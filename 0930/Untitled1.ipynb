{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76028ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 16:56:51,311\tINFO services.py:1265 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-10-06 16:57:00,549\tINFO trainer.py:714 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-10-06 16:57:00,549\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2021-10-06 16:57:00,550\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-10-06 16:57:06,773\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takumi\\anaconda3\\envs\\AI_Ray\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import Logger, UnifiedLogger, pretty_print\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.examples.models.shared_weights_model import TF2SharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.agents.ppo import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from environment_rllib import MyEnv\n",
    "from settings.initial_settings import *\n",
    "from settings.reset_conditions import reset_conditions\n",
    "#from modules.models import MyConv2DModel_v0B_Small_CBAM_1DConv_Share\n",
    "from modules.models import DenseNetModelLarge\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from modules.savers import save_conditions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "PROJECT = \"UCAV\"\n",
    "TRIAL_ID = 2\n",
    "TRIAL = 'test_' + str(TRIAL_ID)\n",
    "EVAL_FREQ = 1\n",
    "CONTINUAL = False\n",
    "\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "    timestr = datetime.datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "\n",
    "    def logger_creator(config):\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "ModelCatalog.register_custom_model('my_model', DenseNetModelLarge)\n",
    "\n",
    "# config = {\"env\": MyEnv,\n",
    "#           \"num_workers\": NUM_WORKERS,\n",
    "#           \"num_gpus\": NUM_GPUS,\n",
    "#           \"num_cpus_per_worker\": NUM_CPUS_PER_WORKER,\n",
    "#           \"num_sgd_iter\": NUM_SGD_ITER,\n",
    "#           \"lr\": LEARNING_RATE,\n",
    "#           \"gamma\": GAMMA,  # default=0.99\n",
    "#           \"model\": {\"custom_model\": \"my_model\"}\n",
    "#           # \"framework\": framework\n",
    "#           }  # use tensorflow 2\n",
    "config = {\"env\": MyEnv,\"num_gpus\": 0,\"num_workers\": 1, \"num_cpus_per_worker\": 0,\"num_gpus\": 0}\n",
    "conditions_dir = os.path.join('./' + PROJECT + '/conditions/')\n",
    "\n",
    "if not os.path.exists(conditions_dir):\n",
    "    os.makedirs(conditions_dir)\n",
    "save_conditions(conditions_dir)\n",
    "\n",
    "# PPOTrainer()は、try_import_tfを使うと、なぜかTensorflowのeager modeのエラーになる。\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config,\n",
    "                         logger_creator=custom_log_creator(\n",
    "                             os.path.expanduser(\"./\" + PROJECT + \"/logs\"), TRIAL))\n",
    "\n",
    "if CONTINUAL:\n",
    "    # Continual learning: Need to specify the checkpoint\n",
    "    model_path = PROJECT + '/checkpoints/' + 'fwd_1/checkpoint_000351/checkpoint-351'\n",
    "    trainer.restore(checkpoint_path=model_path)\n",
    "\n",
    "models_dir = os.path.join('./' + PROJECT + '/models/')\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "text_name = models_dir + TRIAL + '.txt'\n",
    "with open(text_name, \"w\") as fp:\n",
    "    trainer.get_policy().model.base_model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))\n",
    "png_name = models_dir + TRIAL + '.png'\n",
    "plot_model(trainer.get_policy().model.base_model, to_file=png_name, show_shapes=True)\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "eval_env = MyEnv({})\n",
    "\n",
    "# Define checkpoint dir\n",
    "check_point_dir = os.path.join('./' + PROJECT + '/checkpoints/', TRIAL)\n",
    "if not os.path.exists(check_point_dir):\n",
    "    os.makedirs(check_point_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7acca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-06 16:57:10,405\tWARNING deprecation.py:39 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "2021-10-06 16:57:12,566\tWARNING deprecation.py:39 -- DeprecationWarning: `compute_action` has been deprecated. Use `compute_single_action` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:0 starting ! -----------------\n",
      "agent_timesteps_total: 4000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-57-12\n",
      "done: false\n",
      "episode_len_mean: 562.3333333333334\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8927000000000005\n",
      "episode_reward_mean: 0.5288666666666701\n",
      "episode_reward_min: -1.1783999999999923\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 3\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0723531246185303\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007047607097774744\n",
      "        model: {}\n",
      "        policy_loss: -0.011758591048419476\n",
      "        total_loss: 0.06090806424617767\n",
      "        vf_explained_var: -0.31641122698783875\n",
      "        vf_loss: 0.07125714421272278\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.400000000000002\n",
      "  ram_util_percent: 42.9\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.054071927773601\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7803555669217394\n",
      "  mean_inference_ms: 0.8686211989677769\n",
      "  mean_raw_obs_processing_ms: 0.08084868145608592\n",
      "time_since_restore: 5.7168495655059814\n",
      "time_this_iter_s: 5.7168495655059814\n",
      "time_total_s: 5.7168495655059814\n",
      "timers:\n",
      "  learn_throughput: 1898.457\n",
      "  learn_time_ms: 2106.975\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1110.682\n",
      "  sample_time_ms: 3601.39\n",
      "  update_time_ms: 2.007\n",
      "timestamp: 1633507032\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:1 starting ! -----------------\n",
      "agent_timesteps_total: 8000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-57-29\n",
      "done: false\n",
      "episode_len_mean: 625.3333333333334\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8927000000000005\n",
      "episode_reward_mean: -0.6541666666666643\n",
      "episode_reward_min: -2.1835500000000003\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 6\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0537967681884766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011123660951852798\n",
      "        model: {}\n",
      "        policy_loss: -0.008445709012448788\n",
      "        total_loss: 0.03726111724972725\n",
      "        vf_explained_var: -0.458697110414505\n",
      "        vf_loss: 0.04348209500312805\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.7625\n",
      "  ram_util_percent: 42.975\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.05217973859680952\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7868357150301472\n",
      "  mean_inference_ms: 0.8629633986044359\n",
      "  mean_raw_obs_processing_ms: 0.08333613937635757\n",
      "time_since_restore: 11.18454885482788\n",
      "time_this_iter_s: 5.467699289321899\n",
      "time_total_s: 11.18454885482788\n",
      "timers:\n",
      "  learn_throughput: 2020.318\n",
      "  learn_time_ms: 1979.886\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1109.696\n",
      "  sample_time_ms: 3604.59\n",
      "  update_time_ms: 1.924\n",
      "timestamp: 1633507049\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:2 starting ! -----------------\n",
      "agent_timesteps_total: 12000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-57-53\n",
      "done: false\n",
      "episode_len_mean: 699.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8927000000000005\n",
      "episode_reward_mean: -0.7719312499999971\n",
      "episode_reward_min: -2.1835500000000003\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 8\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0530126094818115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009373483248054981\n",
      "        model: {}\n",
      "        policy_loss: -0.013845310546457767\n",
      "        total_loss: 0.00879815686494112\n",
      "        vf_explained_var: -0.10778176784515381\n",
      "        vf_loss: 0.02076876163482666\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.170588235294115\n",
      "  ram_util_percent: 43.00588235294117\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.05106402650015027\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7862900325748725\n",
      "  mean_inference_ms: 0.8623659363993118\n",
      "  mean_raw_obs_processing_ms: 0.08325407593297995\n",
      "time_since_restore: 16.615899801254272\n",
      "time_this_iter_s: 5.431350946426392\n",
      "time_total_s: 16.615899801254272\n",
      "timers:\n",
      "  learn_throughput: 2053.154\n",
      "  learn_time_ms: 1948.222\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1116.375\n",
      "  sample_time_ms: 3583.027\n",
      "  update_time_ms: 1.947\n",
      "timestamp: 1633507073\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:3 starting ! -----------------\n",
      "agent_timesteps_total: 16000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-58-12\n",
      "done: false\n",
      "episode_len_mean: 629.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8927000000000005\n",
      "episode_reward_mean: -1.1376291666666642\n",
      "episode_reward_min: -2.1835500000000003\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 12\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0481693744659424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008607166819274426\n",
      "        model: {}\n",
      "        policy_loss: -0.014809709042310715\n",
      "        total_loss: 0.023506619036197662\n",
      "        vf_explained_var: -0.22633479535579681\n",
      "        vf_loss: 0.03659489005804062\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_steps_sampled: 16000\n",
      "  num_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.923076923076923\n",
      "  ram_util_percent: 43.04615384615384\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0499508444340264\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7824303655529458\n",
      "  mean_inference_ms: 0.8615218693015931\n",
      "  mean_raw_obs_processing_ms: 0.08330262358444429\n",
      "time_since_restore: 21.97033715248108\n",
      "time_this_iter_s: 5.354437351226807\n",
      "time_total_s: 21.97033715248108\n",
      "timers:\n",
      "  learn_throughput: 2079.376\n",
      "  learn_time_ms: 1923.654\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1123.071\n",
      "  sample_time_ms: 3561.664\n",
      "  update_time_ms: 1.958\n",
      "timestamp: 1633507092\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:4 starting ! -----------------\n",
      "agent_timesteps_total: 20000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-58-30\n",
      "done: false\n",
      "episode_len_mean: 674.2142857142857\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8927000000000005\n",
      "episode_reward_mean: -1.2872607142857124\n",
      "episode_reward_min: -2.1898000000000044\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 14\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0160064697265625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012016708962619305\n",
      "        model: {}\n",
      "        policy_loss: -0.017753317952156067\n",
      "        total_loss: 0.016537580639123917\n",
      "        vf_explained_var: -0.4646912217140198\n",
      "        vf_loss: 0.031887564808130264\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.957692307692312\n",
      "  ram_util_percent: 43.01923076923077\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04970887453903932\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7831801957800986\n",
      "  mean_inference_ms: 0.8611079897774364\n",
      "  mean_raw_obs_processing_ms: 0.0835764380962076\n",
      "time_since_restore: 27.595940351486206\n",
      "time_this_iter_s: 5.625603199005127\n",
      "time_total_s: 27.595940351486206\n",
      "timers:\n",
      "  learn_throughput: 2080.586\n",
      "  learn_time_ms: 1922.535\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1114.395\n",
      "  sample_time_ms: 3589.39\n",
      "  update_time_ms: 1.966\n",
      "timestamp: 1633507110\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:5 starting ! -----------------\n",
      "agent_timesteps_total: 24000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-58-50\n",
      "done: false\n",
      "episode_len_mean: 652.0588235294117\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.028549999999998\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 17\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0125885009765625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006026575341820717\n",
      "        model: {}\n",
      "        policy_loss: -0.011828646995127201\n",
      "        total_loss: 0.04203527048230171\n",
      "        vf_explained_var: -0.4901210069656372\n",
      "        vf_loss: 0.052658602595329285\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_steps_sampled: 24000\n",
      "  num_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.124999999999996\n",
      "  ram_util_percent: 43.06428571428571\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049460890550825275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7850665542345803\n",
      "  mean_inference_ms: 0.8611315772961107\n",
      "  mean_raw_obs_processing_ms: 0.08405995833574423\n",
      "time_since_restore: 33.168129682540894\n",
      "time_this_iter_s: 5.5721893310546875\n",
      "time_total_s: 33.168129682540894\n",
      "timers:\n",
      "  learn_throughput: 2092.793\n",
      "  learn_time_ms: 1911.321\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1108.162\n",
      "  sample_time_ms: 3609.58\n",
      "  update_time_ms: 1.971\n",
      "timestamp: 1633507130\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:6 starting ! -----------------\n",
      "agent_timesteps_total: 28000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-59-14\n",
      "done: false\n",
      "episode_len_mean: 689.65\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.047809999999997\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 20\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9622366428375244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013705122284591198\n",
      "        model: {}\n",
      "        policy_loss: -0.025264771655201912\n",
      "        total_loss: -0.007161575369536877\n",
      "        vf_explained_var: 0.22458712756633759\n",
      "        vf_loss: 0.015362175181508064\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_steps_sampled: 28000\n",
      "  num_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.742424242424242\n",
      "  ram_util_percent: 43.0\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924744638392986\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7842248685795201\n",
      "  mean_inference_ms: 0.8609767622680854\n",
      "  mean_raw_obs_processing_ms: 0.08430279114302672\n",
      "time_since_restore: 38.39286541938782\n",
      "time_this_iter_s: 5.224735736846924\n",
      "time_total_s: 38.39286541938782\n",
      "timers:\n",
      "  learn_throughput: 2103.899\n",
      "  learn_time_ms: 1901.232\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1118.46\n",
      "  sample_time_ms: 3576.346\n",
      "  update_time_ms: 1.974\n",
      "timestamp: 1633507154\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:7 starting ! -----------------\n",
      "agent_timesteps_total: 32000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-59-38\n",
      "done: false\n",
      "episode_len_mean: 692.6818181818181\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.0573568181818152\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 22\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9618884325027466\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007874633185565472\n",
      "        model: {}\n",
      "        policy_loss: -0.008541693910956383\n",
      "        total_loss: 0.014200553297996521\n",
      "        vf_explained_var: -0.14786207675933838\n",
      "        vf_loss: 0.021167321130633354\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_steps_sampled: 32000\n",
      "  num_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.800000000000004\n",
      "  ram_util_percent: 42.98823529411764\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04910712314281232\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7835299308927016\n",
      "  mean_inference_ms: 0.8606916739412778\n",
      "  mean_raw_obs_processing_ms: 0.08455629263270754\n",
      "time_since_restore: 43.80899000167847\n",
      "time_this_iter_s: 5.416124582290649\n",
      "time_total_s: 43.80899000167847\n",
      "timers:\n",
      "  learn_throughput: 2104.34\n",
      "  learn_time_ms: 1900.833\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1121.006\n",
      "  sample_time_ms: 3568.223\n",
      "  update_time_ms: 1.977\n",
      "timestamp: 1633507178\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:8 starting ! -----------------\n",
      "agent_timesteps_total: 36000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_16-59-51\n",
      "done: false\n",
      "episode_len_mean: 707.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.1036599999999972\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 25\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.94501793384552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009927663952112198\n",
      "        model: {}\n",
      "        policy_loss: -0.007940166629850864\n",
      "        total_loss: 0.015448111109435558\n",
      "        vf_explained_var: -0.1886611431837082\n",
      "        vf_loss: 0.021402744576334953\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_steps_sampled: 36000\n",
      "  num_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.15555555555555\n",
      "  ram_util_percent: 43.016666666666666\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04900196791445752\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7823147655822379\n",
      "  mean_inference_ms: 0.8604998378203748\n",
      "  mean_raw_obs_processing_ms: 0.08492411552362517\n",
      "time_since_restore: 49.23080134391785\n",
      "time_this_iter_s: 5.42181134223938\n",
      "time_total_s: 49.23080134391785\n",
      "timers:\n",
      "  learn_throughput: 2109.097\n",
      "  learn_time_ms: 1896.546\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1121.566\n",
      "  sample_time_ms: 3566.443\n",
      "  update_time_ms: 1.979\n",
      "timestamp: 1633507191\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:9 starting ! -----------------\n",
      "agent_timesteps_total: 40000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-00-08\n",
      "done: false\n",
      "episode_len_mean: 691.2857142857143\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.1064357142857113\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 28\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.926544427871704\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009619095362722874\n",
      "        model: {}\n",
      "        policy_loss: -0.016542213037610054\n",
      "        total_loss: 0.001845526392571628\n",
      "        vf_explained_var: 0.1389540433883667\n",
      "        vf_loss: 0.01646391674876213\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.752173913043475\n",
      "  ram_util_percent: 42.99130434782609\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048894256386581195\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7812415661597081\n",
      "  mean_inference_ms: 0.8605057429404936\n",
      "  mean_raw_obs_processing_ms: 0.08520229478240637\n",
      "time_since_restore: 54.661585092544556\n",
      "time_this_iter_s: 5.430783748626709\n",
      "time_total_s: 54.661585092544556\n",
      "timers:\n",
      "  learn_throughput: 2113.494\n",
      "  learn_time_ms: 1892.601\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1121.598\n",
      "  sample_time_ms: 3566.341\n",
      "  update_time_ms: 1.881\n",
      "timestamp: 1633507208\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:10 starting ! -----------------\n",
      "agent_timesteps_total: 44000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-00-23\n",
      "done: false\n",
      "episode_len_mean: 696.8709677419355\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.1091354838709646\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 31\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9267640113830566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008496327325701714\n",
      "        model: {}\n",
      "        policy_loss: -0.01509306114166975\n",
      "        total_loss: -0.002140073338523507\n",
      "        vf_explained_var: 0.13809950649738312\n",
      "        vf_loss: 0.011253723874688148\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_steps_sampled: 44000\n",
      "  num_steps_trained: 44000\n",
      "iterations_since_restore: 11\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.113636363636363\n",
      "  ram_util_percent: 43.02272727272727\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048808538319084736\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7805750174024553\n",
      "  mean_inference_ms: 0.8603542748376931\n",
      "  mean_raw_obs_processing_ms: 0.08538732205360146\n",
      "time_since_restore: 60.093756437301636\n",
      "time_this_iter_s: 5.43217134475708\n",
      "time_total_s: 60.093756437301636\n",
      "timers:\n",
      "  learn_throughput: 2142.166\n",
      "  learn_time_ms: 1867.269\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1122.54\n",
      "  sample_time_ms: 3563.348\n",
      "  update_time_ms: 1.854\n",
      "timestamp: 1633507223\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:11 starting ! -----------------\n",
      "agent_timesteps_total: 48000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-00-35\n",
      "done: false\n",
      "episode_len_mean: 711.3939393939394\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.1106075757575729\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 33\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9314038753509521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011941976845264435\n",
      "        model: {}\n",
      "        policy_loss: -0.014558124355971813\n",
      "        total_loss: -0.004506183322519064\n",
      "        vf_explained_var: -0.32528042793273926\n",
      "        vf_loss: 0.00766354613006115\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_steps_sampled: 48000\n",
      "  num_steps_trained: 48000\n",
      "iterations_since_restore: 12\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.327777777777776\n",
      "  ram_util_percent: 43.00555555555555\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048758019733027846\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7799599538039107\n",
      "  mean_inference_ms: 0.8602500862739939\n",
      "  mean_raw_obs_processing_ms: 0.0855190937539657\n",
      "time_since_restore: 65.43469667434692\n",
      "time_this_iter_s: 5.340940237045288\n",
      "time_total_s: 65.43469667434692\n",
      "timers:\n",
      "  learn_throughput: 2141.315\n",
      "  learn_time_ms: 1868.011\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.761\n",
      "  sample_time_ms: 3549.999\n",
      "  update_time_ms: 1.868\n",
      "timestamp: 1633507235\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:12 starting ! -----------------\n",
      "agent_timesteps_total: 52000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-00-59\n",
      "done: false\n",
      "episode_len_mean: 720.3888888888889\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.8941500000000002\n",
      "episode_reward_mean: -1.1380819444444414\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 36\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8260242938995361\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01041203923523426\n",
      "        model: {}\n",
      "        policy_loss: -0.010394042357802391\n",
      "        total_loss: 0.025857802480459213\n",
      "        vf_explained_var: -0.06854818761348724\n",
      "        vf_loss: 0.034169428050518036\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_steps_sampled: 52000\n",
      "  num_steps_trained: 52000\n",
      "iterations_since_restore: 13\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.403030303030302\n",
      "  ram_util_percent: 43.012121212121215\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04866618290548182\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7788520313540804\n",
      "  mean_inference_ms: 0.8600941673036243\n",
      "  mean_raw_obs_processing_ms: 0.08565404364630322\n",
      "time_since_restore: 70.71936368942261\n",
      "time_this_iter_s: 5.284667015075684\n",
      "time_total_s: 70.71936368942261\n",
      "timers:\n",
      "  learn_throughput: 2146.739\n",
      "  learn_time_ms: 1863.291\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.938\n",
      "  sample_time_ms: 3540.016\n",
      "  update_time_ms: 1.868\n",
      "timestamp: 1633507259\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:13 starting ! -----------------\n",
      "agent_timesteps_total: 56000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-01-20\n",
      "done: false\n",
      "episode_len_mean: 707.3846153846154\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.902899999999998\n",
      "episode_reward_mean: -1.03412051282051\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 39\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8942639827728271\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010818012058734894\n",
      "        model: {}\n",
      "        policy_loss: -0.01905163563787937\n",
      "        total_loss: 0.018726997077465057\n",
      "        vf_explained_var: -0.18212543427944183\n",
      "        vf_loss: 0.03561503067612648\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_steps_sampled: 56000\n",
      "  num_steps_trained: 56000\n",
      "iterations_since_restore: 14\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.72413793103448\n",
      "  ram_util_percent: 42.975862068965526\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04857508435127169\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7779112180314947\n",
      "  mean_inference_ms: 0.8599467834640685\n",
      "  mean_raw_obs_processing_ms: 0.08577815180433518\n",
      "time_since_restore: 76.1094491481781\n",
      "time_this_iter_s: 5.390085458755493\n",
      "time_total_s: 76.1094491481781\n",
      "timers:\n",
      "  learn_throughput: 2147.333\n",
      "  learn_time_ms: 1862.776\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.649\n",
      "  sample_time_ms: 3544.06\n",
      "  update_time_ms: 1.769\n",
      "timestamp: 1633507280\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:14 starting ! -----------------\n",
      "agent_timesteps_total: 60000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-01-37\n",
      "done: false\n",
      "episode_len_mean: 713.8095238095239\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.902899999999998\n",
      "episode_reward_mean: -0.9220154761904734\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 42\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7677680253982544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010948901064693928\n",
      "        model: {}\n",
      "        policy_loss: -0.012912523001432419\n",
      "        total_loss: 0.01683039590716362\n",
      "        vf_explained_var: 0.026815373450517654\n",
      "        vf_loss: 0.027553139254450798\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_steps_sampled: 60000\n",
      "  num_steps_trained: 60000\n",
      "iterations_since_restore: 15\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.564\n",
      "  ram_util_percent: 42.98799999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04850805641879837\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7769052575392544\n",
      "  mean_inference_ms: 0.8598290853071882\n",
      "  mean_raw_obs_processing_ms: 0.08591344521945296\n",
      "time_since_restore: 81.45855665206909\n",
      "time_this_iter_s: 5.349107503890991\n",
      "time_total_s: 81.45855665206909\n",
      "timers:\n",
      "  learn_throughput: 2155.294\n",
      "  learn_time_ms: 1855.895\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.317\n",
      "  sample_time_ms: 3523.244\n",
      "  update_time_ms: 1.769\n",
      "timestamp: 1633507297\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:15 starting ! -----------------\n",
      "agent_timesteps_total: 64000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-01-54\n",
      "done: false\n",
      "episode_len_mean: 706.8222222222222\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.8681599999999973\n",
      "episode_reward_min: -2.1996000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 45\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9124277830123901\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011132133193314075\n",
      "        model: {}\n",
      "        policy_loss: -0.023399489000439644\n",
      "        total_loss: -0.01129877008497715\n",
      "        vf_explained_var: -0.16688518226146698\n",
      "        vf_loss: 0.00987429078668356\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_steps_sampled: 64000\n",
      "  num_steps_trained: 64000\n",
      "iterations_since_restore: 16\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.36086956521739\n",
      "  ram_util_percent: 43.01304347826087\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04844135614735892\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7758434289505478\n",
      "  mean_inference_ms: 0.8596776834707924\n",
      "  mean_raw_obs_processing_ms: 0.08605503223991003\n",
      "time_since_restore: 86.75637245178223\n",
      "time_this_iter_s: 5.297815799713135\n",
      "time_total_s: 86.75637245178223\n",
      "timers:\n",
      "  learn_throughput: 2155.358\n",
      "  learn_time_ms: 1855.84\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.211\n",
      "  sample_time_ms: 3495.858\n",
      "  update_time_ms: 1.755\n",
      "timestamp: 1633507314\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:16 starting ! -----------------\n",
      "agent_timesteps_total: 68000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-02-18\n",
      "done: false\n",
      "episode_len_mean: 710.2340425531914\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9242978723404229\n",
      "episode_reward_min: -2.21\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 47\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8139681816101074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009453295730054379\n",
      "        model: {}\n",
      "        policy_loss: -0.00751911336556077\n",
      "        total_loss: 0.017161201685667038\n",
      "        vf_explained_var: 0.014566157013177872\n",
      "        vf_loss: 0.022789662703871727\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_steps_sampled: 68000\n",
      "  num_steps_trained: 68000\n",
      "iterations_since_restore: 17\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.514705882352942\n",
      "  ram_util_percent: 42.98823529411764\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04841510864846082\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7752930244173422\n",
      "  mean_inference_ms: 0.8595465259780977\n",
      "  mean_raw_obs_processing_ms: 0.08612849648667038\n",
      "time_since_restore: 92.19267582893372\n",
      "time_this_iter_s: 5.436303377151489\n",
      "time_total_s: 92.19267582893372\n",
      "timers:\n",
      "  learn_throughput: 2154.418\n",
      "  learn_time_ms: 1856.65\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.625\n",
      "  sample_time_ms: 3516.098\n",
      "  update_time_ms: 1.84\n",
      "timestamp: 1633507338\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:17 starting ! -----------------\n",
      "agent_timesteps_total: 72000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-02-33\n",
      "done: false\n",
      "episode_len_mean: 715.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9768059999999977\n",
      "episode_reward_min: -2.21\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 50\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.832688331604004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009011216461658478\n",
      "        model: {}\n",
      "        policy_loss: -0.005779109429568052\n",
      "        total_loss: 0.012531491927802563\n",
      "        vf_explained_var: -0.13257169723510742\n",
      "        vf_loss: 0.01650836504995823\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_steps_sampled: 72000\n",
      "  num_steps_trained: 72000\n",
      "iterations_since_restore: 18\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.0\n",
      "  ram_util_percent: 42.99523809523809\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048365023983782125\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7744295389784733\n",
      "  mean_inference_ms: 0.8593417937813408\n",
      "  mean_raw_obs_processing_ms: 0.08621935140796139\n",
      "time_since_restore: 97.49496674537659\n",
      "time_this_iter_s: 5.302290916442871\n",
      "time_total_s: 97.49496674537659\n",
      "timers:\n",
      "  learn_throughput: 2158.953\n",
      "  learn_time_ms: 1852.75\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.085\n",
      "  sample_time_ms: 3508.511\n",
      "  update_time_ms: 1.84\n",
      "timestamp: 1633507353\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:18 starting ! -----------------\n",
      "agent_timesteps_total: 76000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-02-57\n",
      "done: false\n",
      "episode_len_mean: 726.4230769230769\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9830634615384592\n",
      "episode_reward_min: -2.21\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 52\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7257436513900757\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013490268960595131\n",
      "        model: {}\n",
      "        policy_loss: -0.01178605854511261\n",
      "        total_loss: 0.01228515338152647\n",
      "        vf_explained_var: -0.10302279144525528\n",
      "        vf_loss: 0.02137315645813942\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_steps_sampled: 76000\n",
      "  num_steps_trained: 76000\n",
      "iterations_since_restore: 19\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.53636363636364\n",
      "  ram_util_percent: 42.97272727272727\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048332435779171394\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7738149880947418\n",
      "  mean_inference_ms: 0.8592084566767356\n",
      "  mean_raw_obs_processing_ms: 0.08629947770349689\n",
      "time_since_restore: 102.80021476745605\n",
      "time_this_iter_s: 5.305248022079468\n",
      "time_total_s: 102.80021476745605\n",
      "timers:\n",
      "  learn_throughput: 2160.885\n",
      "  learn_time_ms: 1851.093\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.349\n",
      "  sample_time_ms: 3498.495\n",
      "  update_time_ms: 1.74\n",
      "timestamp: 1633507377\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:19 starting ! -----------------\n",
      "agent_timesteps_total: 80000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-03-15\n",
      "done: false\n",
      "episode_len_mean: 732.2407407407408\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -1.0058805555555534\n",
      "episode_reward_min: -2.21\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 54\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.76703679561615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009628393687307835\n",
      "        model: {}\n",
      "        policy_loss: -0.01179980393499136\n",
      "        total_loss: 0.01362692005932331\n",
      "        vf_explained_var: -0.05662740767002106\n",
      "        vf_loss: 0.023501046001911163\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_steps_sampled: 80000\n",
      "  num_steps_trained: 80000\n",
      "iterations_since_restore: 20\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.792307692307695\n",
      "  ram_util_percent: 42.98461538461538\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04828706787336074\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7732498120836632\n",
      "  mean_inference_ms: 0.8590890504874151\n",
      "  mean_raw_obs_processing_ms: 0.08635683299200664\n",
      "time_since_restore: 108.15986919403076\n",
      "time_this_iter_s: 5.359654426574707\n",
      "time_total_s: 108.15986919403076\n",
      "timers:\n",
      "  learn_throughput: 2162.015\n",
      "  learn_time_ms: 1850.126\n",
      "  load_throughput: 51072194.825\n",
      "  load_time_ms: 0.078\n",
      "  sample_throughput: 1145.317\n",
      "  sample_time_ms: 3492.483\n",
      "  update_time_ms: 1.84\n",
      "timestamp: 1633507395\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:20 starting ! -----------------\n",
      "agent_timesteps_total: 84000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-03-37\n",
      "done: false\n",
      "episode_len_mean: 724.3333333333334\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9605771929824541\n",
      "episode_reward_min: -2.21\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 57\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7461572885513306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009154144674539566\n",
      "        model: {}\n",
      "        policy_loss: -0.013402668759226799\n",
      "        total_loss: 0.017993221059441566\n",
      "        vf_explained_var: -0.13262398540973663\n",
      "        vf_loss: 0.029565060511231422\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.183333333333334\n",
      "  ram_util_percent: 42.99000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04825051406902898\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7724936595714796\n",
      "  mean_inference_ms: 0.8588978122099439\n",
      "  mean_raw_obs_processing_ms: 0.086416480665014\n",
      "time_since_restore: 113.58935642242432\n",
      "time_this_iter_s: 5.429487228393555\n",
      "time_total_s: 113.58935642242432\n",
      "timers:\n",
      "  learn_throughput: 2157.756\n",
      "  learn_time_ms: 1853.777\n",
      "  load_throughput: 51072194.825\n",
      "  load_time_ms: 0.078\n",
      "  sample_throughput: 1146.604\n",
      "  sample_time_ms: 3488.562\n",
      "  update_time_ms: 1.865\n",
      "timestamp: 1633507417\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:21 starting ! -----------------\n",
      "agent_timesteps_total: 88000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-03-55\n",
      "done: false\n",
      "episode_len_mean: 729.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9881099999999977\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 60\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.733946681022644\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009579682722687721\n",
      "        model: {}\n",
      "        policy_loss: -0.011812496930360794\n",
      "        total_loss: 0.010045032948255539\n",
      "        vf_explained_var: -0.17420703172683716\n",
      "        vf_loss: 0.01994159258902073\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_steps_sampled: 88000\n",
      "  num_steps_trained: 88000\n",
      "iterations_since_restore: 22\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.715384615384615\n",
      "  ram_util_percent: 43.0076923076923\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04822288722868264\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7718833023978393\n",
      "  mean_inference_ms: 0.8587238752417827\n",
      "  mean_raw_obs_processing_ms: 0.08645623577783121\n",
      "time_since_restore: 119.03357362747192\n",
      "time_this_iter_s: 5.444217205047607\n",
      "time_total_s: 119.03357362747192\n",
      "timers:\n",
      "  learn_throughput: 2158.796\n",
      "  learn_time_ms: 1852.885\n",
      "  load_throughput: 51072194.825\n",
      "  load_time_ms: 0.078\n",
      "  sample_throughput: 1142.937\n",
      "  sample_time_ms: 3499.756\n",
      "  update_time_ms: 1.842\n",
      "timestamp: 1633507435\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:22 starting ! -----------------\n",
      "agent_timesteps_total: 92000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-04-07\n",
      "done: false\n",
      "episode_len_mean: 725.3492063492064\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9305126984126961\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 63\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7504751682281494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011635950766503811\n",
      "        model: {}\n",
      "        policy_loss: -0.01774832047522068\n",
      "        total_loss: 0.016950275748968124\n",
      "        vf_explained_var: -0.4545820355415344\n",
      "        vf_loss: 0.0323714055120945\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_steps_sampled: 92000\n",
      "  num_steps_trained: 92000\n",
      "iterations_since_restore: 23\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.78125\n",
      "  ram_util_percent: 43.0125\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048214795757889645\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7711915593151727\n",
      "  mean_inference_ms: 0.8585374048560154\n",
      "  mean_raw_obs_processing_ms: 0.08648562196668044\n",
      "time_since_restore: 124.2711410522461\n",
      "time_this_iter_s: 5.23756742477417\n",
      "time_total_s: 124.2711410522461\n",
      "timers:\n",
      "  learn_throughput: 2157.552\n",
      "  learn_time_ms: 1853.953\n",
      "  load_throughput: 51072194.825\n",
      "  load_time_ms: 0.078\n",
      "  sample_throughput: 1144.823\n",
      "  sample_time_ms: 3493.99\n",
      "  update_time_ms: 1.842\n",
      "timestamp: 1633507447\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:23 starting ! -----------------\n",
      "agent_timesteps_total: 96000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-04-23\n",
      "done: false\n",
      "episode_len_mean: 722.8939393939394\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9547181818181796\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 66\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7688217163085938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012744222767651081\n",
      "        model: {}\n",
      "        policy_loss: -0.015650345012545586\n",
      "        total_loss: 0.0013779457658529282\n",
      "        vf_explained_var: 0.021030351519584656\n",
      "        vf_loss: 0.014479448087513447\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_steps_sampled: 96000\n",
      "  num_steps_trained: 96000\n",
      "iterations_since_restore: 24\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.769565217391307\n",
      "  ram_util_percent: 43.247826086956536\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04820313161091064\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7705001857972313\n",
      "  mean_inference_ms: 0.8583748562634334\n",
      "  mean_raw_obs_processing_ms: 0.08651119876348454\n",
      "time_since_restore: 129.6037313938141\n",
      "time_this_iter_s: 5.332590341567993\n",
      "time_total_s: 129.6037313938141\n",
      "timers:\n",
      "  learn_throughput: 2155.385\n",
      "  learn_time_ms: 1855.817\n",
      "  load_throughput: 50442621.768\n",
      "  load_time_ms: 0.079\n",
      "  sample_throughput: 1147.338\n",
      "  sample_time_ms: 3486.33\n",
      "  update_time_ms: 1.841\n",
      "timestamp: 1633507463\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:24 starting ! -----------------\n",
      "agent_timesteps_total: 100000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-04-42\n",
      "done: false\n",
      "episode_len_mean: 722.0434782608696\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9914195652173893\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 69\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.748530387878418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009190382435917854\n",
      "        model: {}\n",
      "        policy_loss: -0.015802722424268723\n",
      "        total_loss: 0.0011599038261920214\n",
      "        vf_explained_var: 0.009571036323904991\n",
      "        vf_loss: 0.015124546363949776\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_steps_sampled: 100000\n",
      "  num_steps_trained: 100000\n",
      "iterations_since_restore: 25\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.92592592592593\n",
      "  ram_util_percent: 43.270370370370365\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04820233498433318\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7698868662115572\n",
      "  mean_inference_ms: 0.8582462247685797\n",
      "  mean_raw_obs_processing_ms: 0.08655010847682088\n",
      "time_since_restore: 135.24913597106934\n",
      "time_this_iter_s: 5.645404577255249\n",
      "time_total_s: 135.24913597106934\n",
      "timers:\n",
      "  learn_throughput: 2133.854\n",
      "  learn_time_ms: 1874.542\n",
      "  load_throughput: 50442621.768\n",
      "  load_time_ms: 0.079\n",
      "  sample_throughput: 1143.812\n",
      "  sample_time_ms: 3497.078\n",
      "  update_time_ms: 1.941\n",
      "timestamp: 1633507482\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:25 starting ! -----------------\n",
      "agent_timesteps_total: 104000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-05-04\n",
      "done: false\n",
      "episode_len_mean: 720.6805555555555\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9403076388888868\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 72\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7040303945541382\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010849838145077229\n",
      "        model: {}\n",
      "        policy_loss: -0.01725655049085617\n",
      "        total_loss: 0.009599575772881508\n",
      "        vf_explained_var: -0.3707568049430847\n",
      "        vf_loss: 0.024686163291335106\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_steps_sampled: 104000\n",
      "  num_steps_trained: 104000\n",
      "iterations_since_restore: 26\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.729032258064514\n",
      "  ram_util_percent: 43.46451612903226\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04820863347136533\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7693083690989836\n",
      "  mean_inference_ms: 0.8581368701974541\n",
      "  mean_raw_obs_processing_ms: 0.08658785043241918\n",
      "time_since_restore: 140.73741936683655\n",
      "time_this_iter_s: 5.488283395767212\n",
      "time_total_s: 140.73741936683655\n",
      "timers:\n",
      "  learn_throughput: 2122.957\n",
      "  learn_time_ms: 1884.164\n",
      "  load_throughput: 50442621.768\n",
      "  load_time_ms: 0.079\n",
      "  sample_throughput: 1140.758\n",
      "  sample_time_ms: 3506.441\n",
      "  update_time_ms: 1.855\n",
      "timestamp: 1633507504\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 104000\n",
      "training_iteration: 26\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:26 starting ! -----------------\n",
      "agent_timesteps_total: 108000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-05-23\n",
      "done: false\n",
      "episode_len_mean: 717.1081081081081\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9455385135135114\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 74\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6569335460662842\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008863807655870914\n",
      "        model: {}\n",
      "        policy_loss: -0.02020193077623844\n",
      "        total_loss: 0.001002633711323142\n",
      "        vf_explained_var: -0.07947481423616409\n",
      "        vf_loss: 0.01943180151283741\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_steps_sampled: 108000\n",
      "  num_steps_trained: 108000\n",
      "iterations_since_restore: 27\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.41538461538461\n",
      "  ram_util_percent: 43.49615384615385\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04821883467782919\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7689873084049379\n",
      "  mean_inference_ms: 0.85805099118696\n",
      "  mean_raw_obs_processing_ms: 0.08661217874633531\n",
      "time_since_restore: 146.27133202552795\n",
      "time_this_iter_s: 5.533912658691406\n",
      "time_total_s: 146.27133202552795\n",
      "timers:\n",
      "  learn_throughput: 2112.478\n",
      "  learn_time_ms: 1893.511\n",
      "  load_throughput: 50442621.768\n",
      "  load_time_ms: 0.079\n",
      "  sample_throughput: 1140.597\n",
      "  sample_time_ms: 3506.937\n",
      "  update_time_ms: 1.755\n",
      "timestamp: 1633507523\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:27 starting ! -----------------\n",
      "agent_timesteps_total: 112000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-05-36\n",
      "done: false\n",
      "episode_len_mean: 720.4935064935065\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9527233766233745\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 77\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7531789541244507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007671448402106762\n",
      "        model: {}\n",
      "        policy_loss: -0.01176762767136097\n",
      "        total_loss: 0.013231626711785793\n",
      "        vf_explained_var: -0.10850075632333755\n",
      "        vf_loss: 0.023464957252144814\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_steps_sampled: 112000\n",
      "  num_steps_trained: 112000\n",
      "iterations_since_restore: 28\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.650000000000006\n",
      "  ram_util_percent: 43.505\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04824255753708049\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.768483863974934\n",
      "  mean_inference_ms: 0.8579736957852342\n",
      "  mean_raw_obs_processing_ms: 0.08665177462464782\n",
      "time_since_restore: 151.74360179901123\n",
      "time_this_iter_s: 5.472269773483276\n",
      "time_total_s: 151.74360179901123\n",
      "timers:\n",
      "  learn_throughput: 2104.95\n",
      "  learn_time_ms: 1900.283\n",
      "  load_throughput: 22357697.228\n",
      "  load_time_ms: 0.179\n",
      "  sample_throughput: 1137.29\n",
      "  sample_time_ms: 3517.134\n",
      "  update_time_ms: 1.755\n",
      "timestamp: 1633507536\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 112000\n",
      "training_iteration: 28\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:28 starting ! -----------------\n",
      "agent_timesteps_total: 116000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-06-01\n",
      "done: false\n",
      "episode_len_mean: 723.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9717812499999978\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 80\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6723520755767822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016758348792791367\n",
      "        model: {}\n",
      "        policy_loss: -0.021100221201777458\n",
      "        total_loss: -0.005308089312165976\n",
      "        vf_explained_var: 0.18561914563179016\n",
      "        vf_loss: 0.012440470047295094\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_steps_sampled: 116000\n",
      "  num_steps_trained: 116000\n",
      "iterations_since_restore: 29\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.66764705882353\n",
      "  ram_util_percent: 43.49411764705882\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048268891818473256\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.767964421373988\n",
      "  mean_inference_ms: 0.8578869164370797\n",
      "  mean_raw_obs_processing_ms: 0.0866907059445827\n",
      "time_since_restore: 157.11296772956848\n",
      "time_this_iter_s: 5.369365930557251\n",
      "time_total_s: 157.11296772956848\n",
      "timers:\n",
      "  learn_throughput: 2095.798\n",
      "  learn_time_ms: 1908.581\n",
      "  load_throughput: 22357697.228\n",
      "  load_time_ms: 0.179\n",
      "  sample_throughput: 1137.922\n",
      "  sample_time_ms: 3515.18\n",
      "  update_time_ms: 1.854\n",
      "timestamp: 1633507561\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:29 starting ! -----------------\n",
      "agent_timesteps_total: 120000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-06-14\n",
      "done: false\n",
      "episode_len_mean: 716.9277108433735\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.8684548192771063\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 83\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.684520959854126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011418419890105724\n",
      "        model: {}\n",
      "        policy_loss: -0.018822791054844856\n",
      "        total_loss: 0.02512640878558159\n",
      "        vf_explained_var: -0.23977579176425934\n",
      "        vf_loss: 0.04166550934314728\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_steps_sampled: 120000\n",
      "  num_steps_trained: 120000\n",
      "iterations_since_restore: 30\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.9388888888889\n",
      "  ram_util_percent: 43.449999999999996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048307231642282414\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7675287610883297\n",
      "  mean_inference_ms: 0.857783856158712\n",
      "  mean_raw_obs_processing_ms: 0.08673849700113882\n",
      "time_since_restore: 162.6718623638153\n",
      "time_this_iter_s: 5.558894634246826\n",
      "time_total_s: 162.6718623638153\n",
      "timers:\n",
      "  learn_throughput: 2085.397\n",
      "  learn_time_ms: 1918.1\n",
      "  load_throughput: 39765859.208\n",
      "  load_time_ms: 0.101\n",
      "  sample_throughput: 1134.599\n",
      "  sample_time_ms: 3525.475\n",
      "  update_time_ms: 1.854\n",
      "timestamp: 1633507574\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 120000\n",
      "training_iteration: 30\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:30 starting ! -----------------\n",
      "agent_timesteps_total: 124000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-06-35\n",
      "done: false\n",
      "episode_len_mean: 710.9069767441861\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.8889726744186025\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 86\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6277014017105103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00815170630812645\n",
      "        model: {}\n",
      "        policy_loss: -0.0059013632126152515\n",
      "        total_loss: 0.020602524280548096\n",
      "        vf_explained_var: -0.1844886988401413\n",
      "        vf_loss: 0.02487354539334774\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_steps_sampled: 124000\n",
      "  num_steps_trained: 124000\n",
      "iterations_since_restore: 31\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.276666666666664\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048358379525158694\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7671407574316972\n",
      "  mean_inference_ms: 0.8576762167535774\n",
      "  mean_raw_obs_processing_ms: 0.08677617696265573\n",
      "time_since_restore: 168.1376073360443\n",
      "time_this_iter_s: 5.465744972229004\n",
      "time_total_s: 168.1376073360443\n",
      "timers:\n",
      "  learn_throughput: 2083.601\n",
      "  learn_time_ms: 1919.754\n",
      "  load_throughput: 39765859.208\n",
      "  load_time_ms: 0.101\n",
      "  sample_throughput: 1133.972\n",
      "  sample_time_ms: 3527.423\n",
      "  update_time_ms: 1.854\n",
      "timestamp: 1633507595\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 124000\n",
      "training_iteration: 31\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:31 starting ! -----------------\n",
      "agent_timesteps_total: 128000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-06-55\n",
      "done: false\n",
      "episode_len_mean: 709.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9108038888888867\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 90\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7758278846740723\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010426700115203857\n",
      "        model: {}\n",
      "        policy_loss: -0.012898516841232777\n",
      "        total_loss: 0.011142609640955925\n",
      "        vf_explained_var: 0.2469840794801712\n",
      "        vf_loss: 0.021955788135528564\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_steps_sampled: 128000\n",
      "  num_steps_trained: 128000\n",
      "iterations_since_restore: 32\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.61724137931035\n",
      "  ram_util_percent: 43.49655172413794\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04841879592052353\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7667770129125974\n",
      "  mean_inference_ms: 0.8576010426131332\n",
      "  mean_raw_obs_processing_ms: 0.0868313500343187\n",
      "time_since_restore: 173.85088992118835\n",
      "time_this_iter_s: 5.713282585144043\n",
      "time_total_s: 173.85088992118835\n",
      "timers:\n",
      "  learn_throughput: 2075.394\n",
      "  learn_time_ms: 1927.345\n",
      "  load_throughput: 39765859.208\n",
      "  load_time_ms: 0.101\n",
      "  sample_throughput: 1127.83\n",
      "  sample_time_ms: 3546.635\n",
      "  update_time_ms: 1.878\n",
      "timestamp: 1633507615\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 128000\n",
      "training_iteration: 32\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:32 starting ! -----------------\n",
      "agent_timesteps_total: 132000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-07-12\n",
      "done: false\n",
      "episode_len_mean: 706.752688172043\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.8857602150537613\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 93\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7765175104141235\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00817445944994688\n",
      "        model: {}\n",
      "        policy_loss: -0.015234095975756645\n",
      "        total_loss: 0.02446010336279869\n",
      "        vf_explained_var: -0.2999003231525421\n",
      "        vf_loss: 0.038059309124946594\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_steps_sampled: 132000\n",
      "  num_steps_trained: 132000\n",
      "iterations_since_restore: 33\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.42608695652173\n",
      "  ram_util_percent: 43.55652173913044\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04846915869998186\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7665853251827512\n",
      "  mean_inference_ms: 0.8575863773022803\n",
      "  mean_raw_obs_processing_ms: 0.0868784300689722\n",
      "time_since_restore: 179.48615646362305\n",
      "time_this_iter_s: 5.635266542434692\n",
      "time_total_s: 179.48615646362305\n",
      "timers:\n",
      "  learn_throughput: 2075.696\n",
      "  learn_time_ms: 1927.065\n",
      "  load_throughput: 39765859.208\n",
      "  load_time_ms: 0.101\n",
      "  sample_throughput: 1115.253\n",
      "  sample_time_ms: 3586.631\n",
      "  update_time_ms: 1.877\n",
      "timestamp: 1633507632\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 132000\n",
      "training_iteration: 33\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:33 starting ! -----------------\n",
      "agent_timesteps_total: 136000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-07-26\n",
      "done: false\n",
      "episode_len_mean: 709.5368421052632\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9025847368421032\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 95\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7710779905319214\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014381290413439274\n",
      "        model: {}\n",
      "        policy_loss: -0.02053546905517578\n",
      "        total_loss: 0.006061117630451918\n",
      "        vf_explained_var: -0.06664212048053741\n",
      "        vf_loss: 0.023720329627394676\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_steps_sampled: 136000\n",
      "  num_steps_trained: 136000\n",
      "iterations_since_restore: 34\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.9\n",
      "  ram_util_percent: 43.495\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04850651520068757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7664664441787056\n",
      "  mean_inference_ms: 0.8575670374023019\n",
      "  mean_raw_obs_processing_ms: 0.08691314377794451\n",
      "time_since_restore: 184.90930247306824\n",
      "time_this_iter_s: 5.42314600944519\n",
      "time_total_s: 184.90930247306824\n",
      "timers:\n",
      "  learn_throughput: 2076.052\n",
      "  learn_time_ms: 1926.734\n",
      "  load_throughput: 40156093.825\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1112.325\n",
      "  sample_time_ms: 3596.07\n",
      "  update_time_ms: 1.957\n",
      "timestamp: 1633507646\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 136000\n",
      "training_iteration: 34\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:34 starting ! -----------------\n",
      "agent_timesteps_total: 140000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-07-46\n",
      "done: false\n",
      "episode_len_mean: 710.4285714285714\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9095515306122428\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 98\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.769729733467102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011088354513049126\n",
      "        model: {}\n",
      "        policy_loss: -0.015460869297385216\n",
      "        total_loss: 0.005935576744377613\n",
      "        vf_explained_var: 0.013858267106115818\n",
      "        vf_loss: 0.019178777933120728\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_steps_sampled: 140000\n",
      "  num_steps_trained: 140000\n",
      "iterations_since_restore: 35\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.31785714285714\n",
      "  ram_util_percent: 43.425000000000004\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04856201310215666\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7662665527741338\n",
      "  mean_inference_ms: 0.8575353897936726\n",
      "  mean_raw_obs_processing_ms: 0.08695683529054576\n",
      "time_since_restore: 190.28232622146606\n",
      "time_this_iter_s: 5.373023748397827\n",
      "time_total_s: 190.28232622146606\n",
      "timers:\n",
      "  learn_throughput: 2089.907\n",
      "  learn_time_ms: 1913.961\n",
      "  load_throughput: 40156093.825\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1116.765\n",
      "  sample_time_ms: 3581.773\n",
      "  update_time_ms: 1.801\n",
      "timestamp: 1633507666\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 140000\n",
      "training_iteration: 35\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:35 starting ! -----------------\n",
      "agent_timesteps_total: 144000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-08-09\n",
      "done: false\n",
      "episode_len_mean: 705.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9229744999999978\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 102\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7100833654403687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009020685218274593\n",
      "        model: {}\n",
      "        policy_loss: -0.019472721964120865\n",
      "        total_loss: -0.0047296457923948765\n",
      "        vf_explained_var: 0.0454041063785553\n",
      "        vf_loss: 0.012938937172293663\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_steps_sampled: 144000\n",
      "  num_steps_trained: 144000\n",
      "iterations_since_restore: 36\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.13939393939394\n",
      "  ram_util_percent: 43.46666666666667\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048526298972667574\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7657151634458991\n",
      "  mean_inference_ms: 0.8572497837378773\n",
      "  mean_raw_obs_processing_ms: 0.0871354236220773\n",
      "time_since_restore: 195.60675716400146\n",
      "time_this_iter_s: 5.3244309425354\n",
      "time_total_s: 195.60675716400146\n",
      "timers:\n",
      "  learn_throughput: 2102.724\n",
      "  learn_time_ms: 1902.295\n",
      "  load_throughput: 40156093.825\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1118.276\n",
      "  sample_time_ms: 3576.935\n",
      "  update_time_ms: 1.901\n",
      "timestamp: 1633507689\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 144000\n",
      "training_iteration: 36\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:36 starting ! -----------------\n",
      "agent_timesteps_total: 148000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-08-33\n",
      "done: false\n",
      "episode_len_mean: 710.21\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9735024999999977\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 104\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.549646019935608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009920981712639332\n",
      "        model: {}\n",
      "        policy_loss: -0.01571531966328621\n",
      "        total_loss: 0.0015156687004491687\n",
      "        vf_explained_var: 0.06517227739095688\n",
      "        vf_loss: 0.015246786177158356\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_steps_sampled: 148000\n",
      "  num_steps_trained: 148000\n",
      "iterations_since_restore: 37\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.065625\n",
      "  ram_util_percent: 43.425000000000004\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04849166626525011\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7651512195578068\n",
      "  mean_inference_ms: 0.8571100005372996\n",
      "  mean_raw_obs_processing_ms: 0.08723647679996428\n",
      "time_since_restore: 200.94833207130432\n",
      "time_this_iter_s: 5.3415749073028564\n",
      "time_total_s: 200.94833207130432\n",
      "timers:\n",
      "  learn_throughput: 2112.283\n",
      "  learn_time_ms: 1893.686\n",
      "  load_throughput: 40156093.825\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1121.595\n",
      "  sample_time_ms: 3566.35\n",
      "  update_time_ms: 1.917\n",
      "timestamp: 1633507713\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 148000\n",
      "training_iteration: 37\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:37 starting ! -----------------\n",
      "agent_timesteps_total: 152000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-08-52\n",
      "done: false\n",
      "episode_len_mean: 707.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.906\n",
      "episode_reward_mean: -0.9525864999999978\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 107\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.624794363975525\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007927389815449715\n",
      "        model: {}\n",
      "        policy_loss: -0.01391892321407795\n",
      "        total_loss: 0.0012942827306687832\n",
      "        vf_explained_var: 0.13593313097953796\n",
      "        vf_loss: 0.013627724722027779\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_steps_sampled: 152000\n",
      "  num_steps_trained: 152000\n",
      "iterations_since_restore: 38\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.682142857142857\n",
      "  ram_util_percent: 43.40357142857143\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04852030250468693\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7641900445934392\n",
      "  mean_inference_ms: 0.8570473606148681\n",
      "  mean_raw_obs_processing_ms: 0.08734189289244364\n",
      "time_since_restore: 206.32851314544678\n",
      "time_this_iter_s: 5.380181074142456\n",
      "time_total_s: 206.32851314544678\n",
      "timers:\n",
      "  learn_throughput: 2122.793\n",
      "  learn_time_ms: 1884.31\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1121.523\n",
      "  sample_time_ms: 3566.579\n",
      "  update_time_ms: 1.917\n",
      "timestamp: 1633507732\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 152000\n",
      "training_iteration: 38\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:38 starting ! -----------------\n",
      "agent_timesteps_total: 156000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-09-10\n",
      "done: false\n",
      "episode_len_mean: 711.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.9020254999999979\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 110\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5427610874176025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009651272557675838\n",
      "        model: {}\n",
      "        policy_loss: -0.01735943742096424\n",
      "        total_loss: 0.008554835803806782\n",
      "        vf_explained_var: 0.08327247947454453\n",
      "        vf_loss: 0.023984018713235855\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_steps_sampled: 156000\n",
      "  num_steps_trained: 156000\n",
      "iterations_since_restore: 39\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.412000000000003\n",
      "  ram_util_percent: 43.408\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048592045056772976\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7635614876190582\n",
      "  mean_inference_ms: 0.856923391287773\n",
      "  mean_raw_obs_processing_ms: 0.08749351924989603\n",
      "time_since_restore: 211.55926060676575\n",
      "time_this_iter_s: 5.23074746131897\n",
      "time_total_s: 211.55926060676575\n",
      "timers:\n",
      "  learn_throughput: 2132.264\n",
      "  learn_time_ms: 1875.941\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1123.182\n",
      "  sample_time_ms: 3561.312\n",
      "  update_time_ms: 1.898\n",
      "timestamp: 1633507750\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 156000\n",
      "training_iteration: 39\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:39 starting ! -----------------\n",
      "agent_timesteps_total: 160000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-09-33\n",
      "done: false\n",
      "episode_len_mean: 704.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8711579999999977\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 114\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6286048889160156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008138172328472137\n",
      "        model: {}\n",
      "        policy_loss: -0.00955288764089346\n",
      "        total_loss: 0.010933568701148033\n",
      "        vf_explained_var: 0.13559319078922272\n",
      "        vf_loss: 0.01885882019996643\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_steps_sampled: 160000\n",
      "  num_steps_trained: 160000\n",
      "iterations_since_restore: 40\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.07575757575757\n",
      "  ram_util_percent: 43.39393939393939\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04866436964295933\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7625776383947175\n",
      "  mean_inference_ms: 0.8567848425573945\n",
      "  mean_raw_obs_processing_ms: 0.08765967848860093\n",
      "time_since_restore: 216.94293928146362\n",
      "time_this_iter_s: 5.383678674697876\n",
      "time_total_s: 216.94293928146362\n",
      "timers:\n",
      "  learn_throughput: 2135.829\n",
      "  learn_time_ms: 1872.809\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.727\n",
      "  sample_time_ms: 3546.958\n",
      "  update_time_ms: 1.908\n",
      "timestamp: 1633507773\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 160000\n",
      "training_iteration: 40\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:40 starting ! -----------------\n",
      "agent_timesteps_total: 164000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-09-45\n",
      "done: false\n",
      "episode_len_mean: 707.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.9308334999999979\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 117\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6437134742736816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009958391077816486\n",
      "        model: {}\n",
      "        policy_loss: -0.014323724433779716\n",
      "        total_loss: -0.0015422562137246132\n",
      "        vf_explained_var: -0.07743627578020096\n",
      "        vf_loss: 0.010789795778691769\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_steps_sampled: 164000\n",
      "  num_steps_trained: 164000\n",
      "iterations_since_restore: 41\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.746666666666666\n",
      "  ram_util_percent: 43.393333333333324\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04871311118300628\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7614738531456604\n",
      "  mean_inference_ms: 0.8566003059902275\n",
      "  mean_raw_obs_processing_ms: 0.08772043859604045\n",
      "time_since_restore: 222.31810784339905\n",
      "time_this_iter_s: 5.375168561935425\n",
      "time_total_s: 222.31810784339905\n",
      "timers:\n",
      "  learn_throughput: 2141.711\n",
      "  learn_time_ms: 1867.666\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.982\n",
      "  sample_time_ms: 3543.015\n",
      "  update_time_ms: 1.909\n",
      "timestamp: 1633507785\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:41 starting ! -----------------\n",
      "agent_timesteps_total: 168000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-10-03\n",
      "done: false\n",
      "episode_len_mean: 691.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.889032499999998\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 121\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5972330570220947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007953234016895294\n",
      "        model: {}\n",
      "        policy_loss: -0.014355544932186604\n",
      "        total_loss: 0.009212564677000046\n",
      "        vf_explained_var: -0.08164244890213013\n",
      "        vf_loss: 0.021977460011839867\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_steps_sampled: 168000\n",
      "  num_steps_trained: 168000\n",
      "iterations_since_restore: 42\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.93703703703704\n",
      "  ram_util_percent: 43.37037037037038\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.048794032781958714\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7605959320888703\n",
      "  mean_inference_ms: 0.85642915481506\n",
      "  mean_raw_obs_processing_ms: 0.08781405628088679\n",
      "time_since_restore: 227.7449436187744\n",
      "time_this_iter_s: 5.426835775375366\n",
      "time_total_s: 227.7449436187744\n",
      "timers:\n",
      "  learn_throughput: 2146.216\n",
      "  learn_time_ms: 1863.745\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.873\n",
      "  sample_time_ms: 3518.424\n",
      "  update_time_ms: 1.809\n",
      "timestamp: 1633507803\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 168000\n",
      "training_iteration: 42\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:42 starting ! -----------------\n",
      "agent_timesteps_total: 172000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-10-22\n",
      "done: false\n",
      "episode_len_mean: 688.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8481969999999983\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 124\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5624713897705078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01147247664630413\n",
      "        model: {}\n",
      "        policy_loss: -0.019824039191007614\n",
      "        total_loss: 0.01207154244184494\n",
      "        vf_explained_var: 0.014863289892673492\n",
      "        vf_loss: 0.0296010822057724\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_steps_sampled: 172000\n",
      "  num_steps_trained: 172000\n",
      "iterations_since_restore: 43\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.092307692307692\n",
      "  ram_util_percent: 43.43461538461539\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04885318302476724\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7600457806110239\n",
      "  mean_inference_ms: 0.8563431146860612\n",
      "  mean_raw_obs_processing_ms: 0.08784398721828185\n",
      "time_since_restore: 233.15463304519653\n",
      "time_this_iter_s: 5.409689426422119\n",
      "time_total_s: 233.15463304519653\n",
      "timers:\n",
      "  learn_throughput: 2144.103\n",
      "  learn_time_ms: 1865.582\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.8\n",
      "  sample_time_ms: 3494.061\n",
      "  update_time_ms: 1.711\n",
      "timestamp: 1633507822\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 172000\n",
      "training_iteration: 43\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:43 starting ! -----------------\n",
      "agent_timesteps_total: 176000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-10-46\n",
      "done: false\n",
      "episode_len_mean: 690.41\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8578714999999982\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 127\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6706777811050415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012171431444585323\n",
      "        model: {}\n",
      "        policy_loss: -0.017806081101298332\n",
      "        total_loss: 0.0026253454852849245\n",
      "        vf_explained_var: -0.09152019023895264\n",
      "        vf_loss: 0.0179971344769001\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_steps_sampled: 176000\n",
      "  num_steps_trained: 176000\n",
      "iterations_since_restore: 44\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.900000000000002\n",
      "  ram_util_percent: 43.40882352941177\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04891014302057533\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7595118022919668\n",
      "  mean_inference_ms: 0.8562217584684287\n",
      "  mean_raw_obs_processing_ms: 0.08786761137311765\n",
      "time_since_restore: 238.42863297462463\n",
      "time_this_iter_s: 5.273999929428101\n",
      "time_total_s: 238.42863297462463\n",
      "timers:\n",
      "  learn_throughput: 2146.138\n",
      "  learn_time_ms: 1863.813\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.116\n",
      "  sample_time_ms: 3480.935\n",
      "  update_time_ms: 1.731\n",
      "timestamp: 1633507846\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 176000\n",
      "training_iteration: 44\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:44 starting ! -----------------\n",
      "agent_timesteps_total: 180000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-11-03\n",
      "done: false\n",
      "episode_len_mean: 688.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8584619999999981\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 129\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5706194639205933\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010079467669129372\n",
      "        model: {}\n",
      "        policy_loss: -0.013705425895750523\n",
      "        total_loss: 0.003313599154353142\n",
      "        vf_explained_var: 0.17834283411502838\n",
      "        vf_loss: 0.015003126114606857\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_steps_sampled: 180000\n",
      "  num_steps_trained: 180000\n",
      "iterations_since_restore: 45\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.241666666666664\n",
      "  ram_util_percent: 43.4\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04894841120569952\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7591305490001921\n",
      "  mean_inference_ms: 0.8561372703190492\n",
      "  mean_raw_obs_processing_ms: 0.08789149186020698\n",
      "time_since_restore: 243.72045826911926\n",
      "time_this_iter_s: 5.291825294494629\n",
      "time_total_s: 243.72045826911926\n",
      "timers:\n",
      "  learn_throughput: 2153.91\n",
      "  learn_time_ms: 1857.088\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.58\n",
      "  sample_time_ms: 3479.532\n",
      "  update_time_ms: 1.687\n",
      "timestamp: 1633507863\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 180000\n",
      "training_iteration: 45\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:45 starting ! -----------------\n",
      "agent_timesteps_total: 184000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-11-27\n",
      "done: false\n",
      "episode_len_mean: 687.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8679769999999981\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 132\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6026180982589722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009984048083424568\n",
      "        model: {}\n",
      "        policy_loss: -0.010036241263151169\n",
      "        total_loss: 0.011390515603125095\n",
      "        vf_explained_var: 0.04433160275220871\n",
      "        vf_loss: 0.019429950043559074\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_steps_sampled: 184000\n",
      "  num_steps_trained: 184000\n",
      "iterations_since_restore: 46\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.142424242424244\n",
      "  ram_util_percent: 43.38787878787879\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04900648784806341\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7585866802954024\n",
      "  mean_inference_ms: 0.8560463635636789\n",
      "  mean_raw_obs_processing_ms: 0.08792593543577404\n",
      "time_since_restore: 249.19848012924194\n",
      "time_this_iter_s: 5.478021860122681\n",
      "time_total_s: 249.19848012924194\n",
      "timers:\n",
      "  learn_throughput: 2150.332\n",
      "  learn_time_ms: 1860.178\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.491\n",
      "  sample_time_ms: 3491.951\n",
      "  update_time_ms: 1.687\n",
      "timestamp: 1633507887\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 184000\n",
      "training_iteration: 46\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:46 starting ! -----------------\n",
      "agent_timesteps_total: 188000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-11-47\n",
      "done: false\n",
      "episode_len_mean: 687.71\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8676134999999983\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 134\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6578381061553955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011323473416268826\n",
      "        model: {}\n",
      "        policy_loss: -0.018691271543502808\n",
      "        total_loss: -0.003244953230023384\n",
      "        vf_explained_var: -0.24780765175819397\n",
      "        vf_loss: 0.013181624934077263\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_steps_sampled: 188000\n",
      "  num_steps_trained: 188000\n",
      "iterations_since_restore: 47\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.342857142857145\n",
      "  ram_util_percent: 43.39285714285715\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04904719310637692\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7583145907418686\n",
      "  mean_inference_ms: 0.8559767018969475\n",
      "  mean_raw_obs_processing_ms: 0.08795060508108056\n",
      "time_since_restore: 254.61719059944153\n",
      "time_this_iter_s: 5.418710470199585\n",
      "time_total_s: 254.61719059944153\n",
      "timers:\n",
      "  learn_throughput: 2141.32\n",
      "  learn_time_ms: 1868.007\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.617\n",
      "  sample_time_ms: 3491.569\n",
      "  update_time_ms: 1.787\n",
      "timestamp: 1633507907\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 188000\n",
      "training_iteration: 47\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:47 starting ! -----------------\n",
      "agent_timesteps_total: 192000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-12-01\n",
      "done: false\n",
      "episode_len_mean: 690.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8793909999999979\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 137\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6054731607437134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00908399187028408\n",
      "        model: {}\n",
      "        policy_loss: -0.018462680280208588\n",
      "        total_loss: 0.0024293253663927317\n",
      "        vf_explained_var: 0.03772377967834473\n",
      "        vf_loss: 0.019075199961662292\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_steps_sampled: 192000\n",
      "  num_steps_trained: 192000\n",
      "iterations_since_restore: 48\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.24285714285714\n",
      "  ram_util_percent: 43.37619047619047\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0491174035366757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7579493993639855\n",
      "  mean_inference_ms: 0.8558675322668888\n",
      "  mean_raw_obs_processing_ms: 0.08798913837831035\n",
      "time_since_restore: 259.9250326156616\n",
      "time_this_iter_s: 5.307842016220093\n",
      "time_total_s: 259.9250326156616\n",
      "timers:\n",
      "  learn_throughput: 2138.833\n",
      "  learn_time_ms: 1870.178\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.702\n",
      "  sample_time_ms: 3482.191\n",
      "  update_time_ms: 1.787\n",
      "timestamp: 1633507921\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 192000\n",
      "training_iteration: 48\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:48 starting ! -----------------\n",
      "agent_timesteps_total: 196000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-12-14\n",
      "done: false\n",
      "episode_len_mean: 687.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.939225499999998\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 141\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.684525966644287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011135249398648739\n",
      "        model: {}\n",
      "        policy_loss: -0.022494109347462654\n",
      "        total_loss: 0.00191108335275203\n",
      "        vf_explained_var: 0.16672858595848083\n",
      "        vf_loss: 0.022178148850798607\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_steps_sampled: 196000\n",
      "  num_steps_trained: 196000\n",
      "iterations_since_restore: 49\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.605555555555554\n",
      "  ram_util_percent: 43.37222222222222\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049214954534386325\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7574902943849583\n",
      "  mean_inference_ms: 0.8557286358780212\n",
      "  mean_raw_obs_processing_ms: 0.08803154981981443\n",
      "time_since_restore: 265.24192237854004\n",
      "time_this_iter_s: 5.316889762878418\n",
      "time_total_s: 265.24192237854004\n",
      "timers:\n",
      "  learn_throughput: 2139.44\n",
      "  learn_time_ms: 1869.648\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.722\n",
      "  sample_time_ms: 3491.247\n",
      "  update_time_ms: 1.806\n",
      "timestamp: 1633507934\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 196000\n",
      "training_iteration: 49\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:49 starting ! -----------------\n",
      "agent_timesteps_total: 200000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-12-33\n",
      "done: false\n",
      "episode_len_mean: 687.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.9394849999999981\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 143\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6754554510116577\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014880984090268612\n",
      "        model: {}\n",
      "        policy_loss: -0.023944169282913208\n",
      "        total_loss: -0.013610911555588245\n",
      "        vf_explained_var: -0.27369555830955505\n",
      "        vf_loss: 0.007357058580964804\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_steps_sampled: 200000\n",
      "  num_steps_trained: 200000\n",
      "iterations_since_restore: 50\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.311111111111114\n",
      "  ram_util_percent: 43.3888888888889\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049264112280457135\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7573284860778418\n",
      "  mean_inference_ms: 0.8556606799957059\n",
      "  mean_raw_obs_processing_ms: 0.0880440419915434\n",
      "time_since_restore: 270.65526032447815\n",
      "time_this_iter_s: 5.41333794593811\n",
      "time_total_s: 270.65526032447815\n",
      "timers:\n",
      "  learn_throughput: 2146.475\n",
      "  learn_time_ms: 1863.52\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.757\n",
      "  sample_time_ms: 3500.307\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633507953\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200000\n",
      "training_iteration: 50\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:50 starting ! -----------------\n",
      "agent_timesteps_total: 204000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-12-45\n",
      "done: false\n",
      "episode_len_mean: 686.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.929471999999998\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 146\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6221400499343872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01062043011188507\n",
      "        model: {}\n",
      "        policy_loss: -0.019348403438925743\n",
      "        total_loss: 0.004713508300483227\n",
      "        vf_explained_var: -0.05166850611567497\n",
      "        vf_loss: 0.02193782851099968\n",
      "  num_agent_steps_sampled: 204000\n",
      "  num_agent_steps_trained: 204000\n",
      "  num_steps_sampled: 204000\n",
      "  num_steps_trained: 204000\n",
      "iterations_since_restore: 51\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.318749999999998\n",
      "  ram_util_percent: 43.4\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04933675335758823\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.757118536562979\n",
      "  mean_inference_ms: 0.8555683407022536\n",
      "  mean_raw_obs_processing_ms: 0.08806110073159694\n",
      "time_since_restore: 276.1002576351166\n",
      "time_this_iter_s: 5.444997310638428\n",
      "time_total_s: 276.1002576351166\n",
      "timers:\n",
      "  learn_throughput: 2139.799\n",
      "  learn_time_ms: 1869.335\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.366\n",
      "  sample_time_ms: 3501.506\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633507965\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 204000\n",
      "training_iteration: 51\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:51 starting ! -----------------\n",
      "agent_timesteps_total: 208000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-12-59\n",
      "done: false\n",
      "episode_len_mean: 678.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.9088064999999979\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 150\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6684097051620483\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009965674951672554\n",
      "        model: {}\n",
      "        policy_loss: -0.0226532481610775\n",
      "        total_loss: -0.0045729936100542545\n",
      "        vf_explained_var: 0.1872883439064026\n",
      "        vf_loss: 0.01608712039887905\n",
      "  num_agent_steps_sampled: 208000\n",
      "  num_agent_steps_trained: 208000\n",
      "  num_steps_sampled: 208000\n",
      "  num_steps_trained: 208000\n",
      "iterations_since_restore: 52\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.909999999999997\n",
      "  ram_util_percent: 43.415\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0494318662433168\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7568481198609135\n",
      "  mean_inference_ms: 0.855486669956712\n",
      "  mean_raw_obs_processing_ms: 0.0880885756561784\n",
      "time_since_restore: 281.5180060863495\n",
      "time_this_iter_s: 5.41774845123291\n",
      "time_total_s: 281.5180060863495\n",
      "timers:\n",
      "  learn_throughput: 2139.121\n",
      "  learn_time_ms: 1869.927\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.9\n",
      "  sample_time_ms: 3499.867\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633507979\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 208000\n",
      "training_iteration: 52\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:52 starting ! -----------------\n",
      "agent_timesteps_total: 212000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-13-14\n",
      "done: false\n",
      "episode_len_mean: 657.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.9185404999999978\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 154\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6504061222076416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010344232432544231\n",
      "        model: {}\n",
      "        policy_loss: -0.01867021806538105\n",
      "        total_loss: 0.013162409886717796\n",
      "        vf_explained_var: -0.09734036773443222\n",
      "        vf_loss: 0.02976377308368683\n",
      "  num_agent_steps_sampled: 212000\n",
      "  num_agent_steps_trained: 212000\n",
      "  num_steps_sampled: 212000\n",
      "  num_steps_trained: 212000\n",
      "iterations_since_restore: 53\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.719047619047615\n",
      "  ram_util_percent: 43.44285714285714\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04954237220467415\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7566742602386672\n",
      "  mean_inference_ms: 0.8554098729952684\n",
      "  mean_raw_obs_processing_ms: 0.08810289712773768\n",
      "time_since_restore: 286.8294219970703\n",
      "time_this_iter_s: 5.311415910720825\n",
      "time_total_s: 286.8294219970703\n",
      "timers:\n",
      "  learn_throughput: 2141.959\n",
      "  learn_time_ms: 1867.45\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.322\n",
      "  sample_time_ms: 3492.469\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633507994\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 212000\n",
      "training_iteration: 53\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:53 starting ! -----------------\n",
      "agent_timesteps_total: 216000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-13-36\n",
      "done: false\n",
      "episode_len_mean: 657.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.9072434999999979\n",
      "episode_reward_min: -2.2139499999999996\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 158\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7032934427261353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010242046788334846\n",
      "        model: {}\n",
      "        policy_loss: -0.0031232195906341076\n",
      "        total_loss: 0.032730452716350555\n",
      "        vf_explained_var: -0.23591360449790955\n",
      "        vf_loss: 0.03380526229739189\n",
      "  num_agent_steps_sampled: 216000\n",
      "  num_agent_steps_trained: 216000\n",
      "  num_steps_sampled: 216000\n",
      "  num_steps_trained: 216000\n",
      "iterations_since_restore: 54\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.836666666666662\n",
      "  ram_util_percent: 43.42000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04963785833816388\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7564622429615301\n",
      "  mean_inference_ms: 0.855353675377531\n",
      "  mean_raw_obs_processing_ms: 0.08814419277798739\n",
      "time_since_restore: 292.1870722770691\n",
      "time_this_iter_s: 5.357650279998779\n",
      "time_total_s: 292.1870722770691\n",
      "timers:\n",
      "  learn_throughput: 2140.713\n",
      "  learn_time_ms: 1868.537\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.948\n",
      "  sample_time_ms: 3499.721\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633508016\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 216000\n",
      "training_iteration: 54\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:54 starting ! -----------------\n",
      "agent_timesteps_total: 220000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-13-53\n",
      "done: false\n",
      "episode_len_mean: 661.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8968054999999978\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 160\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4850183725357056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009956251829862595\n",
      "        model: {}\n",
      "        policy_loss: -0.027941200882196426\n",
      "        total_loss: -0.003415594110265374\n",
      "        vf_explained_var: 0.04520595073699951\n",
      "        vf_loss: 0.02253435179591179\n",
      "  num_agent_steps_sampled: 220000\n",
      "  num_agent_steps_trained: 220000\n",
      "  num_steps_sampled: 220000\n",
      "  num_steps_trained: 220000\n",
      "iterations_since_restore: 55\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.16\n",
      "  ram_util_percent: 43.43200000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04968250966120771\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7563208238745056\n",
      "  mean_inference_ms: 0.8553195866350315\n",
      "  mean_raw_obs_processing_ms: 0.08816986993022866\n",
      "time_since_restore: 297.43093085289\n",
      "time_this_iter_s: 5.243858575820923\n",
      "time_total_s: 297.43093085289\n",
      "timers:\n",
      "  learn_throughput: 2141.814\n",
      "  learn_time_ms: 1867.576\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.158\n",
      "  sample_time_ms: 3496.022\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633508033\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 220000\n",
      "training_iteration: 55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:55 starting ! -----------------\n",
      "agent_timesteps_total: 224000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-14-17\n",
      "done: false\n",
      "episode_len_mean: 663.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8970114999999979\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 162\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6834207773208618\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010563583113253117\n",
      "        model: {}\n",
      "        policy_loss: -0.01249787025153637\n",
      "        total_loss: 0.003294644644483924\n",
      "        vf_explained_var: -0.022247420623898506\n",
      "        vf_loss: 0.013679804280400276\n",
      "  num_agent_steps_sampled: 224000\n",
      "  num_agent_steps_trained: 224000\n",
      "  num_steps_sampled: 224000\n",
      "  num_steps_trained: 224000\n",
      "iterations_since_restore: 56\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.545454545454547\n",
      "  ram_util_percent: 43.412121212121214\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049721029950171275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7562227099302996\n",
      "  mean_inference_ms: 0.8552957754856633\n",
      "  mean_raw_obs_processing_ms: 0.08819786421785296\n",
      "time_since_restore: 302.72644662857056\n",
      "time_this_iter_s: 5.295515775680542\n",
      "time_total_s: 302.72644662857056\n",
      "timers:\n",
      "  learn_throughput: 2142.43\n",
      "  learn_time_ms: 1867.039\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.013\n",
      "  sample_time_ms: 3478.222\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633508057\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 224000\n",
      "training_iteration: 56\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:56 starting ! -----------------\n",
      "agent_timesteps_total: 228000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-14-31\n",
      "done: false\n",
      "episode_len_mean: 663.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8975339999999978\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 164\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5487217903137207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007937892340123653\n",
      "        model: {}\n",
      "        policy_loss: -0.005911736749112606\n",
      "        total_loss: 0.007121210917830467\n",
      "        vf_explained_var: -0.05414632335305214\n",
      "        vf_loss: 0.011445362120866776\n",
      "  num_agent_steps_sampled: 228000\n",
      "  num_agent_steps_trained: 228000\n",
      "  num_steps_sampled: 228000\n",
      "  num_steps_trained: 228000\n",
      "iterations_since_restore: 57\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.52\n",
      "  ram_util_percent: 43.39999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049760591287440314\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7561424150263916\n",
      "  mean_inference_ms: 0.8552698007317733\n",
      "  mean_raw_obs_processing_ms: 0.08822458336640687\n",
      "time_since_restore: 308.17030215263367\n",
      "time_this_iter_s: 5.44385552406311\n",
      "time_total_s: 308.17030215263367\n",
      "timers:\n",
      "  learn_throughput: 2145.708\n",
      "  learn_time_ms: 1864.187\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.188\n",
      "  sample_time_ms: 3483.751\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633508071\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 228000\n",
      "training_iteration: 57\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:57 starting ! -----------------\n",
      "agent_timesteps_total: 232000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-14-49\n",
      "done: false\n",
      "episode_len_mean: 665.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8385469999999978\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 168\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7475411891937256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009515712969005108\n",
      "        model: {}\n",
      "        policy_loss: -0.02112302929162979\n",
      "        total_loss: 0.0047256434336304665\n",
      "        vf_explained_var: -0.08994700759649277\n",
      "        vf_loss: 0.023945530876517296\n",
      "  num_agent_steps_sampled: 232000\n",
      "  num_agent_steps_trained: 232000\n",
      "  num_steps_sampled: 232000\n",
      "  num_steps_trained: 232000\n",
      "iterations_since_restore: 58\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.331999999999997\n",
      "  ram_util_percent: 43.444\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049841241322225456\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7560210713338361\n",
      "  mean_inference_ms: 0.8551967877493208\n",
      "  mean_raw_obs_processing_ms: 0.08827141472372743\n",
      "time_since_restore: 313.7041850090027\n",
      "time_this_iter_s: 5.5338828563690186\n",
      "time_total_s: 313.7041850090027\n",
      "timers:\n",
      "  learn_throughput: 2133.761\n",
      "  learn_time_ms: 1874.624\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.186\n",
      "  sample_time_ms: 3495.935\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633508089\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 232000\n",
      "training_iteration: 58\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:58 starting ! -----------------\n",
      "agent_timesteps_total: 236000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-15-01\n",
      "done: false\n",
      "episode_len_mean: 663.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8385304999999978\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 171\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6850454807281494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009110401384532452\n",
      "        model: {}\n",
      "        policy_loss: -0.016914470121264458\n",
      "        total_loss: 0.0003132861165795475\n",
      "        vf_explained_var: -0.06065685674548149\n",
      "        vf_loss: 0.01540567446500063\n",
      "  num_agent_steps_sampled: 236000\n",
      "  num_agent_steps_trained: 236000\n",
      "  num_steps_sampled: 236000\n",
      "  num_steps_trained: 236000\n",
      "iterations_since_restore: 59\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.741176470588236\n",
      "  ram_util_percent: 43.4\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04989251174838107\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7558969715374492\n",
      "  mean_inference_ms: 0.8551247222326311\n",
      "  mean_raw_obs_processing_ms: 0.08830338158553461\n",
      "time_since_restore: 319.0020968914032\n",
      "time_this_iter_s: 5.297911882400513\n",
      "time_total_s: 319.0020968914032\n",
      "timers:\n",
      "  learn_throughput: 2124.474\n",
      "  learn_time_ms: 1882.819\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.548\n",
      "  sample_time_ms: 3485.693\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633508101\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 236000\n",
      "training_iteration: 59\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:59 starting ! -----------------\n",
      "agent_timesteps_total: 240000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-15-23\n",
      "done: false\n",
      "episode_len_mean: 668.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.847917999999998\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 173\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5018550157546997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010068021714687347\n",
      "        model: {}\n",
      "        policy_loss: -0.011496016755700111\n",
      "        total_loss: 0.017785251140594482\n",
      "        vf_explained_var: 0.0948028415441513\n",
      "        vf_loss: 0.027267660945653915\n",
      "  num_agent_steps_sampled: 240000\n",
      "  num_agent_steps_trained: 240000\n",
      "  num_steps_sampled: 240000\n",
      "  num_steps_trained: 240000\n",
      "iterations_since_restore: 60\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.40645161290323\n",
      "  ram_util_percent: 43.4451612903226\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04992001121178841\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7557857341752799\n",
      "  mean_inference_ms: 0.8550811386958438\n",
      "  mean_raw_obs_processing_ms: 0.08832237741784395\n",
      "time_since_restore: 324.2398374080658\n",
      "time_this_iter_s: 5.237740516662598\n",
      "time_total_s: 324.2398374080658\n",
      "timers:\n",
      "  learn_throughput: 2124.489\n",
      "  learn_time_ms: 1882.806\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1153.309\n",
      "  sample_time_ms: 3468.282\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633508123\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 240000\n",
      "training_iteration: 60\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:60 starting ! -----------------\n",
      "agent_timesteps_total: 244000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-15-35\n",
      "done: false\n",
      "episode_len_mean: 664.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8475469999999982\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 176\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.545778512954712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01106109656393528\n",
      "        model: {}\n",
      "        policy_loss: -0.019631650298833847\n",
      "        total_loss: -0.004431749694049358\n",
      "        vf_explained_var: 0.014531513676047325\n",
      "        vf_loss: 0.012987674213945866\n",
      "  num_agent_steps_sampled: 244000\n",
      "  num_agent_steps_trained: 244000\n",
      "  num_steps_sampled: 244000\n",
      "  num_steps_trained: 244000\n",
      "iterations_since_restore: 61\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.841176470588234\n",
      "  ram_util_percent: 43.423529411764704\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04995250505840697\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7556059983008909\n",
      "  mean_inference_ms: 0.8550030352317443\n",
      "  mean_raw_obs_processing_ms: 0.08834610957083011\n",
      "time_since_restore: 329.5127763748169\n",
      "time_this_iter_s: 5.272938966751099\n",
      "time_total_s: 329.5127763748169\n",
      "timers:\n",
      "  learn_throughput: 2133.012\n",
      "  learn_time_ms: 1875.282\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1156.523\n",
      "  sample_time_ms: 3458.642\n",
      "  update_time_ms: 1.795\n",
      "timestamp: 1633508135\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 244000\n",
      "training_iteration: 61\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:61 starting ! -----------------\n",
      "agent_timesteps_total: 248000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-15-59\n",
      "done: false\n",
      "episode_len_mean: 664.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.7977379999999981\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 179\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6601378917694092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008327815681695938\n",
      "        model: {}\n",
      "        policy_loss: -0.023092782124876976\n",
      "        total_loss: 0.003927694633603096\n",
      "        vf_explained_var: -0.24533379077911377\n",
      "        vf_loss: 0.025354912504553795\n",
      "  num_agent_steps_sampled: 248000\n",
      "  num_agent_steps_trained: 248000\n",
      "  num_steps_sampled: 248000\n",
      "  num_steps_trained: 248000\n",
      "iterations_since_restore: 62\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.976470588235294\n",
      "  ram_util_percent: 43.43529411764707\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04997997333798763\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7554538362930826\n",
      "  mean_inference_ms: 0.8549257333088298\n",
      "  mean_raw_obs_processing_ms: 0.08836812014294158\n",
      "time_since_restore: 334.8400766849518\n",
      "time_this_iter_s: 5.327300310134888\n",
      "time_total_s: 334.8400766849518\n",
      "timers:\n",
      "  learn_throughput: 2138.385\n",
      "  learn_time_ms: 1870.57\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1157.999\n",
      "  sample_time_ms: 3454.236\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633508159\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 248000\n",
      "training_iteration: 62\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:62 starting ! -----------------\n",
      "agent_timesteps_total: 252000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-16-23\n",
      "done: false\n",
      "episode_len_mean: 665.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8477389999999981\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 182\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6109726428985596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010302448645234108\n",
      "        model: {}\n",
      "        policy_loss: -0.040348656475543976\n",
      "        total_loss: -0.022045869380235672\n",
      "        vf_explained_var: 0.1423765867948532\n",
      "        vf_loss: 0.016242297366261482\n",
      "  num_agent_steps_sampled: 252000\n",
      "  num_agent_steps_trained: 252000\n",
      "  num_steps_sampled: 252000\n",
      "  num_steps_trained: 252000\n",
      "iterations_since_restore: 63\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.857575757575756\n",
      "  ram_util_percent: 43.415151515151514\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.050000362633926515\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7552814574592231\n",
      "  mean_inference_ms: 0.8548613919399632\n",
      "  mean_raw_obs_processing_ms: 0.08838152083990682\n",
      "time_since_restore: 340.1732497215271\n",
      "time_this_iter_s: 5.333173036575317\n",
      "time_total_s: 340.1732497215271\n",
      "timers:\n",
      "  learn_throughput: 2135.826\n",
      "  learn_time_ms: 1872.812\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1158.015\n",
      "  sample_time_ms: 3454.187\n",
      "  update_time_ms: 1.894\n",
      "timestamp: 1633508183\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 252000\n",
      "training_iteration: 63\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:63 starting ! -----------------\n",
      "agent_timesteps_total: 256000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-16-43\n",
      "done: false\n",
      "episode_len_mean: 670.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.848828999999998\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 185\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.518821120262146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008042343892157078\n",
      "        model: {}\n",
      "        policy_loss: -0.009259967133402824\n",
      "        total_loss: 0.006609440315514803\n",
      "        vf_explained_var: 0.12246984988451004\n",
      "        vf_loss: 0.014260940253734589\n",
      "  num_agent_steps_sampled: 256000\n",
      "  num_agent_steps_trained: 256000\n",
      "  num_steps_sampled: 256000\n",
      "  num_steps_trained: 256000\n",
      "iterations_since_restore: 64\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.507142857142856\n",
      "  ram_util_percent: 43.40000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.05001005897238884\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7550791782904273\n",
      "  mean_inference_ms: 0.8548007161020402\n",
      "  mean_raw_obs_processing_ms: 0.08839386385955639\n",
      "time_since_restore: 345.4646737575531\n",
      "time_this_iter_s: 5.291424036026001\n",
      "time_total_s: 345.4646737575531\n",
      "timers:\n",
      "  learn_throughput: 2138.027\n",
      "  learn_time_ms: 1870.884\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1159.581\n",
      "  sample_time_ms: 3449.522\n",
      "  update_time_ms: 1.894\n",
      "timestamp: 1633508203\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 256000\n",
      "training_iteration: 64\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:64 starting ! -----------------\n",
      "agent_timesteps_total: 260000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-16-55\n",
      "done: false\n",
      "episode_len_mean: 674.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93265\n",
      "episode_reward_mean: -0.8600394999999981\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 187\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6168252229690552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011754358187317848\n",
      "        model: {}\n",
      "        policy_loss: -0.017773214727640152\n",
      "        total_loss: 0.0065497164614498615\n",
      "        vf_explained_var: 0.08633682876825333\n",
      "        vf_loss: 0.021972056478261948\n",
      "  num_agent_steps_sampled: 260000\n",
      "  num_agent_steps_trained: 260000\n",
      "  num_steps_sampled: 260000\n",
      "  num_steps_trained: 260000\n",
      "iterations_since_restore: 65\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.922222222222224\n",
      "  ram_util_percent: 43.449999999999996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.05001418094453822\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7549271952680072\n",
      "  mean_inference_ms: 0.8547598384120374\n",
      "  mean_raw_obs_processing_ms: 0.08839772895759897\n",
      "time_since_restore: 350.94468903541565\n",
      "time_this_iter_s: 5.480015277862549\n",
      "time_total_s: 350.94468903541565\n",
      "timers:\n",
      "  learn_throughput: 2137.0\n",
      "  learn_time_ms: 1871.783\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1152.037\n",
      "  sample_time_ms: 3472.11\n",
      "  update_time_ms: 1.873\n",
      "timestamp: 1633508215\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 260000\n",
      "training_iteration: 65\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:65 starting ! -----------------\n",
      "agent_timesteps_total: 264000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-17-11\n",
      "done: false\n",
      "episode_len_mean: 672.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.7482119999999982\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 191\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.539736032485962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008046630769968033\n",
      "        model: {}\n",
      "        policy_loss: -0.01525516901165247\n",
      "        total_loss: 0.028243787586688995\n",
      "        vf_explained_var: -0.10787113755941391\n",
      "        vf_loss: 0.04188963398337364\n",
      "  num_agent_steps_sampled: 264000\n",
      "  num_agent_steps_trained: 264000\n",
      "  num_steps_sampled: 264000\n",
      "  num_steps_trained: 264000\n",
      "iterations_since_restore: 66\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.781818181818178\n",
      "  ram_util_percent: 43.39999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.05001976468365062\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7545270060924665\n",
      "  mean_inference_ms: 0.8546452839270904\n",
      "  mean_raw_obs_processing_ms: 0.08839982836956004\n",
      "time_since_restore: 356.2462224960327\n",
      "time_this_iter_s: 5.301533460617065\n",
      "time_total_s: 356.2462224960327\n",
      "timers:\n",
      "  learn_throughput: 2137.454\n",
      "  learn_time_ms: 1871.385\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1151.697\n",
      "  sample_time_ms: 3473.135\n",
      "  update_time_ms: 1.873\n",
      "timestamp: 1633508231\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 264000\n",
      "training_iteration: 66\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:66 starting ! -----------------\n",
      "agent_timesteps_total: 268000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-17-23\n",
      "done: false\n",
      "episode_len_mean: 674.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.7375044999999981\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 193\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4956340789794922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014041148126125336\n",
      "        model: {}\n",
      "        policy_loss: -0.017334120348095894\n",
      "        total_loss: 0.040874332189559937\n",
      "        vf_explained_var: 0.15815383195877075\n",
      "        vf_loss: 0.05540022626519203\n",
      "  num_agent_steps_sampled: 268000\n",
      "  num_agent_steps_trained: 268000\n",
      "  num_steps_sampled: 268000\n",
      "  num_steps_trained: 268000\n",
      "iterations_since_restore: 67\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.5875\n",
      "  ram_util_percent: 43.3125\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.05001555084999585\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7542957382444371\n",
      "  mean_inference_ms: 0.8545655162786686\n",
      "  mean_raw_obs_processing_ms: 0.08839648498837507\n",
      "time_since_restore: 361.5462095737457\n",
      "time_this_iter_s: 5.299987077713013\n",
      "time_total_s: 361.5462095737457\n",
      "timers:\n",
      "  learn_throughput: 2143.445\n",
      "  learn_time_ms: 1866.154\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1154.74\n",
      "  sample_time_ms: 3463.984\n",
      "  update_time_ms: 1.773\n",
      "timestamp: 1633508243\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 268000\n",
      "training_iteration: 67\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:67 starting ! -----------------\n",
      "agent_timesteps_total: 272000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-17-45\n",
      "done: false\n",
      "episode_len_mean: 672.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.737969499999998\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 196\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4832218885421753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010272940620779991\n",
      "        model: {}\n",
      "        policy_loss: -0.018927302211523056\n",
      "        total_loss: -0.0007497715414501727\n",
      "        vf_explained_var: 0.08948297798633575\n",
      "        vf_loss: 0.016122940927743912\n",
      "  num_agent_steps_sampled: 272000\n",
      "  num_agent_steps_trained: 272000\n",
      "  num_steps_sampled: 272000\n",
      "  num_steps_trained: 272000\n",
      "iterations_since_restore: 68\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.26129032258065\n",
      "  ram_util_percent: 43.32580645161289\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04999807363013308\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7539587156215443\n",
      "  mean_inference_ms: 0.8544528851824486\n",
      "  mean_raw_obs_processing_ms: 0.0883846747850585\n",
      "time_since_restore: 366.8564534187317\n",
      "time_this_iter_s: 5.310243844985962\n",
      "time_total_s: 366.8564534187317\n",
      "timers:\n",
      "  learn_throughput: 2157.43\n",
      "  learn_time_ms: 1854.058\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1158.201\n",
      "  sample_time_ms: 3453.631\n",
      "  update_time_ms: 1.874\n",
      "timestamp: 1633508265\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 272000\n",
      "training_iteration: 68\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:68 starting ! -----------------\n",
      "agent_timesteps_total: 276000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-18-08\n",
      "done: false\n",
      "episode_len_mean: 676.31\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.6775344999999979\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 199\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4119644165039062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012406487949192524\n",
      "        model: {}\n",
      "        policy_loss: -0.011903718113899231\n",
      "        total_loss: 0.033749647438526154\n",
      "        vf_explained_var: 0.06840844452381134\n",
      "        vf_loss: 0.043172068893909454\n",
      "  num_agent_steps_sampled: 276000\n",
      "  num_agent_steps_trained: 276000\n",
      "  num_steps_sampled: 276000\n",
      "  num_steps_trained: 276000\n",
      "iterations_since_restore: 69\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.21818181818182\n",
      "  ram_util_percent: 43.35151515151515\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04997615162026415\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7536286077058755\n",
      "  mean_inference_ms: 0.8543579626161868\n",
      "  mean_raw_obs_processing_ms: 0.08837581750877882\n",
      "time_since_restore: 372.2209074497223\n",
      "time_this_iter_s: 5.364454030990601\n",
      "time_total_s: 372.2209074497223\n",
      "timers:\n",
      "  learn_throughput: 2160.707\n",
      "  learn_time_ms: 1851.246\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1155.023\n",
      "  sample_time_ms: 3463.134\n",
      "  update_time_ms: 1.874\n",
      "timestamp: 1633508288\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 276000\n",
      "training_iteration: 69\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:69 starting ! -----------------\n",
      "agent_timesteps_total: 280000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-18-21\n",
      "done: false\n",
      "episode_len_mean: 673.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.657423999999998\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 203\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5325143337249756\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010226180776953697\n",
      "        model: {}\n",
      "        policy_loss: -0.0120172668248415\n",
      "        total_loss: 0.052624840289354324\n",
      "        vf_explained_var: -0.2634902596473694\n",
      "        vf_loss: 0.0625968649983406\n",
      "  num_agent_steps_sampled: 280000\n",
      "  num_agent_steps_trained: 280000\n",
      "  num_steps_sampled: 280000\n",
      "  num_steps_trained: 280000\n",
      "iterations_since_restore: 70\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.383333333333333\n",
      "  ram_util_percent: 43.34444444444444\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04994253331560839\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.753197106902648\n",
      "  mean_inference_ms: 0.8542454804140235\n",
      "  mean_raw_obs_processing_ms: 0.08836349048431352\n",
      "time_since_restore: 377.63250160217285\n",
      "time_this_iter_s: 5.4115941524505615\n",
      "time_total_s: 377.63250160217285\n",
      "timers:\n",
      "  learn_throughput: 2149.671\n",
      "  learn_time_ms: 1860.75\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1152.426\n",
      "  sample_time_ms: 3470.938\n",
      "  update_time_ms: 1.864\n",
      "timestamp: 1633508301\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 280000\n",
      "training_iteration: 70\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:70 starting ! -----------------\n",
      "agent_timesteps_total: 284000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-18-44\n",
      "done: false\n",
      "episode_len_mean: 671.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.6167589999999984\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 206\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6089879274368286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010088684968650341\n",
      "        model: {}\n",
      "        policy_loss: -0.02312445640563965\n",
      "        total_loss: 0.0014533945359289646\n",
      "        vf_explained_var: -0.31974464654922485\n",
      "        vf_loss: 0.0225601177662611\n",
      "  num_agent_steps_sampled: 284000\n",
      "  num_agent_steps_trained: 284000\n",
      "  num_steps_sampled: 284000\n",
      "  num_steps_trained: 284000\n",
      "iterations_since_restore: 71\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.159375\n",
      "  ram_util_percent: 43.321875000000006\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049916476910982964\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7528973239037728\n",
      "  mean_inference_ms: 0.8541476198317457\n",
      "  mean_raw_obs_processing_ms: 0.08835631883874914\n",
      "time_since_restore: 382.9646039009094\n",
      "time_this_iter_s: 5.332102298736572\n",
      "time_total_s: 382.9646039009094\n",
      "timers:\n",
      "  learn_throughput: 2149.295\n",
      "  learn_time_ms: 1861.075\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.583\n",
      "  sample_time_ms: 3476.497\n",
      "  update_time_ms: 1.764\n",
      "timestamp: 1633508324\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 284000\n",
      "training_iteration: 71\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:71 starting ! -----------------\n",
      "agent_timesteps_total: 288000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-19-04\n",
      "done: false\n",
      "episode_len_mean: 666.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.5561704999999983\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 209\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4348564147949219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008446384221315384\n",
      "        model: {}\n",
      "        policy_loss: -0.012673203833401203\n",
      "        total_loss: 0.025121642276644707\n",
      "        vf_explained_var: -0.18525758385658264\n",
      "        vf_loss: 0.03610556945204735\n",
      "  num_agent_steps_sampled: 288000\n",
      "  num_agent_steps_trained: 288000\n",
      "  num_steps_sampled: 288000\n",
      "  num_steps_trained: 288000\n",
      "iterations_since_restore: 72\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.46896551724138\n",
      "  ram_util_percent: 43.39310344827587\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04989800141220061\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7526219305768729\n",
      "  mean_inference_ms: 0.8540540643172134\n",
      "  mean_raw_obs_processing_ms: 0.08834655621074652\n",
      "time_since_restore: 388.29233169555664\n",
      "time_this_iter_s: 5.327727794647217\n",
      "time_total_s: 388.29233169555664\n",
      "timers:\n",
      "  learn_throughput: 2149.398\n",
      "  learn_time_ms: 1860.986\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.515\n",
      "  sample_time_ms: 3476.703\n",
      "  update_time_ms: 1.665\n",
      "timestamp: 1633508344\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 288000\n",
      "training_iteration: 72\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:72 starting ! -----------------\n",
      "agent_timesteps_total: 292000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-19-23\n",
      "done: false\n",
      "episode_len_mean: 665.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.5068719999999983\n",
      "episode_reward_min: -2.2043000000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 212\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4208056926727295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009049399755895138\n",
      "        model: {}\n",
      "        policy_loss: -0.015663154423236847\n",
      "        total_loss: 0.004297408740967512\n",
      "        vf_explained_var: 0.21855053305625916\n",
      "        vf_loss: 0.018150679767131805\n",
      "  num_agent_steps_sampled: 292000\n",
      "  num_agent_steps_trained: 292000\n",
      "  num_steps_sampled: 292000\n",
      "  num_steps_trained: 292000\n",
      "iterations_since_restore: 73\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.908\n",
      "  ram_util_percent: 43.33999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049890458050355156\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7523594050780328\n",
      "  mean_inference_ms: 0.8539640025010127\n",
      "  mean_raw_obs_processing_ms: 0.0883369111210743\n",
      "time_since_restore: 393.61500120162964\n",
      "time_this_iter_s: 5.322669506072998\n",
      "time_total_s: 393.61500120162964\n",
      "timers:\n",
      "  learn_throughput: 2150.354\n",
      "  learn_time_ms: 1860.159\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.57\n",
      "  sample_time_ms: 3476.537\n",
      "  update_time_ms: 1.665\n",
      "timestamp: 1633508363\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 292000\n",
      "training_iteration: 73\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:73 starting ! -----------------\n",
      "agent_timesteps_total: 296000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-19-38\n",
      "done: false\n",
      "episode_len_mean: 671.85\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.4571899999999983\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 215\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4127362966537476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008763395249843597\n",
      "        model: {}\n",
      "        policy_loss: -0.01538561936467886\n",
      "        total_loss: 0.014295734465122223\n",
      "        vf_explained_var: 0.09313881397247314\n",
      "        vf_loss: 0.0279286690056324\n",
      "  num_agent_steps_sampled: 296000\n",
      "  num_agent_steps_trained: 296000\n",
      "  num_steps_sampled: 296000\n",
      "  num_steps_trained: 296000\n",
      "iterations_since_restore: 74\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.80909090909091\n",
      "  ram_util_percent: 43.34545454545454\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04988834072471529\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7521105093966155\n",
      "  mean_inference_ms: 0.8538793570863252\n",
      "  mean_raw_obs_processing_ms: 0.08832493741951432\n",
      "time_since_restore: 399.10169219970703\n",
      "time_this_iter_s: 5.486690998077393\n",
      "time_total_s: 399.10169219970703\n",
      "timers:\n",
      "  learn_throughput: 2141.32\n",
      "  learn_time_ms: 1868.007\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.737\n",
      "  sample_time_ms: 3488.157\n",
      "  update_time_ms: 1.565\n",
      "timestamp: 1633508378\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 296000\n",
      "training_iteration: 74\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:74 starting ! -----------------\n",
      "agent_timesteps_total: 300000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-19-59\n",
      "done: false\n",
      "episode_len_mean: 680.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.44762199999999824\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 217\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.383090615272522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01189574133604765\n",
      "        model: {}\n",
      "        policy_loss: -0.010273479856550694\n",
      "        total_loss: 0.01925545372068882\n",
      "        vf_explained_var: 0.07528872042894363\n",
      "        vf_loss: 0.027149783447384834\n",
      "  num_agent_steps_sampled: 300000\n",
      "  num_agent_steps_trained: 300000\n",
      "  num_steps_sampled: 300000\n",
      "  num_steps_trained: 300000\n",
      "iterations_since_restore: 75\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.443333333333335\n",
      "  ram_util_percent: 43.339999999999996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04988205706708925\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7519335609844698\n",
      "  mean_inference_ms: 0.8538375626727941\n",
      "  mean_raw_obs_processing_ms: 0.08831617766537114\n",
      "time_since_restore: 404.4532561302185\n",
      "time_this_iter_s: 5.351563930511475\n",
      "time_total_s: 404.4532561302185\n",
      "timers:\n",
      "  learn_throughput: 2138.53\n",
      "  learn_time_ms: 1870.443\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1151.796\n",
      "  sample_time_ms: 3472.837\n",
      "  update_time_ms: 1.586\n",
      "timestamp: 1633508399\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 300000\n",
      "training_iteration: 75\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:75 starting ! -----------------\n",
      "agent_timesteps_total: 304000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-20-17\n",
      "done: false\n",
      "episode_len_mean: 684.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.4079724999999982\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 220\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.43300199508667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012813219800591469\n",
      "        model: {}\n",
      "        policy_loss: -0.018693862482905388\n",
      "        total_loss: 0.03512212261557579\n",
      "        vf_explained_var: -0.2992680072784424\n",
      "        vf_loss: 0.05125334486365318\n",
      "  num_agent_steps_sampled: 304000\n",
      "  num_agent_steps_trained: 304000\n",
      "  num_steps_sampled: 304000\n",
      "  num_steps_trained: 304000\n",
      "iterations_since_restore: 76\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.991999999999997\n",
      "  ram_util_percent: 43.308\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04987186554779667\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7516685545199899\n",
      "  mean_inference_ms: 0.8537708795154043\n",
      "  mean_raw_obs_processing_ms: 0.0883047657482909\n",
      "time_since_restore: 409.7974741458893\n",
      "time_this_iter_s: 5.344218015670776\n",
      "time_total_s: 409.7974741458893\n",
      "timers:\n",
      "  learn_throughput: 2139.572\n",
      "  learn_time_ms: 1869.533\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.117\n",
      "  sample_time_ms: 3477.908\n",
      "  update_time_ms: 1.586\n",
      "timestamp: 1633508417\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 304000\n",
      "training_iteration: 76\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:76 starting ! -----------------\n",
      "agent_timesteps_total: 308000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-20-41\n",
      "done: false\n",
      "episode_len_mean: 680.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.4086959999999983\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 224\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.463592767715454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009494001977145672\n",
      "        model: {}\n",
      "        policy_loss: -0.014478064142167568\n",
      "        total_loss: 0.04397941008210182\n",
      "        vf_explained_var: -0.40849950909614563\n",
      "        vf_loss: 0.05655866116285324\n",
      "  num_agent_steps_sampled: 308000\n",
      "  num_agent_steps_trained: 308000\n",
      "  num_steps_sampled: 308000\n",
      "  num_steps_trained: 308000\n",
      "iterations_since_restore: 77\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.654545454545456\n",
      "  ram_util_percent: 43.31818181818182\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04985894780775354\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7513325253923311\n",
      "  mean_inference_ms: 0.8536505848371456\n",
      "  mean_raw_obs_processing_ms: 0.08828199365708772\n",
      "time_since_restore: 415.1360733509064\n",
      "time_this_iter_s: 5.33859920501709\n",
      "time_total_s: 415.1360733509064\n",
      "timers:\n",
      "  learn_throughput: 2133.997\n",
      "  learn_time_ms: 1874.417\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.459\n",
      "  sample_time_ms: 3476.874\n",
      "  update_time_ms: 1.686\n",
      "timestamp: 1633508441\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 308000\n",
      "training_iteration: 77\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:77 starting ! -----------------\n",
      "agent_timesteps_total: 312000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-20-57\n",
      "done: false\n",
      "episode_len_mean: 681.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.3485784999999983\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 226\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3351411819458008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007055421359837055\n",
      "        model: {}\n",
      "        policy_loss: -0.01455745380371809\n",
      "        total_loss: 0.006926753558218479\n",
      "        vf_explained_var: -0.39692285656929016\n",
      "        vf_loss: 0.02007313258945942\n",
      "  num_agent_steps_sampled: 312000\n",
      "  num_agent_steps_trained: 312000\n",
      "  num_steps_sampled: 312000\n",
      "  num_steps_trained: 312000\n",
      "iterations_since_restore: 78\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.27391304347826\n",
      "  ram_util_percent: 43.33478260869563\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049853139137911055\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511987102510963\n",
      "  mean_inference_ms: 0.8535808584675632\n",
      "  mean_raw_obs_processing_ms: 0.08826946020476888\n",
      "time_since_restore: 420.50012707710266\n",
      "time_this_iter_s: 5.364053726196289\n",
      "time_total_s: 420.50012707710266\n",
      "timers:\n",
      "  learn_throughput: 2132.841\n",
      "  learn_time_ms: 1875.432\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.995\n",
      "  sample_time_ms: 3481.302\n",
      "  update_time_ms: 1.686\n",
      "timestamp: 1633508457\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 312000\n",
      "training_iteration: 78\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:78 starting ! -----------------\n",
      "agent_timesteps_total: 316000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-21-10\n",
      "done: false\n",
      "episode_len_mean: 680.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.3583524999999984\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 229\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.469314694404602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010228005237877369\n",
      "        model: {}\n",
      "        policy_loss: -0.020612914115190506\n",
      "        total_loss: 0.0381854809820652\n",
      "        vf_explained_var: -0.25415730476379395\n",
      "        vf_loss: 0.05675278976559639\n",
      "  num_agent_steps_sampled: 316000\n",
      "  num_agent_steps_trained: 316000\n",
      "  num_steps_sampled: 316000\n",
      "  num_steps_trained: 316000\n",
      "iterations_since_restore: 79\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.79444444444444\n",
      "  ram_util_percent: 43.322222222222216\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0498455891092721\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7510044639550348\n",
      "  mean_inference_ms: 0.8534772345566849\n",
      "  mean_raw_obs_processing_ms: 0.08824784079379895\n",
      "time_since_restore: 425.78276085853577\n",
      "time_this_iter_s: 5.2826337814331055\n",
      "time_total_s: 425.78276085853577\n",
      "timers:\n",
      "  learn_throughput: 2138.628\n",
      "  learn_time_ms: 1870.358\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.0\n",
      "  sample_time_ms: 3478.26\n",
      "  update_time_ms: 1.708\n",
      "timestamp: 1633508470\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 316000\n",
      "training_iteration: 79\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:79 starting ! -----------------\n",
      "agent_timesteps_total: 320000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-21-24\n",
      "done: false\n",
      "episode_len_mean: 684.59\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.3487254999999984\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 231\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.355891227722168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010408828966319561\n",
      "        model: {}\n",
      "        policy_loss: -0.02373570390045643\n",
      "        total_loss: 0.0029455532785505056\n",
      "        vf_explained_var: 0.1882469207048416\n",
      "        vf_loss: 0.02459949627518654\n",
      "  num_agent_steps_sampled: 320000\n",
      "  num_agent_steps_trained: 320000\n",
      "  num_steps_sampled: 320000\n",
      "  num_steps_trained: 320000\n",
      "iterations_since_restore: 80\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.885\n",
      "  ram_util_percent: 43.30499999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049840574979423506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7508624571222211\n",
      "  mean_inference_ms: 0.853403245876391\n",
      "  mean_raw_obs_processing_ms: 0.08823508492627745\n",
      "time_since_restore: 431.1062023639679\n",
      "time_this_iter_s: 5.323441505432129\n",
      "time_total_s: 431.1062023639679\n",
      "timers:\n",
      "  learn_throughput: 2149.721\n",
      "  learn_time_ms: 1860.706\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.755\n",
      "  sample_time_ms: 3479.001\n",
      "  update_time_ms: 1.718\n",
      "timestamp: 1633508484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 320000\n",
      "training_iteration: 80\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:80 starting ! -----------------\n",
      "agent_timesteps_total: 324000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-21-48\n",
      "done: false\n",
      "episode_len_mean: 687.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.3588814999999984\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 233\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3757001161575317\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01160306017845869\n",
      "        model: {}\n",
      "        policy_loss: -0.012109427712857723\n",
      "        total_loss: 0.0047531211748719215\n",
      "        vf_explained_var: -0.061927665024995804\n",
      "        vf_loss: 0.014541932381689548\n",
      "  num_agent_steps_sampled: 324000\n",
      "  num_agent_steps_trained: 324000\n",
      "  num_steps_sampled: 324000\n",
      "  num_steps_trained: 324000\n",
      "iterations_since_restore: 81\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.666666666666668\n",
      "  ram_util_percent: 43.32424242424242\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04983686607714819\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7507210268207538\n",
      "  mean_inference_ms: 0.8533353552564192\n",
      "  mean_raw_obs_processing_ms: 0.08822003369616288\n",
      "time_since_restore: 436.6470868587494\n",
      "time_this_iter_s: 5.540884494781494\n",
      "time_total_s: 436.6470868587494\n",
      "timers:\n",
      "  learn_throughput: 2127.763\n",
      "  learn_time_ms: 1879.909\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.229\n",
      "  sample_time_ms: 3480.594\n",
      "  update_time_ms: 1.818\n",
      "timestamp: 1633508508\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 324000\n",
      "training_iteration: 81\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:81 starting ! -----------------\n",
      "agent_timesteps_total: 328000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-22-07\n",
      "done: false\n",
      "episode_len_mean: 681.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.27807749999999837\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 237\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.448365330696106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012237140908837318\n",
      "        model: {}\n",
      "        policy_loss: -0.019241435453295708\n",
      "        total_loss: 0.014334501698613167\n",
      "        vf_explained_var: 0.12267392128705978\n",
      "        vf_loss: 0.031128503382205963\n",
      "  num_agent_steps_sampled: 328000\n",
      "  num_agent_steps_trained: 328000\n",
      "  num_steps_sampled: 328000\n",
      "  num_steps_trained: 328000\n",
      "iterations_since_restore: 82\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.437037037037037\n",
      "  ram_util_percent: 43.33333333333332\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04982820892048974\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7504496515333078\n",
      "  mean_inference_ms: 0.85321995707022\n",
      "  mean_raw_obs_processing_ms: 0.08818707639158273\n",
      "time_since_restore: 441.95729517936707\n",
      "time_this_iter_s: 5.310208320617676\n",
      "time_total_s: 441.95729517936707\n",
      "timers:\n",
      "  learn_throughput: 2128.157\n",
      "  learn_time_ms: 1879.561\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.66\n",
      "  sample_time_ms: 3479.288\n",
      "  update_time_ms: 1.817\n",
      "timestamp: 1633508527\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 328000\n",
      "training_iteration: 82\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:82 starting ! -----------------\n",
      "agent_timesteps_total: 332000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-22-25\n",
      "done: false\n",
      "episode_len_mean: 678.91\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.2782604999999984\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 240\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4645155668258667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012344148010015488\n",
      "        model: {}\n",
      "        policy_loss: -0.019613051787018776\n",
      "        total_loss: 0.0011770878918468952\n",
      "        vf_explained_var: 0.0164813045412302\n",
      "        vf_loss: 0.018321309238672256\n",
      "  num_agent_steps_sampled: 332000\n",
      "  num_agent_steps_trained: 332000\n",
      "  num_steps_sampled: 332000\n",
      "  num_steps_trained: 332000\n",
      "iterations_since_restore: 83\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.83076923076923\n",
      "  ram_util_percent: 43.31923076923076\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04981884133130634\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7502946309513695\n",
      "  mean_inference_ms: 0.8531343660270896\n",
      "  mean_raw_obs_processing_ms: 0.08815944489403016\n",
      "time_since_restore: 447.427903175354\n",
      "time_this_iter_s: 5.4706079959869385\n",
      "time_total_s: 447.427903175354\n",
      "timers:\n",
      "  learn_throughput: 2130.08\n",
      "  learn_time_ms: 1877.864\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.245\n",
      "  sample_time_ms: 3495.755\n",
      "  update_time_ms: 1.865\n",
      "timestamp: 1633508545\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 332000\n",
      "training_iteration: 83\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:83 starting ! -----------------\n",
      "agent_timesteps_total: 336000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-22-38\n",
      "done: false\n",
      "episode_len_mean: 682.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.26925849999999824\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 243\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.396268367767334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008124045096337795\n",
      "        model: {}\n",
      "        policy_loss: -0.008795664645731449\n",
      "        total_loss: 0.011810743249952793\n",
      "        vf_explained_var: 0.05294005572795868\n",
      "        vf_loss: 0.018981602042913437\n",
      "  num_agent_steps_sampled: 336000\n",
      "  num_agent_steps_trained: 336000\n",
      "  num_steps_sampled: 336000\n",
      "  num_steps_trained: 336000\n",
      "iterations_since_restore: 84\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.83888888888889\n",
      "  ram_util_percent: 43.37222222222222\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049808466554628\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7501430272898889\n",
      "  mean_inference_ms: 0.8530551603471036\n",
      "  mean_raw_obs_processing_ms: 0.08813316849778573\n",
      "time_since_restore: 452.88417887687683\n",
      "time_this_iter_s: 5.456275701522827\n",
      "time_total_s: 452.88417887687683\n",
      "timers:\n",
      "  learn_throughput: 2137.702\n",
      "  learn_time_ms: 1871.168\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.05\n",
      "  sample_time_ms: 3499.408\n",
      "  update_time_ms: 1.965\n",
      "timestamp: 1633508558\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 336000\n",
      "training_iteration: 84\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:84 starting ! -----------------\n",
      "agent_timesteps_total: 340000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-22-51\n",
      "done: false\n",
      "episode_len_mean: 675.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.2586614999999983\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 247\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4179191589355469\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010456563904881477\n",
      "        model: {}\n",
      "        policy_loss: -0.017922205850481987\n",
      "        total_loss: 0.008250725455582142\n",
      "        vf_explained_var: 0.05298199504613876\n",
      "        vf_loss: 0.02408161759376526\n",
      "  num_agent_steps_sampled: 340000\n",
      "  num_agent_steps_trained: 340000\n",
      "  num_steps_sampled: 340000\n",
      "  num_steps_trained: 340000\n",
      "iterations_since_restore: 85\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.783333333333335\n",
      "  ram_util_percent: 43.35555555555555\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04979459760461518\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499345797610824\n",
      "  mean_inference_ms: 0.8529845982078527\n",
      "  mean_raw_obs_processing_ms: 0.088098727636005\n",
      "time_since_restore: 458.39112854003906\n",
      "time_this_iter_s: 5.5069496631622314\n",
      "time_total_s: 458.39112854003906\n",
      "timers:\n",
      "  learn_throughput: 2139.077\n",
      "  learn_time_ms: 1869.965\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.58\n",
      "  sample_time_ms: 3516.237\n",
      "  update_time_ms: 1.865\n",
      "timestamp: 1633508571\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 340000\n",
      "training_iteration: 85\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:85 starting ! -----------------\n",
      "agent_timesteps_total: 344000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-23-15\n",
      "done: false\n",
      "episode_len_mean: 680.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.2093534999999984\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 249\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3509647846221924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0150296064093709\n",
      "        model: {}\n",
      "        policy_loss: -0.022697949782013893\n",
      "        total_loss: 0.0007293277885764837\n",
      "        vf_explained_var: -0.056182581931352615\n",
      "        vf_loss: 0.020421354100108147\n",
      "  num_agent_steps_sampled: 344000\n",
      "  num_agent_steps_trained: 344000\n",
      "  num_steps_sampled: 344000\n",
      "  num_steps_trained: 344000\n",
      "iterations_since_restore: 86\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.34705882352941\n",
      "  ram_util_percent: 43.385294117647064\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04978854099491744\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498289232066989\n",
      "  mean_inference_ms: 0.8529458627432206\n",
      "  mean_raw_obs_processing_ms: 0.08808426183985735\n",
      "time_since_restore: 463.72635102272034\n",
      "time_this_iter_s: 5.335222482681274\n",
      "time_total_s: 463.72635102272034\n",
      "timers:\n",
      "  learn_throughput: 2139.636\n",
      "  learn_time_ms: 1869.477\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1137.706\n",
      "  sample_time_ms: 3515.845\n",
      "  update_time_ms: 1.865\n",
      "timestamp: 1633508595\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 344000\n",
      "training_iteration: 86\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:86 starting ! -----------------\n",
      "agent_timesteps_total: 348000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-23-39\n",
      "done: false\n",
      "episode_len_mean: 693.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.2004479999999982\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 252\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.354257345199585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006591458339244127\n",
      "        model: {}\n",
      "        policy_loss: -0.01088150404393673\n",
      "        total_loss: 0.006192754954099655\n",
      "        vf_explained_var: 0.30218741297721863\n",
      "        vf_loss: 0.015755966305732727\n",
      "  num_agent_steps_sampled: 348000\n",
      "  num_agent_steps_trained: 348000\n",
      "  num_steps_sampled: 348000\n",
      "  num_steps_trained: 348000\n",
      "iterations_since_restore: 87\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.44848484848485\n",
      "  ram_util_percent: 43.35151515151515\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04977661711170739\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496787159686429\n",
      "  mean_inference_ms: 0.8528890261846251\n",
      "  mean_raw_obs_processing_ms: 0.08806016192876526\n",
      "time_since_restore: 469.0582592487335\n",
      "time_this_iter_s: 5.331908226013184\n",
      "time_total_s: 469.0582592487335\n",
      "timers:\n",
      "  learn_throughput: 2144.876\n",
      "  learn_time_ms: 1864.91\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1136.431\n",
      "  sample_time_ms: 3519.79\n",
      "  update_time_ms: 1.865\n",
      "timestamp: 1633508619\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 348000\n",
      "training_iteration: 87\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:87 starting ! -----------------\n",
      "agent_timesteps_total: 352000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-23-57\n",
      "done: false\n",
      "episode_len_mean: 701.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.19087449999999817\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 254\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.290070652961731\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01042863354086876\n",
      "        model: {}\n",
      "        policy_loss: -0.014549213461577892\n",
      "        total_loss: 0.0031359693966805935\n",
      "        vf_explained_var: 0.3407319188117981\n",
      "        vf_loss: 0.015599457547068596\n",
      "  num_agent_steps_sampled: 352000\n",
      "  num_agent_steps_trained: 352000\n",
      "  num_steps_sampled: 352000\n",
      "  num_steps_trained: 352000\n",
      "iterations_since_restore: 88\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.71923076923077\n",
      "  ram_util_percent: 43.35384615384616\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049767364191919745\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495793895155728\n",
      "  mean_inference_ms: 0.8528522640971602\n",
      "  mean_raw_obs_processing_ms: 0.08804466095180466\n",
      "time_since_restore: 474.46426367759705\n",
      "time_this_iter_s: 5.406004428863525\n",
      "time_total_s: 474.46426367759705\n",
      "timers:\n",
      "  learn_throughput: 2137.701\n",
      "  learn_time_ms: 1871.169\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1137.115\n",
      "  sample_time_ms: 3517.675\n",
      "  update_time_ms: 1.765\n",
      "timestamp: 1633508637\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 352000\n",
      "training_iteration: 88\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:88 starting ! -----------------\n",
      "agent_timesteps_total: 356000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-24-21\n",
      "done: false\n",
      "episode_len_mean: 696.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.1307044999999982\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 257\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.274862289428711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00977913849055767\n",
      "        model: {}\n",
      "        policy_loss: -0.01631474308669567\n",
      "        total_loss: 0.0024300457444041967\n",
      "        vf_explained_var: 0.12922029197216034\n",
      "        vf_loss: 0.016788965091109276\n",
      "  num_agent_steps_sampled: 356000\n",
      "  num_agent_steps_trained: 356000\n",
      "  num_steps_sampled: 356000\n",
      "  num_steps_trained: 356000\n",
      "iterations_since_restore: 89\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.393939393939394\n",
      "  ram_util_percent: 43.345454545454544\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04975635022768359\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494429091508856\n",
      "  mean_inference_ms: 0.8528015011228536\n",
      "  mean_raw_obs_processing_ms: 0.0880197162652924\n",
      "time_since_restore: 479.95220947265625\n",
      "time_this_iter_s: 5.487945795059204\n",
      "time_total_s: 479.95220947265625\n",
      "timers:\n",
      "  learn_throughput: 2130.15\n",
      "  learn_time_ms: 1877.802\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1132.646\n",
      "  sample_time_ms: 3531.555\n",
      "  update_time_ms: 1.643\n",
      "timestamp: 1633508661\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 356000\n",
      "training_iteration: 89\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:89 starting ! -----------------\n",
      "agent_timesteps_total: 360000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-24-45\n",
      "done: false\n",
      "episode_len_mean: 695.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.1300994999999982\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 260\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.273123025894165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008950364775955677\n",
      "        model: {}\n",
      "        policy_loss: -0.012594042345881462\n",
      "        total_loss: 0.022159801796078682\n",
      "        vf_explained_var: -0.004357410129159689\n",
      "        vf_loss: 0.03296377509832382\n",
      "  num_agent_steps_sampled: 360000\n",
      "  num_agent_steps_trained: 360000\n",
      "  num_steps_sampled: 360000\n",
      "  num_steps_trained: 360000\n",
      "iterations_since_restore: 90\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.0\n",
      "  ram_util_percent: 43.35588235294117\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04974926458050599\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493343993927426\n",
      "  mean_inference_ms: 0.8527724086609237\n",
      "  mean_raw_obs_processing_ms: 0.08799557343344891\n",
      "time_since_restore: 485.6535367965698\n",
      "time_this_iter_s: 5.701327323913574\n",
      "time_total_s: 485.6535367965698\n",
      "timers:\n",
      "  learn_throughput: 2114.49\n",
      "  learn_time_ms: 1891.709\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1125.009\n",
      "  sample_time_ms: 3555.526\n",
      "  update_time_ms: 1.643\n",
      "timestamp: 1633508685\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 360000\n",
      "training_iteration: 90\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:90 starting ! -----------------\n",
      "agent_timesteps_total: 364000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-24-59\n",
      "done: false\n",
      "episode_len_mean: 683.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.05929849999999824\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 264\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3173338174819946\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01210454199463129\n",
      "        model: {}\n",
      "        policy_loss: -0.021061090752482414\n",
      "        total_loss: 0.0016662618145346642\n",
      "        vf_explained_var: 0.3183707594871521\n",
      "        vf_loss: 0.020306440070271492\n",
      "  num_agent_steps_sampled: 364000\n",
      "  num_agent_steps_trained: 364000\n",
      "  num_steps_sampled: 364000\n",
      "  num_steps_trained: 364000\n",
      "iterations_since_restore: 91\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.859999999999996\n",
      "  ram_util_percent: 43.37499999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04973455449938742\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7492245017485026\n",
      "  mean_inference_ms: 0.8527449817291245\n",
      "  mean_raw_obs_processing_ms: 0.08796246250834037\n",
      "time_since_restore: 490.98958683013916\n",
      "time_this_iter_s: 5.336050033569336\n",
      "time_total_s: 490.98958683013916\n",
      "timers:\n",
      "  learn_throughput: 2136.021\n",
      "  learn_time_ms: 1872.641\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1125.436\n",
      "  sample_time_ms: 3554.177\n",
      "  update_time_ms: 1.643\n",
      "timestamp: 1633508699\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 364000\n",
      "training_iteration: 91\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:91 starting ! -----------------\n",
      "agent_timesteps_total: 368000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-25-23\n",
      "done: false\n",
      "episode_len_mean: 683.65\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.11954599999999832\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 267\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3857849836349487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014794990420341492\n",
      "        model: {}\n",
      "        policy_loss: -0.020624184980988503\n",
      "        total_loss: -0.003458093386143446\n",
      "        vf_explained_var: -0.05495499074459076\n",
      "        vf_loss: 0.01420709490776062\n",
      "  num_agent_steps_sampled: 368000\n",
      "  num_agent_steps_trained: 368000\n",
      "  num_steps_sampled: 368000\n",
      "  num_steps_trained: 368000\n",
      "iterations_since_restore: 92\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.082352941176474\n",
      "  ram_util_percent: 43.35882352941176\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04972114405722375\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491389436831295\n",
      "  mean_inference_ms: 0.8527300011417396\n",
      "  mean_raw_obs_processing_ms: 0.08794025817556267\n",
      "time_since_restore: 496.49154806137085\n",
      "time_this_iter_s: 5.5019612312316895\n",
      "time_total_s: 496.49154806137085\n",
      "timers:\n",
      "  learn_throughput: 2130.981\n",
      "  learn_time_ms: 1877.069\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1120.806\n",
      "  sample_time_ms: 3568.861\n",
      "  update_time_ms: 1.743\n",
      "timestamp: 1633508723\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 368000\n",
      "training_iteration: 92\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:92 starting ! -----------------\n",
      "agent_timesteps_total: 372000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-25-36\n",
      "done: false\n",
      "episode_len_mean: 686.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.060051999999998246\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 270\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3008482456207275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009043646976351738\n",
      "        model: {}\n",
      "        policy_loss: -0.01775563880801201\n",
      "        total_loss: 0.006041312590241432\n",
      "        vf_explained_var: 0.1347033977508545\n",
      "        vf_loss: 0.021988218650221825\n",
      "  num_agent_steps_sampled: 372000\n",
      "  num_agent_steps_trained: 372000\n",
      "  num_steps_sampled: 372000\n",
      "  num_steps_trained: 372000\n",
      "iterations_since_restore: 93\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.016666666666666\n",
      "  ram_util_percent: 43.388888888888886\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049711462301744794\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490841336658391\n",
      "  mean_inference_ms: 0.8527211188375827\n",
      "  mean_raw_obs_processing_ms: 0.08791663403831702\n",
      "time_since_restore: 502.05682826042175\n",
      "time_this_iter_s: 5.565280199050903\n",
      "time_total_s: 502.05682826042175\n",
      "timers:\n",
      "  learn_throughput: 2115.731\n",
      "  learn_time_ms: 1890.599\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1122.088\n",
      "  sample_time_ms: 3564.782\n",
      "  update_time_ms: 1.693\n",
      "timestamp: 1633508736\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 372000\n",
      "training_iteration: 93\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:93 starting ! -----------------\n",
      "agent_timesteps_total: 376000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-26-00\n",
      "done: false\n",
      "episode_len_mean: 685.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.05117049999999813\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 273\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.270234227180481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010155008174479008\n",
      "        model: {}\n",
      "        policy_loss: -0.017291735857725143\n",
      "        total_loss: 0.008365307003259659\n",
      "        vf_explained_var: 0.17752188444137573\n",
      "        vf_loss: 0.023626040667295456\n",
      "  num_agent_steps_sampled: 376000\n",
      "  num_agent_steps_trained: 376000\n",
      "  num_steps_sampled: 376000\n",
      "  num_steps_trained: 376000\n",
      "iterations_since_restore: 94\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.827272727272728\n",
      "  ram_util_percent: 43.412121212121214\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049707456371306656\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490587112783049\n",
      "  mean_inference_ms: 0.8527082832631138\n",
      "  mean_raw_obs_processing_ms: 0.08789305934488566\n",
      "time_since_restore: 507.41894340515137\n",
      "time_this_iter_s: 5.362115144729614\n",
      "time_total_s: 507.41894340515137\n",
      "timers:\n",
      "  learn_throughput: 2113.44\n",
      "  learn_time_ms: 1892.649\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1125.723\n",
      "  sample_time_ms: 3553.271\n",
      "  update_time_ms: 1.693\n",
      "timestamp: 1633508760\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 376000\n",
      "training_iteration: 94\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:94 starting ! -----------------\n",
      "agent_timesteps_total: 380000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-26-14\n",
      "done: false\n",
      "episode_len_mean: 680.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.07096749999999813\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 275\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3232709169387817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010361414402723312\n",
      "        model: {}\n",
      "        policy_loss: -0.015260040760040283\n",
      "        total_loss: -0.005215927492827177\n",
      "        vf_explained_var: -0.05777906998991966\n",
      "        vf_loss: 0.007971830666065216\n",
      "  num_agent_steps_sampled: 380000\n",
      "  num_agent_steps_trained: 380000\n",
      "  num_steps_sampled: 380000\n",
      "  num_steps_trained: 380000\n",
      "iterations_since_restore: 95\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.744999999999997\n",
      "  ram_util_percent: 43.404999999999994\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049704324658385576\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490611516596869\n",
      "  mean_inference_ms: 0.8526956903174558\n",
      "  mean_raw_obs_processing_ms: 0.08787854139793984\n",
      "time_since_restore: 512.7283034324646\n",
      "time_this_iter_s: 5.309360027313232\n",
      "time_total_s: 512.7283034324646\n",
      "timers:\n",
      "  learn_throughput: 2114.473\n",
      "  learn_time_ms: 1891.725\n",
      "  load_throughput: 41343558.403\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1131.726\n",
      "  sample_time_ms: 3534.424\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633508774\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 380000\n",
      "training_iteration: 95\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:95 starting ! -----------------\n",
      "agent_timesteps_total: 384000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-26-31\n",
      "done: false\n",
      "episode_len_mean: 692.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.11157649999999801\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 278\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3152180910110474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00975228101015091\n",
      "        model: {}\n",
      "        policy_loss: -0.02058863453567028\n",
      "        total_loss: -0.010787300765514374\n",
      "        vf_explained_var: 0.4198153614997864\n",
      "        vf_loss: 0.007850880734622478\n",
      "  num_agent_steps_sampled: 384000\n",
      "  num_agent_steps_trained: 384000\n",
      "  num_steps_sampled: 384000\n",
      "  num_steps_trained: 384000\n",
      "iterations_since_restore: 96\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.570833333333336\n",
      "  ram_util_percent: 43.383333333333326\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04969980158435635\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749064894425223\n",
      "  mean_inference_ms: 0.852673143229812\n",
      "  mean_raw_obs_processing_ms: 0.08785837148351872\n",
      "time_since_restore: 518.0556168556213\n",
      "time_this_iter_s: 5.327313423156738\n",
      "time_total_s: 518.0556168556213\n",
      "timers:\n",
      "  learn_throughput: 2111.628\n",
      "  learn_time_ms: 1894.273\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.743\n",
      "  sample_time_ms: 3531.25\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633508791\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 384000\n",
      "training_iteration: 96\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:96 starting ! -----------------\n",
      "agent_timesteps_total: 388000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-26-54\n",
      "done: false\n",
      "episode_len_mean: 694.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.12287949999999791\n",
      "episode_reward_min: -2.204749999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 280\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3545756340026855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011741289868950844\n",
      "        model: {}\n",
      "        policy_loss: -0.01526687666773796\n",
      "        total_loss: -0.004290646407753229\n",
      "        vf_explained_var: 0.19203197956085205\n",
      "        vf_loss: 0.008627965115010738\n",
      "  num_agent_steps_sampled: 388000\n",
      "  num_agent_steps_trained: 388000\n",
      "  num_steps_sampled: 388000\n",
      "  num_steps_trained: 388000\n",
      "iterations_since_restore: 97\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.654545454545456\n",
      "  ram_util_percent: 43.38787878787878\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049695866296546154\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490788261042366\n",
      "  mean_inference_ms: 0.8526649745007978\n",
      "  mean_raw_obs_processing_ms: 0.08784480048889172\n",
      "time_since_restore: 523.7134475708008\n",
      "time_this_iter_s: 5.657830715179443\n",
      "time_total_s: 523.7134475708008\n",
      "timers:\n",
      "  learn_throughput: 2092.024\n",
      "  learn_time_ms: 1912.024\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.02\n",
      "  sample_time_ms: 3546.037\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633508814\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 388000\n",
      "training_iteration: 97\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:97 starting ! -----------------\n",
      "agent_timesteps_total: 392000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-27-18\n",
      "done: false\n",
      "episode_len_mean: 700.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.12419499999999799\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 282\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3719843626022339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009217211045324802\n",
      "        model: {}\n",
      "        policy_loss: -0.014594634994864464\n",
      "        total_loss: -0.006397218443453312\n",
      "        vf_explained_var: 0.09140796959400177\n",
      "        vf_loss: 0.006353973876684904\n",
      "  num_agent_steps_sampled: 392000\n",
      "  num_agent_steps_trained: 392000\n",
      "  num_steps_sampled: 392000\n",
      "  num_steps_trained: 392000\n",
      "iterations_since_restore: 98\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.323529411764707\n",
      "  ram_util_percent: 43.38235294117648\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049691679144542374\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.74909547024783\n",
      "  mean_inference_ms: 0.8526551314907445\n",
      "  mean_raw_obs_processing_ms: 0.08783353424908075\n",
      "time_since_restore: 529.0823750495911\n",
      "time_this_iter_s: 5.368927478790283\n",
      "time_total_s: 529.0823750495911\n",
      "timers:\n",
      "  learn_throughput: 2098.311\n",
      "  learn_time_ms: 1906.295\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.381\n",
      "  sample_time_ms: 3548.047\n",
      "  update_time_ms: 1.894\n",
      "timestamp: 1633508838\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 392000\n",
      "training_iteration: 98\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:98 starting ! -----------------\n",
      "agent_timesteps_total: 396000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-27-31\n",
      "done: false\n",
      "episode_len_mean: 709.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.184958999999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 284\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4101896286010742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012450847774744034\n",
      "        model: {}\n",
      "        policy_loss: -0.01657114364206791\n",
      "        total_loss: 0.008740022778511047\n",
      "        vf_explained_var: -0.05125884339213371\n",
      "        vf_loss: 0.02282099984586239\n",
      "  num_agent_steps_sampled: 396000\n",
      "  num_agent_steps_trained: 396000\n",
      "  num_steps_sampled: 396000\n",
      "  num_steps_trained: 396000\n",
      "iterations_since_restore: 99\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.54705882352941\n",
      "  ram_util_percent: 43.3470588235294\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04968553533819703\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491149405631887\n",
      "  mean_inference_ms: 0.8526492507692188\n",
      "  mean_raw_obs_processing_ms: 0.08782330084806671\n",
      "time_since_restore: 534.4170050621033\n",
      "time_this_iter_s: 5.334630012512207\n",
      "time_total_s: 534.4170050621033\n",
      "timers:\n",
      "  learn_throughput: 2103.474\n",
      "  learn_time_ms: 1901.616\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.744\n",
      "  sample_time_ms: 3537.495\n",
      "  update_time_ms: 1.994\n",
      "timestamp: 1633508851\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 396000\n",
      "training_iteration: 99\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:99 starting ! -----------------\n",
      "agent_timesteps_total: 400000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-27-53\n",
      "done: false\n",
      "episode_len_mean: 701.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.11412549999999795\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 287\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3922187089920044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009391297586262226\n",
      "        model: {}\n",
      "        policy_loss: -0.01883290521800518\n",
      "        total_loss: -0.001213645446114242\n",
      "        vf_explained_var: 0.28872331976890564\n",
      "        vf_loss: 0.015741005539894104\n",
      "  num_agent_steps_sampled: 400000\n",
      "  num_agent_steps_trained: 400000\n",
      "  num_steps_sampled: 400000\n",
      "  num_steps_trained: 400000\n",
      "iterations_since_restore: 100\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.365624999999994\n",
      "  ram_util_percent: 43.3875\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049675402078992036\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491333671782265\n",
      "  mean_inference_ms: 0.8526275372729755\n",
      "  mean_raw_obs_processing_ms: 0.08781380140730864\n",
      "time_since_restore: 540.030665397644\n",
      "time_this_iter_s: 5.6136603355407715\n",
      "time_total_s: 540.030665397644\n",
      "timers:\n",
      "  learn_throughput: 2092.104\n",
      "  learn_time_ms: 1911.951\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.988\n",
      "  sample_time_ms: 3518.067\n",
      "  update_time_ms: 2.076\n",
      "timestamp: 1633508873\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400000\n",
      "training_iteration: 100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:100 starting ! -----------------\n",
      "agent_timesteps_total: 404000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-28-10\n",
      "done: false\n",
      "episode_len_mean: 699.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9354000000000005\n",
      "episode_reward_mean: -0.054067499999997874\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 290\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2389700412750244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010936051607131958\n",
      "        model: {}\n",
      "        policy_loss: -0.020082183182239532\n",
      "        total_loss: 0.01356405671685934\n",
      "        vf_explained_var: -0.027440158650279045\n",
      "        vf_loss: 0.0314590260386467\n",
      "  num_agent_steps_sampled: 404000\n",
      "  num_agent_steps_trained: 404000\n",
      "  num_steps_sampled: 404000\n",
      "  num_steps_trained: 404000\n",
      "iterations_since_restore: 101\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.6\n",
      "  ram_util_percent: 43.652173913043484\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04966686215055804\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491626218511975\n",
      "  mean_inference_ms: 0.852593628363971\n",
      "  mean_raw_obs_processing_ms: 0.08780429001706093\n",
      "time_since_restore: 545.4033391475677\n",
      "time_this_iter_s: 5.372673749923706\n",
      "time_total_s: 545.4033391475677\n",
      "timers:\n",
      "  learn_throughput: 2086.468\n",
      "  learn_time_ms: 1917.115\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.491\n",
      "  sample_time_ms: 3516.512\n",
      "  update_time_ms: 2.076\n",
      "timestamp: 1633508890\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 404000\n",
      "training_iteration: 101\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:101 starting ! -----------------\n",
      "agent_timesteps_total: 408000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-28-24\n",
      "done: false\n",
      "episode_len_mean: 707.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.16518699999999792\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 293\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1413912773132324\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008727246895432472\n",
      "        model: {}\n",
      "        policy_loss: -0.01480728480964899\n",
      "        total_loss: 0.0023285478819161654\n",
      "        vf_explained_var: -0.12136156111955643\n",
      "        vf_loss: 0.015390378423035145\n",
      "  num_agent_steps_sampled: 408000\n",
      "  num_agent_steps_trained: 408000\n",
      "  num_steps_sampled: 408000\n",
      "  num_steps_trained: 408000\n",
      "iterations_since_restore: 102\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.365000000000002\n",
      "  ram_util_percent: 43.53\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049660750991441155\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491912602877252\n",
      "  mean_inference_ms: 0.8525610810338939\n",
      "  mean_raw_obs_processing_ms: 0.08779519049105743\n",
      "time_since_restore: 550.7103335857391\n",
      "time_this_iter_s: 5.306994438171387\n",
      "time_total_s: 550.7103335857391\n",
      "timers:\n",
      "  learn_throughput: 2091.513\n",
      "  learn_time_ms: 1912.491\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.363\n",
      "  sample_time_ms: 3501.514\n",
      "  update_time_ms: 2.076\n",
      "timestamp: 1633508904\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 408000\n",
      "training_iteration: 102\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:102 starting ! -----------------\n",
      "agent_timesteps_total: 412000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-28-48\n",
      "done: false\n",
      "episode_len_mean: 707.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.165007999999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 295\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3453021049499512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009030391462147236\n",
      "        model: {}\n",
      "        policy_loss: -0.01378930825740099\n",
      "        total_loss: 0.0008059145184233785\n",
      "        vf_explained_var: -0.08624467998743057\n",
      "        vf_loss: 0.01278914138674736\n",
      "  num_agent_steps_sampled: 412000\n",
      "  num_agent_steps_trained: 412000\n",
      "  num_steps_sampled: 412000\n",
      "  num_steps_trained: 412000\n",
      "iterations_since_restore: 103\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.28181818181818\n",
      "  ram_util_percent: 43.49696969696969\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04965916590397862\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7492125968551164\n",
      "  mean_inference_ms: 0.8525494589687495\n",
      "  mean_raw_obs_processing_ms: 0.08778895290544955\n",
      "time_since_restore: 556.2442629337311\n",
      "time_this_iter_s: 5.533929347991943\n",
      "time_total_s: 556.2442629337311\n",
      "timers:\n",
      "  learn_throughput: 2091.668\n",
      "  learn_time_ms: 1912.349\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.329\n",
      "  sample_time_ms: 3498.557\n",
      "  update_time_ms: 2.078\n",
      "timestamp: 1633508928\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 412000\n",
      "training_iteration: 103\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:103 starting ! -----------------\n",
      "agent_timesteps_total: 416000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-29-01\n",
      "done: false\n",
      "episode_len_mean: 702.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.22368849999999807\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 298\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2750945091247559\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007851185277104378\n",
      "        model: {}\n",
      "        policy_loss: -0.006652092095464468\n",
      "        total_loss: 0.01230445597320795\n",
      "        vf_explained_var: -0.06726937741041183\n",
      "        vf_loss: 0.017386315390467644\n",
      "  num_agent_steps_sampled: 416000\n",
      "  num_agent_steps_trained: 416000\n",
      "  num_steps_sampled: 416000\n",
      "  num_steps_trained: 416000\n",
      "iterations_since_restore: 104\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.5\n",
      "  ram_util_percent: 43.52105263157895\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04965629107162202\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7492514291497016\n",
      "  mean_inference_ms: 0.8525232622051117\n",
      "  mean_raw_obs_processing_ms: 0.08778118645771174\n",
      "time_since_restore: 561.7417528629303\n",
      "time_this_iter_s: 5.497489929199219\n",
      "time_total_s: 561.7417528629303\n",
      "timers:\n",
      "  learn_throughput: 2073.55\n",
      "  learn_time_ms: 1929.059\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.39\n",
      "  sample_time_ms: 3495.313\n",
      "  update_time_ms: 2.037\n",
      "timestamp: 1633508941\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 416000\n",
      "training_iteration: 104\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:104 starting ! -----------------\n",
      "agent_timesteps_total: 420000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-29-24\n",
      "done: false\n",
      "episode_len_mean: 702.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.16337049999999803\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 302\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3656585216522217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009483975358307362\n",
      "        model: {}\n",
      "        policy_loss: -0.012423329055309296\n",
      "        total_loss: 0.0077887410297989845\n",
      "        vf_explained_var: 0.06591235846281052\n",
      "        vf_loss: 0.018315276131033897\n",
      "  num_agent_steps_sampled: 420000\n",
      "  num_agent_steps_trained: 420000\n",
      "  num_steps_sampled: 420000\n",
      "  num_steps_trained: 420000\n",
      "iterations_since_restore: 105\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.906060606060603\n",
      "  ram_util_percent: 43.472727272727276\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04965689244133598\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493169169021937\n",
      "  mean_inference_ms: 0.8524911203370791\n",
      "  mean_raw_obs_processing_ms: 0.08776836094146745\n",
      "time_since_restore: 567.1294255256653\n",
      "time_this_iter_s: 5.387672662734985\n",
      "time_total_s: 567.1294255256653\n",
      "timers:\n",
      "  learn_throughput: 2072.907\n",
      "  learn_time_ms: 1929.657\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.037\n",
      "  sample_time_ms: 3502.514\n",
      "  update_time_ms: 2.036\n",
      "timestamp: 1633508964\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 420000\n",
      "training_iteration: 105\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:105 starting ! -----------------\n",
      "agent_timesteps_total: 424000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-29-40\n",
      "done: false\n",
      "episode_len_mean: 712.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.21408499999999794\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 304\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2207056283950806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014793254435062408\n",
      "        model: {}\n",
      "        policy_loss: -0.012854558415710926\n",
      "        total_loss: -0.0002892371267080307\n",
      "        vf_explained_var: 0.2885720729827881\n",
      "        vf_loss: 0.009606671519577503\n",
      "  num_agent_steps_sampled: 424000\n",
      "  num_agent_steps_trained: 424000\n",
      "  num_steps_sampled: 424000\n",
      "  num_steps_trained: 424000\n",
      "iterations_since_restore: 106\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.30869565217391\n",
      "  ram_util_percent: 43.4695652173913\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04966009696932572\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493570718876181\n",
      "  mean_inference_ms: 0.8524731602082264\n",
      "  mean_raw_obs_processing_ms: 0.08776161820358493\n",
      "time_since_restore: 572.542804479599\n",
      "time_this_iter_s: 5.413378953933716\n",
      "time_total_s: 572.542804479599\n",
      "timers:\n",
      "  learn_throughput: 2075.329\n",
      "  learn_time_ms: 1927.405\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.547\n",
      "  sample_time_ms: 3513.251\n",
      "  update_time_ms: 2.036\n",
      "timestamp: 1633508980\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 424000\n",
      "training_iteration: 106\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:106 starting ! -----------------\n",
      "agent_timesteps_total: 428000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-29-54\n",
      "done: false\n",
      "episode_len_mean: 713.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.26505949999999795\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 306\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.350797176361084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008145532570779324\n",
      "        model: {}\n",
      "        policy_loss: -0.006052764132618904\n",
      "        total_loss: 0.004774636588990688\n",
      "        vf_explained_var: -0.04042466729879379\n",
      "        vf_loss: 0.00919829960912466\n",
      "  num_agent_steps_sampled: 428000\n",
      "  num_agent_steps_trained: 428000\n",
      "  num_steps_sampled: 428000\n",
      "  num_steps_trained: 428000\n",
      "iterations_since_restore: 107\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.966666666666665\n",
      "  ram_util_percent: 43.43333333333333\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049663316107211204\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493949265717015\n",
      "  mean_inference_ms: 0.8524539128380567\n",
      "  mean_raw_obs_processing_ms: 0.08775407808011657\n",
      "time_since_restore: 577.8345313072205\n",
      "time_this_iter_s: 5.29172682762146\n",
      "time_total_s: 577.8345313072205\n",
      "timers:\n",
      "  learn_throughput: 2096.17\n",
      "  learn_time_ms: 1908.242\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.191\n",
      "  sample_time_ms: 3495.919\n",
      "  update_time_ms: 2.036\n",
      "timestamp: 1633508994\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 428000\n",
      "training_iteration: 107\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:107 starting ! -----------------\n",
      "agent_timesteps_total: 432000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-30-13\n",
      "done: false\n",
      "episode_len_mean: 719.89\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.38651299999999794\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 309\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2039707899093628\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01064000092446804\n",
      "        model: {}\n",
      "        policy_loss: -0.014307616278529167\n",
      "        total_loss: 1.8485643522581086e-05\n",
      "        vf_explained_var: 0.269415020942688\n",
      "        vf_loss: 0.012198103591799736\n",
      "  num_agent_steps_sampled: 432000\n",
      "  num_agent_steps_trained: 432000\n",
      "  num_steps_sampled: 432000\n",
      "  num_steps_trained: 432000\n",
      "iterations_since_restore: 108\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.374074074074073\n",
      "  ram_util_percent: 43.414814814814825\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04966769345185428\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494536496523848\n",
      "  mean_inference_ms: 0.8524228902960458\n",
      "  mean_raw_obs_processing_ms: 0.08774594965345343\n",
      "time_since_restore: 583.1746578216553\n",
      "time_this_iter_s: 5.3401265144348145\n",
      "time_total_s: 583.1746578216553\n",
      "timers:\n",
      "  learn_throughput: 2096.134\n",
      "  learn_time_ms: 1908.275\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.117\n",
      "  sample_time_ms: 3493.092\n",
      "  update_time_ms: 2.037\n",
      "timestamp: 1633509013\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 432000\n",
      "training_iteration: 108\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:108 starting ! -----------------\n",
      "agent_timesteps_total: 436000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-30-25\n",
      "done: false\n",
      "episode_len_mean: 721.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9131500000000004\n",
      "episode_reward_mean: -0.374694999999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 312\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0492186546325684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0108797587454319\n",
      "        model: {}\n",
      "        policy_loss: -0.019196299836039543\n",
      "        total_loss: 0.005595034454017878\n",
      "        vf_explained_var: 0.001939627924002707\n",
      "        vf_loss: 0.022615378722548485\n",
      "  num_agent_steps_sampled: 436000\n",
      "  num_agent_steps_trained: 436000\n",
      "  num_steps_sampled: 436000\n",
      "  num_steps_trained: 436000\n",
      "iterations_since_restore: 109\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.129411764705882\n",
      "  ram_util_percent: 43.40588235294118\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049670365012473454\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749525750518102\n",
      "  mean_inference_ms: 0.852397662368026\n",
      "  mean_raw_obs_processing_ms: 0.08773761140442948\n",
      "time_since_restore: 588.5901486873627\n",
      "time_this_iter_s: 5.4154908657073975\n",
      "time_total_s: 588.5901486873627\n",
      "timers:\n",
      "  learn_throughput: 2099.057\n",
      "  learn_time_ms: 1905.617\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.642\n",
      "  sample_time_ms: 3503.726\n",
      "  update_time_ms: 2.037\n",
      "timestamp: 1633509025\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 436000\n",
      "training_iteration: 109\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:109 starting ! -----------------\n",
      "agent_timesteps_total: 440000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-30-39\n",
      "done: false\n",
      "episode_len_mean: 718.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.313012499999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 315\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.367136836051941\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011447482742369175\n",
      "        model: {}\n",
      "        policy_loss: -0.01723276823759079\n",
      "        total_loss: 0.020477307960391045\n",
      "        vf_explained_var: 0.15792492032051086\n",
      "        vf_loss: 0.035420577973127365\n",
      "  num_agent_steps_sampled: 440000\n",
      "  num_agent_steps_trained: 440000\n",
      "  num_steps_sampled: 440000\n",
      "  num_steps_trained: 440000\n",
      "iterations_since_restore: 110\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.6\n",
      "  ram_util_percent: 43.39999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049669241778404635\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495698016955559\n",
      "  mean_inference_ms: 0.8523772130535252\n",
      "  mean_raw_obs_processing_ms: 0.0877340791507402\n",
      "time_since_restore: 593.8742656707764\n",
      "time_this_iter_s: 5.284116983413696\n",
      "time_total_s: 593.8742656707764\n",
      "timers:\n",
      "  learn_throughput: 2126.926\n",
      "  learn_time_ms: 1880.649\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.13\n",
      "  sample_time_ms: 3496.106\n",
      "  update_time_ms: 1.855\n",
      "timestamp: 1633509039\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 440000\n",
      "training_iteration: 110\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:110 starting ! -----------------\n",
      "agent_timesteps_total: 444000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-30-50\n",
      "done: false\n",
      "episode_len_mean: 718.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.3125909999999981\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 317\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2159957885742188\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011909636668860912\n",
      "        model: {}\n",
      "        policy_loss: -0.013837353326380253\n",
      "        total_loss: 0.030100734904408455\n",
      "        vf_explained_var: -0.02303868718445301\n",
      "        vf_loss: 0.04155615344643593\n",
      "  num_agent_steps_sampled: 444000\n",
      "  num_agent_steps_trained: 444000\n",
      "  num_steps_sampled: 444000\n",
      "  num_steps_trained: 444000\n",
      "iterations_since_restore: 111\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.606250000000003\n",
      "  ram_util_percent: 43.4375\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049670996407663044\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496033102892545\n",
      "  mean_inference_ms: 0.8523623265067287\n",
      "  mean_raw_obs_processing_ms: 0.0877362484453265\n",
      "time_since_restore: 599.2844767570496\n",
      "time_this_iter_s: 5.410211086273193\n",
      "time_total_s: 599.2844767570496\n",
      "timers:\n",
      "  learn_throughput: 2133.334\n",
      "  learn_time_ms: 1874.999\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.032\n",
      "  sample_time_ms: 3505.598\n",
      "  update_time_ms: 1.855\n",
      "timestamp: 1633509050\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 444000\n",
      "training_iteration: 111\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:111 starting ! -----------------\n",
      "agent_timesteps_total: 448000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-31-14\n",
      "done: false\n",
      "episode_len_mean: 721.39\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.42241599999999807\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 320\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1358836889266968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009380636736750603\n",
      "        model: {}\n",
      "        policy_loss: -0.010400302708148956\n",
      "        total_loss: 0.006398080848157406\n",
      "        vf_explained_var: -0.03892505541443825\n",
      "        vf_loss: 0.014922256581485271\n",
      "  num_agent_steps_sampled: 448000\n",
      "  num_agent_steps_trained: 448000\n",
      "  num_steps_sampled: 448000\n",
      "  num_steps_trained: 448000\n",
      "iterations_since_restore: 112\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.37878787878788\n",
      "  ram_util_percent: 43.41818181818182\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04967392056839125\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496505722621221\n",
      "  mean_inference_ms: 0.8523340579412887\n",
      "  mean_raw_obs_processing_ms: 0.08773740084487695\n",
      "time_since_restore: 604.5350205898285\n",
      "time_this_iter_s: 5.250543832778931\n",
      "time_total_s: 604.5350205898285\n",
      "timers:\n",
      "  learn_throughput: 2132.867\n",
      "  learn_time_ms: 1875.41\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.959\n",
      "  sample_time_ms: 3499.688\n",
      "  update_time_ms: 1.855\n",
      "timestamp: 1633509074\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 448000\n",
      "training_iteration: 112\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:112 starting ! -----------------\n",
      "agent_timesteps_total: 452000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-31-28\n",
      "done: false\n",
      "episode_len_mean: 721.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.41216699999999806\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 322\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2087476253509521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011760101653635502\n",
      "        model: {}\n",
      "        policy_loss: -0.021658752113580704\n",
      "        total_loss: -0.0009411409264430404\n",
      "        vf_explained_var: 0.3020158112049103\n",
      "        vf_loss: 0.01836559548974037\n",
      "  num_agent_steps_sampled: 452000\n",
      "  num_agent_steps_trained: 452000\n",
      "  num_steps_sampled: 452000\n",
      "  num_steps_trained: 452000\n",
      "iterations_since_restore: 113\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.47\n",
      "  ram_util_percent: 43.415\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04967436957102966\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496779285334104\n",
      "  mean_inference_ms: 0.8523254696202693\n",
      "  mean_raw_obs_processing_ms: 0.08774029620547302\n",
      "time_since_restore: 609.8556501865387\n",
      "time_this_iter_s: 5.320629596710205\n",
      "time_total_s: 609.8556501865387\n",
      "timers:\n",
      "  learn_throughput: 2147.142\n",
      "  learn_time_ms: 1862.942\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.87\n",
      "  sample_time_ms: 3490.796\n",
      "  update_time_ms: 1.855\n",
      "timestamp: 1633509088\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 452000\n",
      "training_iteration: 113\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:113 starting ! -----------------\n",
      "agent_timesteps_total: 456000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-31-48\n",
      "done: false\n",
      "episode_len_mean: 731.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.462321999999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 325\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0944873094558716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009193329140543938\n",
      "        model: {}\n",
      "        policy_loss: -0.011503017507493496\n",
      "        total_loss: -0.0007478055194951594\n",
      "        vf_explained_var: 0.20340986549854279\n",
      "        vf_loss: 0.00891654472798109\n",
      "  num_agent_steps_sampled: 456000\n",
      "  num_agent_steps_trained: 456000\n",
      "  num_steps_sampled: 456000\n",
      "  num_steps_trained: 456000\n",
      "iterations_since_restore: 114\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.949999999999996\n",
      "  ram_util_percent: 43.4357142857143\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04967337094834087\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497213636737726\n",
      "  mean_inference_ms: 0.8523073319955233\n",
      "  mean_raw_obs_processing_ms: 0.0877474000862124\n",
      "time_since_restore: 615.3632500171661\n",
      "time_this_iter_s: 5.507599830627441\n",
      "time_total_s: 615.3632500171661\n",
      "timers:\n",
      "  learn_throughput: 2150.847\n",
      "  learn_time_ms: 1859.733\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1144.479\n",
      "  sample_time_ms: 3495.041\n",
      "  update_time_ms: 1.896\n",
      "timestamp: 1633509108\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 456000\n",
      "training_iteration: 114\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:114 starting ! -----------------\n",
      "agent_timesteps_total: 460000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-32-01\n",
      "done: false\n",
      "episode_len_mean: 723.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.39255849999999787\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 329\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2063955068588257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007513599004596472\n",
      "        model: {}\n",
      "        policy_loss: -0.013892308808863163\n",
      "        total_loss: 0.008856559172272682\n",
      "        vf_explained_var: 0.03505982458591461\n",
      "        vf_loss: 0.021246150135993958\n",
      "  num_agent_steps_sampled: 460000\n",
      "  num_agent_steps_trained: 460000\n",
      "  num_steps_sampled: 460000\n",
      "  num_steps_trained: 460000\n",
      "iterations_since_restore: 115\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.589473684210528\n",
      "  ram_util_percent: 43.44210526315789\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04967369631124789\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497794015215434\n",
      "  mean_inference_ms: 0.8522903487633954\n",
      "  mean_raw_obs_processing_ms: 0.08775841999058706\n",
      "time_since_restore: 620.7187640666962\n",
      "time_this_iter_s: 5.355514049530029\n",
      "time_total_s: 620.7187640666962\n",
      "timers:\n",
      "  learn_throughput: 2150.486\n",
      "  learn_time_ms: 1860.045\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1145.606\n",
      "  sample_time_ms: 3491.602\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633509121\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 460000\n",
      "training_iteration: 115\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:115 starting ! -----------------\n",
      "agent_timesteps_total: 464000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-32-21\n",
      "done: false\n",
      "episode_len_mean: 705.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.2817309999999979\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 333\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.214281678199768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008397897705435753\n",
      "        model: {}\n",
      "        policy_loss: -0.013195278123021126\n",
      "        total_loss: 0.019765274599194527\n",
      "        vf_explained_var: 0.10381222516298294\n",
      "        vf_loss: 0.03128097206354141\n",
      "  num_agent_steps_sampled: 464000\n",
      "  num_agent_steps_trained: 464000\n",
      "  num_steps_sampled: 464000\n",
      "  num_steps_trained: 464000\n",
      "iterations_since_restore: 116\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.885185185185183\n",
      "  ram_util_percent: 43.429629629629645\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049671674548143745\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749847318488018\n",
      "  mean_inference_ms: 0.8522742286196031\n",
      "  mean_raw_obs_processing_ms: 0.08777409136343223\n",
      "time_since_restore: 626.0743319988251\n",
      "time_this_iter_s: 5.355567932128906\n",
      "time_total_s: 626.0743319988251\n",
      "timers:\n",
      "  learn_throughput: 2149.198\n",
      "  learn_time_ms: 1861.159\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1147.835\n",
      "  sample_time_ms: 3484.82\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633509141\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 464000\n",
      "training_iteration: 116\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:116 starting ! -----------------\n",
      "agent_timesteps_total: 468000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-32-40\n",
      "done: false\n",
      "episode_len_mean: 705.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.281689999999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 336\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.078123688697815\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013571720570325851\n",
      "        model: {}\n",
      "        policy_loss: -0.015002231113612652\n",
      "        total_loss: 0.01327795721590519\n",
      "        vf_explained_var: 0.03442779555916786\n",
      "        vf_loss: 0.025565844029188156\n",
      "  num_agent_steps_sampled: 468000\n",
      "  num_agent_steps_trained: 468000\n",
      "  num_steps_sampled: 468000\n",
      "  num_steps_trained: 468000\n",
      "iterations_since_restore: 117\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.514814814814812\n",
      "  ram_util_percent: 43.42222222222223\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049667624803341345\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498961163299781\n",
      "  mean_inference_ms: 0.852264336818887\n",
      "  mean_raw_obs_processing_ms: 0.0877889073993368\n",
      "time_since_restore: 631.3627133369446\n",
      "time_this_iter_s: 5.288381338119507\n",
      "time_total_s: 631.3627133369446\n",
      "timers:\n",
      "  learn_throughput: 2149.863\n",
      "  learn_time_ms: 1860.583\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1147.752\n",
      "  sample_time_ms: 3485.075\n",
      "  update_time_ms: 1.705\n",
      "timestamp: 1633509160\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 468000\n",
      "training_iteration: 117\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:117 starting ! -----------------\n",
      "agent_timesteps_total: 472000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-32-52\n",
      "done: false\n",
      "episode_len_mean: 705.39\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.400980499999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 340\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3280129432678223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009456934407353401\n",
      "        model: {}\n",
      "        policy_loss: -0.009888865053653717\n",
      "        total_loss: 0.008334197103977203\n",
      "        vf_explained_var: 0.05781963840126991\n",
      "        vf_loss: 0.01633167639374733\n",
      "  num_agent_steps_sampled: 472000\n",
      "  num_agent_steps_trained: 472000\n",
      "  num_steps_sampled: 472000\n",
      "  num_steps_trained: 472000\n",
      "iterations_since_restore: 118\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.84705882352941\n",
      "  ram_util_percent: 43.48823529411765\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049665433791068254\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499496246661604\n",
      "  mean_inference_ms: 0.8522584093338201\n",
      "  mean_raw_obs_processing_ms: 0.08780901934259981\n",
      "time_since_restore: 636.8806188106537\n",
      "time_this_iter_s: 5.5179054737091064\n",
      "time_total_s: 636.8806188106537\n",
      "timers:\n",
      "  learn_throughput: 2147.984\n",
      "  learn_time_ms: 1862.211\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1142.475\n",
      "  sample_time_ms: 3501.172\n",
      "  update_time_ms: 1.705\n",
      "timestamp: 1633509172\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 472000\n",
      "training_iteration: 118\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:118 starting ! -----------------\n",
      "agent_timesteps_total: 476000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-33-04\n",
      "done: false\n",
      "episode_len_mean: 698.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.34889749999999803\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 343\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.289918303489685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01155679952353239\n",
      "        model: {}\n",
      "        policy_loss: -0.016649441793560982\n",
      "        total_loss: -0.005438859108835459\n",
      "        vf_explained_var: 0.3638487458229065\n",
      "        vf_loss: 0.008899222128093243\n",
      "  num_agent_steps_sampled: 476000\n",
      "  num_agent_steps_trained: 476000\n",
      "  num_steps_sampled: 476000\n",
      "  num_steps_trained: 476000\n",
      "iterations_since_restore: 119\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.470588235294116\n",
      "  ram_util_percent: 43.45882352941177\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049665576888520836\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749965550815439\n",
      "  mean_inference_ms: 0.8522511936611177\n",
      "  mean_raw_obs_processing_ms: 0.08782656372961417\n",
      "time_since_restore: 642.2457406520844\n",
      "time_this_iter_s: 5.365121841430664\n",
      "time_total_s: 642.2457406520844\n",
      "timers:\n",
      "  learn_throughput: 2145.878\n",
      "  learn_time_ms: 1864.039\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1144.731\n",
      "  sample_time_ms: 3494.27\n",
      "  update_time_ms: 1.705\n",
      "timestamp: 1633509184\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 476000\n",
      "training_iteration: 119\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:119 starting ! -----------------\n",
      "agent_timesteps_total: 480000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-33-24\n",
      "done: false\n",
      "episode_len_mean: 700.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.308688999999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 347\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2313915491104126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010057754814624786\n",
      "        model: {}\n",
      "        policy_loss: -0.014603346586227417\n",
      "        total_loss: 0.0057512191124260426\n",
      "        vf_explained_var: 0.11306966096162796\n",
      "        vf_loss: 0.018343016505241394\n",
      "  num_agent_steps_sampled: 480000\n",
      "  num_agent_steps_trained: 480000\n",
      "  num_steps_sampled: 480000\n",
      "  num_steps_trained: 480000\n",
      "iterations_since_restore: 120\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.17142857142857\n",
      "  ram_util_percent: 43.482142857142854\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04966624801702972\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749967852447337\n",
      "  mean_inference_ms: 0.8522199238993241\n",
      "  mean_raw_obs_processing_ms: 0.08784794221039555\n",
      "time_since_restore: 647.5522468090057\n",
      "time_this_iter_s: 5.306506156921387\n",
      "time_total_s: 647.5522468090057\n",
      "timers:\n",
      "  learn_throughput: 2144.944\n",
      "  learn_time_ms: 1864.85\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1144.268\n",
      "  sample_time_ms: 3495.685\n",
      "  update_time_ms: 1.805\n",
      "timestamp: 1633509204\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 480000\n",
      "training_iteration: 120\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:120 starting ! -----------------\n",
      "agent_timesteps_total: 484000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-33-35\n",
      "done: false\n",
      "episode_len_mean: 697.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.298209499999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 350\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0776877403259277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010929360054433346\n",
      "        model: {}\n",
      "        policy_loss: -0.012577989138662815\n",
      "        total_loss: 0.01069817878305912\n",
      "        vf_explained_var: 0.17049826681613922\n",
      "        vf_loss: 0.021090297028422356\n",
      "  num_agent_steps_sampled: 484000\n",
      "  num_agent_steps_trained: 484000\n",
      "  num_steps_sampled: 484000\n",
      "  num_steps_trained: 484000\n",
      "iterations_since_restore: 121\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.506666666666664\n",
      "  ram_util_percent: 43.453333333333326\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04966838157085566\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499684470404049\n",
      "  mean_inference_ms: 0.8522022477446963\n",
      "  mean_raw_obs_processing_ms: 0.08786322390764535\n",
      "time_since_restore: 652.905446767807\n",
      "time_this_iter_s: 5.3531999588012695\n",
      "time_total_s: 652.905446767807\n",
      "timers:\n",
      "  learn_throughput: 2144.233\n",
      "  learn_time_ms: 1865.469\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1146.353\n",
      "  sample_time_ms: 3489.325\n",
      "  update_time_ms: 1.705\n",
      "timestamp: 1633509215\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 484000\n",
      "training_iteration: 121\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:121 starting ! -----------------\n",
      "agent_timesteps_total: 488000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-33-52\n",
      "done: false\n",
      "episode_len_mean: 695.09\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.2475314999999981\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 353\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.073453426361084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007535166572779417\n",
      "        model: {}\n",
      "        policy_loss: -0.012674739584326744\n",
      "        total_loss: 0.0017221266170963645\n",
      "        vf_explained_var: 0.4102582335472107\n",
      "        vf_loss: 0.012889831326901913\n",
      "  num_agent_steps_sampled: 488000\n",
      "  num_agent_steps_trained: 488000\n",
      "  num_steps_sampled: 488000\n",
      "  num_steps_trained: 488000\n",
      "iterations_since_restore: 122\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.117391304347834\n",
      "  ram_util_percent: 43.43043478260869\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04967284354918301\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749978281325286\n",
      "  mean_inference_ms: 0.8521906983247893\n",
      "  mean_raw_obs_processing_ms: 0.08788074158207129\n",
      "time_since_restore: 658.3824374675751\n",
      "time_this_iter_s: 5.476990699768066\n",
      "time_total_s: 658.3824374675751\n",
      "timers:\n",
      "  learn_throughput: 2143.836\n",
      "  learn_time_ms: 1865.814\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1139.068\n",
      "  sample_time_ms: 3511.642\n",
      "  update_time_ms: 1.705\n",
      "timestamp: 1633509232\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 488000\n",
      "training_iteration: 122\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:122 starting ! -----------------\n",
      "agent_timesteps_total: 492000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-34-08\n",
      "done: false\n",
      "episode_len_mean: 685.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.14680999999999805\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 357\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0861109495162964\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008056573569774628\n",
      "        model: {}\n",
      "        policy_loss: -0.01696518249809742\n",
      "        total_loss: 0.011895900592207909\n",
      "        vf_explained_var: 0.04956521838903427\n",
      "        vf_loss: 0.027249760925769806\n",
      "  num_agent_steps_sampled: 492000\n",
      "  num_agent_steps_trained: 492000\n",
      "  num_steps_sampled: 492000\n",
      "  num_steps_trained: 492000\n",
      "iterations_since_restore: 123\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.733333333333334\n",
      "  ram_util_percent: 43.43333333333334\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04968092234296715\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499959507830323\n",
      "  mean_inference_ms: 0.8521718538284424\n",
      "  mean_raw_obs_processing_ms: 0.08790377793544275\n",
      "time_since_restore: 663.889274597168\n",
      "time_this_iter_s: 5.5068371295928955\n",
      "time_total_s: 663.889274597168\n",
      "timers:\n",
      "  learn_throughput: 2135.331\n",
      "  learn_time_ms: 1873.246\n",
      "  load_throughput: 41160981.354\n",
      "  load_time_ms: 0.097\n",
      "  sample_throughput: 1135.455\n",
      "  sample_time_ms: 3522.816\n",
      "  update_time_ms: 1.705\n",
      "timestamp: 1633509248\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 492000\n",
      "training_iteration: 123\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:123 starting ! -----------------\n",
      "agent_timesteps_total: 496000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-34-31\n",
      "done: false\n",
      "episode_len_mean: 680.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.207420499999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 360\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1829246282577515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009771057404577732\n",
      "        model: {}\n",
      "        policy_loss: -0.02384628728032112\n",
      "        total_loss: -0.003633377607911825\n",
      "        vf_explained_var: 0.08039578795433044\n",
      "        vf_loss: 0.018258698284626007\n",
      "  num_agent_steps_sampled: 496000\n",
      "  num_agent_steps_trained: 496000\n",
      "  num_steps_sampled: 496000\n",
      "  num_steps_trained: 496000\n",
      "iterations_since_restore: 124\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.43030303030303\n",
      "  ram_util_percent: 43.442424242424245\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04968446215379459\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499911296928374\n",
      "  mean_inference_ms: 0.8521386810876372\n",
      "  mean_raw_obs_processing_ms: 0.08791756666109368\n",
      "time_since_restore: 669.1849255561829\n",
      "time_this_iter_s: 5.295650959014893\n",
      "time_total_s: 669.1849255561829\n",
      "timers:\n",
      "  learn_throughput: 2154.107\n",
      "  learn_time_ms: 1856.918\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.984\n",
      "  sample_time_ms: 3518.078\n",
      "  update_time_ms: 1.705\n",
      "timestamp: 1633509271\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 496000\n",
      "training_iteration: 124\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:124 starting ! -----------------\n",
      "agent_timesteps_total: 500000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-34-55\n",
      "done: false\n",
      "episode_len_mean: 691.23\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.20750049999999798\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 362\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.088750958442688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011997091583907604\n",
      "        model: {}\n",
      "        policy_loss: -0.012261595577001572\n",
      "        total_loss: 0.03010585345327854\n",
      "        vf_explained_var: 0.00976150669157505\n",
      "        vf_loss: 0.03996802866458893\n",
      "  num_agent_steps_sampled: 500000\n",
      "  num_agent_steps_trained: 500000\n",
      "  num_steps_sampled: 500000\n",
      "  num_steps_trained: 500000\n",
      "iterations_since_restore: 125\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.2030303030303\n",
      "  ram_util_percent: 43.44242424242425\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04968774160011826\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499836131947785\n",
      "  mean_inference_ms: 0.8521160539681003\n",
      "  mean_raw_obs_processing_ms: 0.08792898310167228\n",
      "time_since_restore: 674.4902393817902\n",
      "time_this_iter_s: 5.3053138256073\n",
      "time_total_s: 674.4902393817902\n",
      "timers:\n",
      "  learn_throughput: 2155.253\n",
      "  learn_time_ms: 1855.931\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.327\n",
      "  sample_time_ms: 3513.928\n",
      "  update_time_ms: 1.804\n",
      "timestamp: 1633509295\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 500000\n",
      "training_iteration: 125\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:125 starting ! -----------------\n",
      "agent_timesteps_total: 504000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-35-09\n",
      "done: false\n",
      "episode_len_mean: 702.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.308025999999998\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 364\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.333127737045288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010485298000276089\n",
      "        model: {}\n",
      "        policy_loss: -0.01613648608326912\n",
      "        total_loss: -0.0028954176232218742\n",
      "        vf_explained_var: 0.14112251996994019\n",
      "        vf_loss: 0.011144005693495274\n",
      "  num_agent_steps_sampled: 504000\n",
      "  num_agent_steps_trained: 504000\n",
      "  num_steps_sampled: 504000\n",
      "  num_steps_trained: 504000\n",
      "iterations_since_restore: 126\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.54\n",
      "  ram_util_percent: 43.43999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049690128414575395\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499708520533445\n",
      "  mean_inference_ms: 0.8520921701876154\n",
      "  mean_raw_obs_processing_ms: 0.08793924771182585\n",
      "time_since_restore: 679.8345346450806\n",
      "time_this_iter_s: 5.344295263290405\n",
      "time_total_s: 679.8345346450806\n",
      "timers:\n",
      "  learn_throughput: 2146.454\n",
      "  learn_time_ms: 1863.539\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.198\n",
      "  sample_time_ms: 3505.087\n",
      "  update_time_ms: 1.917\n",
      "timestamp: 1633509309\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 504000\n",
      "training_iteration: 126\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:126 starting ! -----------------\n",
      "agent_timesteps_total: 508000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-35-30\n",
      "done: false\n",
      "episode_len_mean: 691.21\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.18573599999999818\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 368\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2202646732330322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011811373755335808\n",
      "        model: {}\n",
      "        policy_loss: -0.02104508876800537\n",
      "        total_loss: 0.005210301373153925\n",
      "        vf_explained_var: 0.10205905139446259\n",
      "        vf_loss: 0.023893117904663086\n",
      "  num_agent_steps_sampled: 508000\n",
      "  num_agent_steps_trained: 508000\n",
      "  num_steps_sampled: 508000\n",
      "  num_steps_trained: 508000\n",
      "iterations_since_restore: 127\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.110714285714288\n",
      "  ram_util_percent: 43.442857142857164\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04969247744357449\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499064703237833\n",
      "  mean_inference_ms: 0.8520440535730746\n",
      "  mean_raw_obs_processing_ms: 0.08795826393242039\n",
      "time_since_restore: 685.0637409687042\n",
      "time_this_iter_s: 5.229206323623657\n",
      "time_total_s: 685.0637409687042\n",
      "timers:\n",
      "  learn_throughput: 2147.053\n",
      "  learn_time_ms: 1863.018\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.959\n",
      "  sample_time_ms: 3499.688\n",
      "  update_time_ms: 1.809\n",
      "timestamp: 1633509330\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 508000\n",
      "training_iteration: 127\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:127 starting ! -----------------\n",
      "agent_timesteps_total: 512000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-35-47\n",
      "done: false\n",
      "episode_len_mean: 698.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.23629899999999815\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 370\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1618670225143433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010762039572000504\n",
      "        model: {}\n",
      "        policy_loss: -0.00927155651152134\n",
      "        total_loss: 0.013413199223577976\n",
      "        vf_explained_var: 0.06071510910987854\n",
      "        vf_loss: 0.02053234912455082\n",
      "  num_agent_steps_sampled: 512000\n",
      "  num_agent_steps_trained: 512000\n",
      "  num_steps_sampled: 512000\n",
      "  num_steps_trained: 512000\n",
      "iterations_since_restore: 128\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.232\n",
      "  ram_util_percent: 43.44\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04969393594682403\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498679324423227\n",
      "  mean_inference_ms: 0.8520174396723997\n",
      "  mean_raw_obs_processing_ms: 0.08796670577092555\n",
      "time_since_restore: 690.4730536937714\n",
      "time_this_iter_s: 5.409312725067139\n",
      "time_total_s: 690.4730536937714\n",
      "timers:\n",
      "  learn_throughput: 2138.823\n",
      "  learn_time_ms: 1870.188\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.885\n",
      "  sample_time_ms: 3481.637\n",
      "  update_time_ms: 1.809\n",
      "timestamp: 1633509347\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 512000\n",
      "training_iteration: 128\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:128 starting ! -----------------\n",
      "agent_timesteps_total: 516000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-36-06\n",
      "done: false\n",
      "episode_len_mean: 695.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.12575049999999824\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 374\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2368122339248657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010820652358233929\n",
      "        model: {}\n",
      "        policy_loss: -0.018889037892222404\n",
      "        total_loss: -0.0021646206732839346\n",
      "        vf_explained_var: 0.08279048651456833\n",
      "        vf_loss: 0.014560289680957794\n",
      "  num_agent_steps_sampled: 516000\n",
      "  num_agent_steps_trained: 516000\n",
      "  num_steps_sampled: 516000\n",
      "  num_steps_trained: 516000\n",
      "iterations_since_restore: 129\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.133333333333333\n",
      "  ram_util_percent: 43.444444444444464\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049694318671822515\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497732511563321\n",
      "  mean_inference_ms: 0.8519767542858375\n",
      "  mean_raw_obs_processing_ms: 0.08798413199127954\n",
      "time_since_restore: 695.7153203487396\n",
      "time_this_iter_s: 5.242266654968262\n",
      "time_total_s: 695.7153203487396\n",
      "timers:\n",
      "  learn_throughput: 2140.899\n",
      "  learn_time_ms: 1868.374\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1152.304\n",
      "  sample_time_ms: 3471.306\n",
      "  update_time_ms: 1.809\n",
      "timestamp: 1633509366\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 516000\n",
      "training_iteration: 129\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:129 starting ! -----------------\n",
      "agent_timesteps_total: 520000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-36-17\n",
      "done: false\n",
      "episode_len_mean: 692.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.1259809999999983\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 376\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2998228073120117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012585651129484177\n",
      "        model: {}\n",
      "        policy_loss: -0.019401315599679947\n",
      "        total_loss: -0.00039999434375204146\n",
      "        vf_explained_var: 0.08808322995901108\n",
      "        vf_loss: 0.01648419350385666\n",
      "  num_agent_steps_sampled: 520000\n",
      "  num_agent_steps_trained: 520000\n",
      "  num_steps_sampled: 520000\n",
      "  num_steps_trained: 520000\n",
      "iterations_since_restore: 130\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.18666666666667\n",
      "  ram_util_percent: 43.40666666666666\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04969647997609451\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497262820066507\n",
      "  mean_inference_ms: 0.8519619219700172\n",
      "  mean_raw_obs_processing_ms: 0.08799149578172333\n",
      "time_since_restore: 701.0674564838409\n",
      "time_this_iter_s: 5.352136135101318\n",
      "time_total_s: 701.0674564838409\n",
      "timers:\n",
      "  learn_throughput: 2141.049\n",
      "  learn_time_ms: 1868.243\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.764\n",
      "  sample_time_ms: 3475.953\n",
      "  update_time_ms: 1.788\n",
      "timestamp: 1633509377\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 520000\n",
      "training_iteration: 130\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:130 starting ! -----------------\n",
      "agent_timesteps_total: 524000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-36-32\n",
      "done: false\n",
      "episode_len_mean: 685.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: -0.0648539999999983\n",
      "episode_reward_min: -2.2133500000000037\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 379\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.178169846534729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00935938861221075\n",
      "        model: {}\n",
      "        policy_loss: 0.0027430083137005568\n",
      "        total_loss: 0.018753038719296455\n",
      "        vf_explained_var: 0.4334986209869385\n",
      "        vf_loss: 0.01413815189152956\n",
      "  num_agent_steps_sampled: 524000\n",
      "  num_agent_steps_trained: 524000\n",
      "  num_steps_sampled: 524000\n",
      "  num_steps_trained: 524000\n",
      "iterations_since_restore: 131\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.885714285714293\n",
      "  ram_util_percent: 43.49047619047619\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049699313492842005\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496612518625447\n",
      "  mean_inference_ms: 0.8519382901323183\n",
      "  mean_raw_obs_processing_ms: 0.08800206820003449\n",
      "time_since_restore: 706.475700378418\n",
      "time_this_iter_s: 5.408243894577026\n",
      "time_total_s: 706.475700378418\n",
      "timers:\n",
      "  learn_throughput: 2139.96\n",
      "  learn_time_ms: 1869.194\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.242\n",
      "  sample_time_ms: 3480.556\n",
      "  update_time_ms: 1.788\n",
      "timestamp: 1633509392\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 524000\n",
      "training_iteration: 131\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:131 starting ! -----------------\n",
      "agent_timesteps_total: 528000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-36-47\n",
      "done: false\n",
      "episode_len_mean: 674.09\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.037143000000001626\n",
      "episode_reward_min: -2.1890000000000005\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 382\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2935903072357178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010836894623935223\n",
      "        model: {}\n",
      "        policy_loss: -0.017613742500543594\n",
      "        total_loss: 0.01781628280878067\n",
      "        vf_explained_var: 0.027401570230722427\n",
      "        vf_loss: 0.03326265141367912\n",
      "  num_agent_steps_sampled: 528000\n",
      "  num_agent_steps_trained: 528000\n",
      "  num_steps_sampled: 528000\n",
      "  num_steps_trained: 528000\n",
      "iterations_since_restore: 132\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.081818181818186\n",
      "  ram_util_percent: 43.46363636363636\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04970223134745986\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495910608378832\n",
      "  mean_inference_ms: 0.8519125285727874\n",
      "  mean_raw_obs_processing_ms: 0.0880131862097271\n",
      "time_since_restore: 711.965900182724\n",
      "time_this_iter_s: 5.49019980430603\n",
      "time_total_s: 711.965900182724\n",
      "timers:\n",
      "  learn_throughput: 2128.717\n",
      "  learn_time_ms: 1879.066\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1152.086\n",
      "  sample_time_ms: 3471.962\n",
      "  update_time_ms: 1.788\n",
      "timestamp: 1633509407\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 528000\n",
      "training_iteration: 132\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:132 starting ! -----------------\n",
      "agent_timesteps_total: 532000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-37-06\n",
      "done: false\n",
      "episode_len_mean: 669.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.037688500000001665\n",
      "episode_reward_min: -2.182999999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 385\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2195731401443481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00740113016217947\n",
      "        model: {}\n",
      "        policy_loss: -0.013957012444734573\n",
      "        total_loss: 0.002691912464797497\n",
      "        vf_explained_var: 0.08896725624799728\n",
      "        vf_loss: 0.015168695710599422\n",
      "  num_agent_steps_sampled: 532000\n",
      "  num_agent_steps_trained: 532000\n",
      "  num_steps_sampled: 532000\n",
      "  num_steps_trained: 532000\n",
      "iterations_since_restore: 133\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.238461538461536\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049705519594340906\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495212037030176\n",
      "  mean_inference_ms: 0.8518882874068752\n",
      "  mean_raw_obs_processing_ms: 0.088024588112589\n",
      "time_since_restore: 717.2770595550537\n",
      "time_this_iter_s: 5.311159372329712\n",
      "time_total_s: 717.2770595550537\n",
      "timers:\n",
      "  learn_throughput: 2137.856\n",
      "  learn_time_ms: 1871.034\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1155.948\n",
      "  sample_time_ms: 3460.364\n",
      "  update_time_ms: 1.788\n",
      "timestamp: 1633509426\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 532000\n",
      "training_iteration: 133\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:133 starting ! -----------------\n",
      "agent_timesteps_total: 536000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-37-17\n",
      "done: false\n",
      "episode_len_mean: 662.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.08841800000000154\n",
      "episode_reward_min: -2.182999999999999\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 390\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2363964319229126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008464755490422249\n",
      "        model: {}\n",
      "        policy_loss: -0.01614583283662796\n",
      "        total_loss: 0.023265594616532326\n",
      "        vf_explained_var: 0.251971036195755\n",
      "        vf_loss: 0.037718478590250015\n",
      "  num_agent_steps_sampled: 536000\n",
      "  num_agent_steps_trained: 536000\n",
      "  num_steps_sampled: 536000\n",
      "  num_steps_trained: 536000\n",
      "iterations_since_restore: 134\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.075\n",
      "  ram_util_percent: 43.45\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04971158143134903\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494026960768255\n",
      "  mean_inference_ms: 0.8518581163484342\n",
      "  mean_raw_obs_processing_ms: 0.08803943437593072\n",
      "time_since_restore: 722.7765510082245\n",
      "time_this_iter_s: 5.499491453170776\n",
      "time_total_s: 722.7765510082245\n",
      "timers:\n",
      "  learn_throughput: 2119.657\n",
      "  learn_time_ms: 1887.098\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1154.488\n",
      "  sample_time_ms: 3464.738\n",
      "  update_time_ms: 1.788\n",
      "timestamp: 1633509437\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 536000\n",
      "training_iteration: 134\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:134 starting ! -----------------\n",
      "agent_timesteps_total: 540000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-37-37\n",
      "done: false\n",
      "episode_len_mean: 660.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.1295485000000015\n",
      "episode_reward_min: -2.182999999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 393\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3438606262207031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015161202289164066\n",
      "        model: {}\n",
      "        policy_loss: -0.013285405933856964\n",
      "        total_loss: 0.021010305732488632\n",
      "        vf_explained_var: -0.23772041499614716\n",
      "        vf_loss: 0.03126347064971924\n",
      "  num_agent_steps_sampled: 540000\n",
      "  num_agent_steps_trained: 540000\n",
      "  num_steps_sampled: 540000\n",
      "  num_steps_trained: 540000\n",
      "iterations_since_restore: 135\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.739285714285707\n",
      "  ram_util_percent: 43.45714285714286\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04971590856775936\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493209632561864\n",
      "  mean_inference_ms: 0.8518433016589388\n",
      "  mean_raw_obs_processing_ms: 0.08804767684735353\n",
      "time_since_restore: 727.9663918018341\n",
      "time_this_iter_s: 5.189840793609619\n",
      "time_total_s: 727.9663918018341\n",
      "timers:\n",
      "  learn_throughput: 2119.999\n",
      "  learn_time_ms: 1886.794\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1158.243\n",
      "  sample_time_ms: 3453.506\n",
      "  update_time_ms: 1.788\n",
      "timestamp: 1633509457\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 540000\n",
      "training_iteration: 135\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:135 starting ! -----------------\n",
      "agent_timesteps_total: 544000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-37-50\n",
      "done: false\n",
      "episode_len_mean: 663.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.14065800000000153\n",
      "episode_reward_min: -2.181649999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 395\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1522232294082642\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008523802272975445\n",
      "        model: {}\n",
      "        policy_loss: -0.009105348028242588\n",
      "        total_loss: 0.011238187551498413\n",
      "        vf_explained_var: 0.09545319527387619\n",
      "        vf_loss: 0.018638774752616882\n",
      "  num_agent_steps_sampled: 544000\n",
      "  num_agent_steps_trained: 544000\n",
      "  num_steps_sampled: 544000\n",
      "  num_steps_trained: 544000\n",
      "iterations_since_restore: 136\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.233333333333334\n",
      "  ram_util_percent: 43.483333333333334\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049720492641370136\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749259598058547\n",
      "  mean_inference_ms: 0.8518365139013018\n",
      "  mean_raw_obs_processing_ms: 0.08805473305413272\n",
      "time_since_restore: 733.3903467655182\n",
      "time_this_iter_s: 5.423954963684082\n",
      "time_total_s: 733.3903467655182\n",
      "timers:\n",
      "  learn_throughput: 2128.216\n",
      "  learn_time_ms: 1879.509\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1153.166\n",
      "  sample_time_ms: 3468.71\n",
      "  update_time_ms: 1.775\n",
      "timestamp: 1633509470\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 544000\n",
      "training_iteration: 136\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:136 starting ! -----------------\n",
      "agent_timesteps_total: 548000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-38-04\n",
      "done: false\n",
      "episode_len_mean: 669.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.1486520000000016\n",
      "episode_reward_min: -2.181649999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 397\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0819413661956787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011079550720751286\n",
      "        model: {}\n",
      "        policy_loss: -0.01981183886528015\n",
      "        total_loss: 0.003468138165771961\n",
      "        vf_explained_var: 0.29469555616378784\n",
      "        vf_loss: 0.021064072847366333\n",
      "  num_agent_steps_sampled: 548000\n",
      "  num_agent_steps_trained: 548000\n",
      "  num_steps_sampled: 548000\n",
      "  num_steps_trained: 548000\n",
      "iterations_since_restore: 137\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.594736842105263\n",
      "  ram_util_percent: 43.473684210526315\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04972716780294539\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7492056434134908\n",
      "  mean_inference_ms: 0.8518277008210258\n",
      "  mean_raw_obs_processing_ms: 0.08806118301297737\n",
      "time_since_restore: 738.8423507213593\n",
      "time_this_iter_s: 5.4520039558410645\n",
      "time_total_s: 738.8423507213593\n",
      "timers:\n",
      "  learn_throughput: 2118.274\n",
      "  learn_time_ms: 1888.33\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.724\n",
      "  sample_time_ms: 3482.124\n",
      "  update_time_ms: 1.892\n",
      "timestamp: 1633509484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 548000\n",
      "training_iteration: 137\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:137 starting ! -----------------\n",
      "agent_timesteps_total: 552000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-38-24\n",
      "done: false\n",
      "episode_len_mean: 676.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.14886500000000158\n",
      "episode_reward_min: -2.181649999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 399\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.247784972190857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010169199667870998\n",
      "        model: {}\n",
      "        policy_loss: 0.00033627633820287883\n",
      "        total_loss: 0.01791658252477646\n",
      "        vf_explained_var: -0.03465891256928444\n",
      "        vf_loss: 0.015546462498605251\n",
      "  num_agent_steps_sampled: 552000\n",
      "  num_agent_steps_trained: 552000\n",
      "  num_steps_sampled: 552000\n",
      "  num_steps_trained: 552000\n",
      "iterations_since_restore: 138\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.04827586206897\n",
      "  ram_util_percent: 43.489655172413784\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049732218410744995\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491478432357834\n",
      "  mean_inference_ms: 0.8518189754643715\n",
      "  mean_raw_obs_processing_ms: 0.0880683357545327\n",
      "time_since_restore: 744.1653065681458\n",
      "time_this_iter_s: 5.322955846786499\n",
      "time_total_s: 744.1653065681458\n",
      "timers:\n",
      "  learn_throughput: 2127.368\n",
      "  learn_time_ms: 1880.257\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.909\n",
      "  sample_time_ms: 3481.563\n",
      "  update_time_ms: 1.892\n",
      "timestamp: 1633509504\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 552000\n",
      "training_iteration: 138\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:138 starting ! -----------------\n",
      "agent_timesteps_total: 556000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-38-46\n",
      "done: false\n",
      "episode_len_mean: 677.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.15909200000000168\n",
      "episode_reward_min: -2.181649999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 401\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0275627374649048\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010223634541034698\n",
      "        model: {}\n",
      "        policy_loss: -0.013692460022866726\n",
      "        total_loss: 0.003925865050405264\n",
      "        vf_explained_var: -0.038381148129701614\n",
      "        vf_loss: 0.015573596581816673\n",
      "  num_agent_steps_sampled: 556000\n",
      "  num_agent_steps_trained: 556000\n",
      "  num_steps_sampled: 556000\n",
      "  num_steps_trained: 556000\n",
      "iterations_since_restore: 139\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.27741935483871\n",
      "  ram_util_percent: 43.48064516129032\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049736208549010626\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490876715624729\n",
      "  mean_inference_ms: 0.851808394625913\n",
      "  mean_raw_obs_processing_ms: 0.08807552499229271\n",
      "time_since_restore: 749.4822146892548\n",
      "time_this_iter_s: 5.316908121109009\n",
      "time_total_s: 749.4822146892548\n",
      "timers:\n",
      "  learn_throughput: 2124.147\n",
      "  learn_time_ms: 1883.108\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.45\n",
      "  sample_time_ms: 3485.99\n",
      "  update_time_ms: 1.892\n",
      "timestamp: 1633509526\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 556000\n",
      "training_iteration: 139\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:139 starting ! -----------------\n",
      "agent_timesteps_total: 560000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-39-03\n",
      "done: false\n",
      "episode_len_mean: 681.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914900000000001\n",
      "episode_reward_mean: 0.15088450000000145\n",
      "episode_reward_min: -2.181649999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 404\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2096869945526123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011367352679371834\n",
      "        model: {}\n",
      "        policy_loss: -0.01838347688317299\n",
      "        total_loss: -0.001008317805826664\n",
      "        vf_explained_var: 0.013474849984049797\n",
      "        vf_loss: 0.015101687051355839\n",
      "  num_agent_steps_sampled: 560000\n",
      "  num_agent_steps_trained: 560000\n",
      "  num_steps_sampled: 560000\n",
      "  num_steps_trained: 560000\n",
      "iterations_since_restore: 140\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.89166666666667\n",
      "  ram_util_percent: 43.487500000000004\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04973851288300292\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489833247430907\n",
      "  mean_inference_ms: 0.851792758306368\n",
      "  mean_raw_obs_processing_ms: 0.08808798205271685\n",
      "time_since_restore: 754.7507183551788\n",
      "time_this_iter_s: 5.268503665924072\n",
      "time_total_s: 754.7507183551788\n",
      "timers:\n",
      "  learn_throughput: 2123.056\n",
      "  learn_time_ms: 1884.077\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.505\n",
      "  sample_time_ms: 3476.734\n",
      "  update_time_ms: 1.913\n",
      "timestamp: 1633509543\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 560000\n",
      "training_iteration: 140\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:140 starting ! -----------------\n",
      "agent_timesteps_total: 564000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-39-18\n",
      "done: false\n",
      "episode_len_mean: 672.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.2727455000000014\n",
      "episode_reward_min: -2.169350000000001\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 407\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.172990322113037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010723710991442204\n",
      "        model: {}\n",
      "        policy_loss: -0.01831020414829254\n",
      "        total_loss: 0.02276861108839512\n",
      "        vf_explained_var: -0.09134241938591003\n",
      "        vf_loss: 0.038934070616960526\n",
      "  num_agent_steps_sampled: 564000\n",
      "  num_agent_steps_trained: 564000\n",
      "  num_steps_sampled: 564000\n",
      "  num_steps_trained: 564000\n",
      "iterations_since_restore: 141\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.169999999999998\n",
      "  ram_util_percent: 43.464999999999996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04973839161424172\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7488810379102558\n",
      "  mean_inference_ms: 0.8517787129037572\n",
      "  mean_raw_obs_processing_ms: 0.08810252390272556\n",
      "time_since_restore: 760.0997130870819\n",
      "time_this_iter_s: 5.348994731903076\n",
      "time_total_s: 760.0997130870819\n",
      "timers:\n",
      "  learn_throughput: 2119.196\n",
      "  learn_time_ms: 1887.509\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1153.638\n",
      "  sample_time_ms: 3467.292\n",
      "  update_time_ms: 1.913\n",
      "timestamp: 1633509558\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 564000\n",
      "training_iteration: 141\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:141 starting ! -----------------\n",
      "agent_timesteps_total: 568000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-39-34\n",
      "done: false\n",
      "episode_len_mean: 676.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.2629800000000014\n",
      "episode_reward_min: -2.169350000000001\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 410\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3152942657470703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013791453093290329\n",
      "        model: {}\n",
      "        policy_loss: -0.019087422639131546\n",
      "        total_loss: 0.009463898837566376\n",
      "        vf_explained_var: 0.260000079870224\n",
      "        vf_loss: 0.025793032720685005\n",
      "  num_agent_steps_sampled: 568000\n",
      "  num_agent_steps_trained: 568000\n",
      "  num_steps_sampled: 568000\n",
      "  num_steps_trained: 568000\n",
      "iterations_since_restore: 142\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.269565217391307\n",
      "  ram_util_percent: 43.46521739130435\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04973578688530317\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7487711082101484\n",
      "  mean_inference_ms: 0.8517644930297729\n",
      "  mean_raw_obs_processing_ms: 0.08811692183267272\n",
      "time_since_restore: 765.3499224185944\n",
      "time_this_iter_s: 5.250209331512451\n",
      "time_total_s: 765.3499224185944\n",
      "timers:\n",
      "  learn_throughput: 2130.604\n",
      "  learn_time_ms: 1877.402\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1158.275\n",
      "  sample_time_ms: 3453.412\n",
      "  update_time_ms: 1.876\n",
      "timestamp: 1633509574\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 568000\n",
      "training_iteration: 142\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:142 starting ! -----------------\n",
      "agent_timesteps_total: 572000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-39-52\n",
      "done: false\n",
      "episode_len_mean: 676.15\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.26153850000000145\n",
      "episode_reward_min: -2.169350000000001\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 413\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2548290491104126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009726197458803654\n",
      "        model: {}\n",
      "        policy_loss: -0.022272875532507896\n",
      "        total_loss: -0.007791847921907902\n",
      "        vf_explained_var: 0.17922717332839966\n",
      "        vf_loss: 0.012535784393548965\n",
      "  num_agent_steps_sampled: 572000\n",
      "  num_agent_steps_trained: 572000\n",
      "  num_steps_sampled: 572000\n",
      "  num_steps_trained: 572000\n",
      "iterations_since_restore: 143\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.972\n",
      "  ram_util_percent: 43.44000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04973448555186246\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.74866309220748\n",
      "  mean_inference_ms: 0.8517483275545557\n",
      "  mean_raw_obs_processing_ms: 0.08813055492189997\n",
      "time_since_restore: 770.7925100326538\n",
      "time_this_iter_s: 5.442587614059448\n",
      "time_total_s: 770.7925100326538\n",
      "timers:\n",
      "  learn_throughput: 2122.29\n",
      "  learn_time_ms: 1884.757\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1156.313\n",
      "  sample_time_ms: 3459.27\n",
      "  update_time_ms: 1.876\n",
      "timestamp: 1633509592\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 572000\n",
      "training_iteration: 143\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:143 starting ! -----------------\n",
      "agent_timesteps_total: 576000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-40-05\n",
      "done: false\n",
      "episode_len_mean: 676.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.20101500000000158\n",
      "episode_reward_min: -2.169350000000001\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 415\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3443413972854614\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007990666665136814\n",
      "        model: {}\n",
      "        policy_loss: -0.017069995403289795\n",
      "        total_loss: -0.00463373027741909\n",
      "        vf_explained_var: 0.17440199851989746\n",
      "        vf_loss: 0.010838134214282036\n",
      "  num_agent_steps_sampled: 576000\n",
      "  num_agent_steps_trained: 576000\n",
      "  num_steps_sampled: 576000\n",
      "  num_steps_trained: 576000\n",
      "iterations_since_restore: 144\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.263157894736842\n",
      "  ram_util_percent: 43.426315789473676\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04973271378321464\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7485922429518749\n",
      "  mean_inference_ms: 0.8517357941519097\n",
      "  mean_raw_obs_processing_ms: 0.08813939985892556\n",
      "time_since_restore: 776.0746188163757\n",
      "time_this_iter_s: 5.282108783721924\n",
      "time_total_s: 776.0746188163757\n",
      "timers:\n",
      "  learn_throughput: 2137.585\n",
      "  learn_time_ms: 1871.271\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1159.104\n",
      "  sample_time_ms: 3450.943\n",
      "  update_time_ms: 1.776\n",
      "timestamp: 1633509605\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 576000\n",
      "training_iteration: 144\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:144 starting ! -----------------\n",
      "agent_timesteps_total: 580000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-40-23\n",
      "done: false\n",
      "episode_len_mean: 663.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.3114390000000016\n",
      "episode_reward_min: -2.169350000000001\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 419\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1566369533538818\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016775408759713173\n",
      "        model: {}\n",
      "        policy_loss: -0.023525148630142212\n",
      "        total_loss: -0.004301971290260553\n",
      "        vf_explained_var: 0.5418320298194885\n",
      "        vf_loss: 0.015868093818426132\n",
      "  num_agent_steps_sampled: 580000\n",
      "  num_agent_steps_trained: 580000\n",
      "  num_steps_sampled: 580000\n",
      "  num_steps_trained: 580000\n",
      "iterations_since_restore: 145\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.795833333333334\n",
      "  ram_util_percent: 43.425000000000004\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04972656634628759\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7484594544826748\n",
      "  mean_inference_ms: 0.8517096727936988\n",
      "  mean_raw_obs_processing_ms: 0.0881537008337792\n",
      "time_since_restore: 781.4950714111328\n",
      "time_this_iter_s: 5.42045259475708\n",
      "time_total_s: 781.4950714111328\n",
      "timers:\n",
      "  learn_throughput: 2131.94\n",
      "  learn_time_ms: 1876.225\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1153.024\n",
      "  sample_time_ms: 3469.138\n",
      "  update_time_ms: 1.676\n",
      "timestamp: 1633509623\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 580000\n",
      "training_iteration: 145\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:145 starting ! -----------------\n",
      "agent_timesteps_total: 584000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-40-41\n",
      "done: false\n",
      "episode_len_mean: 658.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.3609315000000015\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 423\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2108699083328247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01069526094943285\n",
      "        model: {}\n",
      "        policy_loss: -0.012975340709090233\n",
      "        total_loss: 0.003289744257926941\n",
      "        vf_explained_var: 0.019642014056444168\n",
      "        vf_loss: 0.01412603072822094\n",
      "  num_agent_steps_sampled: 584000\n",
      "  num_agent_steps_trained: 584000\n",
      "  num_steps_sampled: 584000\n",
      "  num_steps_trained: 584000\n",
      "iterations_since_restore: 146\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.515384615384615\n",
      "  ram_util_percent: 43.407692307692315\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049719876823098016\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7483457505779945\n",
      "  mean_inference_ms: 0.8516834668226426\n",
      "  mean_raw_obs_processing_ms: 0.08816681662957336\n",
      "time_since_restore: 786.8642904758453\n",
      "time_this_iter_s: 5.369219064712524\n",
      "time_total_s: 786.8642904758453\n",
      "timers:\n",
      "  learn_throughput: 2133.23\n",
      "  learn_time_ms: 1875.091\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1154.414\n",
      "  sample_time_ms: 3464.962\n",
      "  update_time_ms: 1.676\n",
      "timestamp: 1633509641\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 584000\n",
      "training_iteration: 146\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:146 starting ! -----------------\n",
      "agent_timesteps_total: 588000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-40-59\n",
      "done: false\n",
      "episode_len_mean: 659.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.3608600000000014\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 425\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2852814197540283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011885923333466053\n",
      "        model: {}\n",
      "        policy_loss: -0.016634272411465645\n",
      "        total_loss: -0.004605683498084545\n",
      "        vf_explained_var: -0.028917469084262848\n",
      "        vf_loss: 0.009651410393416882\n",
      "  num_agent_steps_sampled: 588000\n",
      "  num_agent_steps_trained: 588000\n",
      "  num_steps_sampled: 588000\n",
      "  num_steps_trained: 588000\n",
      "iterations_since_restore: 147\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.836\n",
      "  ram_util_percent: 43.388000000000005\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0497170983439861\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7482818824583606\n",
      "  mean_inference_ms: 0.851673073224433\n",
      "  mean_raw_obs_processing_ms: 0.08817381376518917\n",
      "time_since_restore: 792.1440613269806\n",
      "time_this_iter_s: 5.279770851135254\n",
      "time_total_s: 792.1440613269806\n",
      "timers:\n",
      "  learn_throughput: 2141.717\n",
      "  learn_time_ms: 1867.661\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1157.687\n",
      "  sample_time_ms: 3455.166\n",
      "  update_time_ms: 1.658\n",
      "timestamp: 1633509659\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 588000\n",
      "training_iteration: 147\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:147 starting ! -----------------\n",
      "agent_timesteps_total: 592000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-41-17\n",
      "done: false\n",
      "episode_len_mean: 665.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.4017275000000014\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 428\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.134425163269043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01052964199334383\n",
      "        model: {}\n",
      "        policy_loss: -0.018874166533350945\n",
      "        total_loss: 0.0041861990466713905\n",
      "        vf_explained_var: -0.031353384256362915\n",
      "        vf_loss: 0.02095443196594715\n",
      "  num_agent_steps_sampled: 592000\n",
      "  num_agent_steps_trained: 592000\n",
      "  num_steps_sampled: 592000\n",
      "  num_steps_trained: 592000\n",
      "iterations_since_restore: 148\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.923076923076923\n",
      "  ram_util_percent: 43.400000000000006\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04971268744250227\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7481798894933948\n",
      "  mean_inference_ms: 0.8516574302658129\n",
      "  mean_raw_obs_processing_ms: 0.08818593437545821\n",
      "time_since_restore: 797.5211980342865\n",
      "time_this_iter_s: 5.377136707305908\n",
      "time_total_s: 797.5211980342865\n",
      "timers:\n",
      "  learn_throughput: 2132.828\n",
      "  learn_time_ms: 1875.445\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1158.467\n",
      "  sample_time_ms: 3452.838\n",
      "  update_time_ms: 1.558\n",
      "timestamp: 1633509677\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 592000\n",
      "training_iteration: 148\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:148 starting ! -----------------\n",
      "agent_timesteps_total: 596000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-41-30\n",
      "done: false\n",
      "episode_len_mean: 672.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.29106550000000136\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 431\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1440966129302979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01244964450597763\n",
      "        model: {}\n",
      "        policy_loss: -0.018594112247228622\n",
      "        total_loss: -0.0015948731452226639\n",
      "        vf_explained_var: 0.023923521861433983\n",
      "        vf_loss: 0.014509310014545918\n",
      "  num_agent_steps_sampled: 596000\n",
      "  num_agent_steps_trained: 596000\n",
      "  num_steps_sampled: 596000\n",
      "  num_steps_trained: 596000\n",
      "iterations_since_restore: 149\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.316666666666666\n",
      "  ram_util_percent: 43.37777777777777\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04970901199575101\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7480789838930607\n",
      "  mean_inference_ms: 0.8516466053009907\n",
      "  mean_raw_obs_processing_ms: 0.08819358816517031\n",
      "time_since_restore: 802.8902506828308\n",
      "time_this_iter_s: 5.3690526485443115\n",
      "time_total_s: 802.8902506828308\n",
      "timers:\n",
      "  learn_throughput: 2133.477\n",
      "  learn_time_ms: 1874.874\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1156.447\n",
      "  sample_time_ms: 3458.869\n",
      "  update_time_ms: 1.459\n",
      "timestamp: 1633509690\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 596000\n",
      "training_iteration: 149\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:149 starting ! -----------------\n",
      "agent_timesteps_total: 600000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-41-54\n",
      "done: false\n",
      "episode_len_mean: 677.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.27175600000000133\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 434\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.276888370513916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009348719380795956\n",
      "        model: {}\n",
      "        policy_loss: -0.01834343746304512\n",
      "        total_loss: -0.008097867481410503\n",
      "        vf_explained_var: 0.21686698496341705\n",
      "        vf_loss: 0.00837582815438509\n",
      "  num_agent_steps_sampled: 600000\n",
      "  num_agent_steps_trained: 600000\n",
      "  num_steps_sampled: 600000\n",
      "  num_steps_trained: 600000\n",
      "iterations_since_restore: 150\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.56470588235294\n",
      "  ram_util_percent: 43.35294117647059\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04970654755281581\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7479766279392431\n",
      "  mean_inference_ms: 0.8516356570500325\n",
      "  mean_raw_obs_processing_ms: 0.08820000801266041\n",
      "time_since_restore: 808.2849721908569\n",
      "time_this_iter_s: 5.394721508026123\n",
      "time_total_s: 808.2849721908569\n",
      "timers:\n",
      "  learn_throughput: 2124.678\n",
      "  learn_time_ms: 1882.638\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1154.873\n",
      "  sample_time_ms: 3463.585\n",
      "  update_time_ms: 1.46\n",
      "timestamp: 1633509714\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 600000\n",
      "training_iteration: 150\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:150 starting ! -----------------\n",
      "agent_timesteps_total: 604000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-42-17\n",
      "done: false\n",
      "episode_len_mean: 674.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.25184000000000123\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 436\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3311620950698853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01185584720224142\n",
      "        model: {}\n",
      "        policy_loss: -0.026247741654515266\n",
      "        total_loss: -0.02120809070765972\n",
      "        vf_explained_var: 0.06402864307165146\n",
      "        vf_loss: 0.0026684857439249754\n",
      "  num_agent_steps_sampled: 604000\n",
      "  num_agent_steps_trained: 604000\n",
      "  num_steps_sampled: 604000\n",
      "  num_steps_trained: 604000\n",
      "iterations_since_restore: 151\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.881249999999998\n",
      "  ram_util_percent: 43.39375\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04970496391663277\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.747905881071489\n",
      "  mean_inference_ms: 0.8516290367059394\n",
      "  mean_raw_obs_processing_ms: 0.08820377548200607\n",
      "time_since_restore: 813.5417311191559\n",
      "time_this_iter_s: 5.25675892829895\n",
      "time_total_s: 813.5417311191559\n",
      "timers:\n",
      "  learn_throughput: 2129.956\n",
      "  learn_time_ms: 1877.973\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1156.364\n",
      "  sample_time_ms: 3459.12\n",
      "  update_time_ms: 1.56\n",
      "timestamp: 1633509737\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 604000\n",
      "training_iteration: 151\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:151 starting ! -----------------\n",
      "agent_timesteps_total: 608000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-42-30\n",
      "done: false\n",
      "episode_len_mean: 680.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.3108725000000013\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 439\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1622599363327026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010248910635709763\n",
      "        model: {}\n",
      "        policy_loss: -0.021902740001678467\n",
      "        total_loss: -0.010273231193423271\n",
      "        vf_explained_var: 0.1587573140859604\n",
      "        vf_loss: 0.009579726494848728\n",
      "  num_agent_steps_sampled: 608000\n",
      "  num_agent_steps_trained: 608000\n",
      "  num_steps_sampled: 608000\n",
      "  num_steps_trained: 608000\n",
      "iterations_since_restore: 152\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.511111111111106\n",
      "  ram_util_percent: 43.338888888888874\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04969985127742613\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7477928784057083\n",
      "  mean_inference_ms: 0.8516074705546046\n",
      "  mean_raw_obs_processing_ms: 0.08821069230754658\n",
      "time_since_restore: 818.8647165298462\n",
      "time_this_iter_s: 5.322985410690308\n",
      "time_total_s: 818.8647165298462\n",
      "timers:\n",
      "  learn_throughput: 2129.526\n",
      "  learn_time_ms: 1878.353\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1154.029\n",
      "  sample_time_ms: 3466.118\n",
      "  update_time_ms: 1.597\n",
      "timestamp: 1633509750\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 608000\n",
      "training_iteration: 152\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:152 starting ! -----------------\n",
      "agent_timesteps_total: 612000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-42-44\n",
      "done: false\n",
      "episode_len_mean: 684.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.31091700000000133\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 441\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2594584226608276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012393992394208908\n",
      "        model: {}\n",
      "        policy_loss: -0.01635557785630226\n",
      "        total_loss: -0.0036241880152374506\n",
      "        vf_explained_var: 0.22101955115795135\n",
      "        vf_loss: 0.010252594947814941\n",
      "  num_agent_steps_sampled: 612000\n",
      "  num_agent_steps_trained: 612000\n",
      "  num_steps_sampled: 612000\n",
      "  num_steps_trained: 612000\n",
      "iterations_since_restore: 153\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.615\n",
      "  ram_util_percent: 43.394999999999996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049695936498806485\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7477127034692574\n",
      "  mean_inference_ms: 0.8515931172519049\n",
      "  mean_raw_obs_processing_ms: 0.0882145243277489\n",
      "time_since_restore: 824.1235282421112\n",
      "time_this_iter_s: 5.258811712265015\n",
      "time_total_s: 824.1235282421112\n",
      "timers:\n",
      "  learn_throughput: 2136.874\n",
      "  learn_time_ms: 1871.893\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1158.021\n",
      "  sample_time_ms: 3454.167\n",
      "  update_time_ms: 1.597\n",
      "timestamp: 1633509764\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 612000\n",
      "training_iteration: 153\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:153 starting ! -----------------\n",
      "agent_timesteps_total: 616000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-43-01\n",
      "done: false\n",
      "episode_len_mean: 691.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.2411310000000012\n",
      "episode_reward_min: -2.171750000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 444\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3048982620239258\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011839274317026138\n",
      "        model: {}\n",
      "        policy_loss: -0.016547808423638344\n",
      "        total_loss: -0.0052186837419867516\n",
      "        vf_explained_var: -0.22392159700393677\n",
      "        vf_loss: 0.0089612677693367\n",
      "  num_agent_steps_sampled: 616000\n",
      "  num_agent_steps_trained: 616000\n",
      "  num_steps_sampled: 616000\n",
      "  num_steps_trained: 616000\n",
      "iterations_since_restore: 154\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.065217391304348\n",
      "  ram_util_percent: 43.39130434782608\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04968779732685118\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7475899076796455\n",
      "  mean_inference_ms: 0.851570531272739\n",
      "  mean_raw_obs_processing_ms: 0.08821937300808796\n",
      "time_since_restore: 829.3771283626556\n",
      "time_this_iter_s: 5.253600120544434\n",
      "time_total_s: 829.3771283626556\n",
      "timers:\n",
      "  learn_throughput: 2139.021\n",
      "  learn_time_ms: 1870.014\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1158.373\n",
      "  sample_time_ms: 3453.119\n",
      "  update_time_ms: 1.597\n",
      "timestamp: 1633509781\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 616000\n",
      "training_iteration: 154\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:154 starting ! -----------------\n",
      "agent_timesteps_total: 620000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-43-17\n",
      "done: false\n",
      "episode_len_mean: 697.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.12046750000000124\n",
      "episode_reward_min: -2.179999999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 447\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2424269914627075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011359109543263912\n",
      "        model: {}\n",
      "        policy_loss: -0.015411784872412682\n",
      "        total_loss: 0.0031302769202739\n",
      "        vf_explained_var: -0.05138184875249863\n",
      "        vf_loss: 0.016270238906145096\n",
      "  num_agent_steps_sampled: 620000\n",
      "  num_agent_steps_trained: 620000\n",
      "  num_steps_sampled: 620000\n",
      "  num_steps_trained: 620000\n",
      "iterations_since_restore: 155\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.958333333333332\n",
      "  ram_util_percent: 43.458333333333336\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049681208607623484\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7474620987362772\n",
      "  mean_inference_ms: 0.8515468403591017\n",
      "  mean_raw_obs_processing_ms: 0.08822650052220919\n",
      "time_since_restore: 834.6559767723083\n",
      "time_this_iter_s: 5.27884840965271\n",
      "time_total_s: 834.6559767723083\n",
      "timers:\n",
      "  learn_throughput: 2144.536\n",
      "  learn_time_ms: 1865.205\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1161.52\n",
      "  sample_time_ms: 3443.763\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633509797\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 620000\n",
      "training_iteration: 155\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:155 starting ! -----------------\n",
      "agent_timesteps_total: 624000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-43-31\n",
      "done: false\n",
      "episode_len_mean: 696.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.1009640000000011\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 449\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.200956106185913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012156404554843903\n",
      "        model: {}\n",
      "        policy_loss: -0.017172647640109062\n",
      "        total_loss: 0.0006960007594898343\n",
      "        vf_explained_var: 0.17724327743053436\n",
      "        vf_loss: 0.015437372028827667\n",
      "  num_agent_steps_sampled: 624000\n",
      "  num_agent_steps_trained: 624000\n",
      "  num_steps_sampled: 624000\n",
      "  num_steps_trained: 624000\n",
      "iterations_since_restore: 156\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.04736842105263\n",
      "  ram_util_percent: 43.39473684210526\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049676199591305696\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7473791980623057\n",
      "  mean_inference_ms: 0.8515294589829726\n",
      "  mean_raw_obs_processing_ms: 0.08823123745649905\n",
      "time_since_restore: 839.9967393875122\n",
      "time_this_iter_s: 5.340762615203857\n",
      "time_total_s: 839.9967393875122\n",
      "timers:\n",
      "  learn_throughput: 2144.875\n",
      "  learn_time_ms: 1864.911\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1162.437\n",
      "  sample_time_ms: 3441.048\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633509811\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 624000\n",
      "training_iteration: 156\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:156 starting ! -----------------\n",
      "agent_timesteps_total: 628000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-43-48\n",
      "done: false\n",
      "episode_len_mean: 703.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: 0.04085900000000108\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 452\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1800639629364014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012121554464101791\n",
      "        model: {}\n",
      "        policy_loss: -0.017119862139225006\n",
      "        total_loss: -0.0069201355800032616\n",
      "        vf_explained_var: 0.1421317607164383\n",
      "        vf_loss: 0.007775416132062674\n",
      "  num_agent_steps_sampled: 628000\n",
      "  num_agent_steps_trained: 628000\n",
      "  num_steps_sampled: 628000\n",
      "  num_steps_trained: 628000\n",
      "iterations_since_restore: 157\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.662500000000005\n",
      "  ram_util_percent: 43.425000000000004\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049666749581638595\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472421520335034\n",
      "  mean_inference_ms: 0.8515017747476482\n",
      "  mean_raw_obs_processing_ms: 0.08823787464286513\n",
      "time_since_restore: 845.4148414134979\n",
      "time_this_iter_s: 5.418102025985718\n",
      "time_total_s: 845.4148414134979\n",
      "timers:\n",
      "  learn_throughput: 2129.184\n",
      "  learn_time_ms: 1878.654\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1162.444\n",
      "  sample_time_ms: 3441.027\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633509828\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 628000\n",
      "training_iteration: 157\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:157 starting ! -----------------\n",
      "agent_timesteps_total: 632000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-44-01\n",
      "done: false\n",
      "episode_len_mean: 711.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.916999999999999\n",
      "episode_reward_mean: -0.0693304999999989\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 454\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3588294982910156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009366219863295555\n",
      "        model: {}\n",
      "        policy_loss: -0.019629767164587975\n",
      "        total_loss: -0.004536549560725689\n",
      "        vf_explained_var: 0.3528241515159607\n",
      "        vf_loss: 0.013219974935054779\n",
      "  num_agent_steps_sampled: 632000\n",
      "  num_agent_steps_trained: 632000\n",
      "  num_steps_sampled: 632000\n",
      "  num_steps_trained: 632000\n",
      "iterations_since_restore: 158\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.216666666666665\n",
      "  ram_util_percent: 43.36111111111111\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04966020788359149\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7471392192610175\n",
      "  mean_inference_ms: 0.8514833237631669\n",
      "  mean_raw_obs_processing_ms: 0.08824215075030445\n",
      "time_since_restore: 850.7359228134155\n",
      "time_this_iter_s: 5.3210813999176025\n",
      "time_total_s: 850.7359228134155\n",
      "timers:\n",
      "  learn_throughput: 2131.936\n",
      "  learn_time_ms: 1876.229\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1163.51\n",
      "  sample_time_ms: 3437.874\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633509841\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 632000\n",
      "training_iteration: 158\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:158 starting ! -----------------\n",
      "agent_timesteps_total: 636000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-44-20\n",
      "done: false\n",
      "episode_len_mean: 716.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.12878999999999888\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 457\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0702894926071167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009288116358220577\n",
      "        model: {}\n",
      "        policy_loss: -0.01586628518998623\n",
      "        total_loss: 0.018741440027952194\n",
      "        vf_explained_var: 0.22203579545021057\n",
      "        vf_loss: 0.03275011107325554\n",
      "  num_agent_steps_sampled: 636000\n",
      "  num_agent_steps_trained: 636000\n",
      "  num_steps_sampled: 636000\n",
      "  num_steps_trained: 636000\n",
      "iterations_since_restore: 159\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.03333333333333\n",
      "  ram_util_percent: 43.403703703703705\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04964941578382904\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7469845068213233\n",
      "  mean_inference_ms: 0.8514561499216612\n",
      "  mean_raw_obs_processing_ms: 0.08824773106145584\n",
      "time_since_restore: 856.1835720539093\n",
      "time_this_iter_s: 5.447649240493774\n",
      "time_total_s: 856.1835720539093\n",
      "timers:\n",
      "  learn_throughput: 2125.622\n",
      "  learn_time_ms: 1881.802\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1162.792\n",
      "  sample_time_ms: 3439.996\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633509860\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 636000\n",
      "training_iteration: 159\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:159 starting ! -----------------\n",
      "agent_timesteps_total: 640000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-44-44\n",
      "done: false\n",
      "episode_len_mean: 721.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.13902299999999898\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 459\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1683964729309082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010271342471241951\n",
      "        model: {}\n",
      "        policy_loss: -0.015483703464269638\n",
      "        total_loss: 0.009341923519968987\n",
      "        vf_explained_var: -0.12239714711904526\n",
      "        vf_loss: 0.022771351039409637\n",
      "  num_agent_steps_sampled: 640000\n",
      "  num_agent_steps_trained: 640000\n",
      "  num_steps_sampled: 640000\n",
      "  num_steps_trained: 640000\n",
      "iterations_since_restore: 160\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.329411764705885\n",
      "  ram_util_percent: 43.38529411764706\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04964230815947093\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7468871894641262\n",
      "  mean_inference_ms: 0.8514439138951366\n",
      "  mean_raw_obs_processing_ms: 0.08825245965957254\n",
      "time_since_restore: 861.8701775074005\n",
      "time_this_iter_s: 5.686605453491211\n",
      "time_total_s: 861.8701775074005\n",
      "timers:\n",
      "  learn_throughput: 2111.505\n",
      "  learn_time_ms: 1894.383\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1157.184\n",
      "  sample_time_ms: 3456.667\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633509884\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 640000\n",
      "training_iteration: 160\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:160 starting ! -----------------\n",
      "agent_timesteps_total: 644000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-45-03\n",
      "done: false\n",
      "episode_len_mean: 717.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.13931599999999889\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 462\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1594083309173584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009064274840056896\n",
      "        model: {}\n",
      "        policy_loss: -0.01568211242556572\n",
      "        total_loss: -0.0068799639120697975\n",
      "        vf_explained_var: 0.08113566786050797\n",
      "        vf_loss: 0.006989290472120047\n",
      "  num_agent_steps_sampled: 644000\n",
      "  num_agent_steps_trained: 644000\n",
      "  num_steps_sampled: 644000\n",
      "  num_steps_trained: 644000\n",
      "iterations_since_restore: 161\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.011111111111113\n",
      "  ram_util_percent: 43.407407407407426\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049632603760945075\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7467502127271628\n",
      "  mean_inference_ms: 0.8514297974991204\n",
      "  mean_raw_obs_processing_ms: 0.08825921909928493\n",
      "time_since_restore: 867.369989156723\n",
      "time_this_iter_s: 5.49981164932251\n",
      "time_total_s: 867.369989156723\n",
      "timers:\n",
      "  learn_throughput: 2105.294\n",
      "  learn_time_ms: 1899.972\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.942\n",
      "  sample_time_ms: 3475.414\n",
      "  update_time_ms: 1.676\n",
      "timestamp: 1633509903\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 644000\n",
      "training_iteration: 161\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:161 starting ! -----------------\n",
      "agent_timesteps_total: 648000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-45-23\n",
      "done: false\n",
      "episode_len_mean: 714.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.1399174999999988\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 464\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1634507179260254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013577945530414581\n",
      "        model: {}\n",
      "        policy_loss: -0.018253853544592857\n",
      "        total_loss: -0.001502663129940629\n",
      "        vf_explained_var: 0.05051315575838089\n",
      "        vf_loss: 0.01403560396283865\n",
      "  num_agent_steps_sampled: 648000\n",
      "  num_agent_steps_trained: 648000\n",
      "  num_steps_sampled: 648000\n",
      "  num_steps_trained: 648000\n",
      "iterations_since_restore: 162\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.992592592592587\n",
      "  ram_util_percent: 43.414814814814825\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049626985312413725\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7466631279681252\n",
      "  mean_inference_ms: 0.851420172635461\n",
      "  mean_raw_obs_processing_ms: 0.08826431971308502\n",
      "time_since_restore: 872.6448376178741\n",
      "time_this_iter_s: 5.274848461151123\n",
      "time_total_s: 872.6448376178741\n",
      "timers:\n",
      "  learn_throughput: 2106.31\n",
      "  learn_time_ms: 1899.056\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1152.297\n",
      "  sample_time_ms: 3471.327\n",
      "  update_time_ms: 1.775\n",
      "timestamp: 1633509923\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 648000\n",
      "training_iteration: 162\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:162 starting ! -----------------\n",
      "agent_timesteps_total: 652000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-45-43\n",
      "done: false\n",
      "episode_len_mean: 720.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.1503274999999988\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 467\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2568789720535278\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012933480553328991\n",
      "        model: {}\n",
      "        policy_loss: -0.02599893882870674\n",
      "        total_loss: -0.008782997727394104\n",
      "        vf_explained_var: -0.12934717535972595\n",
      "        vf_loss: 0.014629238285124302\n",
      "  num_agent_steps_sampled: 652000\n",
      "  num_agent_steps_trained: 652000\n",
      "  num_steps_sampled: 652000\n",
      "  num_steps_trained: 652000\n",
      "iterations_since_restore: 163\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.653571428571432\n",
      "  ram_util_percent: 43.39285714285716\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04961928615065538\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7465422058080997\n",
      "  mean_inference_ms: 0.8514038469907615\n",
      "  mean_raw_obs_processing_ms: 0.08827170094939227\n",
      "time_since_restore: 877.94291639328\n",
      "time_this_iter_s: 5.298078775405884\n",
      "time_total_s: 877.94291639328\n",
      "timers:\n",
      "  learn_throughput: 2106.313\n",
      "  learn_time_ms: 1899.053\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.982\n",
      "  sample_time_ms: 3475.293\n",
      "  update_time_ms: 1.676\n",
      "timestamp: 1633509943\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 652000\n",
      "training_iteration: 163\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:163 starting ! -----------------\n",
      "agent_timesteps_total: 656000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-45-56\n",
      "done: false\n",
      "episode_len_mean: 726.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.2011859999999986\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 469\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1049715280532837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010348847135901451\n",
      "        model: {}\n",
      "        policy_loss: -0.017315149307250977\n",
      "        total_loss: -0.007184379734098911\n",
      "        vf_explained_var: 0.339486688375473\n",
      "        vf_loss: 0.008060996420681477\n",
      "  num_agent_steps_sampled: 656000\n",
      "  num_agent_steps_trained: 656000\n",
      "  num_steps_sampled: 656000\n",
      "  num_steps_trained: 656000\n",
      "iterations_since_restore: 164\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.347368421052632\n",
      "  ram_util_percent: 43.431578947368415\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04961305544503439\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7464608028726565\n",
      "  mean_inference_ms: 0.8513916287512023\n",
      "  mean_raw_obs_processing_ms: 0.08827844230977874\n",
      "time_since_restore: 883.2504742145538\n",
      "time_this_iter_s: 5.307557821273804\n",
      "time_total_s: 883.2504742145538\n",
      "timers:\n",
      "  learn_throughput: 2105.548\n",
      "  learn_time_ms: 1899.743\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.398\n",
      "  sample_time_ms: 3480.083\n",
      "  update_time_ms: 1.775\n",
      "timestamp: 1633509956\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 656000\n",
      "training_iteration: 164\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:164 starting ! -----------------\n",
      "agent_timesteps_total: 660000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-46-19\n",
      "done: false\n",
      "episode_len_mean: 730.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.3222449999999986\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 472\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.112728238105774\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01059434749186039\n",
      "        model: {}\n",
      "        policy_loss: -0.017331920564174652\n",
      "        total_loss: -0.002604136010631919\n",
      "        vf_explained_var: -0.09703706949949265\n",
      "        vf_loss: 0.01260891743004322\n",
      "  num_agent_steps_sampled: 660000\n",
      "  num_agent_steps_trained: 660000\n",
      "  num_steps_sampled: 660000\n",
      "  num_steps_trained: 660000\n",
      "iterations_since_restore: 165\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.76666666666667\n",
      "  ram_util_percent: 43.3969696969697\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04960302569013339\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7463519466883213\n",
      "  mean_inference_ms: 0.851373271060844\n",
      "  mean_raw_obs_processing_ms: 0.08828959637582832\n",
      "time_since_restore: 888.6622602939606\n",
      "time_this_iter_s: 5.411786079406738\n",
      "time_total_s: 888.6622602939606\n",
      "timers:\n",
      "  learn_throughput: 2102.076\n",
      "  learn_time_ms: 1902.88\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.14\n",
      "  sample_time_ms: 3489.974\n",
      "  update_time_ms: 1.876\n",
      "timestamp: 1633509979\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 660000\n",
      "training_iteration: 165\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:165 starting ! -----------------\n",
      "agent_timesteps_total: 664000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-46-43\n",
      "done: false\n",
      "episode_len_mean: 729.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.27220299999999864\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 475\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1660557985305786\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009342367760837078\n",
      "        model: {}\n",
      "        policy_loss: -0.017580442130565643\n",
      "        total_loss: -0.0041137635707855225\n",
      "        vf_explained_var: 0.13675548136234283\n",
      "        vf_loss: 0.011598209850490093\n",
      "  num_agent_steps_sampled: 664000\n",
      "  num_agent_steps_trained: 664000\n",
      "  num_steps_sampled: 664000\n",
      "  num_steps_trained: 664000\n",
      "iterations_since_restore: 166\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.271875\n",
      "  ram_util_percent: 43.409375000000004\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04959223654649\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7462431477143465\n",
      "  mean_inference_ms: 0.8513527115268315\n",
      "  mean_raw_obs_processing_ms: 0.08830051959953023\n",
      "time_since_restore: 893.9561994075775\n",
      "time_this_iter_s: 5.293939113616943\n",
      "time_total_s: 893.9561994075775\n",
      "timers:\n",
      "  learn_throughput: 2098.93\n",
      "  learn_time_ms: 1905.733\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.621\n",
      "  sample_time_ms: 3482.437\n",
      "  update_time_ms: 1.876\n",
      "timestamp: 1633510003\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 664000\n",
      "training_iteration: 166\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:166 starting ! -----------------\n",
      "agent_timesteps_total: 668000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-47-03\n",
      "done: false\n",
      "episode_len_mean: 729.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.2622379999999986\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 477\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0728003978729248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0079253064468503\n",
      "        model: {}\n",
      "        policy_loss: -0.017149537801742554\n",
      "        total_loss: 0.0013872464187443256\n",
      "        vf_explained_var: -0.02058420144021511\n",
      "        vf_loss: 0.016951721161603928\n",
      "  num_agent_steps_sampled: 668000\n",
      "  num_agent_steps_trained: 668000\n",
      "  num_steps_sampled: 668000\n",
      "  num_steps_trained: 668000\n",
      "iterations_since_restore: 167\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.57931034482759\n",
      "  ram_util_percent: 43.441379310344836\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04958543425972703\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7461706290633751\n",
      "  mean_inference_ms: 0.8513376409995552\n",
      "  mean_raw_obs_processing_ms: 0.0883074813300339\n",
      "time_since_restore: 899.3109991550446\n",
      "time_this_iter_s: 5.354799747467041\n",
      "time_total_s: 899.3109991550446\n",
      "timers:\n",
      "  learn_throughput: 2113.417\n",
      "  learn_time_ms: 1892.669\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.364\n",
      "  sample_time_ms: 3489.292\n",
      "  update_time_ms: 1.909\n",
      "timestamp: 1633510023\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 668000\n",
      "training_iteration: 167\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:167 starting ! -----------------\n",
      "agent_timesteps_total: 672000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-47-22\n",
      "done: false\n",
      "episode_len_mean: 731.91\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.32255799999999857\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 481\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1831964254379272\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010631452314555645\n",
      "        model: {}\n",
      "        policy_loss: -0.01601683720946312\n",
      "        total_loss: 0.0037604114040732384\n",
      "        vf_explained_var: -0.24154922366142273\n",
      "        vf_loss: 0.017650961875915527\n",
      "  num_agent_steps_sampled: 672000\n",
      "  num_agent_steps_trained: 672000\n",
      "  num_steps_sampled: 672000\n",
      "  num_steps_trained: 672000\n",
      "iterations_since_restore: 168\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.222222222222225\n",
      "  ram_util_percent: 43.433333333333344\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04957239928958905\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7460217761482829\n",
      "  mean_inference_ms: 0.8513023452505871\n",
      "  mean_raw_obs_processing_ms: 0.08831896514410116\n",
      "time_since_restore: 904.617625951767\n",
      "time_this_iter_s: 5.306626796722412\n",
      "time_total_s: 904.617625951767\n",
      "timers:\n",
      "  learn_throughput: 2120.682\n",
      "  learn_time_ms: 1886.186\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.757\n",
      "  sample_time_ms: 3494.192\n",
      "  update_time_ms: 2.085\n",
      "timestamp: 1633510042\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 672000\n",
      "training_iteration: 168\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:168 starting ! -----------------\n",
      "agent_timesteps_total: 676000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-47-36\n",
      "done: false\n",
      "episode_len_mean: 734.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.3921249999999986\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 483\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1086426973342896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010107841342687607\n",
      "        model: {}\n",
      "        policy_loss: -0.017227984964847565\n",
      "        total_loss: 0.0015992472181096673\n",
      "        vf_explained_var: -0.009680901654064655\n",
      "        vf_loss: 0.016805661842226982\n",
      "  num_agent_steps_sampled: 676000\n",
      "  num_agent_steps_trained: 676000\n",
      "  num_steps_sampled: 676000\n",
      "  num_steps_trained: 676000\n",
      "iterations_since_restore: 169\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.13157894736842\n",
      "  ram_util_percent: 43.40526315789473\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049566242921036244\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.745952659120423\n",
      "  mean_inference_ms: 0.8512845343706317\n",
      "  mean_raw_obs_processing_ms: 0.08832221370097408\n",
      "time_since_restore: 910.0878212451935\n",
      "time_this_iter_s: 5.470195293426514\n",
      "time_total_s: 910.0878212451935\n",
      "timers:\n",
      "  learn_throughput: 2119.325\n",
      "  learn_time_ms: 1887.394\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.398\n",
      "  sample_time_ms: 3495.289\n",
      "  update_time_ms: 2.185\n",
      "timestamp: 1633510056\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 676000\n",
      "training_iteration: 169\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:169 starting ! -----------------\n",
      "agent_timesteps_total: 680000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-47-59\n",
      "done: false\n",
      "episode_len_mean: 734.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.3822989999999987\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 486\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2107889652252197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010539178736507893\n",
      "        model: {}\n",
      "        policy_loss: -0.023037679493427277\n",
      "        total_loss: -0.006753732915967703\n",
      "        vf_explained_var: 0.1022670567035675\n",
      "        vf_loss: 0.014176110737025738\n",
      "  num_agent_steps_sampled: 680000\n",
      "  num_agent_steps_trained: 680000\n",
      "  num_steps_sampled: 680000\n",
      "  num_steps_trained: 680000\n",
      "iterations_since_restore: 170\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.312121212121212\n",
      "  ram_util_percent: 43.43030303030303\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04955675893789783\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.745846856898575\n",
      "  mean_inference_ms: 0.8512608923873494\n",
      "  mean_raw_obs_processing_ms: 0.08832620422867211\n",
      "time_since_restore: 915.3902409076691\n",
      "time_this_iter_s: 5.302419662475586\n",
      "time_total_s: 915.3902409076691\n",
      "timers:\n",
      "  learn_throughput: 2144.413\n",
      "  learn_time_ms: 1865.312\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.736\n",
      "  sample_time_ms: 3479.059\n",
      "  update_time_ms: 2.085\n",
      "timestamp: 1633510079\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 680000\n",
      "training_iteration: 170\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:170 starting ! -----------------\n",
      "agent_timesteps_total: 684000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-48-14\n",
      "done: false\n",
      "episode_len_mean: 745.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.4924014999999987\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 489\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2016687393188477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01121972780674696\n",
      "        model: {}\n",
      "        policy_loss: -0.018222512677311897\n",
      "        total_loss: 0.004433728288859129\n",
      "        vf_explained_var: -0.13105632364749908\n",
      "        vf_loss: 0.020412301644682884\n",
      "  num_agent_steps_sampled: 684000\n",
      "  num_agent_steps_trained: 684000\n",
      "  num_steps_sampled: 684000\n",
      "  num_steps_trained: 684000\n",
      "iterations_since_restore: 171\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.366666666666667\n",
      "  ram_util_percent: 43.44285714285714\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0495462317682446\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.745740376416282\n",
      "  mean_inference_ms: 0.8512362899847468\n",
      "  mean_raw_obs_processing_ms: 0.08833193617087137\n",
      "time_since_restore: 920.7180480957031\n",
      "time_this_iter_s: 5.327807188034058\n",
      "time_total_s: 920.7180480957031\n",
      "timers:\n",
      "  learn_throughput: 2149.477\n",
      "  learn_time_ms: 1860.918\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1153.986\n",
      "  sample_time_ms: 3466.247\n",
      "  update_time_ms: 2.106\n",
      "timestamp: 1633510094\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 684000\n",
      "training_iteration: 171\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:171 starting ! -----------------\n",
      "agent_timesteps_total: 688000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-48-34\n",
      "done: false\n",
      "episode_len_mean: 745.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5534734999999986\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 492\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2555062770843506\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011728985235095024\n",
      "        model: {}\n",
      "        policy_loss: -0.02290579304099083\n",
      "        total_loss: -0.003776012919843197\n",
      "        vf_explained_var: -0.20216721296310425\n",
      "        vf_loss: 0.016783984377980232\n",
      "  num_agent_steps_sampled: 688000\n",
      "  num_agent_steps_trained: 688000\n",
      "  num_steps_sampled: 688000\n",
      "  num_steps_trained: 688000\n",
      "iterations_since_restore: 172\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.900000000000002\n",
      "  ram_util_percent: 43.45555555555556\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04953454490036615\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7456469966292617\n",
      "  mean_inference_ms: 0.8512120222064837\n",
      "  mean_raw_obs_processing_ms: 0.08833714469137723\n",
      "time_since_restore: 926.073178768158\n",
      "time_this_iter_s: 5.355130672454834\n",
      "time_total_s: 926.073178768158\n",
      "timers:\n",
      "  learn_throughput: 2148.563\n",
      "  learn_time_ms: 1861.71\n",
      "  load_throughput: 40108094.669\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1151.532\n",
      "  sample_time_ms: 3473.633\n",
      "  update_time_ms: 2.007\n",
      "timestamp: 1633510114\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 688000\n",
      "training_iteration: 172\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:172 starting ! -----------------\n",
      "agent_timesteps_total: 692000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-48-58\n",
      "done: false\n",
      "episode_len_mean: 740.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5543139999999985\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 494\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9434823393821716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00945038441568613\n",
      "        model: {}\n",
      "        policy_loss: -0.013997248373925686\n",
      "        total_loss: 0.000544488022569567\n",
      "        vf_explained_var: 0.025934631004929543\n",
      "        vf_loss: 0.012651653960347176\n",
      "  num_agent_steps_sampled: 692000\n",
      "  num_agent_steps_trained: 692000\n",
      "  num_steps_sampled: 692000\n",
      "  num_steps_trained: 692000\n",
      "iterations_since_restore: 173\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.164705882352937\n",
      "  ram_util_percent: 43.45294117647059\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049526094442256696\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7455940079476671\n",
      "  mean_inference_ms: 0.8511919058494888\n",
      "  mean_raw_obs_processing_ms: 0.08833997109239505\n",
      "time_since_restore: 931.4941856861115\n",
      "time_this_iter_s: 5.421006917953491\n",
      "time_total_s: 931.4941856861115\n",
      "timers:\n",
      "  learn_throughput: 2145.039\n",
      "  learn_time_ms: 1864.768\n",
      "  load_throughput: 40108094.669\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1148.497\n",
      "  sample_time_ms: 3482.811\n",
      "  update_time_ms: 2.105\n",
      "timestamp: 1633510138\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 692000\n",
      "training_iteration: 173\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:173 starting ! -----------------\n",
      "agent_timesteps_total: 696000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-49-14\n",
      "done: false\n",
      "episode_len_mean: 740.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5631149999999987\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 497\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2515795230865479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008364035747945309\n",
      "        model: {}\n",
      "        policy_loss: -0.0160745307803154\n",
      "        total_loss: -0.009440279565751553\n",
      "        vf_explained_var: -0.04177585616707802\n",
      "        vf_loss: 0.0049614435993134975\n",
      "  num_agent_steps_sampled: 696000\n",
      "  num_agent_steps_trained: 696000\n",
      "  num_steps_sampled: 696000\n",
      "  num_steps_trained: 696000\n",
      "iterations_since_restore: 174\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.04347826086956\n",
      "  ram_util_percent: 43.43043478260869\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04951076483435953\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7455072235102869\n",
      "  mean_inference_ms: 0.8511585367891967\n",
      "  mean_raw_obs_processing_ms: 0.08834479292239784\n",
      "time_since_restore: 936.8748786449432\n",
      "time_this_iter_s: 5.380692958831787\n",
      "time_total_s: 936.8748786449432\n",
      "timers:\n",
      "  learn_throughput: 2131.62\n",
      "  learn_time_ms: 1876.507\n",
      "  load_throughput: 40108094.669\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1150.003\n",
      "  sample_time_ms: 3478.252\n",
      "  update_time_ms: 2.006\n",
      "timestamp: 1633510154\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 696000\n",
      "training_iteration: 174\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:174 starting ! -----------------\n",
      "agent_timesteps_total: 700000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-49-35\n",
      "done: false\n",
      "episode_len_mean: 731.43\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5629249999999988\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 500\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1998482942581177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008476954884827137\n",
      "        model: {}\n",
      "        policy_loss: -0.01485280878841877\n",
      "        total_loss: 0.03222983703017235\n",
      "        vf_explained_var: -0.04861299321055412\n",
      "        vf_loss: 0.045387256890535355\n",
      "  num_agent_steps_sampled: 700000\n",
      "  num_agent_steps_trained: 700000\n",
      "  num_steps_sampled: 700000\n",
      "  num_steps_trained: 700000\n",
      "iterations_since_restore: 175\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.39\n",
      "  ram_util_percent: 43.483333333333334\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04949910512610168\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7454277205807414\n",
      "  mean_inference_ms: 0.8511249336382752\n",
      "  mean_raw_obs_processing_ms: 0.08834827602889643\n",
      "time_since_restore: 942.261388540268\n",
      "time_this_iter_s: 5.386509895324707\n",
      "time_total_s: 942.261388540268\n",
      "timers:\n",
      "  learn_throughput: 2134.195\n",
      "  learn_time_ms: 1874.243\n",
      "  load_throughput: 40108094.669\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1150.05\n",
      "  sample_time_ms: 3478.111\n",
      "  update_time_ms: 1.905\n",
      "timestamp: 1633510175\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 700000\n",
      "training_iteration: 175\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:175 starting ! -----------------\n",
      "agent_timesteps_total: 704000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-49-52\n",
      "done: false\n",
      "episode_len_mean: 727.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5142979999999987\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 503\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0788655281066895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008992154151201248\n",
      "        model: {}\n",
      "        policy_loss: -0.01655161939561367\n",
      "        total_loss: 0.0030005655717104673\n",
      "        vf_explained_var: -0.036535222083330154\n",
      "        vf_loss: 0.017753759399056435\n",
      "  num_agent_steps_sampled: 704000\n",
      "  num_agent_steps_trained: 704000\n",
      "  num_steps_sampled: 704000\n",
      "  num_steps_trained: 704000\n",
      "iterations_since_restore: 176\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.97391304347826\n",
      "  ram_util_percent: 43.439130434782605\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049488861038417424\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.74536803811711\n",
      "  mean_inference_ms: 0.851101418328303\n",
      "  mean_raw_obs_processing_ms: 0.08835148071673325\n",
      "time_since_restore: 947.8415014743805\n",
      "time_this_iter_s: 5.580112934112549\n",
      "time_total_s: 947.8415014743805\n",
      "timers:\n",
      "  learn_throughput: 2136.746\n",
      "  learn_time_ms: 1872.006\n",
      "  load_throughput: 40108094.669\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1139.904\n",
      "  sample_time_ms: 3509.066\n",
      "  update_time_ms: 1.905\n",
      "timestamp: 1633510192\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 704000\n",
      "training_iteration: 176\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:176 starting ! -----------------\n",
      "agent_timesteps_total: 708000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-50-15\n",
      "done: false\n",
      "episode_len_mean: 731.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5762394999999985\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 506\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9876477122306824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00961996242403984\n",
      "        model: {}\n",
      "        policy_loss: -0.020692631602287292\n",
      "        total_loss: 0.028736384585499763\n",
      "        vf_explained_var: -0.18387861549854279\n",
      "        vf_loss: 0.04750502482056618\n",
      "  num_agent_steps_sampled: 708000\n",
      "  num_agent_steps_trained: 708000\n",
      "  num_steps_sampled: 708000\n",
      "  num_steps_trained: 708000\n",
      "iterations_since_restore: 177\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.087878787878783\n",
      "  ram_util_percent: 43.42424242424242\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04947887672947922\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7453200045137457\n",
      "  mean_inference_ms: 0.8510817266106295\n",
      "  mean_raw_obs_processing_ms: 0.08835125190757688\n",
      "time_since_restore: 953.2574031352997\n",
      "time_this_iter_s: 5.4159016609191895\n",
      "time_total_s: 953.2574031352997\n",
      "timers:\n",
      "  learn_throughput: 2136.915\n",
      "  learn_time_ms: 1871.857\n",
      "  load_throughput: 20114154.178\n",
      "  load_time_ms: 0.199\n",
      "  sample_throughput: 1137.916\n",
      "  sample_time_ms: 3515.199\n",
      "  update_time_ms: 1.871\n",
      "timestamp: 1633510215\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 708000\n",
      "training_iteration: 177\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:177 starting ! -----------------\n",
      "agent_timesteps_total: 712000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-50-28\n",
      "done: false\n",
      "episode_len_mean: 721.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5255459999999986\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 509\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2110055685043335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013434814289212227\n",
      "        model: {}\n",
      "        policy_loss: -0.02407449670135975\n",
      "        total_loss: -0.010408876463770866\n",
      "        vf_explained_var: -0.239447683095932\n",
      "        vf_loss: 0.010978658683598042\n",
      "  num_agent_steps_sampled: 712000\n",
      "  num_agent_steps_trained: 712000\n",
      "  num_steps_sampled: 712000\n",
      "  num_steps_trained: 712000\n",
      "iterations_since_restore: 178\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.333333333333332\n",
      "  ram_util_percent: 43.42777777777778\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049471566433355\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7452721434941363\n",
      "  mean_inference_ms: 0.8510631355712885\n",
      "  mean_raw_obs_processing_ms: 0.08834947237040211\n",
      "time_since_restore: 958.5560421943665\n",
      "time_this_iter_s: 5.2986390590667725\n",
      "time_total_s: 958.5560421943665\n",
      "timers:\n",
      "  learn_throughput: 2134.403\n",
      "  learn_time_ms: 1874.06\n",
      "  load_throughput: 20114154.178\n",
      "  load_time_ms: 0.199\n",
      "  sample_throughput: 1138.856\n",
      "  sample_time_ms: 3512.297\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633510228\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 712000\n",
      "training_iteration: 178\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:178 starting ! -----------------\n",
      "agent_timesteps_total: 716000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-50-43\n",
      "done: false\n",
      "episode_len_mean: 721.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5157549999999985\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 513\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1572422981262207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01060427725315094\n",
      "        model: {}\n",
      "        policy_loss: -0.01772373355925083\n",
      "        total_loss: 0.0024760698433965445\n",
      "        vf_explained_var: 0.1127517893910408\n",
      "        vf_loss: 0.018078943714499474\n",
      "  num_agent_steps_sampled: 716000\n",
      "  num_agent_steps_trained: 716000\n",
      "  num_steps_sampled: 716000\n",
      "  num_steps_trained: 716000\n",
      "iterations_since_restore: 179\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.53809523809524\n",
      "  ram_util_percent: 43.43809523809523\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04946066600268658\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.74521994335012\n",
      "  mean_inference_ms: 0.8510404939265217\n",
      "  mean_raw_obs_processing_ms: 0.088345347230939\n",
      "time_since_restore: 963.9942965507507\n",
      "time_this_iter_s: 5.438254356384277\n",
      "time_total_s: 963.9942965507507\n",
      "timers:\n",
      "  learn_throughput: 2142.268\n",
      "  learn_time_ms: 1867.18\n",
      "  load_throughput: 20114154.178\n",
      "  load_time_ms: 0.199\n",
      "  sample_throughput: 1137.665\n",
      "  sample_time_ms: 3515.974\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633510243\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 716000\n",
      "training_iteration: 179\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:179 starting ! -----------------\n",
      "agent_timesteps_total: 720000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-51-00\n",
      "done: false\n",
      "episode_len_mean: 718.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.45586249999999856\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 515\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2542800903320312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010733036324381828\n",
      "        model: {}\n",
      "        policy_loss: -0.019864855334162712\n",
      "        total_loss: 0.012594741769134998\n",
      "        vf_explained_var: 0.2488245815038681\n",
      "        vf_loss: 0.030313000082969666\n",
      "  num_agent_steps_sampled: 720000\n",
      "  num_agent_steps_trained: 720000\n",
      "  num_steps_sampled: 720000\n",
      "  num_steps_trained: 720000\n",
      "iterations_since_restore: 180\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.995833333333337\n",
      "  ram_util_percent: 43.44583333333333\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049455589062782135\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451982124662219\n",
      "  mean_inference_ms: 0.8510301909084219\n",
      "  mean_raw_obs_processing_ms: 0.08834137985600382\n",
      "time_since_restore: 969.35107254982\n",
      "time_this_iter_s: 5.356775999069214\n",
      "time_total_s: 969.35107254982\n",
      "timers:\n",
      "  learn_throughput: 2136.686\n",
      "  learn_time_ms: 1872.058\n",
      "  load_throughput: 20114154.178\n",
      "  load_time_ms: 0.199\n",
      "  sample_throughput: 1137.513\n",
      "  sample_time_ms: 3516.443\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633510260\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 720000\n",
      "training_iteration: 180\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:180 starting ! -----------------\n",
      "agent_timesteps_total: 724000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-51-15\n",
      "done: false\n",
      "episode_len_mean: 726.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5581964999999984\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 518\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1403952836990356\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010672002099454403\n",
      "        model: {}\n",
      "        policy_loss: -0.01568268984556198\n",
      "        total_loss: 0.007261691149324179\n",
      "        vf_explained_var: -0.03723028302192688\n",
      "        vf_loss: 0.02080998197197914\n",
      "  num_agent_steps_sampled: 724000\n",
      "  num_agent_steps_trained: 724000\n",
      "  num_steps_sampled: 724000\n",
      "  num_steps_trained: 724000\n",
      "iterations_since_restore: 181\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.60454545454546\n",
      "  ram_util_percent: 43.40909090909091\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049449981933464476\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451635857415162\n",
      "  mean_inference_ms: 0.8510107995363293\n",
      "  mean_raw_obs_processing_ms: 0.0883355545096602\n",
      "time_since_restore: 974.7571852207184\n",
      "time_this_iter_s: 5.4061126708984375\n",
      "time_total_s: 974.7571852207184\n",
      "timers:\n",
      "  learn_throughput: 2128.963\n",
      "  learn_time_ms: 1878.849\n",
      "  load_throughput: 20114154.178\n",
      "  load_time_ms: 0.199\n",
      "  sample_throughput: 1137.235\n",
      "  sample_time_ms: 3517.303\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633510275\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 724000\n",
      "training_iteration: 181\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:181 starting ! -----------------\n",
      "agent_timesteps_total: 728000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-51-29\n",
      "done: false\n",
      "episode_len_mean: 728.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.6190024999999985\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 521\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1454904079437256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010371224954724312\n",
      "        model: {}\n",
      "        policy_loss: -0.01694122888147831\n",
      "        total_loss: 0.0017350524431094527\n",
      "        vf_explained_var: -0.07117778807878494\n",
      "        vf_loss: 0.016602037474513054\n",
      "  num_agent_steps_sampled: 728000\n",
      "  num_agent_steps_trained: 728000\n",
      "  num_steps_sampled: 728000\n",
      "  num_steps_trained: 728000\n",
      "iterations_since_restore: 182\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.36842105263158\n",
      "  ram_util_percent: 43.431578947368415\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049444153207620065\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451290144068196\n",
      "  mean_inference_ms: 0.8509920245675143\n",
      "  mean_raw_obs_processing_ms: 0.08833020311446489\n",
      "time_since_restore: 980.1579692363739\n",
      "time_this_iter_s: 5.400784015655518\n",
      "time_total_s: 980.1579692363739\n",
      "timers:\n",
      "  learn_throughput: 2126.478\n",
      "  learn_time_ms: 1881.045\n",
      "  load_throughput: 40349244.829\n",
      "  load_time_ms: 0.099\n",
      "  sample_throughput: 1136.498\n",
      "  sample_time_ms: 3519.585\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633510289\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 728000\n",
      "training_iteration: 182\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:182 starting ! -----------------\n",
      "agent_timesteps_total: 732000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-51-49\n",
      "done: false\n",
      "episode_len_mean: 737.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5589939999999985\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 523\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9879088401794434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006840821355581284\n",
      "        model: {}\n",
      "        policy_loss: -0.014876906760036945\n",
      "        total_loss: 0.0202218908816576\n",
      "        vf_explained_var: -0.27593544125556946\n",
      "        vf_loss: 0.033730629831552505\n",
      "  num_agent_steps_sampled: 732000\n",
      "  num_agent_steps_trained: 732000\n",
      "  num_steps_sampled: 732000\n",
      "  num_steps_trained: 732000\n",
      "iterations_since_restore: 183\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.275000000000002\n",
      "  ram_util_percent: 43.439285714285724\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04943982853669829\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451099579027902\n",
      "  mean_inference_ms: 0.8509816838236277\n",
      "  mean_raw_obs_processing_ms: 0.08832688678752781\n",
      "time_since_restore: 985.6430284976959\n",
      "time_this_iter_s: 5.4850592613220215\n",
      "time_total_s: 985.6430284976959\n",
      "timers:\n",
      "  learn_throughput: 2128.145\n",
      "  learn_time_ms: 1879.572\n",
      "  load_throughput: 40349244.829\n",
      "  load_time_ms: 0.099\n",
      "  sample_throughput: 1133.962\n",
      "  sample_time_ms: 3527.454\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633510309\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 732000\n",
      "training_iteration: 183\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:183 starting ! -----------------\n",
      "agent_timesteps_total: 736000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-52-13\n",
      "done: false\n",
      "episode_len_mean: 731.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5597599999999985\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 526\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1380864381790161\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010773107409477234\n",
      "        model: {}\n",
      "        policy_loss: -0.02945673279464245\n",
      "        total_loss: -0.01705561764538288\n",
      "        vf_explained_var: -0.14510679244995117\n",
      "        vf_loss: 0.010246493853628635\n",
      "  num_agent_steps_sampled: 736000\n",
      "  num_agent_steps_trained: 736000\n",
      "  num_steps_sampled: 736000\n",
      "  num_steps_trained: 736000\n",
      "iterations_since_restore: 184\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.884848484848483\n",
      "  ram_util_percent: 43.448484848484846\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04943314947335641\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450898589927607\n",
      "  mean_inference_ms: 0.850967802191396\n",
      "  mean_raw_obs_processing_ms: 0.08832003424903265\n",
      "time_since_restore: 991.06121134758\n",
      "time_this_iter_s: 5.418182849884033\n",
      "time_total_s: 991.06121134758\n",
      "timers:\n",
      "  learn_throughput: 2131.879\n",
      "  learn_time_ms: 1876.279\n",
      "  load_throughput: 40349244.829\n",
      "  load_time_ms: 0.099\n",
      "  sample_throughput: 1131.688\n",
      "  sample_time_ms: 3534.543\n",
      "  update_time_ms: 1.852\n",
      "timestamp: 1633510333\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 736000\n",
      "training_iteration: 184\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:184 starting ! -----------------\n",
      "agent_timesteps_total: 740000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-52-36\n",
      "done: false\n",
      "episode_len_mean: 733.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.6204074999999984\n",
      "episode_reward_min: -2.192050000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 529\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0945894718170166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009640839882194996\n",
      "        model: {}\n",
      "        policy_loss: -0.01641640067100525\n",
      "        total_loss: 0.005055269692093134\n",
      "        vf_explained_var: -0.176669642329216\n",
      "        vf_loss: 0.019543493166565895\n",
      "  num_agent_steps_sampled: 740000\n",
      "  num_agent_steps_trained: 740000\n",
      "  num_steps_sampled: 740000\n",
      "  num_steps_trained: 740000\n",
      "iterations_since_restore: 185\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.824242424242424\n",
      "  ram_util_percent: 43.46060606060607\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04942630515974619\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450750725676201\n",
      "  mean_inference_ms: 0.8509560949659861\n",
      "  mean_raw_obs_processing_ms: 0.08831153311069592\n",
      "time_since_restore: 996.4360427856445\n",
      "time_this_iter_s: 5.374831438064575\n",
      "time_total_s: 996.4360427856445\n",
      "timers:\n",
      "  learn_throughput: 2132.429\n",
      "  learn_time_ms: 1875.795\n",
      "  load_throughput: 40349244.829\n",
      "  load_time_ms: 0.099\n",
      "  sample_throughput: 1131.877\n",
      "  sample_time_ms: 3533.954\n",
      "  update_time_ms: 1.852\n",
      "timestamp: 1633510356\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 740000\n",
      "training_iteration: 185\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:185 starting ! -----------------\n",
      "agent_timesteps_total: 744000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-52-55\n",
      "done: false\n",
      "episode_len_mean: 737.59\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.6220629999999985\n",
      "episode_reward_min: -2.201249999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 531\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1758981943130493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01030999980866909\n",
      "        model: {}\n",
      "        policy_loss: -0.020259512588381767\n",
      "        total_loss: -0.0036949911154806614\n",
      "        vf_explained_var: 0.3525034785270691\n",
      "        vf_loss: 0.014502523466944695\n",
      "  num_agent_steps_sampled: 744000\n",
      "  num_agent_steps_trained: 744000\n",
      "  num_steps_sampled: 744000\n",
      "  num_steps_trained: 744000\n",
      "iterations_since_restore: 186\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.137037037037036\n",
      "  ram_util_percent: 43.48148148148148\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04942124284094309\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450638858698855\n",
      "  mean_inference_ms: 0.850948323409174\n",
      "  mean_raw_obs_processing_ms: 0.08830549151902871\n",
      "time_since_restore: 1001.7601153850555\n",
      "time_this_iter_s: 5.324072599411011\n",
      "time_total_s: 1001.7601153850555\n",
      "timers:\n",
      "  learn_throughput: 2132.179\n",
      "  learn_time_ms: 1876.015\n",
      "  load_throughput: 40349244.829\n",
      "  load_time_ms: 0.099\n",
      "  sample_throughput: 1140.219\n",
      "  sample_time_ms: 3508.097\n",
      "  update_time_ms: 1.752\n",
      "timestamp: 1633510375\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 744000\n",
      "training_iteration: 186\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:186 starting ! -----------------\n",
      "agent_timesteps_total: 748000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-53-19\n",
      "done: false\n",
      "episode_len_mean: 738.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.6733404999999982\n",
      "episode_reward_min: -2.201249999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 534\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1880481243133545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017508989199995995\n",
      "        model: {}\n",
      "        policy_loss: -0.024530712515115738\n",
      "        total_loss: -0.0059167505241930485\n",
      "        vf_explained_var: 0.059018395841121674\n",
      "        vf_loss: 0.015112165361642838\n",
      "  num_agent_steps_sampled: 748000\n",
      "  num_agent_steps_trained: 748000\n",
      "  num_steps_sampled: 748000\n",
      "  num_steps_trained: 748000\n",
      "iterations_since_restore: 187\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.766666666666666\n",
      "  ram_util_percent: 43.45757575757576\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04941420429660776\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.745049314095898\n",
      "  mean_inference_ms: 0.8509439884643509\n",
      "  mean_raw_obs_processing_ms: 0.08829493421907379\n",
      "time_since_restore: 1007.1991286277771\n",
      "time_this_iter_s: 5.439013242721558\n",
      "time_total_s: 1007.1991286277771\n",
      "timers:\n",
      "  learn_throughput: 2127.771\n",
      "  learn_time_ms: 1879.901\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.727\n",
      "  sample_time_ms: 3506.534\n",
      "  update_time_ms: 1.852\n",
      "timestamp: 1633510399\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 748000\n",
      "training_iteration: 187\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:187 starting ! -----------------\n",
      "agent_timesteps_total: 752000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-53-42\n",
      "done: false\n",
      "episode_len_mean: 733.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5438209999999981\n",
      "episode_reward_min: -2.201249999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 537\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9686856269836426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00941685400903225\n",
      "        model: {}\n",
      "        policy_loss: -0.0002782223164103925\n",
      "        total_loss: 0.013925396837294102\n",
      "        vf_explained_var: 0.3705572783946991\n",
      "        vf_loss: 0.012320243753492832\n",
      "  num_agent_steps_sampled: 752000\n",
      "  num_agent_steps_trained: 752000\n",
      "  num_steps_sampled: 752000\n",
      "  num_steps_trained: 752000\n",
      "iterations_since_restore: 188\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.196969696969695\n",
      "  ram_util_percent: 43.481818181818184\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04940796990306555\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450454693724718\n",
      "  mean_inference_ms: 0.8509426683470628\n",
      "  mean_raw_obs_processing_ms: 0.08828557728486419\n",
      "time_since_restore: 1012.6161720752716\n",
      "time_this_iter_s: 5.417043447494507\n",
      "time_total_s: 1012.6161720752716\n",
      "timers:\n",
      "  learn_throughput: 2131.102\n",
      "  learn_time_ms: 1876.963\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.946\n",
      "  sample_time_ms: 3521.295\n",
      "  update_time_ms: 1.952\n",
      "timestamp: 1633510422\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 752000\n",
      "training_iteration: 188\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:188 starting ! -----------------\n",
      "agent_timesteps_total: 756000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-54-00\n",
      "done: false\n",
      "episode_len_mean: 731.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5431674999999981\n",
      "episode_reward_min: -2.201249999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 540\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.038877248764038\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011955584399402142\n",
      "        model: {}\n",
      "        policy_loss: -0.020313890650868416\n",
      "        total_loss: -0.006674397271126509\n",
      "        vf_explained_var: 0.2604667544364929\n",
      "        vf_loss: 0.011248377151787281\n",
      "  num_agent_steps_sampled: 756000\n",
      "  num_agent_steps_trained: 756000\n",
      "  num_steps_sampled: 756000\n",
      "  num_steps_trained: 756000\n",
      "iterations_since_restore: 189\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.232\n",
      "  ram_util_percent: 43.48\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049402129187773286\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.745037592437288\n",
      "  mean_inference_ms: 0.8509426516583392\n",
      "  mean_raw_obs_processing_ms: 0.08827569143250688\n",
      "time_since_restore: 1017.8811268806458\n",
      "time_this_iter_s: 5.2649548053741455\n",
      "time_total_s: 1017.8811268806458\n",
      "timers:\n",
      "  learn_throughput: 2130.939\n",
      "  learn_time_ms: 1877.107\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.605\n",
      "  sample_time_ms: 3503.839\n",
      "  update_time_ms: 1.952\n",
      "timestamp: 1633510440\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 756000\n",
      "training_iteration: 189\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:189 starting ! -----------------\n",
      "agent_timesteps_total: 760000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-54-14\n",
      "done: false\n",
      "episode_len_mean: 729.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5525749999999983\n",
      "episode_reward_min: -2.201249999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 543\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1972661018371582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015029186382889748\n",
      "        model: {}\n",
      "        policy_loss: -0.01156151108443737\n",
      "        total_loss: 0.023274514824151993\n",
      "        vf_explained_var: 0.013787856325507164\n",
      "        vf_loss: 0.03183019161224365\n",
      "  num_agent_steps_sampled: 760000\n",
      "  num_agent_steps_trained: 760000\n",
      "  num_steps_sampled: 760000\n",
      "  num_steps_trained: 760000\n",
      "iterations_since_restore: 190\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.72\n",
      "  ram_util_percent: 43.489999999999995\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04939618507967259\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.745032608359015\n",
      "  mean_inference_ms: 0.8509464843825186\n",
      "  mean_raw_obs_processing_ms: 0.08826486042891114\n",
      "time_since_restore: 1023.1412925720215\n",
      "time_this_iter_s: 5.260165691375732\n",
      "time_total_s: 1023.1412925720215\n",
      "timers:\n",
      "  learn_throughput: 2136.334\n",
      "  learn_time_ms: 1872.366\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.191\n",
      "  sample_time_ms: 3498.978\n",
      "  update_time_ms: 1.952\n",
      "timestamp: 1633510454\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 760000\n",
      "training_iteration: 190\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:190 starting ! -----------------\n",
      "agent_timesteps_total: 764000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-54-28\n",
      "done: false\n",
      "episode_len_mean: 730.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5532764999999983\n",
      "episode_reward_min: -2.201249999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 546\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0339723825454712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009550503455102444\n",
      "        model: {}\n",
      "        policy_loss: -0.016282545402646065\n",
      "        total_loss: 0.011672494001686573\n",
      "        vf_explained_var: -0.018213551491498947\n",
      "        vf_loss: 0.0260449405759573\n",
      "  num_agent_steps_sampled: 764000\n",
      "  num_agent_steps_trained: 764000\n",
      "  num_steps_sampled: 764000\n",
      "  num_steps_trained: 764000\n",
      "iterations_since_restore: 191\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.015789473684215\n",
      "  ram_util_percent: 43.473684210526315\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0493885044588746\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450354961575886\n",
      "  mean_inference_ms: 0.8509514087079711\n",
      "  mean_raw_obs_processing_ms: 0.08825259702866979\n",
      "time_since_restore: 1028.4565699100494\n",
      "time_this_iter_s: 5.315277338027954\n",
      "time_total_s: 1028.4565699100494\n",
      "timers:\n",
      "  learn_throughput: 2145.325\n",
      "  learn_time_ms: 1864.52\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.527\n",
      "  sample_time_ms: 3497.949\n",
      "  update_time_ms: 1.909\n",
      "timestamp: 1633510468\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 764000\n",
      "training_iteration: 191\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:191 starting ! -----------------\n",
      "agent_timesteps_total: 768000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-54-44\n",
      "done: false\n",
      "episode_len_mean: 734.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5444439999999983\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 548\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0710370540618896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008473032154142857\n",
      "        model: {}\n",
      "        policy_loss: -0.01481656264513731\n",
      "        total_loss: 0.006069606635719538\n",
      "        vf_explained_var: 0.13760001957416534\n",
      "        vf_loss: 0.0191915612667799\n",
      "  num_agent_steps_sampled: 768000\n",
      "  num_agent_steps_trained: 768000\n",
      "  num_steps_sampled: 768000\n",
      "  num_steps_trained: 768000\n",
      "iterations_since_restore: 192\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.813043478260866\n",
      "  ram_util_percent: 43.49130434782609\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04938267468065835\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450425483913363\n",
      "  mean_inference_ms: 0.8509598079673362\n",
      "  mean_raw_obs_processing_ms: 0.08824582890392209\n",
      "time_since_restore: 1034.0886964797974\n",
      "time_this_iter_s: 5.632126569747925\n",
      "time_total_s: 1034.0886964797974\n",
      "timers:\n",
      "  learn_throughput: 2135.573\n",
      "  learn_time_ms: 1873.034\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.801\n",
      "  sample_time_ms: 3512.467\n",
      "  update_time_ms: 1.909\n",
      "timestamp: 1633510484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 768000\n",
      "training_iteration: 192\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:192 starting ! -----------------\n",
      "agent_timesteps_total: 772000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-54-58\n",
      "done: false\n",
      "episode_len_mean: 735.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.5440979999999982\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 550\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1514697074890137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011324433609843254\n",
      "        model: {}\n",
      "        policy_loss: -0.022737761959433556\n",
      "        total_loss: -0.0034968808759003878\n",
      "        vf_explained_var: -0.06862757354974747\n",
      "        vf_loss: 0.01697598770260811\n",
      "  num_agent_steps_sampled: 772000\n",
      "  num_agent_steps_trained: 772000\n",
      "  num_steps_sampled: 772000\n",
      "  num_steps_trained: 772000\n",
      "iterations_since_restore: 193\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.675\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04937643601796024\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450481024106037\n",
      "  mean_inference_ms: 0.8509668710275343\n",
      "  mean_raw_obs_processing_ms: 0.08823941853485742\n",
      "time_since_restore: 1039.3391637802124\n",
      "time_this_iter_s: 5.250467300415039\n",
      "time_total_s: 1039.3391637802124\n",
      "timers:\n",
      "  learn_throughput: 2136.989\n",
      "  learn_time_ms: 1871.793\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.006\n",
      "  sample_time_ms: 3490.383\n",
      "  update_time_ms: 1.91\n",
      "timestamp: 1633510498\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 772000\n",
      "training_iteration: 193\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:193 starting ! -----------------\n",
      "agent_timesteps_total: 776000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-55-10\n",
      "done: false\n",
      "episode_len_mean: 718.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9209500000000004\n",
      "episode_reward_mean: -0.49359999999999826\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 555\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1937206983566284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010467617772519588\n",
      "        model: {}\n",
      "        policy_loss: -0.021011024713516235\n",
      "        total_loss: 0.030461205169558525\n",
      "        vf_explained_var: 0.07901111245155334\n",
      "        vf_loss: 0.04937870427966118\n",
      "  num_agent_steps_sampled: 776000\n",
      "  num_agent_steps_trained: 776000\n",
      "  num_steps_sampled: 776000\n",
      "  num_steps_trained: 776000\n",
      "iterations_since_restore: 194\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.964705882352945\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04935913312567536\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450767156323237\n",
      "  mean_inference_ms: 0.8509879000960667\n",
      "  mean_raw_obs_processing_ms: 0.08822263610265789\n",
      "time_since_restore: 1044.7121765613556\n",
      "time_this_iter_s: 5.3730127811431885\n",
      "time_total_s: 1044.7121765613556\n",
      "timers:\n",
      "  learn_throughput: 2146.099\n",
      "  learn_time_ms: 1863.847\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.846\n",
      "  sample_time_ms: 3493.92\n",
      "  update_time_ms: 1.952\n",
      "timestamp: 1633510510\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 776000\n",
      "training_iteration: 194\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:194 starting ! -----------------\n",
      "agent_timesteps_total: 780000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-55-23\n",
      "done: false\n",
      "episode_len_mean: 719.91\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914099999999999\n",
      "episode_reward_mean: -0.5636209999999984\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 557\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2486377954483032\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01044362224638462\n",
      "        model: {}\n",
      "        policy_loss: -0.018606146797537804\n",
      "        total_loss: -0.0068132998421788216\n",
      "        vf_explained_var: 0.2675403654575348\n",
      "        vf_loss: 0.009704126976430416\n",
      "  num_agent_steps_sampled: 780000\n",
      "  num_agent_steps_trained: 780000\n",
      "  num_steps_sampled: 780000\n",
      "  num_steps_trained: 780000\n",
      "iterations_since_restore: 195\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.227777777777774\n",
      "  ram_util_percent: 43.455555555555556\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04935242247471075\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450875886303315\n",
      "  mean_inference_ms: 0.8509933263828064\n",
      "  mean_raw_obs_processing_ms: 0.08821562937828734\n",
      "time_since_restore: 1050.0630023479462\n",
      "time_this_iter_s: 5.350825786590576\n",
      "time_total_s: 1050.0630023479462\n",
      "timers:\n",
      "  learn_throughput: 2135.465\n",
      "  learn_time_ms: 1873.128\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.709\n",
      "  sample_time_ms: 3482.171\n",
      "  update_time_ms: 1.875\n",
      "timestamp: 1633510523\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 780000\n",
      "training_iteration: 195\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:195 starting ! -----------------\n",
      "agent_timesteps_total: 784000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-55-43\n",
      "done: false\n",
      "episode_len_mean: 716.15\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.914099999999999\n",
      "episode_reward_mean: -0.5129969999999985\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 560\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.080740213394165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0098353810608387\n",
      "        model: {}\n",
      "        policy_loss: -0.015662208199501038\n",
      "        total_loss: 0.0251690112054348\n",
      "        vf_explained_var: -0.05113959312438965\n",
      "        vf_loss: 0.0388641394674778\n",
      "  num_agent_steps_sampled: 784000\n",
      "  num_agent_steps_trained: 784000\n",
      "  num_steps_sampled: 784000\n",
      "  num_steps_trained: 784000\n",
      "iterations_since_restore: 196\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.732142857142858\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04934385303395633\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7450962291999794\n",
      "  mean_inference_ms: 0.8509970572712531\n",
      "  mean_raw_obs_processing_ms: 0.08820493127543168\n",
      "time_since_restore: 1055.4589190483093\n",
      "time_this_iter_s: 5.395916700363159\n",
      "time_total_s: 1055.4589190483093\n",
      "timers:\n",
      "  learn_throughput: 2136.77\n",
      "  learn_time_ms: 1871.984\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.951\n",
      "  sample_time_ms: 3490.55\n",
      "  update_time_ms: 1.974\n",
      "timestamp: 1633510543\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 784000\n",
      "training_iteration: 196\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:196 starting ! -----------------\n",
      "agent_timesteps_total: 788000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-56-00\n",
      "done: false\n",
      "episode_len_mean: 714.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.4627044999999985\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 563\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.162868618965149\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0111585333943367\n",
      "        model: {}\n",
      "        policy_loss: -0.01875743456184864\n",
      "        total_loss: 0.05587707832455635\n",
      "        vf_explained_var: -0.222934752702713\n",
      "        vf_loss: 0.07240280508995056\n",
      "  num_agent_steps_sampled: 788000\n",
      "  num_agent_steps_trained: 788000\n",
      "  num_steps_sampled: 788000\n",
      "  num_steps_trained: 788000\n",
      "iterations_since_restore: 197\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.349999999999998\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04933548603728262\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451011971850838\n",
      "  mean_inference_ms: 0.8510001003363092\n",
      "  mean_raw_obs_processing_ms: 0.0881972782564315\n",
      "time_since_restore: 1060.8669373989105\n",
      "time_this_iter_s: 5.408018350601196\n",
      "time_total_s: 1060.8669373989105\n",
      "timers:\n",
      "  learn_throughput: 2139.417\n",
      "  learn_time_ms: 1869.669\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.165\n",
      "  sample_time_ms: 3489.9\n",
      "  update_time_ms: 1.835\n",
      "timestamp: 1633510560\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 788000\n",
      "training_iteration: 197\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:197 starting ! -----------------\n",
      "agent_timesteps_total: 792000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-56-16\n",
      "done: false\n",
      "episode_len_mean: 712.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.48195149999999853\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 565\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0738602876663208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011277508921921253\n",
      "        model: {}\n",
      "        policy_loss: -0.01440111268311739\n",
      "        total_loss: 0.0015472109662368894\n",
      "        vf_explained_var: -0.2091604769229889\n",
      "        vf_loss: 0.013692822307348251\n",
      "  num_agent_steps_sampled: 792000\n",
      "  num_agent_steps_trained: 792000\n",
      "  num_steps_sampled: 792000\n",
      "  num_steps_trained: 792000\n",
      "iterations_since_restore: 198\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.527272727272727\n",
      "  ram_util_percent: 43.49090909090909\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049330038487450156\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451086950152\n",
      "  mean_inference_ms: 0.8510023625899202\n",
      "  mean_raw_obs_processing_ms: 0.08819433404616497\n",
      "time_since_restore: 1066.2720901966095\n",
      "time_this_iter_s: 5.405152797698975\n",
      "time_total_s: 1066.2720901966095\n",
      "timers:\n",
      "  learn_throughput: 2137.289\n",
      "  learn_time_ms: 1871.53\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.14\n",
      "  sample_time_ms: 3486.932\n",
      "  update_time_ms: 1.736\n",
      "timestamp: 1633510576\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 792000\n",
      "training_iteration: 198\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:198 starting ! -----------------\n",
      "agent_timesteps_total: 796000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-56-36\n",
      "done: false\n",
      "episode_len_mean: 712.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.4819229999999985\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 568\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1219274997711182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014495277777314186\n",
      "        model: {}\n",
      "        policy_loss: -0.02363252267241478\n",
      "        total_loss: -0.0037261086981743574\n",
      "        vf_explained_var: -0.10208810865879059\n",
      "        vf_loss: 0.017007360234856606\n",
      "  num_agent_steps_sampled: 796000\n",
      "  num_agent_steps_trained: 796000\n",
      "  num_steps_sampled: 796000\n",
      "  num_steps_trained: 796000\n",
      "iterations_since_restore: 199\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.542857142857137\n",
      "  ram_util_percent: 43.48571428571428\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04932225955989626\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451185839693109\n",
      "  mean_inference_ms: 0.8510060641224829\n",
      "  mean_raw_obs_processing_ms: 0.0881897852624627\n",
      "time_since_restore: 1071.5635681152344\n",
      "time_this_iter_s: 5.291477918624878\n",
      "time_total_s: 1071.5635681152344\n",
      "timers:\n",
      "  learn_throughput: 2137.839\n",
      "  learn_time_ms: 1871.049\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.121\n",
      "  sample_time_ms: 3490.032\n",
      "  update_time_ms: 1.835\n",
      "timestamp: 1633510596\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 796000\n",
      "training_iteration: 199\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:199 starting ! -----------------\n",
      "agent_timesteps_total: 800000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-56-50\n",
      "done: false\n",
      "episode_len_mean: 701.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.43036399999999864\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 571\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1519718170166016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010907309129834175\n",
      "        model: {}\n",
      "        policy_loss: -0.01945541426539421\n",
      "        total_loss: 0.016526460647583008\n",
      "        vf_explained_var: -0.07331395149230957\n",
      "        vf_loss: 0.033800408244132996\n",
      "  num_agent_steps_sampled: 800000\n",
      "  num_agent_steps_trained: 800000\n",
      "  num_steps_sampled: 800000\n",
      "  num_steps_trained: 800000\n",
      "iterations_since_restore: 200\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.568421052631578\n",
      "  ram_util_percent: 43.515789473684215\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04931456864021021\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451314383163594\n",
      "  mean_inference_ms: 0.851016210324358\n",
      "  mean_raw_obs_processing_ms: 0.08818492545241419\n",
      "time_since_restore: 1077.0715687274933\n",
      "time_this_iter_s: 5.508000612258911\n",
      "time_total_s: 1077.0715687274933\n",
      "timers:\n",
      "  learn_throughput: 2131.211\n",
      "  learn_time_ms: 1876.867\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.938\n",
      "  sample_time_ms: 3508.963\n",
      "  update_time_ms: 1.835\n",
      "timestamp: 1633510610\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 800000\n",
      "training_iteration: 200\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:200 starting ! -----------------\n",
      "agent_timesteps_total: 804000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-57-04\n",
      "done: false\n",
      "episode_len_mean: 712.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.5208964999999984\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 574\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0617129802703857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008842975832521915\n",
      "        model: {}\n",
      "        policy_loss: -0.016530966386198997\n",
      "        total_loss: 0.006799977272748947\n",
      "        vf_explained_var: -0.014589645899832249\n",
      "        vf_loss: 0.021562345325946808\n",
      "  num_agent_steps_sampled: 804000\n",
      "  num_agent_steps_trained: 804000\n",
      "  num_steps_sampled: 804000\n",
      "  num_steps_trained: 804000\n",
      "iterations_since_restore: 201\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.5\n",
      "  ram_util_percent: 43.49047619047619\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04930834583156733\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7451488982701835\n",
      "  mean_inference_ms: 0.8510275471150731\n",
      "  mean_raw_obs_processing_ms: 0.08817921553648377\n",
      "time_since_restore: 1082.4590320587158\n",
      "time_this_iter_s: 5.387463331222534\n",
      "time_total_s: 1082.4590320587158\n",
      "timers:\n",
      "  learn_throughput: 2130.93\n",
      "  learn_time_ms: 1877.114\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.753\n",
      "  sample_time_ms: 3515.7\n",
      "  update_time_ms: 1.878\n",
      "timestamp: 1633510624\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 804000\n",
      "training_iteration: 201\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:201 starting ! -----------------\n",
      "agent_timesteps_total: 808000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-57-18\n",
      "done: false\n",
      "episode_len_mean: 703.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.46894599999999853\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 577\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0265824794769287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009442225098609924\n",
      "        model: {}\n",
      "        policy_loss: -0.008931683376431465\n",
      "        total_loss: 0.029719488695263863\n",
      "        vf_explained_var: -0.17158491909503937\n",
      "        vf_loss: 0.03676273301243782\n",
      "  num_agent_steps_sampled: 808000\n",
      "  num_agent_steps_trained: 808000\n",
      "  num_steps_sampled: 808000\n",
      "  num_steps_trained: 808000\n",
      "iterations_since_restore: 202\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.99473684210526\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049301452464934836\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.745176945961046\n",
      "  mean_inference_ms: 0.8510399179848107\n",
      "  mean_raw_obs_processing_ms: 0.08817589224611337\n",
      "time_since_restore: 1087.950564622879\n",
      "time_this_iter_s: 5.491532564163208\n",
      "time_total_s: 1087.950564622879\n",
      "timers:\n",
      "  learn_throughput: 2143.83\n",
      "  learn_time_ms: 1865.819\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.612\n",
      "  sample_time_ms: 3513.049\n",
      "  update_time_ms: 1.878\n",
      "timestamp: 1633510638\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 808000\n",
      "training_iteration: 202\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:202 starting ! -----------------\n",
      "agent_timesteps_total: 812000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-57-34\n",
      "done: false\n",
      "episode_len_mean: 698.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.4490529999999984\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 581\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0769693851470947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010154517367482185\n",
      "        model: {}\n",
      "        policy_loss: -0.0203996691852808\n",
      "        total_loss: -0.001479326281696558\n",
      "        vf_explained_var: 0.1519695371389389\n",
      "        vf_loss: 0.01688944362103939\n",
      "  num_agent_steps_sampled: 812000\n",
      "  num_agent_steps_trained: 812000\n",
      "  num_steps_sampled: 812000\n",
      "  num_steps_trained: 812000\n",
      "iterations_since_restore: 203\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.49545454545455\n",
      "  ram_util_percent: 43.49090909090909\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049290871204091494\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7452185496027962\n",
      "  mean_inference_ms: 0.8510628273126621\n",
      "  mean_raw_obs_processing_ms: 0.08817164806385325\n",
      "time_since_restore: 1093.3718693256378\n",
      "time_this_iter_s: 5.421304702758789\n",
      "time_total_s: 1093.3718693256378\n",
      "timers:\n",
      "  learn_throughput: 2140.216\n",
      "  learn_time_ms: 1868.97\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.128\n",
      "  sample_time_ms: 3526.938\n",
      "  update_time_ms: 1.878\n",
      "timestamp: 1633510654\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 812000\n",
      "training_iteration: 203\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:203 starting ! -----------------\n",
      "agent_timesteps_total: 816000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-57-57\n",
      "done: false\n",
      "episode_len_mean: 690.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.3791274999999985\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 585\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1015430688858032\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01864253543317318\n",
      "        model: {}\n",
      "        policy_loss: -0.02507280744612217\n",
      "        total_loss: -0.0018528907094150782\n",
      "        vf_explained_var: 0.13707171380519867\n",
      "        vf_loss: 0.0194914061576128\n",
      "  num_agent_steps_sampled: 816000\n",
      "  num_agent_steps_trained: 816000\n",
      "  num_steps_sampled: 816000\n",
      "  num_steps_trained: 816000\n",
      "iterations_since_restore: 204\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.991176470588236\n",
      "  ram_util_percent: 43.51470588235294\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928213934581709\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7452557237805151\n",
      "  mean_inference_ms: 0.8510878012879044\n",
      "  mean_raw_obs_processing_ms: 0.08817082635649108\n",
      "time_since_restore: 1098.797390460968\n",
      "time_this_iter_s: 5.4255211353302\n",
      "time_total_s: 1098.797390460968\n",
      "timers:\n",
      "  learn_throughput: 2138.832\n",
      "  learn_time_ms: 1870.18\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.821\n",
      "  sample_time_ms: 3531.009\n",
      "  update_time_ms: 1.878\n",
      "timestamp: 1633510677\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 816000\n",
      "training_iteration: 204\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:204 starting ! -----------------\n",
      "agent_timesteps_total: 820000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-58-21\n",
      "done: false\n",
      "episode_len_mean: 680.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.2694194999999984\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 589\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.084725022315979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010373945347964764\n",
      "        model: {}\n",
      "        policy_loss: -0.02302302047610283\n",
      "        total_loss: -0.0029595575761049986\n",
      "        vf_explained_var: 0.25561416149139404\n",
      "        vf_loss: 0.017988676205277443\n",
      "  num_agent_steps_sampled: 820000\n",
      "  num_agent_steps_trained: 820000\n",
      "  num_steps_sampled: 820000\n",
      "  num_steps_trained: 820000\n",
      "iterations_since_restore: 205\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.49090909090909\n",
      "  ram_util_percent: 43.55454545454546\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049274596366062615\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7452954027186945\n",
      "  mean_inference_ms: 0.8511097832415742\n",
      "  mean_raw_obs_processing_ms: 0.08816994299443798\n",
      "time_since_restore: 1104.2513411045074\n",
      "time_this_iter_s: 5.453950643539429\n",
      "time_total_s: 1104.2513411045074\n",
      "timers:\n",
      "  learn_throughput: 2133.11\n",
      "  learn_time_ms: 1875.197\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.088\n",
      "  sample_time_ms: 3536.419\n",
      "  update_time_ms: 1.955\n",
      "timestamp: 1633510701\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 820000\n",
      "training_iteration: 205\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:205 starting ! -----------------\n",
      "agent_timesteps_total: 824000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-58-36\n",
      "done: false\n",
      "episode_len_mean: 683.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.2600399999999984\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 591\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0299382209777832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011145583353936672\n",
      "        model: {}\n",
      "        policy_loss: -0.01953793689608574\n",
      "        total_loss: -0.00014508821186609566\n",
      "        vf_explained_var: 0.20568564534187317\n",
      "        vf_loss: 0.017163727432489395\n",
      "  num_agent_steps_sampled: 824000\n",
      "  num_agent_steps_trained: 824000\n",
      "  num_steps_sampled: 824000\n",
      "  num_steps_trained: 824000\n",
      "iterations_since_restore: 206\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.947619047619046\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049271410661920416\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7453211824687065\n",
      "  mean_inference_ms: 0.8511202201156611\n",
      "  mean_raw_obs_processing_ms: 0.08817037661696774\n",
      "time_since_restore: 1109.7894706726074\n",
      "time_this_iter_s: 5.538129568099976\n",
      "time_total_s: 1109.7894706726074\n",
      "timers:\n",
      "  learn_throughput: 2126.345\n",
      "  learn_time_ms: 1881.162\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.457\n",
      "  sample_time_ms: 3544.662\n",
      "  update_time_ms: 1.956\n",
      "timestamp: 1633510716\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 824000\n",
      "training_iteration: 206\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:206 starting ! -----------------\n",
      "agent_timesteps_total: 828000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-59-00\n",
      "done: false\n",
      "episode_len_mean: 689.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.31097699999999845\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 594\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0939881801605225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009616829454898834\n",
      "        model: {}\n",
      "        policy_loss: -0.03152710199356079\n",
      "        total_loss: -0.02067873813211918\n",
      "        vf_explained_var: -0.027203237637877464\n",
      "        vf_loss: 0.00892499927431345\n",
      "  num_agent_steps_sampled: 828000\n",
      "  num_agent_steps_trained: 828000\n",
      "  num_steps_sampled: 828000\n",
      "  num_steps_trained: 828000\n",
      "iterations_since_restore: 207\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.14242424242424\n",
      "  ram_util_percent: 43.518181818181816\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049266949258940526\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7453603723785607\n",
      "  mean_inference_ms: 0.851138226332268\n",
      "  mean_raw_obs_processing_ms: 0.08817232420668215\n",
      "time_since_restore: 1115.2712922096252\n",
      "time_this_iter_s: 5.481821537017822\n",
      "time_total_s: 1115.2712922096252\n",
      "timers:\n",
      "  learn_throughput: 2124.576\n",
      "  learn_time_ms: 1882.729\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.612\n",
      "  sample_time_ms: 3550.468\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633510740\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 828000\n",
      "training_iteration: 207\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:207 starting ! -----------------\n",
      "agent_timesteps_total: 832000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-59-13\n",
      "done: false\n",
      "episode_len_mean: 678.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.20075249999999845\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 597\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.129794955253601\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010827245190739632\n",
      "        model: {}\n",
      "        policy_loss: -0.02190142683684826\n",
      "        total_loss: -0.006426387000828981\n",
      "        vf_explained_var: 0.21981912851333618\n",
      "        vf_loss: 0.013309591449797153\n",
      "  num_agent_steps_sampled: 832000\n",
      "  num_agent_steps_trained: 832000\n",
      "  num_steps_sampled: 832000\n",
      "  num_steps_trained: 832000\n",
      "iterations_since_restore: 208\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.647368421052636\n",
      "  ram_util_percent: 43.52105263157895\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926238977707368\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7454008820457406\n",
      "  mean_inference_ms: 0.8511581972398917\n",
      "  mean_raw_obs_processing_ms: 0.08817420975826482\n",
      "time_since_restore: 1120.5761926174164\n",
      "time_this_iter_s: 5.304900407791138\n",
      "time_total_s: 1120.5761926174164\n",
      "timers:\n",
      "  learn_throughput: 2125.551\n",
      "  learn_time_ms: 1881.865\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.552\n",
      "  sample_time_ms: 3541.227\n",
      "  update_time_ms: 2.095\n",
      "timestamp: 1633510753\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 832000\n",
      "training_iteration: 208\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:208 starting ! -----------------\n",
      "agent_timesteps_total: 836000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-59-33\n",
      "done: false\n",
      "episode_len_mean: 675.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.1403309999999984\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 601\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1167680025100708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014806609600782394\n",
      "        model: {}\n",
      "        policy_loss: -0.02715645171701908\n",
      "        total_loss: -0.004233688581734896\n",
      "        vf_explained_var: 0.1849166601896286\n",
      "        vf_loss: 0.019961440935730934\n",
      "  num_agent_steps_sampled: 836000\n",
      "  num_agent_steps_trained: 836000\n",
      "  num_steps_sampled: 836000\n",
      "  num_steps_trained: 836000\n",
      "iterations_since_restore: 209\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.35555555555556\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925178196478142\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7454515152878919\n",
      "  mean_inference_ms: 0.8511831446906154\n",
      "  mean_raw_obs_processing_ms: 0.0881784151259601\n",
      "time_since_restore: 1125.9609818458557\n",
      "time_this_iter_s: 5.384789228439331\n",
      "time_total_s: 1125.9609818458557\n",
      "timers:\n",
      "  learn_throughput: 2124.888\n",
      "  learn_time_ms: 1882.452\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.778\n",
      "  sample_time_ms: 3549.944\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633510773\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 836000\n",
      "training_iteration: 209\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:209 starting ! -----------------\n",
      "agent_timesteps_total: 840000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_17-59-56\n",
      "done: false\n",
      "episode_len_mean: 674.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.1298439999999984\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 604\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0842865705490112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013486267067492008\n",
      "        model: {}\n",
      "        policy_loss: -0.0166119784116745\n",
      "        total_loss: 0.00025913535500876606\n",
      "        vf_explained_var: 0.07391732931137085\n",
      "        vf_loss: 0.014173862524330616\n",
      "  num_agent_steps_sampled: 840000\n",
      "  num_agent_steps_trained: 840000\n",
      "  num_steps_sampled: 840000\n",
      "  num_steps_trained: 840000\n",
      "iterations_since_restore: 210\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.769696969696973\n",
      "  ram_util_percent: 43.52727272727273\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492439645702378\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7454742976757487\n",
      "  mean_inference_ms: 0.8511960678611014\n",
      "  mean_raw_obs_processing_ms: 0.08818278940629046\n",
      "time_since_restore: 1131.334017276764\n",
      "time_this_iter_s: 5.373035430908203\n",
      "time_total_s: 1131.334017276764\n",
      "timers:\n",
      "  learn_throughput: 2122.149\n",
      "  learn_time_ms: 1884.882\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.853\n",
      "  sample_time_ms: 3534.029\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633510796\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 840000\n",
      "training_iteration: 210\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:210 starting ! -----------------\n",
      "agent_timesteps_total: 844000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-00-10\n",
      "done: false\n",
      "episode_len_mean: 679.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.17944799999999844\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 606\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0007094144821167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011995342560112476\n",
      "        model: {}\n",
      "        policy_loss: -0.018643323332071304\n",
      "        total_loss: 0.005611212458461523\n",
      "        vf_explained_var: -0.06207117810845375\n",
      "        vf_loss: 0.021855471655726433\n",
      "  num_agent_steps_sampled: 844000\n",
      "  num_agent_steps_trained: 844000\n",
      "  num_steps_sampled: 844000\n",
      "  num_steps_trained: 844000\n",
      "iterations_since_restore: 211\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.725\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923968542005591\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7454880045942438\n",
      "  mean_inference_ms: 0.8512037906875544\n",
      "  mean_raw_obs_processing_ms: 0.0881859279186327\n",
      "time_since_restore: 1136.7080781459808\n",
      "time_this_iter_s: 5.374060869216919\n",
      "time_total_s: 1136.7080781459808\n",
      "timers:\n",
      "  learn_throughput: 2122.234\n",
      "  learn_time_ms: 1884.806\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.214\n",
      "  sample_time_ms: 3532.902\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633510810\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 844000\n",
      "training_iteration: 211\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:211 starting ! -----------------\n",
      "agent_timesteps_total: 848000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-00-29\n",
      "done: false\n",
      "episode_len_mean: 675.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.08018049999999849\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 610\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1639987230300903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010346004739403725\n",
      "        model: {}\n",
      "        policy_loss: -0.02140417881309986\n",
      "        total_loss: 0.0284590981900692\n",
      "        vf_explained_var: -0.19493886828422546\n",
      "        vf_loss: 0.047794073820114136\n",
      "  num_agent_steps_sampled: 848000\n",
      "  num_agent_steps_trained: 848000\n",
      "  num_steps_sampled: 848000\n",
      "  num_steps_trained: 848000\n",
      "iterations_since_restore: 212\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.303703703703704\n",
      "  ram_util_percent: 43.4962962962963\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923030985149939\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7455267236351446\n",
      "  mean_inference_ms: 0.8512194388868926\n",
      "  mean_raw_obs_processing_ms: 0.08819431006902632\n",
      "time_since_restore: 1142.2088749408722\n",
      "time_this_iter_s: 5.500796794891357\n",
      "time_total_s: 1142.2088749408722\n",
      "timers:\n",
      "  learn_throughput: 2113.756\n",
      "  learn_time_ms: 1892.366\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.374\n",
      "  sample_time_ms: 3526.173\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633510829\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 848000\n",
      "training_iteration: 212\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:212 starting ! -----------------\n",
      "agent_timesteps_total: 852000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-00-45\n",
      "done: false\n",
      "episode_len_mean: 675.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.08047999999999847\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 614\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0526920557022095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01296808011829853\n",
      "        model: {}\n",
      "        policy_loss: -0.01971658319234848\n",
      "        total_loss: 0.00913090631365776\n",
      "        vf_explained_var: 0.038209863007068634\n",
      "        vf_loss: 0.026253879070281982\n",
      "  num_agent_steps_sampled: 852000\n",
      "  num_agent_steps_trained: 852000\n",
      "  num_steps_sampled: 852000\n",
      "  num_steps_trained: 852000\n",
      "iterations_since_restore: 213\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.431818181818183\n",
      "  ram_util_percent: 43.46363636363637\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049220712692740046\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7455620644072315\n",
      "  mean_inference_ms: 0.8512338756343155\n",
      "  mean_raw_obs_processing_ms: 0.08820163007375875\n",
      "time_since_restore: 1147.6602268218994\n",
      "time_this_iter_s: 5.451351881027222\n",
      "time_total_s: 1147.6602268218994\n",
      "timers:\n",
      "  learn_throughput: 2106.787\n",
      "  learn_time_ms: 1898.626\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.469\n",
      "  sample_time_ms: 3522.773\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633510845\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 852000\n",
      "training_iteration: 213\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:213 starting ! -----------------\n",
      "agent_timesteps_total: 856000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-01-02\n",
      "done: false\n",
      "episode_len_mean: 667.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.08947049999999847\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 617\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0035474300384521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009001387283205986\n",
      "        model: {}\n",
      "        policy_loss: -0.018074043095111847\n",
      "        total_loss: 0.006176230497658253\n",
      "        vf_explained_var: -0.27519193291664124\n",
      "        vf_loss: 0.022449994459748268\n",
      "  num_agent_steps_sampled: 856000\n",
      "  num_agent_steps_trained: 856000\n",
      "  num_steps_sampled: 856000\n",
      "  num_steps_trained: 856000\n",
      "iterations_since_restore: 214\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.072000000000003\n",
      "  ram_util_percent: 43.472\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492150845107277\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7455891267718181\n",
      "  mean_inference_ms: 0.8512480868835094\n",
      "  mean_raw_obs_processing_ms: 0.08820737862644311\n",
      "time_since_restore: 1153.2612295150757\n",
      "time_this_iter_s: 5.6010026931762695\n",
      "time_total_s: 1153.2612295150757\n",
      "timers:\n",
      "  learn_throughput: 2085.969\n",
      "  learn_time_ms: 1917.574\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.957\n",
      "  sample_time_ms: 3521.259\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633510862\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 856000\n",
      "training_iteration: 214\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:214 starting ! -----------------\n",
      "agent_timesteps_total: 860000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-01-25\n",
      "done: false\n",
      "episode_len_mean: 669.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.08893849999999855\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 619\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9974396824836731\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011538085527718067\n",
      "        model: {}\n",
      "        policy_loss: -0.022173337638378143\n",
      "        total_loss: -0.008770589716732502\n",
      "        vf_explained_var: 0.3013223111629486\n",
      "        vf_loss: 0.011095134541392326\n",
      "  num_agent_steps_sampled: 860000\n",
      "  num_agent_steps_trained: 860000\n",
      "  num_steps_sampled: 860000\n",
      "  num_steps_trained: 860000\n",
      "iterations_since_restore: 215\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.384375\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049211775595338236\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7456084395193099\n",
      "  mean_inference_ms: 0.8512579715853126\n",
      "  mean_raw_obs_processing_ms: 0.08821054331920247\n",
      "time_since_restore: 1158.6585776805878\n",
      "time_this_iter_s: 5.397348165512085\n",
      "time_total_s: 1158.6585776805878\n",
      "timers:\n",
      "  learn_throughput: 2100.922\n",
      "  learn_time_ms: 1903.926\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.401\n",
      "  sample_time_ms: 3529.201\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633510885\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 860000\n",
      "training_iteration: 215\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:215 starting ! -----------------\n",
      "agent_timesteps_total: 864000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-01-50\n",
      "done: false\n",
      "episode_len_mean: 672.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.11912649999999832\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 622\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0335700511932373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01099656242877245\n",
      "        model: {}\n",
      "        policy_loss: -0.01582174375653267\n",
      "        total_loss: -0.0020618485286831856\n",
      "        vf_explained_var: 0.327629953622818\n",
      "        vf_loss: 0.011560583487153053\n",
      "  num_agent_steps_sampled: 864000\n",
      "  num_agent_steps_trained: 864000\n",
      "  num_steps_sampled: 864000\n",
      "  num_steps_trained: 864000\n",
      "iterations_since_restore: 216\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.894117647058824\n",
      "  ram_util_percent: 43.49411764705882\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920859391134633\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7456411668442514\n",
      "  mean_inference_ms: 0.8512764044648427\n",
      "  mean_raw_obs_processing_ms: 0.088215777036743\n",
      "time_since_restore: 1164.2499713897705\n",
      "time_this_iter_s: 5.591393709182739\n",
      "time_total_s: 1164.2499713897705\n",
      "timers:\n",
      "  learn_throughput: 2102.714\n",
      "  learn_time_ms: 1902.303\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.165\n",
      "  sample_time_ms: 3536.178\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633510910\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 864000\n",
      "training_iteration: 216\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:216 starting ! -----------------\n",
      "agent_timesteps_total: 868000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-02-13\n",
      "done: false\n",
      "episode_len_mean: 663.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.017773499999998374\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 625\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9758156538009644\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012975264340639114\n",
      "        model: {}\n",
      "        policy_loss: -0.02701568976044655\n",
      "        total_loss: -0.0006082693580538034\n",
      "        vf_explained_var: -0.042252425104379654\n",
      "        vf_loss: 0.023812368512153625\n",
      "  num_agent_steps_sampled: 868000\n",
      "  num_agent_steps_trained: 868000\n",
      "  num_steps_sampled: 868000\n",
      "  num_steps_trained: 868000\n",
      "iterations_since_restore: 217\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.415151515151518\n",
      "  ram_util_percent: 43.47575757575758\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920621912139045\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7456692218541088\n",
      "  mean_inference_ms: 0.8512893810154403\n",
      "  mean_raw_obs_processing_ms: 0.08822217174201474\n",
      "time_since_restore: 1169.576907634735\n",
      "time_this_iter_s: 5.3269362449646\n",
      "time_total_s: 1169.576907634735\n",
      "timers:\n",
      "  learn_throughput: 2104.651\n",
      "  learn_time_ms: 1900.553\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.595\n",
      "  sample_time_ms: 3522.381\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633510933\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 868000\n",
      "training_iteration: 217\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:217 starting ! -----------------\n",
      "agent_timesteps_total: 872000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-02-27\n",
      "done: false\n",
      "episode_len_mean: 665.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.007989999999998294\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 628\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9915010333061218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012778989970684052\n",
      "        model: {}\n",
      "        policy_loss: -0.018565738573670387\n",
      "        total_loss: 0.004404631908982992\n",
      "        vf_explained_var: 0.2162700593471527\n",
      "        vf_loss: 0.0204145684838295\n",
      "  num_agent_steps_sampled: 872000\n",
      "  num_agent_steps_trained: 872000\n",
      "  num_steps_sampled: 872000\n",
      "  num_steps_trained: 872000\n",
      "iterations_since_restore: 218\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.531578947368423\n",
      "  ram_util_percent: 43.49473684210526\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920422396269058\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7456937030175973\n",
      "  mean_inference_ms: 0.8513000681118544\n",
      "  mean_raw_obs_processing_ms: 0.08822936150113593\n",
      "time_since_restore: 1174.8780536651611\n",
      "time_this_iter_s: 5.301146030426025\n",
      "time_total_s: 1174.8780536651611\n",
      "timers:\n",
      "  learn_throughput: 2104.434\n",
      "  learn_time_ms: 1900.748\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.77\n",
      "  sample_time_ms: 3521.84\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633510947\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 872000\n",
      "training_iteration: 218\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:218 starting ! -----------------\n",
      "agent_timesteps_total: 876000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-02-41\n",
      "done: false\n",
      "episode_len_mean: 657.65\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.0032940000000017244\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 631\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.068711757659912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010307706892490387\n",
      "        model: {}\n",
      "        policy_loss: -0.029055671766400337\n",
      "        total_loss: -0.01453065313398838\n",
      "        vf_explained_var: 0.19808480143547058\n",
      "        vf_loss: 0.01246347650885582\n",
      "  num_agent_steps_sampled: 876000\n",
      "  num_agent_steps_trained: 876000\n",
      "  num_steps_sampled: 876000\n",
      "  num_steps_trained: 876000\n",
      "iterations_since_restore: 219\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.750000000000007\n",
      "  ram_util_percent: 43.485\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920276200225419\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7457217243088904\n",
      "  mean_inference_ms: 0.851310117509742\n",
      "  mean_raw_obs_processing_ms: 0.08823847781102293\n",
      "time_since_restore: 1180.2923092842102\n",
      "time_this_iter_s: 5.414255619049072\n",
      "time_total_s: 1180.2923092842102\n",
      "timers:\n",
      "  learn_throughput: 2105.517\n",
      "  learn_time_ms: 1899.771\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.511\n",
      "  sample_time_ms: 3525.748\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633510961\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 876000\n",
      "training_iteration: 219\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:219 starting ! -----------------\n",
      "agent_timesteps_total: 880000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-02-57\n",
      "done: false\n",
      "episode_len_mean: 661.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.02404350000000174\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 634\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1033622026443481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01090828888118267\n",
      "        model: {}\n",
      "        policy_loss: -0.020730139687657356\n",
      "        total_loss: -0.008513723500072956\n",
      "        vf_explained_var: 0.3082164525985718\n",
      "        vf_loss: 0.010034757666289806\n",
      "  num_agent_steps_sampled: 880000\n",
      "  num_agent_steps_trained: 880000\n",
      "  num_steps_sampled: 880000\n",
      "  num_steps_trained: 880000\n",
      "iterations_since_restore: 220\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.969565217391306\n",
      "  ram_util_percent: 43.51304347826087\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920068102238551\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7457454674864664\n",
      "  mean_inference_ms: 0.851315046671496\n",
      "  mean_raw_obs_processing_ms: 0.08825023562333395\n",
      "time_since_restore: 1185.577368736267\n",
      "time_this_iter_s: 5.285059452056885\n",
      "time_total_s: 1185.577368736267\n",
      "timers:\n",
      "  learn_throughput: 2114.092\n",
      "  learn_time_ms: 1892.065\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.894\n",
      "  sample_time_ms: 3524.558\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633510977\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 880000\n",
      "training_iteration: 220\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:220 starting ! -----------------\n",
      "agent_timesteps_total: 884000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-03-10\n",
      "done: false\n",
      "episode_len_mean: 658.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.025963499999998266\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 638\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0849944353103638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009931027889251709\n",
      "        model: {}\n",
      "        policy_loss: -0.019631490111351013\n",
      "        total_loss: 0.0187385231256485\n",
      "        vf_explained_var: -0.13241516053676605\n",
      "        vf_loss: 0.03638380765914917\n",
      "  num_agent_steps_sampled: 884000\n",
      "  num_agent_steps_trained: 884000\n",
      "  num_steps_sampled: 884000\n",
      "  num_steps_trained: 884000\n",
      "iterations_since_restore: 221\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.670588235294115\n",
      "  ram_util_percent: 43.44117647058823\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04919966970272954\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7457780702980498\n",
      "  mean_inference_ms: 0.8513263569752857\n",
      "  mean_raw_obs_processing_ms: 0.08826594689907279\n",
      "time_since_restore: 1191.091285943985\n",
      "time_this_iter_s: 5.5139172077178955\n",
      "time_total_s: 1191.091285943985\n",
      "timers:\n",
      "  learn_throughput: 2110.47\n",
      "  learn_time_ms: 1895.313\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.442\n",
      "  sample_time_ms: 3535.313\n",
      "  update_time_ms: 1.795\n",
      "timestamp: 1633510990\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 884000\n",
      "training_iteration: 221\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:221 starting ! -----------------\n",
      "agent_timesteps_total: 888000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-03-24\n",
      "done: false\n",
      "episode_len_mean: 658.85\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: -0.007175999999998168\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 641\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9745482206344604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010413557291030884\n",
      "        model: {}\n",
      "        policy_loss: -0.019218239933252335\n",
      "        total_loss: -0.002478670794516802\n",
      "        vf_explained_var: -0.0660739466547966\n",
      "        vf_loss: 0.014656861312687397\n",
      "  num_agent_steps_sampled: 888000\n",
      "  num_agent_steps_trained: 888000\n",
      "  num_steps_sampled: 888000\n",
      "  num_steps_trained: 888000\n",
      "iterations_since_restore: 222\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.355\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049198850967167786\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7458057342624979\n",
      "  mean_inference_ms: 0.8513361907433817\n",
      "  mean_raw_obs_processing_ms: 0.0882780144993698\n",
      "time_since_restore: 1196.4350266456604\n",
      "time_this_iter_s: 5.343740701675415\n",
      "time_total_s: 1196.4350266456604\n",
      "timers:\n",
      "  learn_throughput: 2112.586\n",
      "  learn_time_ms: 1893.414\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.829\n",
      "  sample_time_ms: 3521.657\n",
      "  update_time_ms: 1.754\n",
      "timestamp: 1633511004\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 888000\n",
      "training_iteration: 222\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:222 starting ! -----------------\n",
      "agent_timesteps_total: 892000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-03-38\n",
      "done: false\n",
      "episode_len_mean: 657.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.0023620000000019025\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 643\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1302096843719482\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01374239381402731\n",
      "        model: {}\n",
      "        policy_loss: -0.03345244377851486\n",
      "        total_loss: -0.023834986612200737\n",
      "        vf_explained_var: 0.4150346517562866\n",
      "        vf_loss: 0.006868980824947357\n",
      "  num_agent_steps_sampled: 892000\n",
      "  num_agent_steps_trained: 892000\n",
      "  num_steps_sampled: 892000\n",
      "  num_steps_trained: 892000\n",
      "iterations_since_restore: 223\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.076190476190476\n",
      "  ram_util_percent: 43.47142857142857\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04919865472246004\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7458277803793945\n",
      "  mean_inference_ms: 0.8513424361351215\n",
      "  mean_raw_obs_processing_ms: 0.08828630739990206\n",
      "time_since_restore: 1201.8132588863373\n",
      "time_this_iter_s: 5.37823224067688\n",
      "time_total_s: 1201.8132588863373\n",
      "timers:\n",
      "  learn_throughput: 2123.488\n",
      "  learn_time_ms: 1883.693\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.021\n",
      "  sample_time_ms: 3524.163\n",
      "  update_time_ms: 1.754\n",
      "timestamp: 1633511018\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 892000\n",
      "training_iteration: 223\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:223 starting ! -----------------\n",
      "agent_timesteps_total: 896000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-03-51\n",
      "done: false\n",
      "episode_len_mean: 658.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.012004000000001888\n",
      "episode_reward_min: -2.2288999999999994\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 646\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.056736707687378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00892413780093193\n",
      "        model: {}\n",
      "        policy_loss: -0.015207890421152115\n",
      "        total_loss: -0.003904186887666583\n",
      "        vf_explained_var: -0.17386318743228912\n",
      "        vf_loss: 0.009518877603113651\n",
      "  num_agent_steps_sampled: 896000\n",
      "  num_agent_steps_trained: 896000\n",
      "  num_steps_sampled: 896000\n",
      "  num_steps_trained: 896000\n",
      "iterations_since_restore: 224\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.041176470588233\n",
      "  ram_util_percent: 43.51176470588236\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049199647104920224\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7458650980159456\n",
      "  mean_inference_ms: 0.8513549917270642\n",
      "  mean_raw_obs_processing_ms: 0.0883002614265806\n",
      "time_since_restore: 1207.2992658615112\n",
      "time_this_iter_s: 5.48600697517395\n",
      "time_total_s: 1207.2992658615112\n",
      "timers:\n",
      "  learn_throughput: 2146.652\n",
      "  learn_time_ms: 1863.367\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.172\n",
      "  sample_time_ms: 3533.032\n",
      "  update_time_ms: 1.853\n",
      "timestamp: 1633511031\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 896000\n",
      "training_iteration: 224\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:224 starting ! -----------------\n",
      "agent_timesteps_total: 900000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-04-03\n",
      "done: false\n",
      "episode_len_mean: 651.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.07361550000000187\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 649\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0879499912261963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013352393172681332\n",
      "        model: {}\n",
      "        policy_loss: -0.020532701164484024\n",
      "        total_loss: -0.0025605959817767143\n",
      "        vf_explained_var: 0.07167058438062668\n",
      "        vf_loss: 0.015301624312996864\n",
      "  num_agent_steps_sampled: 900000\n",
      "  num_agent_steps_trained: 900000\n",
      "  num_steps_sampled: 900000\n",
      "  num_steps_trained: 900000\n",
      "iterations_since_restore: 225\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.11176470588235\n",
      "  ram_util_percent: 43.5\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049200566603172\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7458956989126532\n",
      "  mean_inference_ms: 0.851361797877615\n",
      "  mean_raw_obs_processing_ms: 0.08831194674331817\n",
      "time_since_restore: 1212.5981731414795\n",
      "time_this_iter_s: 5.298907279968262\n",
      "time_total_s: 1212.5981731414795\n",
      "timers:\n",
      "  learn_throughput: 2146.05\n",
      "  learn_time_ms: 1863.889\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.492\n",
      "  sample_time_ms: 3522.702\n",
      "  update_time_ms: 1.853\n",
      "timestamp: 1633511043\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 900000\n",
      "training_iteration: 225\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:225 starting ! -----------------\n",
      "agent_timesteps_total: 904000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-04-23\n",
      "done: false\n",
      "episode_len_mean: 656.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.0828195000000019\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 651\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9941574335098267\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00912988930940628\n",
      "        model: {}\n",
      "        policy_loss: -0.019995423033833504\n",
      "        total_loss: -0.002712732646614313\n",
      "        vf_explained_var: -0.2395157665014267\n",
      "        vf_loss: 0.01545671746134758\n",
      "  num_agent_steps_sampled: 904000\n",
      "  num_agent_steps_trained: 904000\n",
      "  num_steps_sampled: 904000\n",
      "  num_steps_trained: 904000\n",
      "iterations_since_restore: 226\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.47931034482758\n",
      "  ram_util_percent: 43.87586206896551\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049202680884038924\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7459229942547926\n",
      "  mean_inference_ms: 0.8513654033150594\n",
      "  mean_raw_obs_processing_ms: 0.08831878420924888\n",
      "time_since_restore: 1218.0433382987976\n",
      "time_this_iter_s: 5.445165157318115\n",
      "time_total_s: 1218.0433382987976\n",
      "timers:\n",
      "  learn_throughput: 2148.206\n",
      "  learn_time_ms: 1862.019\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.596\n",
      "  sample_time_ms: 3510.015\n",
      "  update_time_ms: 1.853\n",
      "timestamp: 1633511063\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 904000\n",
      "training_iteration: 226\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:226 starting ! -----------------\n",
      "agent_timesteps_total: 908000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-04-36\n",
      "done: false\n",
      "episode_len_mean: 660.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.20203500000000194\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 655\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.069871187210083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013014506548643112\n",
      "        model: {}\n",
      "        policy_loss: -0.02696874737739563\n",
      "        total_loss: 0.010937449522316456\n",
      "        vf_explained_var: -0.07425187528133392\n",
      "        vf_loss: 0.035303302109241486\n",
      "  num_agent_steps_sampled: 908000\n",
      "  num_agent_steps_trained: 908000\n",
      "  num_steps_sampled: 908000\n",
      "  num_steps_trained: 908000\n",
      "iterations_since_restore: 227\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.827777777777776\n",
      "  ram_util_percent: 43.988888888888894\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049208202441558865\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7459767356217571\n",
      "  mean_inference_ms: 0.8513679014479336\n",
      "  mean_raw_obs_processing_ms: 0.08833140304065273\n",
      "time_since_restore: 1223.5262897014618\n",
      "time_this_iter_s: 5.482951402664185\n",
      "time_total_s: 1223.5262897014618\n",
      "timers:\n",
      "  learn_throughput: 2129.866\n",
      "  learn_time_ms: 1878.053\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.777\n",
      "  sample_time_ms: 3509.457\n",
      "  update_time_ms: 1.753\n",
      "timestamp: 1633511076\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 908000\n",
      "training_iteration: 227\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:227 starting ! -----------------\n",
      "agent_timesteps_total: 912000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-04-51\n",
      "done: false\n",
      "episode_len_mean: 661.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.21118100000000198\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 657\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9945215582847595\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01142516452819109\n",
      "        model: {}\n",
      "        policy_loss: -0.026996809989213943\n",
      "        total_loss: 0.0003360532864462584\n",
      "        vf_explained_var: 0.12257655709981918\n",
      "        vf_loss: 0.025047829374670982\n",
      "  num_agent_steps_sampled: 912000\n",
      "  num_agent_steps_trained: 912000\n",
      "  num_steps_sampled: 912000\n",
      "  num_steps_trained: 912000\n",
      "iterations_since_restore: 228\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 33.769999999999996\n",
      "  ram_util_percent: 44.010000000000005\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921060398837442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7460106628753667\n",
      "  mean_inference_ms: 0.8513735684110688\n",
      "  mean_raw_obs_processing_ms: 0.08833911619988254\n",
      "time_since_restore: 1229.061187028885\n",
      "time_this_iter_s: 5.534897327423096\n",
      "time_total_s: 1229.061187028885\n",
      "timers:\n",
      "  learn_throughput: 2130.332\n",
      "  learn_time_ms: 1877.641\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.126\n",
      "  sample_time_ms: 3533.175\n",
      "  update_time_ms: 1.753\n",
      "timestamp: 1633511091\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 912000\n",
      "training_iteration: 228\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:228 starting ! -----------------\n",
      "agent_timesteps_total: 916000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-05-05\n",
      "done: false\n",
      "episode_len_mean: 658.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9244000000000003\n",
      "episode_reward_mean: 0.17140750000000207\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 660\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9615613222122192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013489969074726105\n",
      "        model: {}\n",
      "        policy_loss: -0.02281162515282631\n",
      "        total_loss: -0.005251665599644184\n",
      "        vf_explained_var: 0.24537692964076996\n",
      "        vf_loss: 0.014861966483294964\n",
      "  num_agent_steps_sampled: 916000\n",
      "  num_agent_steps_trained: 916000\n",
      "  num_steps_sampled: 916000\n",
      "  num_steps_trained: 916000\n",
      "iterations_since_restore: 229\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.354999999999997\n",
      "  ram_util_percent: 44.055\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921272874728272\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7460584013694617\n",
      "  mean_inference_ms: 0.8513810453908345\n",
      "  mean_raw_obs_processing_ms: 0.08835139906557252\n",
      "time_since_restore: 1234.4047133922577\n",
      "time_this_iter_s: 5.343526363372803\n",
      "time_total_s: 1234.4047133922577\n",
      "timers:\n",
      "  learn_throughput: 2129.945\n",
      "  learn_time_ms: 1877.983\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.476\n",
      "  sample_time_ms: 3525.857\n",
      "  update_time_ms: 1.753\n",
      "timestamp: 1633511105\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 916000\n",
      "training_iteration: 229\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:229 starting ! -----------------\n",
      "agent_timesteps_total: 920000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-05-20\n",
      "done: false\n",
      "episode_len_mean: 656.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9104499999999986\n",
      "episode_reward_mean: 0.1610870000000021\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 663\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0454672574996948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009814458899199963\n",
      "        model: {}\n",
      "        policy_loss: -0.017959335818886757\n",
      "        total_loss: 0.025958526879549026\n",
      "        vf_explained_var: 0.017200471833348274\n",
      "        vf_loss: 0.041954975575208664\n",
      "  num_agent_steps_sampled: 920000\n",
      "  num_agent_steps_trained: 920000\n",
      "  num_steps_sampled: 920000\n",
      "  num_steps_trained: 920000\n",
      "iterations_since_restore: 230\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.799999999999997\n",
      "  ram_util_percent: 44.02857142857143\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049215812980870834\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7461174387857787\n",
      "  mean_inference_ms: 0.8513880339567804\n",
      "  mean_raw_obs_processing_ms: 0.08836045170611523\n",
      "time_since_restore: 1239.940259218216\n",
      "time_this_iter_s: 5.535545825958252\n",
      "time_total_s: 1239.940259218216\n",
      "timers:\n",
      "  learn_throughput: 2129.729\n",
      "  learn_time_ms: 1878.173\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.528\n",
      "  sample_time_ms: 3550.732\n",
      "  update_time_ms: 1.653\n",
      "timestamp: 1633511120\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 920000\n",
      "training_iteration: 230\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:230 starting ! -----------------\n",
      "agent_timesteps_total: 924000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-05-35\n",
      "done: false\n",
      "episode_len_mean: 657.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9104499999999986\n",
      "episode_reward_mean: 0.19033250000000212\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 666\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0867624282836914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010557050816714764\n",
      "        model: {}\n",
      "        policy_loss: -0.021693699061870575\n",
      "        total_loss: -0.00029861056827940047\n",
      "        vf_explained_var: 0.12367865443229675\n",
      "        vf_loss: 0.019283676519989967\n",
      "  num_agent_steps_sampled: 924000\n",
      "  num_agent_steps_trained: 924000\n",
      "  num_steps_sampled: 924000\n",
      "  num_steps_trained: 924000\n",
      "iterations_since_restore: 231\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.35238095238095\n",
      "  ram_util_percent: 44.00952380952381\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921865947352735\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7461731143859377\n",
      "  mean_inference_ms: 0.8513981131843568\n",
      "  mean_raw_obs_processing_ms: 0.0883661315613924\n",
      "time_since_restore: 1245.261699438095\n",
      "time_this_iter_s: 5.32144021987915\n",
      "time_total_s: 1245.261699438095\n",
      "timers:\n",
      "  learn_throughput: 2133.155\n",
      "  learn_time_ms: 1875.157\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.715\n",
      "  sample_time_ms: 3534.457\n",
      "  update_time_ms: 1.653\n",
      "timestamp: 1633511135\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 924000\n",
      "training_iteration: 231\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:231 starting ! -----------------\n",
      "agent_timesteps_total: 928000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-05-58\n",
      "done: false\n",
      "episode_len_mean: 654.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9104499999999986\n",
      "episode_reward_mean: 0.18959200000000215\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 669\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0139293670654297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00823640450835228\n",
      "        model: {}\n",
      "        policy_loss: -0.018032290041446686\n",
      "        total_loss: 0.013133467175066471\n",
      "        vf_explained_var: -0.05326656624674797\n",
      "        vf_loss: 0.029518472030758858\n",
      "  num_agent_steps_sampled: 928000\n",
      "  num_agent_steps_trained: 928000\n",
      "  num_steps_sampled: 928000\n",
      "  num_steps_trained: 928000\n",
      "iterations_since_restore: 232\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.854545454545455\n",
      "  ram_util_percent: 44.03939393939394\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049221471730860014\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.746232100431695\n",
      "  mean_inference_ms: 0.8514053511158092\n",
      "  mean_raw_obs_processing_ms: 0.08837089698401238\n",
      "time_since_restore: 1250.620269536972\n",
      "time_this_iter_s: 5.358570098876953\n",
      "time_total_s: 1250.620269536972\n",
      "timers:\n",
      "  learn_throughput: 2136.656\n",
      "  learn_time_ms: 1872.084\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.292\n",
      "  sample_time_ms: 3538.907\n",
      "  update_time_ms: 1.694\n",
      "timestamp: 1633511158\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 928000\n",
      "training_iteration: 232\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:232 starting ! -----------------\n",
      "agent_timesteps_total: 932000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-06-22\n",
      "done: false\n",
      "episode_len_mean: 659.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9104499999999986\n",
      "episode_reward_mean: 0.12867500000000218\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 671\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0351388454437256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00974961556494236\n",
      "        model: {}\n",
      "        policy_loss: -0.016634896397590637\n",
      "        total_loss: 0.0012032878585159779\n",
      "        vf_explained_var: 0.04929129779338837\n",
      "        vf_loss: 0.015888260677456856\n",
      "  num_agent_steps_sampled: 932000\n",
      "  num_agent_steps_trained: 932000\n",
      "  num_steps_sampled: 932000\n",
      "  num_steps_trained: 932000\n",
      "iterations_since_restore: 233\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.161764705882348\n",
      "  ram_util_percent: 44.01764705882353\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922368329646765\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.746270613946687\n",
      "  mean_inference_ms: 0.8514093236484794\n",
      "  mean_raw_obs_processing_ms: 0.08837310242999606\n",
      "time_since_restore: 1256.022373199463\n",
      "time_this_iter_s: 5.402103662490845\n",
      "time_total_s: 1256.022373199463\n",
      "timers:\n",
      "  learn_throughput: 2135.032\n",
      "  learn_time_ms: 1873.508\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.002\n",
      "  sample_time_ms: 3539.816\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633511182\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 932000\n",
      "training_iteration: 233\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:233 starting ! -----------------\n",
      "agent_timesteps_total: 936000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-06-35\n",
      "done: false\n",
      "episode_len_mean: 650.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9108500000000004\n",
      "episode_reward_mean: 0.2293065000000022\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 675\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.032509446144104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009279483929276466\n",
      "        model: {}\n",
      "        policy_loss: -0.024463173002004623\n",
      "        total_loss: 0.009810666553676128\n",
      "        vf_explained_var: -0.14920289814472198\n",
      "        vf_loss: 0.03241794556379318\n",
      "  num_agent_steps_sampled: 936000\n",
      "  num_agent_steps_trained: 936000\n",
      "  num_steps_sampled: 936000\n",
      "  num_steps_trained: 936000\n",
      "iterations_since_restore: 234\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.855555555555558\n",
      "  ram_util_percent: 44.01111111111111\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049226327807123785\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7463373808958126\n",
      "  mean_inference_ms: 0.8514174897394795\n",
      "  mean_raw_obs_processing_ms: 0.08837848233877862\n",
      "time_since_restore: 1261.3390140533447\n",
      "time_this_iter_s: 5.316640853881836\n",
      "time_total_s: 1261.3390140533447\n",
      "timers:\n",
      "  learn_throughput: 2135.011\n",
      "  learn_time_ms: 1873.527\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.418\n",
      "  sample_time_ms: 3522.931\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633511195\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 936000\n",
      "training_iteration: 234\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:234 starting ! -----------------\n",
      "agent_timesteps_total: 940000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-06-55\n",
      "done: false\n",
      "episode_len_mean: 651.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9108500000000004\n",
      "episode_reward_mean: 0.26969050000000216\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 678\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9753242135047913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010910294950008392\n",
      "        model: {}\n",
      "        policy_loss: -0.01948382332921028\n",
      "        total_loss: -0.004788937047123909\n",
      "        vf_explained_var: 0.153291717171669\n",
      "        vf_loss: 0.01251282636076212\n",
      "  num_agent_steps_sampled: 940000\n",
      "  num_agent_steps_trained: 940000\n",
      "  num_steps_sampled: 940000\n",
      "  num_steps_trained: 940000\n",
      "iterations_since_restore: 235\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.97142857142857\n",
      "  ram_util_percent: 44.035714285714285\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492296989202301\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7463843212456126\n",
      "  mean_inference_ms: 0.8514218309163251\n",
      "  mean_raw_obs_processing_ms: 0.08838156042153233\n",
      "time_since_restore: 1266.8592491149902\n",
      "time_this_iter_s: 5.520235061645508\n",
      "time_total_s: 1266.8592491149902\n",
      "timers:\n",
      "  learn_throughput: 2125.403\n",
      "  learn_time_ms: 1881.996\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1131.07\n",
      "  sample_time_ms: 3536.474\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633511215\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 940000\n",
      "training_iteration: 235\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:235 starting ! -----------------\n",
      "agent_timesteps_total: 944000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-07-08\n",
      "done: false\n",
      "episode_len_mean: 660.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9108500000000004\n",
      "episode_reward_mean: 0.20920750000000218\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 681\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9518505930900574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012807996943593025\n",
      "        model: {}\n",
      "        policy_loss: -0.019815417006611824\n",
      "        total_loss: 4.8543304728809744e-05\n",
      "        vf_explained_var: 0.1159471720457077\n",
      "        vf_loss: 0.017302365973591805\n",
      "  num_agent_steps_sampled: 944000\n",
      "  num_agent_steps_trained: 944000\n",
      "  num_steps_sampled: 944000\n",
      "  num_steps_trained: 944000\n",
      "iterations_since_restore: 236\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.611111111111107\n",
      "  ram_util_percent: 44.01111111111111\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049233950100090525\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7464270693275407\n",
      "  mean_inference_ms: 0.8514234813130013\n",
      "  mean_raw_obs_processing_ms: 0.08838461092053397\n",
      "time_since_restore: 1272.2076642513275\n",
      "time_this_iter_s: 5.34841513633728\n",
      "time_total_s: 1272.2076642513275\n",
      "timers:\n",
      "  learn_throughput: 2121.533\n",
      "  learn_time_ms: 1885.429\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1135.354\n",
      "  sample_time_ms: 3523.13\n",
      "  update_time_ms: 1.795\n",
      "timestamp: 1633511228\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 944000\n",
      "training_iteration: 236\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:236 starting ! -----------------\n",
      "agent_timesteps_total: 948000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-07-21\n",
      "done: false\n",
      "episode_len_mean: 671.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9108500000000004\n",
      "episode_reward_mean: 0.1485760000000022\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 683\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0871150493621826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01095735002309084\n",
      "        model: {}\n",
      "        policy_loss: -0.016553452238440514\n",
      "        total_loss: -0.0030148248188197613\n",
      "        vf_explained_var: -0.09557881206274033\n",
      "        vf_loss: 0.01134716160595417\n",
      "  num_agent_steps_sampled: 948000\n",
      "  num_agent_steps_trained: 948000\n",
      "  num_steps_sampled: 948000\n",
      "  num_steps_trained: 948000\n",
      "iterations_since_restore: 237\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.27894736842105\n",
      "  ram_util_percent: 43.98421052631579\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049236288767910315\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7464547640301272\n",
      "  mean_inference_ms: 0.8514232413792021\n",
      "  mean_raw_obs_processing_ms: 0.0883860896900858\n",
      "time_since_restore: 1277.5576539039612\n",
      "time_this_iter_s: 5.349989652633667\n",
      "time_total_s: 1277.5576539039612\n",
      "timers:\n",
      "  learn_throughput: 2140.279\n",
      "  learn_time_ms: 1868.915\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1134.257\n",
      "  sample_time_ms: 3526.536\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633511241\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 948000\n",
      "training_iteration: 237\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:237 starting ! -----------------\n",
      "agent_timesteps_total: 952000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-07-45\n",
      "done: false\n",
      "episode_len_mean: 677.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.08776550000000226\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 686\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9838423728942871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012177438475191593\n",
      "        model: {}\n",
      "        policy_loss: -0.024298736825585365\n",
      "        total_loss: -0.0035428586415946484\n",
      "        vf_explained_var: 0.30966055393218994\n",
      "        vf_loss: 0.018320390954613686\n",
      "  num_agent_steps_sampled: 952000\n",
      "  num_agent_steps_trained: 952000\n",
      "  num_steps_sampled: 952000\n",
      "  num_steps_trained: 952000\n",
      "iterations_since_restore: 238\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.681818181818183\n",
      "  ram_util_percent: 44.015151515151516\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923985852761701\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7465034681195535\n",
      "  mean_inference_ms: 0.8514220919608786\n",
      "  mean_raw_obs_processing_ms: 0.08838782701279682\n",
      "time_since_restore: 1283.0412752628326\n",
      "time_this_iter_s: 5.48362135887146\n",
      "time_total_s: 1283.0412752628326\n",
      "timers:\n",
      "  learn_throughput: 2135.588\n",
      "  learn_time_ms: 1873.021\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1137.238\n",
      "  sample_time_ms: 3517.294\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633511265\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 952000\n",
      "training_iteration: 238\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:238 starting ! -----------------\n",
      "agent_timesteps_total: 956000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-08-08\n",
      "done: false\n",
      "episode_len_mean: 679.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.08709800000000228\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 689\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9781762957572937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009552043862640858\n",
      "        model: {}\n",
      "        policy_loss: -0.019688498228788376\n",
      "        total_loss: 0.01597224362194538\n",
      "        vf_explained_var: -0.03827756270766258\n",
      "        vf_loss: 0.03375033661723137\n",
      "  num_agent_steps_sampled: 956000\n",
      "  num_agent_steps_trained: 956000\n",
      "  num_steps_sampled: 956000\n",
      "  num_steps_trained: 956000\n",
      "iterations_since_restore: 239\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.972727272727273\n",
      "  ram_util_percent: 44.018181818181816\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924289861738859\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7465533854987545\n",
      "  mean_inference_ms: 0.8514223252099303\n",
      "  mean_raw_obs_processing_ms: 0.08838914413049156\n",
      "time_since_restore: 1288.4459846019745\n",
      "time_this_iter_s: 5.404709339141846\n",
      "time_total_s: 1288.4459846019745\n",
      "timers:\n",
      "  learn_throughput: 2130.802\n",
      "  learn_time_ms: 1877.228\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1136.65\n",
      "  sample_time_ms: 3519.113\n",
      "  update_time_ms: 1.895\n",
      "timestamp: 1633511288\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 956000\n",
      "training_iteration: 239\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:239 starting ! -----------------\n",
      "agent_timesteps_total: 960000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-08-21\n",
      "done: false\n",
      "episode_len_mean: 677.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.14792100000000236\n",
      "episode_reward_min: -2.211050000000006\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 691\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9819145798683167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011984948068857193\n",
      "        model: {}\n",
      "        policy_loss: -0.017759084701538086\n",
      "        total_loss: 0.015458490699529648\n",
      "        vf_explained_var: 0.09711523354053497\n",
      "        vf_loss: 0.030820583924651146\n",
      "  num_agent_steps_sampled: 960000\n",
      "  num_agent_steps_trained: 960000\n",
      "  num_steps_sampled: 960000\n",
      "  num_steps_trained: 960000\n",
      "iterations_since_restore: 240\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.31666666666667\n",
      "  ram_util_percent: 44.02777777777778\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924551634036636\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7465786935670172\n",
      "  mean_inference_ms: 0.8514231191069754\n",
      "  mean_raw_obs_processing_ms: 0.0883894271079198\n",
      "time_since_restore: 1293.7907123565674\n",
      "time_this_iter_s: 5.3447277545928955\n",
      "time_total_s: 1293.7907123565674\n",
      "timers:\n",
      "  learn_throughput: 2129.902\n",
      "  learn_time_ms: 1878.021\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1143.093\n",
      "  sample_time_ms: 3499.277\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633511301\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 960000\n",
      "training_iteration: 240\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:240 starting ! -----------------\n",
      "agent_timesteps_total: 964000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-08-45\n",
      "done: false\n",
      "episode_len_mean: 673.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.1984375000000024\n",
      "episode_reward_min: -2.2022000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 694\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0197861194610596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010588503442704678\n",
      "        model: {}\n",
      "        policy_loss: -0.0162314772605896\n",
      "        total_loss: 0.01353600062429905\n",
      "        vf_explained_var: 0.15697748959064484\n",
      "        vf_loss: 0.027649778872728348\n",
      "  num_agent_steps_sampled: 964000\n",
      "  num_agent_steps_trained: 964000\n",
      "  num_steps_sampled: 964000\n",
      "  num_steps_trained: 964000\n",
      "iterations_since_restore: 241\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.490909090909092\n",
      "  ram_util_percent: 44.084848484848486\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049248977274697486\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7466113181564218\n",
      "  mean_inference_ms: 0.8514209017845716\n",
      "  mean_raw_obs_processing_ms: 0.08838968969998767\n",
      "time_since_restore: 1299.1957252025604\n",
      "time_this_iter_s: 5.405012845993042\n",
      "time_total_s: 1299.1957252025604\n",
      "timers:\n",
      "  learn_throughput: 2119.222\n",
      "  learn_time_ms: 1887.485\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1143.45\n",
      "  sample_time_ms: 3498.184\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633511325\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 964000\n",
      "training_iteration: 241\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:241 starting ! -----------------\n",
      "agent_timesteps_total: 968000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-09-08\n",
      "done: false\n",
      "episode_len_mean: 680.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.09748200000000252\n",
      "episode_reward_min: -2.2022000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 696\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.034407377243042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010733491741120815\n",
      "        model: {}\n",
      "        policy_loss: -0.01382895465940237\n",
      "        total_loss: -9.220928041031584e-05\n",
      "        vf_explained_var: 0.03656608983874321\n",
      "        vf_loss: 0.011590047739446163\n",
      "  num_agent_steps_sampled: 968000\n",
      "  num_agent_steps_trained: 968000\n",
      "  num_steps_sampled: 968000\n",
      "  num_steps_trained: 968000\n",
      "iterations_since_restore: 242\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.894117647058824\n",
      "  ram_util_percent: 44.035294117647055\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925169822275857\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7466310482385653\n",
      "  mean_inference_ms: 0.8514201617512032\n",
      "  mean_raw_obs_processing_ms: 0.08839013289902914\n",
      "time_since_restore: 1304.479082584381\n",
      "time_this_iter_s: 5.283357381820679\n",
      "time_total_s: 1304.479082584381\n",
      "timers:\n",
      "  learn_throughput: 2120.576\n",
      "  learn_time_ms: 1886.28\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1145.53\n",
      "  sample_time_ms: 3491.832\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633511348\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 968000\n",
      "training_iteration: 242\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:242 starting ! -----------------\n",
      "agent_timesteps_total: 972000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-09-32\n",
      "done: false\n",
      "episode_len_mean: 685.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.1572420000000025\n",
      "episode_reward_min: -2.2022000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 699\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9625174403190613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010004309937357903\n",
      "        model: {}\n",
      "        policy_loss: -0.018531939014792442\n",
      "        total_loss: -0.0015162406489253044\n",
      "        vf_explained_var: 0.01151038333773613\n",
      "        vf_loss: 0.01501484028995037\n",
      "  num_agent_steps_sampled: 972000\n",
      "  num_agent_steps_trained: 972000\n",
      "  num_steps_sampled: 972000\n",
      "  num_steps_trained: 972000\n",
      "iterations_since_restore: 243\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.372727272727275\n",
      "  ram_util_percent: 44.012121212121215\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925630958621374\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7466628167192372\n",
      "  mean_inference_ms: 0.8514153126545149\n",
      "  mean_raw_obs_processing_ms: 0.08839192052110487\n",
      "time_since_restore: 1309.9421548843384\n",
      "time_this_iter_s: 5.463072299957275\n",
      "time_total_s: 1309.9421548843384\n",
      "timers:\n",
      "  learn_throughput: 2112.479\n",
      "  learn_time_ms: 1893.51\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1145.927\n",
      "  sample_time_ms: 3490.623\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633511372\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 972000\n",
      "training_iteration: 243\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:243 starting ! -----------------\n",
      "agent_timesteps_total: 976000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-09-49\n",
      "done: false\n",
      "episode_len_mean: 688.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.14604900000000248\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 702\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0365508794784546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009731668047606945\n",
      "        model: {}\n",
      "        policy_loss: -0.020412828773260117\n",
      "        total_loss: -0.0023793329019099474\n",
      "        vf_explained_var: 0.41817614436149597\n",
      "        vf_loss: 0.016087163239717484\n",
      "  num_agent_steps_sampled: 976000\n",
      "  num_agent_steps_trained: 976000\n",
      "  num_steps_sampled: 976000\n",
      "  num_steps_trained: 976000\n",
      "iterations_since_restore: 244\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.500000000000004\n",
      "  ram_util_percent: 44.047826086956526\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049261053554053635\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7466968317275868\n",
      "  mean_inference_ms: 0.8514108526973057\n",
      "  mean_raw_obs_processing_ms: 0.08839382988710559\n",
      "time_since_restore: 1315.5942721366882\n",
      "time_this_iter_s: 5.6521172523498535\n",
      "time_total_s: 1315.5942721366882\n",
      "timers:\n",
      "  learn_throughput: 2084.497\n",
      "  learn_time_ms: 1918.928\n",
      "  load_throughput: 40136880.383\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1143.33\n",
      "  sample_time_ms: 3498.553\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633511389\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 976000\n",
      "training_iteration: 244\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:244 starting ! -----------------\n",
      "agent_timesteps_total: 980000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-10-04\n",
      "done: false\n",
      "episode_len_mean: 694.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.0959280000000025\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 704\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1000138521194458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0095102833583951\n",
      "        model: {}\n",
      "        policy_loss: -0.012575656175613403\n",
      "        total_loss: 0.007629923522472382\n",
      "        vf_explained_var: -0.006678651086986065\n",
      "        vf_loss: 0.0183035247027874\n",
      "  num_agent_steps_sampled: 980000\n",
      "  num_agent_steps_trained: 980000\n",
      "  num_steps_sampled: 980000\n",
      "  num_steps_trained: 980000\n",
      "iterations_since_restore: 245\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.676190476190477\n",
      "  ram_util_percent: 44.023809523809526\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049264521048733266\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7467193639638589\n",
      "  mean_inference_ms: 0.8514072142309905\n",
      "  mean_raw_obs_processing_ms: 0.08839497164660845\n",
      "time_since_restore: 1320.9584758281708\n",
      "time_this_iter_s: 5.364203691482544\n",
      "time_total_s: 1320.9584758281708\n",
      "timers:\n",
      "  learn_throughput: 2087.881\n",
      "  learn_time_ms: 1915.818\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.397\n",
      "  sample_time_ms: 3486.153\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633511404\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 980000\n",
      "training_iteration: 245\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:245 starting ! -----------------\n",
      "agent_timesteps_total: 984000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-10-16\n",
      "done: false\n",
      "episode_len_mean: 688.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.10609200000000252\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 707\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9621076583862305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010715186595916748\n",
      "        model: {}\n",
      "        policy_loss: -0.02236819453537464\n",
      "        total_loss: 0.0010799664305523038\n",
      "        vf_explained_var: 0.27420324087142944\n",
      "        vf_loss: 0.02130512334406376\n",
      "  num_agent_steps_sampled: 984000\n",
      "  num_agent_steps_trained: 984000\n",
      "  num_steps_sampled: 984000\n",
      "  num_steps_trained: 984000\n",
      "iterations_since_restore: 246\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.02352941176471\n",
      "  ram_util_percent: 44.076470588235296\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926829165563326\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7467518552776561\n",
      "  mean_inference_ms: 0.8514026736284817\n",
      "  mean_raw_obs_processing_ms: 0.08839667683083918\n",
      "time_since_restore: 1326.35879945755\n",
      "time_this_iter_s: 5.4003236293792725\n",
      "time_total_s: 1326.35879945755\n",
      "timers:\n",
      "  learn_throughput: 2091.475\n",
      "  learn_time_ms: 1912.526\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.612\n",
      "  sample_time_ms: 3494.634\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633511416\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 984000\n",
      "training_iteration: 246\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:246 starting ! -----------------\n",
      "agent_timesteps_total: 988000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-10-28\n",
      "done: false\n",
      "episode_len_mean: 694.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: -0.0047974999999973985\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 710\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9623770117759705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010642804205417633\n",
      "        model: {}\n",
      "        policy_loss: -0.021382514387369156\n",
      "        total_loss: 0.01688452810049057\n",
      "        vf_explained_var: -0.00027073276578448713\n",
      "        vf_loss: 0.03613848239183426\n",
      "  num_agent_steps_sampled: 988000\n",
      "  num_agent_steps_trained: 988000\n",
      "  num_steps_sampled: 988000\n",
      "  num_steps_trained: 988000\n",
      "iterations_since_restore: 247\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.31666666666667\n",
      "  ram_util_percent: 44.06666666666667\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927139502840377\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7467846018280678\n",
      "  mean_inference_ms: 0.8513975593200174\n",
      "  mean_raw_obs_processing_ms: 0.08839715689730392\n",
      "time_since_restore: 1331.746978521347\n",
      "time_this_iter_s: 5.388179063796997\n",
      "time_total_s: 1331.746978521347\n",
      "timers:\n",
      "  learn_throughput: 2091.561\n",
      "  learn_time_ms: 1912.447\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.35\n",
      "  sample_time_ms: 3498.491\n",
      "  update_time_ms: 1.997\n",
      "timestamp: 1633511428\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 988000\n",
      "training_iteration: 247\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:247 starting ! -----------------\n",
      "agent_timesteps_total: 992000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-10-52\n",
      "done: false\n",
      "episode_len_mean: 699.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: -0.06434449999999735\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 714\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9898609519004822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017444590106606483\n",
      "        model: {}\n",
      "        policy_loss: -0.023032883182168007\n",
      "        total_loss: -0.006598103791475296\n",
      "        vf_explained_var: 0.2258663922548294\n",
      "        vf_loss: 0.012945864349603653\n",
      "  num_agent_steps_sampled: 992000\n",
      "  num_agent_steps_trained: 992000\n",
      "  num_steps_sampled: 992000\n",
      "  num_steps_trained: 992000\n",
      "iterations_since_restore: 248\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.221212121212123\n",
      "  ram_util_percent: 44.05151515151515\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049277732333986204\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7468244735708456\n",
      "  mean_inference_ms: 0.851394927725655\n",
      "  mean_raw_obs_processing_ms: 0.0883985469084282\n",
      "time_since_restore: 1337.2430064678192\n",
      "time_this_iter_s: 5.496027946472168\n",
      "time_total_s: 1337.2430064678192\n",
      "timers:\n",
      "  learn_throughput: 2085.339\n",
      "  learn_time_ms: 1918.153\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.792\n",
      "  sample_time_ms: 3494.084\n",
      "  update_time_ms: 1.996\n",
      "timestamp: 1633511452\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 992000\n",
      "training_iteration: 248\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:248 starting ! -----------------\n",
      "agent_timesteps_total: 996000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-11-06\n",
      "done: false\n",
      "episode_len_mean: 695.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.046627000000002555\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 717\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9987483620643616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011383665725588799\n",
      "        model: {}\n",
      "        policy_loss: -0.015682589262723923\n",
      "        total_loss: 0.0022266197483986616\n",
      "        vf_explained_var: 0.46403512358665466\n",
      "        vf_loss: 0.015632471069693565\n",
      "  num_agent_steps_sampled: 996000\n",
      "  num_agent_steps_trained: 996000\n",
      "  num_steps_sampled: 996000\n",
      "  num_steps_trained: 996000\n",
      "iterations_since_restore: 249\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.652631578947368\n",
      "  ram_util_percent: 44.031578947368416\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928127987383133\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7468514620240613\n",
      "  mean_inference_ms: 0.8513897501894533\n",
      "  mean_raw_obs_processing_ms: 0.08840013285034262\n",
      "time_since_restore: 1342.5347912311554\n",
      "time_this_iter_s: 5.291784763336182\n",
      "time_total_s: 1342.5347912311554\n",
      "timers:\n",
      "  learn_throughput: 2089.415\n",
      "  learn_time_ms: 1914.412\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.256\n",
      "  sample_time_ms: 3486.581\n",
      "  update_time_ms: 1.996\n",
      "timestamp: 1633511466\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 996000\n",
      "training_iteration: 249\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:249 starting ! -----------------\n",
      "agent_timesteps_total: 1000000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-11-21\n",
      "done: false\n",
      "episode_len_mean: 695.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.04621100000000263\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 720\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9671539068222046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012480752542614937\n",
      "        model: {}\n",
      "        policy_loss: -0.022998202592134476\n",
      "        total_loss: -0.006457692012190819\n",
      "        vf_explained_var: 0.5297190546989441\n",
      "        vf_loss: 0.014044356532394886\n",
      "  num_agent_steps_sampled: 1000000\n",
      "  num_agent_steps_trained: 1000000\n",
      "  num_steps_sampled: 1000000\n",
      "  num_steps_trained: 1000000\n",
      "iterations_since_restore: 250\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.936363636363634\n",
      "  ram_util_percent: 44.04090909090909\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049284859063277224\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.746872022595793\n",
      "  mean_inference_ms: 0.851382591207653\n",
      "  mean_raw_obs_processing_ms: 0.08840198814379013\n",
      "time_since_restore: 1347.8719837665558\n",
      "time_this_iter_s: 5.337192535400391\n",
      "time_total_s: 1347.8719837665558\n",
      "timers:\n",
      "  learn_throughput: 2090.349\n",
      "  learn_time_ms: 1913.556\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.219\n",
      "  sample_time_ms: 3486.692\n",
      "  update_time_ms: 1.896\n",
      "timestamp: 1633511481\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1000000\n",
      "training_iteration: 250\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:250 starting ! -----------------\n",
      "agent_timesteps_total: 1004000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-11-41\n",
      "done: false\n",
      "episode_len_mean: 691.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.14697450000000253\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 722\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9402800798416138\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008276713080704212\n",
      "        model: {}\n",
      "        policy_loss: -0.01592620462179184\n",
      "        total_loss: 0.006953957490622997\n",
      "        vf_explained_var: 0.10714472085237503\n",
      "        vf_loss: 0.021224824711680412\n",
      "  num_agent_steps_sampled: 1004000\n",
      "  num_agent_steps_trained: 1004000\n",
      "  num_steps_sampled: 1004000\n",
      "  num_steps_trained: 1004000\n",
      "iterations_since_restore: 251\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.692857142857147\n",
      "  ram_util_percent: 44.02857142857142\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928684671766124\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7468811674530138\n",
      "  mean_inference_ms: 0.8513748397849389\n",
      "  mean_raw_obs_processing_ms: 0.08840300779514898\n",
      "time_since_restore: 1353.2558887004852\n",
      "time_this_iter_s: 5.383904933929443\n",
      "time_total_s: 1353.2558887004852\n",
      "timers:\n",
      "  learn_throughput: 2092.681\n",
      "  learn_time_ms: 1911.423\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.217\n",
      "  sample_time_ms: 3486.7\n",
      "  update_time_ms: 1.877\n",
      "timestamp: 1633511501\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1004000\n",
      "training_iteration: 251\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:251 starting ! -----------------\n",
      "agent_timesteps_total: 1008000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-11-56\n",
      "done: false\n",
      "episode_len_mean: 698.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.0363605000000026\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 726\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0494393110275269\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014242980629205704\n",
      "        model: {}\n",
      "        policy_loss: -0.03405226767063141\n",
      "        total_loss: 0.01633506827056408\n",
      "        vf_explained_var: 0.1773412674665451\n",
      "        vf_loss: 0.04753873124718666\n",
      "  num_agent_steps_sampled: 1008000\n",
      "  num_agent_steps_trained: 1008000\n",
      "  num_steps_sampled: 1008000\n",
      "  num_steps_trained: 1008000\n",
      "iterations_since_restore: 252\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.119999999999997\n",
      "  ram_util_percent: 44.04\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929093087140205\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.746905660825207\n",
      "  mean_inference_ms: 0.8513636948696114\n",
      "  mean_raw_obs_processing_ms: 0.08840436503023633\n",
      "time_since_restore: 1358.8446669578552\n",
      "time_this_iter_s: 5.588778257369995\n",
      "time_total_s: 1358.8446669578552\n",
      "timers:\n",
      "  learn_throughput: 2074.173\n",
      "  learn_time_ms: 1928.48\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.784\n",
      "  sample_time_ms: 3500.224\n",
      "  update_time_ms: 1.877\n",
      "timestamp: 1633511516\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1008000\n",
      "training_iteration: 252\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:252 starting ! -----------------\n",
      "agent_timesteps_total: 1012000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-12-09\n",
      "done: false\n",
      "episode_len_mean: 698.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.036717500000002554\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 728\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9912348389625549\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012026390060782433\n",
      "        model: {}\n",
      "        policy_loss: -0.022986456751823425\n",
      "        total_loss: 0.0004803539486601949\n",
      "        vf_explained_var: 0.15807479619979858\n",
      "        vf_loss: 0.02106153406202793\n",
      "  num_agent_steps_sampled: 1012000\n",
      "  num_agent_steps_trained: 1012000\n",
      "  num_steps_sampled: 1012000\n",
      "  num_steps_trained: 1012000\n",
      "iterations_since_restore: 253\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.665000000000003\n",
      "  ram_util_percent: 44.10000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049292619013653435\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7469180293623984\n",
      "  mean_inference_ms: 0.8513592812208514\n",
      "  mean_raw_obs_processing_ms: 0.08840510331734841\n",
      "time_since_restore: 1364.1771123409271\n",
      "time_this_iter_s: 5.332445383071899\n",
      "time_total_s: 1364.1771123409271\n",
      "timers:\n",
      "  learn_throughput: 2082.408\n",
      "  learn_time_ms: 1920.853\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.504\n",
      "  sample_time_ms: 3494.962\n",
      "  update_time_ms: 1.877\n",
      "timestamp: 1633511529\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1012000\n",
      "training_iteration: 253\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:253 starting ! -----------------\n",
      "agent_timesteps_total: 1016000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-12-21\n",
      "done: false\n",
      "episode_len_mean: 693.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.09672350000000261\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 732\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0678707361221313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011759663932025433\n",
      "        model: {}\n",
      "        policy_loss: -0.027391843497753143\n",
      "        total_loss: -0.005327903665602207\n",
      "        vf_explained_var: 0.25975388288497925\n",
      "        vf_loss: 0.01971200481057167\n",
      "  num_agent_steps_sampled: 1016000\n",
      "  num_agent_steps_trained: 1016000\n",
      "  num_steps_sampled: 1016000\n",
      "  num_steps_trained: 1016000\n",
      "iterations_since_restore: 254\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.025\n",
      "  ram_util_percent: 44.09375\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929568561091407\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7469405145090966\n",
      "  mean_inference_ms: 0.8513489558620807\n",
      "  mean_raw_obs_processing_ms: 0.08840661900689872\n",
      "time_since_restore: 1369.5604557991028\n",
      "time_this_iter_s: 5.383343458175659\n",
      "time_total_s: 1369.5604557991028\n",
      "timers:\n",
      "  learn_throughput: 2107.388\n",
      "  learn_time_ms: 1898.084\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.808\n",
      "  sample_time_ms: 3490.986\n",
      "  update_time_ms: 1.878\n",
      "timestamp: 1633511541\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1016000\n",
      "training_iteration: 254\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:254 starting ! -----------------\n",
      "agent_timesteps_total: 1020000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-12-41\n",
      "done: false\n",
      "episode_len_mean: 693.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.08619550000000262\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 734\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0227982997894287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009321385063230991\n",
      "        model: {}\n",
      "        policy_loss: -0.00931013748049736\n",
      "        total_loss: 0.012953811325132847\n",
      "        vf_explained_var: -0.0698772743344307\n",
      "        vf_loss: 0.0203996691852808\n",
      "  num_agent_steps_sampled: 1020000\n",
      "  num_agent_steps_trained: 1020000\n",
      "  num_steps_sampled: 1020000\n",
      "  num_steps_trained: 1020000\n",
      "iterations_since_restore: 255\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.00714285714286\n",
      "  ram_util_percent: 44.03214285714286\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492973698205606\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7469574907099741\n",
      "  mean_inference_ms: 0.8513426839386892\n",
      "  mean_raw_obs_processing_ms: 0.08840619895301369\n",
      "time_since_restore: 1374.9394211769104\n",
      "time_this_iter_s: 5.378965377807617\n",
      "time_total_s: 1374.9394211769104\n",
      "timers:\n",
      "  learn_throughput: 2115.326\n",
      "  learn_time_ms: 1890.962\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.015\n",
      "  sample_time_ms: 3499.516\n",
      "  update_time_ms: 1.878\n",
      "timestamp: 1633511561\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1020000\n",
      "training_iteration: 255\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:255 starting ! -----------------\n",
      "agent_timesteps_total: 1024000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-13-01\n",
      "done: false\n",
      "episode_len_mean: 700.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.07629400000000258\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 738\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9449978470802307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012308895587921143\n",
      "        model: {}\n",
      "        policy_loss: -0.02609892375767231\n",
      "        total_loss: -0.005663982126861811\n",
      "        vf_explained_var: 0.243376225233078\n",
      "        vf_loss: 0.017973164096474648\n",
      "  num_agent_steps_sampled: 1024000\n",
      "  num_agent_steps_trained: 1024000\n",
      "  num_steps_sampled: 1024000\n",
      "  num_steps_trained: 1024000\n",
      "iterations_since_restore: 256\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.000000000000004\n",
      "  ram_util_percent: 44.046428571428564\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04930117740352852\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7469773343854113\n",
      "  mean_inference_ms: 0.8513219777982206\n",
      "  mean_raw_obs_processing_ms: 0.08840492429255273\n",
      "time_since_restore: 1380.289711713791\n",
      "time_this_iter_s: 5.350290536880493\n",
      "time_total_s: 1380.289711713791\n",
      "timers:\n",
      "  learn_throughput: 2103.674\n",
      "  learn_time_ms: 1901.436\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.016\n",
      "  sample_time_ms: 3484.27\n",
      "  update_time_ms: 1.877\n",
      "timestamp: 1633511581\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1024000\n",
      "training_iteration: 256\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:256 starting ! -----------------\n",
      "agent_timesteps_total: 1028000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-13-20\n",
      "done: false\n",
      "episode_len_mean: 695.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.06640950000000251\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 740\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.982659637928009\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01085103489458561\n",
      "        model: {}\n",
      "        policy_loss: -0.016063213348388672\n",
      "        total_loss: 0.0030880928970873356\n",
      "        vf_explained_var: 0.25816410779953003\n",
      "        vf_loss: 0.0169810950756073\n",
      "  num_agent_steps_sampled: 1028000\n",
      "  num_agent_steps_trained: 1028000\n",
      "  num_steps_sampled: 1028000\n",
      "  num_steps_trained: 1028000\n",
      "iterations_since_restore: 257\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.77037037037037\n",
      "  ram_util_percent: 44.096296296296295\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04930348635965064\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7469910052586914\n",
      "  mean_inference_ms: 0.8513123798920529\n",
      "  mean_raw_obs_processing_ms: 0.08840444375058606\n",
      "time_since_restore: 1385.7013976573944\n",
      "time_this_iter_s: 5.411685943603516\n",
      "time_total_s: 1385.7013976573944\n",
      "timers:\n",
      "  learn_throughput: 2104.939\n",
      "  learn_time_ms: 1900.292\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.829\n",
      "  sample_time_ms: 3487.878\n",
      "  update_time_ms: 1.852\n",
      "timestamp: 1633511600\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1028000\n",
      "training_iteration: 257\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:257 starting ! -----------------\n",
      "agent_timesteps_total: 1032000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-13-44\n",
      "done: false\n",
      "episode_len_mean: 702.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.06578500000000262\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 742\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8821865320205688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008807346224784851\n",
      "        model: {}\n",
      "        policy_loss: -0.01289430446922779\n",
      "        total_loss: -0.001073190476745367\n",
      "        vf_explained_var: -0.16001150012016296\n",
      "        vf_loss: 0.010059641674160957\n",
      "  num_agent_steps_sampled: 1032000\n",
      "  num_agent_steps_trained: 1032000\n",
      "  num_steps_sampled: 1032000\n",
      "  num_steps_trained: 1032000\n",
      "iterations_since_restore: 258\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.269696969696973\n",
      "  ram_util_percent: 44.06969696969697\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04930599961121766\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.747003270221563\n",
      "  mean_inference_ms: 0.8513013437938252\n",
      "  mean_raw_obs_processing_ms: 0.08840448279161707\n",
      "time_since_restore: 1391.0319871902466\n",
      "time_this_iter_s: 5.330589532852173\n",
      "time_total_s: 1391.0319871902466\n",
      "timers:\n",
      "  learn_throughput: 2114.101\n",
      "  learn_time_ms: 1892.057\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.547\n",
      "  sample_time_ms: 3479.632\n",
      "  update_time_ms: 1.753\n",
      "timestamp: 1633511624\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1032000\n",
      "training_iteration: 258\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:258 starting ! -----------------\n",
      "agent_timesteps_total: 1036000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-14-00\n",
      "done: false\n",
      "episode_len_mean: 691.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.23698350000000257\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 747\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0805680751800537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009220761246979237\n",
      "        model: {}\n",
      "        policy_loss: -0.023044200614094734\n",
      "        total_loss: 0.008410691283643246\n",
      "        vf_explained_var: 0.22966036200523376\n",
      "        vf_loss: 0.02961074374616146\n",
      "  num_agent_steps_sampled: 1036000\n",
      "  num_agent_steps_trained: 1036000\n",
      "  num_steps_sampled: 1036000\n",
      "  num_steps_trained: 1036000\n",
      "iterations_since_restore: 259\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.37391304347826\n",
      "  ram_util_percent: 44.07826086956522\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04931111682999188\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470317026079039\n",
      "  mean_inference_ms: 0.8512718780860046\n",
      "  mean_raw_obs_processing_ms: 0.08840426543862823\n",
      "time_since_restore: 1396.491537809372\n",
      "time_this_iter_s: 5.459550619125366\n",
      "time_total_s: 1396.491537809372\n",
      "timers:\n",
      "  learn_throughput: 2109.1\n",
      "  learn_time_ms: 1896.543\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.523\n",
      "  sample_time_ms: 3491.855\n",
      "  update_time_ms: 1.653\n",
      "timestamp: 1633511640\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1036000\n",
      "training_iteration: 259\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:259 starting ! -----------------\n",
      "agent_timesteps_total: 1040000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-14-18\n",
      "done: false\n",
      "episode_len_mean: 691.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.1960320000000027\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 750\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.993896484375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009902716614305973\n",
      "        model: {}\n",
      "        policy_loss: -0.018753856420516968\n",
      "        total_loss: 0.013395681045949459\n",
      "        vf_explained_var: -0.009274152107536793\n",
      "        vf_loss: 0.030168993398547173\n",
      "  num_agent_steps_sampled: 1040000\n",
      "  num_agent_steps_trained: 1040000\n",
      "  num_steps_sampled: 1040000\n",
      "  num_steps_trained: 1040000\n",
      "iterations_since_restore: 260\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.096153846153847\n",
      "  ram_util_percent: 44.092307692307685\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0493135617979775\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470469232522852\n",
      "  mean_inference_ms: 0.8512527003735415\n",
      "  mean_raw_obs_processing_ms: 0.08840563703686778\n",
      "time_since_restore: 1401.7967643737793\n",
      "time_this_iter_s: 5.305226564407349\n",
      "time_total_s: 1401.7967643737793\n",
      "timers:\n",
      "  learn_throughput: 2109.51\n",
      "  learn_time_ms: 1896.175\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.426\n",
      "  sample_time_ms: 3489.103\n",
      "  update_time_ms: 1.653\n",
      "timestamp: 1633511658\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1040000\n",
      "training_iteration: 260\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:260 starting ! -----------------\n",
      "agent_timesteps_total: 1044000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-14-42\n",
      "done: false\n",
      "episode_len_mean: 686.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.13581350000000267\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 753\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1285151243209839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010269063524901867\n",
      "        model: {}\n",
      "        policy_loss: -0.01994761824607849\n",
      "        total_loss: 0.004084372892975807\n",
      "        vf_explained_var: -0.2976583242416382\n",
      "        vf_loss: 0.02197817713022232\n",
      "  num_agent_steps_sampled: 1044000\n",
      "  num_agent_steps_trained: 1044000\n",
      "  num_steps_sampled: 1044000\n",
      "  num_steps_trained: 1044000\n",
      "iterations_since_restore: 261\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.430303030303033\n",
      "  ram_util_percent: 44.03333333333333\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049315102168929335\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.747062677106547\n",
      "  mean_inference_ms: 0.8512336031104912\n",
      "  mean_raw_obs_processing_ms: 0.0884083769023256\n",
      "time_since_restore: 1407.3042843341827\n",
      "time_this_iter_s: 5.507519960403442\n",
      "time_total_s: 1407.3042843341827\n",
      "timers:\n",
      "  learn_throughput: 2103.87\n",
      "  learn_time_ms: 1901.258\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.047\n",
      "  sample_time_ms: 3496.36\n",
      "  update_time_ms: 1.573\n",
      "timestamp: 1633511682\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1044000\n",
      "training_iteration: 261\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:261 starting ! -----------------\n",
      "agent_timesteps_total: 1048000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-15-01\n",
      "done: false\n",
      "episode_len_mean: 689.17\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.07567800000000265\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 756\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9520405530929565\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013551012612879276\n",
      "        model: {}\n",
      "        policy_loss: -0.01649346575140953\n",
      "        total_loss: 0.013072713278234005\n",
      "        vf_explained_var: -0.28748247027397156\n",
      "        vf_loss: 0.02685597725212574\n",
      "  num_agent_steps_sampled: 1048000\n",
      "  num_agent_steps_trained: 1048000\n",
      "  num_steps_sampled: 1048000\n",
      "  num_steps_trained: 1048000\n",
      "iterations_since_restore: 262\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.41481481481481\n",
      "  ram_util_percent: 44.07037037037038\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049316739319433534\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470741860624855\n",
      "  mean_inference_ms: 0.8512136599212411\n",
      "  mean_raw_obs_processing_ms: 0.08841141748366585\n",
      "time_since_restore: 1412.6830096244812\n",
      "time_this_iter_s: 5.378725290298462\n",
      "time_total_s: 1412.6830096244812\n",
      "timers:\n",
      "  learn_throughput: 2117.985\n",
      "  learn_time_ms: 1888.587\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.752\n",
      "  sample_time_ms: 3488.111\n",
      "  update_time_ms: 1.573\n",
      "timestamp: 1633511701\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1048000\n",
      "training_iteration: 262\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:262 starting ! -----------------\n",
      "agent_timesteps_total: 1052000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-15-15\n",
      "done: false\n",
      "episode_len_mean: 688.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.08620300000000267\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 758\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1531903743743896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010146355256438255\n",
      "        model: {}\n",
      "        policy_loss: -0.017403751611709595\n",
      "        total_loss: 0.0019369276706129313\n",
      "        vf_explained_var: 0.11481381952762604\n",
      "        vf_loss: 0.017311405390501022\n",
      "  num_agent_steps_sampled: 1052000\n",
      "  num_agent_steps_trained: 1052000\n",
      "  num_steps_sampled: 1052000\n",
      "  num_steps_trained: 1052000\n",
      "iterations_since_restore: 263\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.855\n",
      "  ram_util_percent: 44.015\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049317371656821936\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470753827083243\n",
      "  mean_inference_ms: 0.8511970002467174\n",
      "  mean_raw_obs_processing_ms: 0.08841350040818403\n",
      "time_since_restore: 1417.8901121616364\n",
      "time_this_iter_s: 5.207102537155151\n",
      "time_total_s: 1417.8901121616364\n",
      "timers:\n",
      "  learn_throughput: 2119.062\n",
      "  learn_time_ms: 1887.628\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.564\n",
      "  sample_time_ms: 3476.555\n",
      "  update_time_ms: 1.558\n",
      "timestamp: 1633511715\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1052000\n",
      "training_iteration: 263\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:263 starting ! -----------------\n",
      "agent_timesteps_total: 1056000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-15-30\n",
      "done: false\n",
      "episode_len_mean: 690.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.14637400000000267\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 761\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9464141726493835\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009496849961578846\n",
      "        model: {}\n",
      "        policy_loss: -0.015933601185679436\n",
      "        total_loss: 0.01967780478298664\n",
      "        vf_explained_var: 0.15966103971004486\n",
      "        vf_loss: 0.033712033182382584\n",
      "  num_agent_steps_sampled: 1056000\n",
      "  num_agent_steps_trained: 1056000\n",
      "  num_steps_sampled: 1056000\n",
      "  num_steps_trained: 1056000\n",
      "iterations_since_restore: 264\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.133333333333336\n",
      "  ram_util_percent: 44.009523809523806\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04931811929207025\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470784562152055\n",
      "  mean_inference_ms: 0.8511680888772679\n",
      "  mean_raw_obs_processing_ms: 0.08841604479104367\n",
      "time_since_restore: 1423.2620613574982\n",
      "time_this_iter_s: 5.371949195861816\n",
      "time_total_s: 1423.2620613574982\n",
      "timers:\n",
      "  learn_throughput: 2121.209\n",
      "  learn_time_ms: 1885.717\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1150.305\n",
      "  sample_time_ms: 3477.339\n",
      "  update_time_ms: 1.557\n",
      "timestamp: 1633511730\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1056000\n",
      "training_iteration: 264\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:264 starting ! -----------------\n",
      "agent_timesteps_total: 1060000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-15-51\n",
      "done: false\n",
      "episode_len_mean: 692.01\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.1573850000000027\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 764\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1031668186187744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016513610258698463\n",
      "        model: {}\n",
      "        policy_loss: -0.03402126953005791\n",
      "        total_loss: 0.008446999825537205\n",
      "        vf_explained_var: 0.231750026345253\n",
      "        vf_loss: 0.03916555270552635\n",
      "  num_agent_steps_sampled: 1060000\n",
      "  num_agent_steps_trained: 1060000\n",
      "  num_steps_sampled: 1060000\n",
      "  num_steps_trained: 1060000\n",
      "iterations_since_restore: 265\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.76896551724138\n",
      "  ram_util_percent: 44.04137931034482\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04931940003038057\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.74707726443083\n",
      "  mean_inference_ms: 0.851140483612594\n",
      "  mean_raw_obs_processing_ms: 0.08841826047025232\n",
      "time_since_restore: 1428.6950023174286\n",
      "time_this_iter_s: 5.43294095993042\n",
      "time_total_s: 1428.6950023174286\n",
      "timers:\n",
      "  learn_throughput: 2119.924\n",
      "  learn_time_ms: 1886.86\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1148.918\n",
      "  sample_time_ms: 3481.538\n",
      "  update_time_ms: 1.557\n",
      "timestamp: 1633511751\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1060000\n",
      "training_iteration: 265\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:265 starting ! -----------------\n",
      "agent_timesteps_total: 1064000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-16-15\n",
      "done: false\n",
      "episode_len_mean: 692.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.21768450000000267\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 768\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1070855855941772\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009815171360969543\n",
      "        model: {}\n",
      "        policy_loss: -0.02178492769598961\n",
      "        total_loss: 0.023402687162160873\n",
      "        vf_explained_var: -0.06732503324747086\n",
      "        vf_loss: 0.043224580585956573\n",
      "  num_agent_steps_sampled: 1064000\n",
      "  num_agent_steps_trained: 1064000\n",
      "  num_steps_sampled: 1064000\n",
      "  num_steps_trained: 1064000\n",
      "iterations_since_restore: 266\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.53235294117647\n",
      "  ram_util_percent: 44.01470588235294\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04932035160261619\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470765201109114\n",
      "  mean_inference_ms: 0.8511077857036874\n",
      "  mean_raw_obs_processing_ms: 0.08842193286773681\n",
      "time_since_restore: 1434.073528289795\n",
      "time_this_iter_s: 5.378525972366333\n",
      "time_total_s: 1434.073528289795\n",
      "timers:\n",
      "  learn_throughput: 2133.926\n",
      "  learn_time_ms: 1874.479\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.967\n",
      "  sample_time_ms: 3496.603\n",
      "  update_time_ms: 1.557\n",
      "timestamp: 1633511775\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1064000\n",
      "training_iteration: 266\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:266 starting ! -----------------\n",
      "agent_timesteps_total: 1068000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-16-36\n",
      "done: false\n",
      "episode_len_mean: 689.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.20795750000000257\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 771\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9284660220146179\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009291587397456169\n",
      "        model: {}\n",
      "        policy_loss: -0.018214689567685127\n",
      "        total_loss: 0.006830303464084864\n",
      "        vf_explained_var: -0.021373366937041283\n",
      "        vf_loss: 0.023186679929494858\n",
      "  num_agent_steps_sampled: 1068000\n",
      "  num_agent_steps_trained: 1068000\n",
      "  num_steps_sampled: 1068000\n",
      "  num_steps_trained: 1068000\n",
      "iterations_since_restore: 267\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.41379310344828\n",
      "  ram_util_percent: 44.00689655172414\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04932027652391982\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470825438056262\n",
      "  mean_inference_ms: 0.8510846398625368\n",
      "  mean_raw_obs_processing_ms: 0.0884264825599702\n",
      "time_since_restore: 1439.6861662864685\n",
      "time_this_iter_s: 5.612637996673584\n",
      "time_total_s: 1439.6861662864685\n",
      "timers:\n",
      "  learn_throughput: 2124.561\n",
      "  learn_time_ms: 1882.742\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.142\n",
      "  sample_time_ms: 3508.336\n",
      "  update_time_ms: 1.58\n",
      "timestamp: 1633511796\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1068000\n",
      "training_iteration: 267\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:267 starting ! -----------------\n",
      "agent_timesteps_total: 1072000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-17-00\n",
      "done: false\n",
      "episode_len_mean: 683.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.2584705000000025\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 773\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0575190782546997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015088830143213272\n",
      "        model: {}\n",
      "        policy_loss: -0.021159814670681953\n",
      "        total_loss: 0.019702773541212082\n",
      "        vf_explained_var: -0.16874752938747406\n",
      "        vf_loss: 0.037844814360141754\n",
      "  num_agent_steps_sampled: 1072000\n",
      "  num_agent_steps_trained: 1072000\n",
      "  num_steps_sampled: 1072000\n",
      "  num_steps_trained: 1072000\n",
      "iterations_since_restore: 268\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.061764705882357\n",
      "  ram_util_percent: 44.035294117647055\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04932077555360963\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470884116114034\n",
      "  mean_inference_ms: 0.8510672680855871\n",
      "  mean_raw_obs_processing_ms: 0.08842990105696913\n",
      "time_since_restore: 1445.0925493240356\n",
      "time_this_iter_s: 5.406383037567139\n",
      "time_total_s: 1445.0925493240356\n",
      "timers:\n",
      "  learn_throughput: 2118.928\n",
      "  learn_time_ms: 1887.747\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.296\n",
      "  sample_time_ms: 3510.94\n",
      "  update_time_ms: 1.68\n",
      "timestamp: 1633511820\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1072000\n",
      "training_iteration: 268\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:268 starting ! -----------------\n",
      "agent_timesteps_total: 1076000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-17-20\n",
      "done: false\n",
      "episode_len_mean: 692.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.14684500000000256\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 775\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9819411039352417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009470623917877674\n",
      "        model: {}\n",
      "        policy_loss: -0.014591141603887081\n",
      "        total_loss: 0.0108006177470088\n",
      "        vf_explained_var: -0.03997308760881424\n",
      "        vf_loss: 0.023497633635997772\n",
      "  num_agent_steps_sampled: 1076000\n",
      "  num_agent_steps_trained: 1076000\n",
      "  num_steps_sampled: 1076000\n",
      "  num_steps_trained: 1076000\n",
      "iterations_since_restore: 269\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.73214285714286\n",
      "  ram_util_percent: 44.010714285714286\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04932117897914637\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7470972001334732\n",
      "  mean_inference_ms: 0.8510497693116273\n",
      "  mean_raw_obs_processing_ms: 0.08843321336743185\n",
      "time_since_restore: 1450.5269632339478\n",
      "time_this_iter_s: 5.434413909912109\n",
      "time_total_s: 1450.5269632339478\n",
      "timers:\n",
      "  learn_throughput: 2124.653\n",
      "  learn_time_ms: 1882.66\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.482\n",
      "  sample_time_ms: 3513.451\n",
      "  update_time_ms: 1.68\n",
      "timestamp: 1633511840\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1076000\n",
      "training_iteration: 269\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:269 starting ! -----------------\n",
      "agent_timesteps_total: 1080000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-17-34\n",
      "done: false\n",
      "episode_len_mean: 698.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.10661900000000257\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 778\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.151318907737732\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012302787974476814\n",
      "        model: {}\n",
      "        policy_loss: -0.02386470139026642\n",
      "        total_loss: 0.03463901951909065\n",
      "        vf_explained_var: 0.07317723333835602\n",
      "        vf_loss: 0.05604316294193268\n",
      "  num_agent_steps_sampled: 1080000\n",
      "  num_agent_steps_trained: 1080000\n",
      "  num_steps_sampled: 1080000\n",
      "  num_steps_trained: 1080000\n",
      "iterations_since_restore: 270\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.352380952380948\n",
      "  ram_util_percent: 44.0\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04932059624305432\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7471067199954292\n",
      "  mean_inference_ms: 0.8510241398129412\n",
      "  mean_raw_obs_processing_ms: 0.0884380428490606\n",
      "time_since_restore: 1455.9281167984009\n",
      "time_this_iter_s: 5.401153564453125\n",
      "time_total_s: 1455.9281167984009\n",
      "timers:\n",
      "  learn_throughput: 2119.307\n",
      "  learn_time_ms: 1887.409\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.931\n",
      "  sample_time_ms: 3518.245\n",
      "  update_time_ms: 1.68\n",
      "timestamp: 1633511854\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1080000\n",
      "training_iteration: 270\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:270 starting ! -----------------\n",
      "agent_timesteps_total: 1084000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-17-54\n",
      "done: false\n",
      "episode_len_mean: 686.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9280500000000007\n",
      "episode_reward_mean: 0.2680255000000024\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 782\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0088489055633545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010865654796361923\n",
      "        model: {}\n",
      "        policy_loss: -0.028887374326586723\n",
      "        total_loss: 0.004031309392303228\n",
      "        vf_explained_var: 0.10526605695486069\n",
      "        vf_loss: 0.030745552852749825\n",
      "  num_agent_steps_sampled: 1084000\n",
      "  num_agent_steps_trained: 1084000\n",
      "  num_steps_sampled: 1084000\n",
      "  num_steps_trained: 1084000\n",
      "iterations_since_restore: 271\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.462962962962962\n",
      "  ram_util_percent: 44.03703703703704\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049316662184748596\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7471257644367857\n",
      "  mean_inference_ms: 0.8509927871790817\n",
      "  mean_raw_obs_processing_ms: 0.08844498176754435\n",
      "time_since_restore: 1461.2997136116028\n",
      "time_this_iter_s: 5.371596813201904\n",
      "time_total_s: 1461.2997136116028\n",
      "timers:\n",
      "  learn_throughput: 2131.962\n",
      "  learn_time_ms: 1876.206\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.681\n",
      "  sample_time_ms: 3515.924\n",
      "  update_time_ms: 1.78\n",
      "timestamp: 1633511874\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1084000\n",
      "training_iteration: 271\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:271 starting ! -----------------\n",
      "agent_timesteps_total: 1088000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-18-18\n",
      "done: false\n",
      "episode_len_mean: 678.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.37913600000000236\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 786\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0652291774749756\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011459046974778175\n",
      "        model: {}\n",
      "        policy_loss: -0.026366127654910088\n",
      "        total_loss: 0.00411513727158308\n",
      "        vf_explained_var: 0.5452303886413574\n",
      "        vf_loss: 0.02818945422768593\n",
      "  num_agent_steps_sampled: 1088000\n",
      "  num_agent_steps_trained: 1088000\n",
      "  num_steps_sampled: 1088000\n",
      "  num_steps_trained: 1088000\n",
      "iterations_since_restore: 272\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.711764705882352\n",
      "  ram_util_percent: 44.023529411764706\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04931106930264747\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7471362996288865\n",
      "  mean_inference_ms: 0.8509616368655257\n",
      "  mean_raw_obs_processing_ms: 0.08845249802899235\n",
      "time_since_restore: 1466.5906059741974\n",
      "time_this_iter_s: 5.2908923625946045\n",
      "time_total_s: 1466.5906059741974\n",
      "timers:\n",
      "  learn_throughput: 2136.662\n",
      "  learn_time_ms: 1872.079\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.188\n",
      "  sample_time_ms: 3511.272\n",
      "  update_time_ms: 1.68\n",
      "timestamp: 1633511898\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1088000\n",
      "training_iteration: 272\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:272 starting ! -----------------\n",
      "agent_timesteps_total: 1092000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-18-31\n",
      "done: false\n",
      "episode_len_mean: 680.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.3297130000000024\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 789\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.017961025238037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01206945814192295\n",
      "        model: {}\n",
      "        policy_loss: -0.018008476123213768\n",
      "        total_loss: -0.0002883142151404172\n",
      "        vf_explained_var: 0.22495587170124054\n",
      "        vf_loss: 0.015306264162063599\n",
      "  num_agent_steps_sampled: 1092000\n",
      "  num_agent_steps_trained: 1092000\n",
      "  num_steps_sampled: 1092000\n",
      "  num_steps_trained: 1092000\n",
      "iterations_since_restore: 273\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.850000000000005\n",
      "  ram_util_percent: 44.03333333333333\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04930702153795107\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7471424604022886\n",
      "  mean_inference_ms: 0.8509379983534554\n",
      "  mean_raw_obs_processing_ms: 0.08845809388892349\n",
      "time_since_restore: 1471.9296038150787\n",
      "time_this_iter_s: 5.338997840881348\n",
      "time_total_s: 1471.9296038150787\n",
      "timers:\n",
      "  learn_throughput: 2136.595\n",
      "  learn_time_ms: 1872.138\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.953\n",
      "  sample_time_ms: 3524.374\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633511911\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1092000\n",
      "training_iteration: 273\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:273 starting ! -----------------\n",
      "agent_timesteps_total: 1096000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-18-46\n",
      "done: false\n",
      "episode_len_mean: 678.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.3298590000000024\n",
      "episode_reward_min: -2.216600000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 791\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.044495940208435\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011410885490477085\n",
      "        model: {}\n",
      "        policy_loss: -0.012623819522559643\n",
      "        total_loss: 0.03523361682891846\n",
      "        vf_explained_var: -0.04064468294382095\n",
      "        vf_loss: 0.04557526111602783\n",
      "  num_agent_steps_sampled: 1096000\n",
      "  num_agent_steps_trained: 1096000\n",
      "  num_steps_sampled: 1096000\n",
      "  num_steps_trained: 1096000\n",
      "iterations_since_restore: 274\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.257142857142856\n",
      "  ram_util_percent: 44.06190476190476\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049304081144241535\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7471499522816336\n",
      "  mean_inference_ms: 0.8509223085686469\n",
      "  mean_raw_obs_processing_ms: 0.08846191011121725\n",
      "time_since_restore: 1477.3504157066345\n",
      "time_this_iter_s: 5.420811891555786\n",
      "time_total_s: 1477.3504157066345\n",
      "timers:\n",
      "  learn_throughput: 2138.234\n",
      "  learn_time_ms: 1870.702\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.935\n",
      "  sample_time_ms: 3530.652\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633511926\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1096000\n",
      "training_iteration: 274\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:274 starting ! -----------------\n",
      "agent_timesteps_total: 1100000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-19-04\n",
      "done: false\n",
      "episode_len_mean: 667.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.42020850000000215\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 796\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9258580207824707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01842038705945015\n",
      "        model: {}\n",
      "        policy_loss: -0.029026299715042114\n",
      "        total_loss: -0.002203374868258834\n",
      "        vf_explained_var: 0.36564862728118896\n",
      "        vf_loss: 0.023138849064707756\n",
      "  num_agent_steps_sampled: 1100000\n",
      "  num_agent_steps_trained: 1100000\n",
      "  num_steps_sampled: 1100000\n",
      "  num_steps_trained: 1100000\n",
      "iterations_since_restore: 275\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.238461538461536\n",
      "  ram_util_percent: 44.05384615384616\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049295455387419744\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7471768558274383\n",
      "  mean_inference_ms: 0.8508910245823054\n",
      "  mean_raw_obs_processing_ms: 0.08846715680104658\n",
      "time_since_restore: 1482.7428739070892\n",
      "time_this_iter_s: 5.392458200454712\n",
      "time_total_s: 1482.7428739070892\n",
      "timers:\n",
      "  learn_throughput: 2137.517\n",
      "  learn_time_ms: 1871.33\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.361\n",
      "  sample_time_ms: 3526.214\n",
      "  update_time_ms: 1.596\n",
      "timestamp: 1633511944\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1100000\n",
      "training_iteration: 275\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:275 starting ! -----------------\n",
      "agent_timesteps_total: 1104000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-19-16\n",
      "done: false\n",
      "episode_len_mean: 667.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.31921300000000236\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 799\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.028112769126892\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009893066249787807\n",
      "        model: {}\n",
      "        policy_loss: -0.02106343023478985\n",
      "        total_loss: -0.005570553243160248\n",
      "        vf_explained_var: 0.307933509349823\n",
      "        vf_loss: 0.0135142607614398\n",
      "  num_agent_steps_sampled: 1104000\n",
      "  num_agent_steps_trained: 1104000\n",
      "  num_steps_sampled: 1104000\n",
      "  num_steps_trained: 1104000\n",
      "iterations_since_restore: 276\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.98235294117647\n",
      "  ram_util_percent: 44.00588235294118\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929000467063137\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.747194076551782\n",
      "  mean_inference_ms: 0.8508743956559338\n",
      "  mean_raw_obs_processing_ms: 0.08846837140005805\n",
      "time_since_restore: 1488.0990643501282\n",
      "time_this_iter_s: 5.35619044303894\n",
      "time_total_s: 1488.0990643501282\n",
      "timers:\n",
      "  learn_throughput: 2135.967\n",
      "  learn_time_ms: 1872.688\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.505\n",
      "  sample_time_ms: 3522.663\n",
      "  update_time_ms: 1.596\n",
      "timestamp: 1633511956\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1104000\n",
      "training_iteration: 276\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:276 starting ! -----------------\n",
      "agent_timesteps_total: 1108000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-19-37\n",
      "done: false\n",
      "episode_len_mean: 655.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.4303280000000024\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 803\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0119400024414062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009937041439116001\n",
      "        model: {}\n",
      "        policy_loss: -0.025515422224998474\n",
      "        total_loss: -0.0005680079339072108\n",
      "        vf_explained_var: 0.2553205192089081\n",
      "        vf_loss: 0.022960010915994644\n",
      "  num_agent_steps_sampled: 1108000\n",
      "  num_agent_steps_trained: 1108000\n",
      "  num_steps_sampled: 1108000\n",
      "  num_steps_trained: 1108000\n",
      "iterations_since_restore: 277\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.50689655172414\n",
      "  ram_util_percent: 44.02758620689655\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928279900062769\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472153072466234\n",
      "  mean_inference_ms: 0.8508520436196199\n",
      "  mean_raw_obs_processing_ms: 0.08846908092246891\n",
      "time_since_restore: 1493.4465816020966\n",
      "time_this_iter_s: 5.347517251968384\n",
      "time_total_s: 1493.4465816020966\n",
      "timers:\n",
      "  learn_throughput: 2144.353\n",
      "  learn_time_ms: 1865.365\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.707\n",
      "  sample_time_ms: 3503.526\n",
      "  update_time_ms: 1.496\n",
      "timestamp: 1633511977\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1108000\n",
      "training_iteration: 277\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:277 starting ! -----------------\n",
      "agent_timesteps_total: 1112000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-19-50\n",
      "done: false\n",
      "episode_len_mean: 655.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.4291515000000025\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 805\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9733104705810547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010049110278487206\n",
      "        model: {}\n",
      "        policy_loss: -0.026487763971090317\n",
      "        total_loss: -0.009400973096489906\n",
      "        vf_explained_var: 0.21576324105262756\n",
      "        vf_loss: 0.015076973475515842\n",
      "  num_agent_steps_sampled: 1112000\n",
      "  num_agent_steps_trained: 1112000\n",
      "  num_steps_sampled: 1112000\n",
      "  num_steps_trained: 1112000\n",
      "iterations_since_restore: 278\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.78888888888889\n",
      "  ram_util_percent: 44.03333333333333\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927954758715277\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472295263027405\n",
      "  mean_inference_ms: 0.8508448924999941\n",
      "  mean_raw_obs_processing_ms: 0.0884691165021279\n",
      "time_since_restore: 1498.9867384433746\n",
      "time_this_iter_s: 5.540156841278076\n",
      "time_total_s: 1498.9867384433746\n",
      "timers:\n",
      "  learn_throughput: 2151.666\n",
      "  learn_time_ms: 1859.025\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.329\n",
      "  sample_time_ms: 3523.206\n",
      "  update_time_ms: 1.396\n",
      "timestamp: 1633511990\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1112000\n",
      "training_iteration: 278\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:278 starting ! -----------------\n",
      "agent_timesteps_total: 1116000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-20-09\n",
      "done: false\n",
      "episode_len_mean: 652.97\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.5301115000000023\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 808\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0029593706130981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010242295451462269\n",
      "        model: {}\n",
      "        policy_loss: -0.022021114826202393\n",
      "        total_loss: -0.007337338291108608\n",
      "        vf_explained_var: 0.37174513936042786\n",
      "        vf_loss: 0.012635321356356144\n",
      "  num_agent_steps_sampled: 1116000\n",
      "  num_agent_steps_trained: 1116000\n",
      "  num_steps_sampled: 1116000\n",
      "  num_steps_trained: 1116000\n",
      "iterations_since_restore: 279\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.466666666666665\n",
      "  ram_util_percent: 44.01111111111112\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927662691789535\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472485677788342\n",
      "  mean_inference_ms: 0.8508335520589179\n",
      "  mean_raw_obs_processing_ms: 0.08846912160820039\n",
      "time_since_restore: 1504.3709981441498\n",
      "time_this_iter_s: 5.3842597007751465\n",
      "time_total_s: 1504.3709981441498\n",
      "timers:\n",
      "  learn_throughput: 2148.803\n",
      "  learn_time_ms: 1861.501\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.67\n",
      "  sample_time_ms: 3515.957\n",
      "  update_time_ms: 1.496\n",
      "timestamp: 1633512009\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1116000\n",
      "training_iteration: 279\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:279 starting ! -----------------\n",
      "agent_timesteps_total: 1120000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-20-27\n",
      "done: false\n",
      "episode_len_mean: 659.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.5400335000000024\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 811\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9873171448707581\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009803780354559422\n",
      "        model: {}\n",
      "        policy_loss: -0.04486655816435814\n",
      "        total_loss: -0.029632654041051865\n",
      "        vf_explained_var: 0.13928845524787903\n",
      "        vf_loss: 0.013273146003484726\n",
      "  num_agent_steps_sampled: 1120000\n",
      "  num_agent_steps_trained: 1120000\n",
      "  num_steps_sampled: 1120000\n",
      "  num_steps_trained: 1120000\n",
      "iterations_since_restore: 280\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.64\n",
      "  ram_util_percent: 43.996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927299346287881\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472590629509596\n",
      "  mean_inference_ms: 0.8508227514103832\n",
      "  mean_raw_obs_processing_ms: 0.08846926163356866\n",
      "time_since_restore: 1509.5974514484406\n",
      "time_this_iter_s: 5.2264533042907715\n",
      "time_total_s: 1509.5974514484406\n",
      "timers:\n",
      "  learn_throughput: 2153.799\n",
      "  learn_time_ms: 1857.184\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.943\n",
      "  sample_time_ms: 3502.801\n",
      "  update_time_ms: 1.496\n",
      "timestamp: 1633512027\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1120000\n",
      "training_iteration: 280\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:280 starting ! -----------------\n",
      "agent_timesteps_total: 1124000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-20-42\n",
      "done: false\n",
      "episode_len_mean: 659.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.6001130000000025\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 814\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9819862246513367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008438865654170513\n",
      "        model: {}\n",
      "        policy_loss: -0.016983307898044586\n",
      "        total_loss: -0.00913187488913536\n",
      "        vf_explained_var: 0.36805260181427\n",
      "        vf_loss: 0.006163663696497679\n",
      "  num_agent_steps_sampled: 1124000\n",
      "  num_agent_steps_trained: 1124000\n",
      "  num_steps_sampled: 1124000\n",
      "  num_steps_trained: 1124000\n",
      "iterations_since_restore: 281\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.476190476190474\n",
      "  ram_util_percent: 43.99523809523809\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926779743043421\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472675424017803\n",
      "  mean_inference_ms: 0.8508098838094249\n",
      "  mean_raw_obs_processing_ms: 0.0884693037803921\n",
      "time_since_restore: 1514.9530794620514\n",
      "time_this_iter_s: 5.35562801361084\n",
      "time_total_s: 1514.9530794620514\n",
      "timers:\n",
      "  learn_throughput: 2146.119\n",
      "  learn_time_ms: 1863.83\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.637\n",
      "  sample_time_ms: 3494.559\n",
      "  update_time_ms: 1.496\n",
      "timestamp: 1633512042\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1124000\n",
      "training_iteration: 281\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:281 starting ! -----------------\n",
      "agent_timesteps_total: 1128000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-21-06\n",
      "done: false\n",
      "episode_len_mean: 664.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.4992045000000025\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 816\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9632439017295837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01143941655755043\n",
      "        model: {}\n",
      "        policy_loss: -0.018007175996899605\n",
      "        total_loss: -0.0025447553489357233\n",
      "        vf_explained_var: 0.2576530873775482\n",
      "        vf_loss: 0.013174536637961864\n",
      "  num_agent_steps_sampled: 1128000\n",
      "  num_agent_steps_trained: 1128000\n",
      "  num_steps_sampled: 1128000\n",
      "  num_steps_trained: 1128000\n",
      "iterations_since_restore: 282\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.673529411764704\n",
      "  ram_util_percent: 44.029411764705884\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049264876632019235\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472778206640743\n",
      "  mean_inference_ms: 0.8508007296695745\n",
      "  mean_raw_obs_processing_ms: 0.08846846263351221\n",
      "time_since_restore: 1520.335117816925\n",
      "time_this_iter_s: 5.382038354873657\n",
      "time_total_s: 1520.335117816925\n",
      "timers:\n",
      "  learn_throughput: 2146.904\n",
      "  learn_time_ms: 1863.148\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.419\n",
      "  sample_time_ms: 3504.409\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633512066\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1128000\n",
      "training_iteration: 282\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:282 starting ! -----------------\n",
      "agent_timesteps_total: 1132000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-21-24\n",
      "done: false\n",
      "episode_len_mean: 666.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9238000000000004\n",
      "episode_reward_mean: 0.4987980000000026\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 819\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9445852041244507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013037039898335934\n",
      "        model: {}\n",
      "        policy_loss: -0.023332515731453896\n",
      "        total_loss: -0.00013969790597911924\n",
      "        vf_explained_var: 0.3531074821949005\n",
      "        vf_loss: 0.02058541215956211\n",
      "  num_agent_steps_sampled: 1132000\n",
      "  num_agent_steps_trained: 1132000\n",
      "  num_steps_sampled: 1132000\n",
      "  num_steps_trained: 1132000\n",
      "iterations_since_restore: 283\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.542307692307695\n",
      "  ram_util_percent: 44.01538461538462\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492599471434256\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7472938134071496\n",
      "  mean_inference_ms: 0.8507856570825528\n",
      "  mean_raw_obs_processing_ms: 0.08846708392879142\n",
      "time_since_restore: 1525.6441612243652\n",
      "time_this_iter_s: 5.3090434074401855\n",
      "time_total_s: 1525.6441612243652\n",
      "timers:\n",
      "  learn_throughput: 2147.978\n",
      "  learn_time_ms: 1862.216\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.094\n",
      "  sample_time_ms: 3502.338\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633512084\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1132000\n",
      "training_iteration: 283\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:283 starting ! -----------------\n",
      "agent_timesteps_total: 1136000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-21-42\n",
      "done: false\n",
      "episode_len_mean: 663.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9222499999999996\n",
      "episode_reward_mean: 0.4484605000000025\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 822\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8942526578903198\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009175736457109451\n",
      "        model: {}\n",
      "        policy_loss: -0.026231640949845314\n",
      "        total_loss: 0.006109267473220825\n",
      "        vf_explained_var: 0.21828125417232513\n",
      "        vf_loss: 0.03050575964152813\n",
      "  num_agent_steps_sampled: 1136000\n",
      "  num_agent_steps_trained: 1136000\n",
      "  num_steps_sampled: 1136000\n",
      "  num_steps_trained: 1136000\n",
      "iterations_since_restore: 284\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.383333333333336\n",
      "  ram_util_percent: 44.025\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925478045566157\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7473127516641825\n",
      "  mean_inference_ms: 0.8507729626781599\n",
      "  mean_raw_obs_processing_ms: 0.08846600336494598\n",
      "time_since_restore: 1531.0694596767426\n",
      "time_this_iter_s: 5.425298452377319\n",
      "time_total_s: 1531.0694596767426\n",
      "timers:\n",
      "  learn_throughput: 2146.971\n",
      "  learn_time_ms: 1863.09\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.23\n",
      "  sample_time_ms: 3501.923\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633512102\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1136000\n",
      "training_iteration: 284\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:284 starting ! -----------------\n",
      "agent_timesteps_total: 1140000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-21-56\n",
      "done: false\n",
      "episode_len_mean: 668.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9222499999999996\n",
      "episode_reward_mean: 0.45745600000000264\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 824\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0031439065933228\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00927484966814518\n",
      "        model: {}\n",
      "        policy_loss: -0.02386740781366825\n",
      "        total_loss: -0.0011127677280455828\n",
      "        vf_explained_var: 0.20066004991531372\n",
      "        vf_loss: 0.02089967206120491\n",
      "  num_agent_steps_sampled: 1140000\n",
      "  num_agent_steps_trained: 1140000\n",
      "  num_steps_sampled: 1140000\n",
      "  num_steps_trained: 1140000\n",
      "iterations_since_restore: 285\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.585714285714282\n",
      "  ram_util_percent: 44.0904761904762\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925125869627818\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7473226639035695\n",
      "  mean_inference_ms: 0.8507644783496374\n",
      "  mean_raw_obs_processing_ms: 0.08846521769806458\n",
      "time_since_restore: 1536.3862612247467\n",
      "time_this_iter_s: 5.31680154800415\n",
      "time_total_s: 1536.3862612247467\n",
      "timers:\n",
      "  learn_throughput: 2147.756\n",
      "  learn_time_ms: 1862.409\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.533\n",
      "  sample_time_ms: 3494.876\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633512116\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1140000\n",
      "training_iteration: 285\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:285 starting ! -----------------\n",
      "agent_timesteps_total: 1144000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-22-20\n",
      "done: false\n",
      "episode_len_mean: 665.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9222499999999996\n",
      "episode_reward_mean: 0.5075990000000025\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 827\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9656625986099243\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01069041807204485\n",
      "        model: {}\n",
      "        policy_loss: -0.0257788747549057\n",
      "        total_loss: -0.0025271119084209204\n",
      "        vf_explained_var: 0.27613839507102966\n",
      "        vf_loss: 0.0211136806756258\n",
      "  num_agent_steps_sampled: 1144000\n",
      "  num_agent_steps_trained: 1144000\n",
      "  num_steps_sampled: 1144000\n",
      "  num_steps_trained: 1144000\n",
      "iterations_since_restore: 286\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.078787878787878\n",
      "  ram_util_percent: 44.03636363636363\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049245598705910956\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7473402228190845\n",
      "  mean_inference_ms: 0.850750201068728\n",
      "  mean_raw_obs_processing_ms: 0.08846343451830994\n",
      "time_since_restore: 1541.8401803970337\n",
      "time_this_iter_s: 5.453919172286987\n",
      "time_total_s: 1541.8401803970337\n",
      "timers:\n",
      "  learn_throughput: 2136.919\n",
      "  learn_time_ms: 1871.854\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.464\n",
      "  sample_time_ms: 3495.087\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633512140\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1144000\n",
      "training_iteration: 286\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:286 starting ! -----------------\n",
      "agent_timesteps_total: 1148000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-22-33\n",
      "done: false\n",
      "episode_len_mean: 667.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.5074555000000026\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 830\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.038499355316162\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014677195809781551\n",
      "        model: {}\n",
      "        policy_loss: -0.018136372789740562\n",
      "        total_loss: 0.027373772114515305\n",
      "        vf_explained_var: 0.049620192497968674\n",
      "        vf_loss: 0.042574699968099594\n",
      "  num_agent_steps_sampled: 1148000\n",
      "  num_agent_steps_trained: 1148000\n",
      "  num_steps_sampled: 1148000\n",
      "  num_steps_trained: 1148000\n",
      "iterations_since_restore: 287\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.92777777777778\n",
      "  ram_util_percent: 44.02777777777778\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924060065852356\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7473596030875862\n",
      "  mean_inference_ms: 0.8507348483218372\n",
      "  mean_raw_obs_processing_ms: 0.08846197489582096\n",
      "time_since_restore: 1547.2220675945282\n",
      "time_this_iter_s: 5.381887197494507\n",
      "time_total_s: 1547.2220675945282\n",
      "timers:\n",
      "  learn_throughput: 2135.256\n",
      "  learn_time_ms: 1873.312\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.823\n",
      "  sample_time_ms: 3497.043\n",
      "  update_time_ms: 1.694\n",
      "timestamp: 1633512153\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1148000\n",
      "training_iteration: 287\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:287 starting ! -----------------\n",
      "agent_timesteps_total: 1152000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-22-44\n",
      "done: false\n",
      "episode_len_mean: 675.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.4460895000000026\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 832\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9586322903633118\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010301965288817883\n",
      "        model: {}\n",
      "        policy_loss: -0.017476987093687057\n",
      "        total_loss: 0.002187899546697736\n",
      "        vf_explained_var: 0.0790686160326004\n",
      "        vf_loss: 0.017604487016797066\n",
      "  num_agent_steps_sampled: 1152000\n",
      "  num_agent_steps_trained: 1152000\n",
      "  num_steps_sampled: 1152000\n",
      "  num_steps_trained: 1152000\n",
      "iterations_since_restore: 288\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.46875\n",
      "  ram_util_percent: 44.025\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923765630764142\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.747374920958451\n",
      "  mean_inference_ms: 0.8507244676881806\n",
      "  mean_raw_obs_processing_ms: 0.08846047766089146\n",
      "time_since_restore: 1552.6344954967499\n",
      "time_this_iter_s: 5.41242790222168\n",
      "time_total_s: 1552.6344954967499\n",
      "timers:\n",
      "  learn_throughput: 2135.948\n",
      "  learn_time_ms: 1872.705\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.8\n",
      "  sample_time_ms: 3484.927\n",
      "  update_time_ms: 1.694\n",
      "timestamp: 1633512164\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1152000\n",
      "training_iteration: 288\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:288 starting ! -----------------\n",
      "agent_timesteps_total: 1156000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-23-04\n",
      "done: false\n",
      "episode_len_mean: 676.89\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.4664895000000028\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 835\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.029831051826477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01105060987174511\n",
      "        model: {}\n",
      "        policy_loss: -0.02780662477016449\n",
      "        total_loss: -0.014395820908248425\n",
      "        vf_explained_var: 0.23739182949066162\n",
      "        vf_loss: 0.011200683191418648\n",
      "  num_agent_steps_sampled: 1156000\n",
      "  num_agent_steps_trained: 1156000\n",
      "  num_steps_sampled: 1156000\n",
      "  num_steps_trained: 1156000\n",
      "iterations_since_restore: 289\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.337931034482757\n",
      "  ram_util_percent: 44.03448275862069\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049233045027555634\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7473923803440379\n",
      "  mean_inference_ms: 0.8507111664598862\n",
      "  mean_raw_obs_processing_ms: 0.08845914765289101\n",
      "time_since_restore: 1557.9161286354065\n",
      "time_this_iter_s: 5.281633138656616\n",
      "time_total_s: 1557.9161286354065\n",
      "timers:\n",
      "  learn_throughput: 2140.234\n",
      "  learn_time_ms: 1868.954\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1149.994\n",
      "  sample_time_ms: 3478.279\n",
      "  update_time_ms: 1.683\n",
      "timestamp: 1633512184\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1156000\n",
      "training_iteration: 289\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:289 starting ! -----------------\n",
      "agent_timesteps_total: 1160000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-23-17\n",
      "done: false\n",
      "episode_len_mean: 676.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.40548550000000266\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 837\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.948891282081604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010847136378288269\n",
      "        model: {}\n",
      "        policy_loss: -0.02213834784924984\n",
      "        total_loss: -0.005262559279799461\n",
      "        vf_explained_var: -0.055922940373420715\n",
      "        vf_loss: 0.014706362038850784\n",
      "  num_agent_steps_sampled: 1160000\n",
      "  num_agent_steps_trained: 1160000\n",
      "  num_steps_sampled: 1160000\n",
      "  num_steps_trained: 1160000\n",
      "iterations_since_restore: 290\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.81764705882353\n",
      "  ram_util_percent: 44.04705882352941\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049229724143612014\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7474050907432567\n",
      "  mean_inference_ms: 0.8507032151319485\n",
      "  mean_raw_obs_processing_ms: 0.08845823038948829\n",
      "time_since_restore: 1563.2880148887634\n",
      "time_this_iter_s: 5.371886253356934\n",
      "time_total_s: 1563.2880148887634\n",
      "timers:\n",
      "  learn_throughput: 2132.756\n",
      "  learn_time_ms: 1875.507\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.351\n",
      "  sample_time_ms: 3486.291\n",
      "  update_time_ms: 1.783\n",
      "timestamp: 1633512197\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1160000\n",
      "training_iteration: 290\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:290 starting ! -----------------\n",
      "agent_timesteps_total: 1164000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-23-36\n",
      "done: false\n",
      "episode_len_mean: 688.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3543960000000027\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 840\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8684203028678894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010511703789234161\n",
      "        model: {}\n",
      "        policy_loss: -0.024898536503314972\n",
      "        total_loss: -0.009349931962788105\n",
      "        vf_explained_var: 0.06202877312898636\n",
      "        vf_loss: 0.01344626396894455\n",
      "  num_agent_steps_sampled: 1164000\n",
      "  num_agent_steps_trained: 1164000\n",
      "  num_steps_sampled: 1164000\n",
      "  num_steps_trained: 1164000\n",
      "iterations_since_restore: 291\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.759259259259256\n",
      "  ram_util_percent: 44.029629629629625\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922512388424706\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7474240547096858\n",
      "  mean_inference_ms: 0.8506900626423995\n",
      "  mean_raw_obs_processing_ms: 0.08845718782790629\n",
      "time_since_restore: 1568.6861567497253\n",
      "time_this_iter_s: 5.398141860961914\n",
      "time_total_s: 1568.6861567497253\n",
      "timers:\n",
      "  learn_throughput: 2140.718\n",
      "  learn_time_ms: 1868.532\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.699\n",
      "  sample_time_ms: 3497.425\n",
      "  update_time_ms: 1.784\n",
      "timestamp: 1633512216\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1164000\n",
      "training_iteration: 291\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:291 starting ! -----------------\n",
      "agent_timesteps_total: 1168000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-23-58\n",
      "done: false\n",
      "episode_len_mean: 682.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.39485400000000254\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 842\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8794530630111694\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012122145853936672\n",
      "        model: {}\n",
      "        policy_loss: -0.022495146840810776\n",
      "        total_loss: -0.0021565489005297422\n",
      "        vf_explained_var: 0.0253202635794878\n",
      "        vf_loss: 0.01791416108608246\n",
      "  num_agent_steps_sampled: 1168000\n",
      "  num_agent_steps_trained: 1168000\n",
      "  num_steps_sampled: 1168000\n",
      "  num_steps_trained: 1168000\n",
      "iterations_since_restore: 292\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.919354838709676\n",
      "  ram_util_percent: 44.054838709677405\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922192160882073\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.747439839013419\n",
      "  mean_inference_ms: 0.8506823831263302\n",
      "  mean_raw_obs_processing_ms: 0.08845614465272146\n",
      "time_since_restore: 1574.1425127983093\n",
      "time_this_iter_s: 5.456356048583984\n",
      "time_total_s: 1574.1425127983093\n",
      "timers:\n",
      "  learn_throughput: 2139.021\n",
      "  learn_time_ms: 1870.014\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.788\n",
      "  sample_time_ms: 3503.277\n",
      "  update_time_ms: 1.785\n",
      "timestamp: 1633512238\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1168000\n",
      "training_iteration: 292\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:292 starting ! -----------------\n",
      "agent_timesteps_total: 1172000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-24-13\n",
      "done: false\n",
      "episode_len_mean: 679.97\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3947535000000024\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 846\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0968459844589233\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010313001461327076\n",
      "        model: {}\n",
      "        policy_loss: -0.0198260098695755\n",
      "        total_loss: 0.019434792920947075\n",
      "        vf_explained_var: -0.1973462551832199\n",
      "        vf_loss: 0.037198204547166824\n",
      "  num_agent_steps_sampled: 1172000\n",
      "  num_agent_steps_trained: 1172000\n",
      "  num_steps_sampled: 1172000\n",
      "  num_steps_trained: 1172000\n",
      "iterations_since_restore: 293\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.0\n",
      "  ram_util_percent: 44.06190476190476\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492162212075102\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.747469215860629\n",
      "  mean_inference_ms: 0.850666802074025\n",
      "  mean_raw_obs_processing_ms: 0.08845236362139343\n",
      "time_since_restore: 1579.5944576263428\n",
      "time_this_iter_s: 5.451944828033447\n",
      "time_total_s: 1579.5944576263428\n",
      "timers:\n",
      "  learn_throughput: 2128.243\n",
      "  learn_time_ms: 1879.485\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.24\n",
      "  sample_time_ms: 3508.034\n",
      "  update_time_ms: 1.785\n",
      "timestamp: 1633512253\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1172000\n",
      "training_iteration: 293\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:293 starting ! -----------------\n",
      "agent_timesteps_total: 1176000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-24-26\n",
      "done: false\n",
      "episode_len_mean: 687.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3251010000000024\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 849\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9904143810272217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01111739594489336\n",
      "        model: {}\n",
      "        policy_loss: -0.01711593009531498\n",
      "        total_loss: 0.006426096428185701\n",
      "        vf_explained_var: -0.2894439101219177\n",
      "        vf_loss: 0.021318545565009117\n",
      "  num_agent_steps_sampled: 1176000\n",
      "  num_agent_steps_trained: 1176000\n",
      "  num_steps_sampled: 1176000\n",
      "  num_steps_trained: 1176000\n",
      "iterations_since_restore: 294\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.373684210526314\n",
      "  ram_util_percent: 44.05263157894738\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049212232486056794\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7474869537447489\n",
      "  mean_inference_ms: 0.8506571597960092\n",
      "  mean_raw_obs_processing_ms: 0.0884478556812849\n",
      "time_since_restore: 1584.846836566925\n",
      "time_this_iter_s: 5.252378940582275\n",
      "time_total_s: 1584.846836566925\n",
      "timers:\n",
      "  learn_throughput: 2129.001\n",
      "  learn_time_ms: 1878.815\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.662\n",
      "  sample_time_ms: 3491.432\n",
      "  update_time_ms: 1.785\n",
      "timestamp: 1633512266\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1176000\n",
      "training_iteration: 294\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:294 starting ! -----------------\n",
      "agent_timesteps_total: 1180000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-24-40\n",
      "done: false\n",
      "episode_len_mean: 688.89\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3854650000000025\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 851\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8764085173606873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013483990915119648\n",
      "        model: {}\n",
      "        policy_loss: -0.02384600043296814\n",
      "        total_loss: 0.0017246067291125655\n",
      "        vf_explained_var: 0.03918156027793884\n",
      "        vf_loss: 0.022873802110552788\n",
      "  num_agent_steps_sampled: 1180000\n",
      "  num_agent_steps_trained: 1180000\n",
      "  num_steps_sampled: 1180000\n",
      "  num_steps_trained: 1180000\n",
      "iterations_since_restore: 295\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.111111111111107\n",
      "  ram_util_percent: 44.09444444444445\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920912479744484\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7475020538971374\n",
      "  mean_inference_ms: 0.8506510274412211\n",
      "  mean_raw_obs_processing_ms: 0.0884446721402971\n",
      "time_since_restore: 1590.3304176330566\n",
      "time_this_iter_s: 5.483581066131592\n",
      "time_total_s: 1590.3304176330566\n",
      "timers:\n",
      "  learn_throughput: 2124.282\n",
      "  learn_time_ms: 1882.989\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.55\n",
      "  sample_time_ms: 3504.009\n",
      "  update_time_ms: 1.885\n",
      "timestamp: 1633512280\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1180000\n",
      "training_iteration: 295\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:295 starting ! -----------------\n",
      "agent_timesteps_total: 1184000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-24-54\n",
      "done: false\n",
      "episode_len_mean: 693.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3341250000000025\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 854\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9410198330879211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010151552967727184\n",
      "        model: {}\n",
      "        policy_loss: -0.024886924773454666\n",
      "        total_loss: -0.013803244568407536\n",
      "        vf_explained_var: 0.3151650130748749\n",
      "        vf_loss: 0.009053368121385574\n",
      "  num_agent_steps_sampled: 1184000\n",
      "  num_agent_steps_trained: 1184000\n",
      "  num_steps_sampled: 1184000\n",
      "  num_steps_trained: 1184000\n",
      "iterations_since_restore: 296\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.415\n",
      "  ram_util_percent: 44.10000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920570950610171\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7475339418394101\n",
      "  mean_inference_ms: 0.8506433009758305\n",
      "  mean_raw_obs_processing_ms: 0.08843908490025656\n",
      "time_since_restore: 1595.9337840080261\n",
      "time_this_iter_s: 5.603366374969482\n",
      "time_total_s: 1595.9337840080261\n",
      "timers:\n",
      "  learn_throughput: 2134.47\n",
      "  learn_time_ms: 1874.002\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.752\n",
      "  sample_time_ms: 3528.108\n",
      "  update_time_ms: 1.885\n",
      "timestamp: 1633512294\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1184000\n",
      "training_iteration: 296\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:296 starting ! -----------------\n",
      "agent_timesteps_total: 1188000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-25-10\n",
      "done: false\n",
      "episode_len_mean: 696.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3331190000000027\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 856\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9112605452537537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009838299825787544\n",
      "        model: {}\n",
      "        policy_loss: -0.01615355722606182\n",
      "        total_loss: -0.006573954131454229\n",
      "        vf_explained_var: 0.2223585546016693\n",
      "        vf_loss: 0.007611940614879131\n",
      "  num_agent_steps_sampled: 1188000\n",
      "  num_agent_steps_trained: 1188000\n",
      "  num_steps_sampled: 1188000\n",
      "  num_steps_trained: 1188000\n",
      "iterations_since_restore: 297\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.727272727272727\n",
      "  ram_util_percent: 44.150000000000006\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920288625137095\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7475582168158356\n",
      "  mean_inference_ms: 0.8506373860800223\n",
      "  mean_raw_obs_processing_ms: 0.088434851225555\n",
      "time_since_restore: 1601.314457654953\n",
      "time_this_iter_s: 5.38067364692688\n",
      "time_total_s: 1601.314457654953\n",
      "timers:\n",
      "  learn_throughput: 2135.796\n",
      "  learn_time_ms: 1872.838\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.437\n",
      "  sample_time_ms: 3529.089\n",
      "  update_time_ms: 1.885\n",
      "timestamp: 1633512310\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1188000\n",
      "training_iteration: 297\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:297 starting ! -----------------\n",
      "agent_timesteps_total: 1192000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-25-31\n",
      "done: false\n",
      "episode_len_mean: 694.85\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3825660000000027\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 859\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9901409149169922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01115532498806715\n",
      "        model: {}\n",
      "        policy_loss: -0.020739721134305\n",
      "        total_loss: 0.002343416679650545\n",
      "        vf_explained_var: 0.09495250880718231\n",
      "        vf_loss: 0.020852072164416313\n",
      "  num_agent_steps_sampled: 1192000\n",
      "  num_agent_steps_trained: 1192000\n",
      "  num_steps_sampled: 1192000\n",
      "  num_steps_trained: 1192000\n",
      "iterations_since_restore: 298\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.04193548387097\n",
      "  ram_util_percent: 44.09999999999998\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04919927857337465\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7476029840012182\n",
      "  mean_inference_ms: 0.8506277596776907\n",
      "  mean_raw_obs_processing_ms: 0.08842855424422623\n",
      "time_since_restore: 1606.680065870285\n",
      "time_this_iter_s: 5.365608215332031\n",
      "time_total_s: 1606.680065870285\n",
      "timers:\n",
      "  learn_throughput: 2134.232\n",
      "  learn_time_ms: 1874.21\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.405\n",
      "  sample_time_ms: 3522.973\n",
      "  update_time_ms: 1.985\n",
      "timestamp: 1633512331\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1192000\n",
      "training_iteration: 298\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:298 starting ! -----------------\n",
      "agent_timesteps_total: 1196000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-25-44\n",
      "done: false\n",
      "episode_len_mean: 695.39\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.37190350000000266\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 862\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9503737688064575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010633292607963085\n",
      "        model: {}\n",
      "        policy_loss: -0.023938924074172974\n",
      "        total_loss: 0.0026039155200123787\n",
      "        vf_explained_var: 0.18757987022399902\n",
      "        vf_loss: 0.024416174739599228\n",
      "  num_agent_steps_sampled: 1196000\n",
      "  num_agent_steps_trained: 1196000\n",
      "  num_steps_sampled: 1196000\n",
      "  num_steps_trained: 1196000\n",
      "iterations_since_restore: 299\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.994117647058825\n",
      "  ram_util_percent: 44.11176470588235\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04919520786966327\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7476483450705369\n",
      "  mean_inference_ms: 0.8506171902231704\n",
      "  mean_raw_obs_processing_ms: 0.08842272439187635\n",
      "time_since_restore: 1612.0501370429993\n",
      "time_this_iter_s: 5.370071172714233\n",
      "time_total_s: 1612.0501370429993\n",
      "timers:\n",
      "  learn_throughput: 2133.661\n",
      "  learn_time_ms: 1874.712\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.71\n",
      "  sample_time_ms: 3531.355\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633512344\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1196000\n",
      "training_iteration: 299\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:299 starting ! -----------------\n",
      "agent_timesteps_total: 1200000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-26-08\n",
      "done: false\n",
      "episode_len_mean: 695.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3105850000000028\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 865\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0033074617385864\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012990685179829597\n",
      "        model: {}\n",
      "        policy_loss: -0.028439655900001526\n",
      "        total_loss: -0.012950615957379341\n",
      "        vf_explained_var: 0.33462268114089966\n",
      "        vf_loss: 0.012890897691249847\n",
      "  num_agent_steps_sampled: 1200000\n",
      "  num_agent_steps_trained: 1200000\n",
      "  num_steps_sampled: 1200000\n",
      "  num_steps_trained: 1200000\n",
      "iterations_since_restore: 300\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.10285714285714\n",
      "  ram_util_percent: 44.099999999999994\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04919100934332239\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7476996317410028\n",
      "  mean_inference_ms: 0.8506063043238883\n",
      "  mean_raw_obs_processing_ms: 0.08841733915687097\n",
      "time_since_restore: 1617.5906920433044\n",
      "time_this_iter_s: 5.540555000305176\n",
      "time_total_s: 1617.5906920433044\n",
      "timers:\n",
      "  learn_throughput: 2140.461\n",
      "  learn_time_ms: 1868.756\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1125.443\n",
      "  sample_time_ms: 3554.155\n",
      "  update_time_ms: 1.896\n",
      "timestamp: 1633512368\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1200000\n",
      "training_iteration: 300\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:300 starting ! -----------------\n",
      "agent_timesteps_total: 1204000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-26-28\n",
      "done: false\n",
      "episode_len_mean: 696.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.36053800000000286\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 867\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9060288071632385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010377941653132439\n",
      "        model: {}\n",
      "        policy_loss: -0.016073590144515038\n",
      "        total_loss: -0.004046086687594652\n",
      "        vf_explained_var: 0.17883038520812988\n",
      "        vf_loss: 0.009951913729310036\n",
      "  num_agent_steps_sampled: 1204000\n",
      "  num_agent_steps_trained: 1204000\n",
      "  num_steps_sampled: 1204000\n",
      "  num_steps_trained: 1204000\n",
      "iterations_since_restore: 301\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.19629629629629\n",
      "  ram_util_percent: 44.099999999999994\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918818537603903\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7477328641341038\n",
      "  mean_inference_ms: 0.8505978811166812\n",
      "  mean_raw_obs_processing_ms: 0.088413413719564\n",
      "time_since_restore: 1622.903898715973\n",
      "time_this_iter_s: 5.313206672668457\n",
      "time_total_s: 1622.903898715973\n",
      "timers:\n",
      "  learn_throughput: 2140.027\n",
      "  learn_time_ms: 1869.135\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.248\n",
      "  sample_time_ms: 3545.321\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633512388\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1204000\n",
      "training_iteration: 301\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:301 starting ! -----------------\n",
      "agent_timesteps_total: 1208000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-26-51\n",
      "done: false\n",
      "episode_len_mean: 703.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.31975800000000293\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 870\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.038630485534668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012504378333687782\n",
      "        model: {}\n",
      "        policy_loss: -0.026617825031280518\n",
      "        total_loss: -0.007277235388755798\n",
      "        vf_explained_var: 0.07589009404182434\n",
      "        vf_loss: 0.016839714720845222\n",
      "  num_agent_steps_sampled: 1208000\n",
      "  num_agent_steps_trained: 1208000\n",
      "  num_steps_sampled: 1208000\n",
      "  num_steps_trained: 1208000\n",
      "iterations_since_restore: 302\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.193939393939395\n",
      "  ram_util_percent: 44.121212121212125\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918450103561023\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7477810040624797\n",
      "  mean_inference_ms: 0.8505871926058515\n",
      "  mean_raw_obs_processing_ms: 0.08840666465519587\n",
      "time_since_restore: 1628.427761554718\n",
      "time_this_iter_s: 5.523862838745117\n",
      "time_total_s: 1628.427761554718\n",
      "timers:\n",
      "  learn_throughput: 2138.413\n",
      "  learn_time_ms: 1870.546\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.561\n",
      "  sample_time_ms: 3550.629\n",
      "  update_time_ms: 1.748\n",
      "timestamp: 1633512411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1208000\n",
      "training_iteration: 302\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:302 starting ! -----------------\n",
      "agent_timesteps_total: 1212000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-27-11\n",
      "done: false\n",
      "episode_len_mean: 708.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.32900050000000314\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 872\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8864784836769104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01339877862483263\n",
      "        model: {}\n",
      "        policy_loss: -0.02028176747262478\n",
      "        total_loss: -0.004031231161206961\n",
      "        vf_explained_var: 0.14832741022109985\n",
      "        vf_loss: 0.013570775277912617\n",
      "  num_agent_steps_sampled: 1212000\n",
      "  num_agent_steps_trained: 1212000\n",
      "  num_steps_sampled: 1212000\n",
      "  num_steps_trained: 1212000\n",
      "iterations_since_restore: 303\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.45714285714286\n",
      "  ram_util_percent: 44.09999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918199870647562\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7478113340813612\n",
      "  mean_inference_ms: 0.8505813620293461\n",
      "  mean_raw_obs_processing_ms: 0.08840162504302079\n",
      "time_since_restore: 1633.8220405578613\n",
      "time_this_iter_s: 5.3942790031433105\n",
      "time_total_s: 1633.8220405578613\n",
      "timers:\n",
      "  learn_throughput: 2146.54\n",
      "  learn_time_ms: 1863.464\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.124\n",
      "  sample_time_ms: 3552.005\n",
      "  update_time_ms: 1.748\n",
      "timestamp: 1633512431\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1212000\n",
      "training_iteration: 303\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:303 starting ! -----------------\n",
      "agent_timesteps_total: 1216000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-27-34\n",
      "done: false\n",
      "episode_len_mean: 706.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3896665000000032\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 875\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8474944233894348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009668301790952682\n",
      "        model: {}\n",
      "        policy_loss: -0.019431741908192635\n",
      "        total_loss: -0.006860945373773575\n",
      "        vf_explained_var: 0.281994104385376\n",
      "        vf_loss: 0.010637132450938225\n",
      "  num_agent_steps_sampled: 1216000\n",
      "  num_agent_steps_trained: 1216000\n",
      "  num_steps_sampled: 1216000\n",
      "  num_steps_trained: 1216000\n",
      "iterations_since_restore: 304\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.08787878787879\n",
      "  ram_util_percent: 44.090909090909086\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049177876505910406\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7478520463910727\n",
      "  mean_inference_ms: 0.8505730394602408\n",
      "  mean_raw_obs_processing_ms: 0.08839397128870267\n",
      "time_since_restore: 1639.1226658821106\n",
      "time_this_iter_s: 5.300625324249268\n",
      "time_total_s: 1639.1226658821106\n",
      "timers:\n",
      "  learn_throughput: 2146.162\n",
      "  learn_time_ms: 1863.793\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1124.682\n",
      "  sample_time_ms: 3556.562\n",
      "  update_time_ms: 1.748\n",
      "timestamp: 1633512454\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1216000\n",
      "training_iteration: 304\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:304 starting ! -----------------\n",
      "agent_timesteps_total: 1220000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-27-58\n",
      "done: false\n",
      "episode_len_mean: 707.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9149499999999993\n",
      "episode_reward_mean: 0.3893645000000032\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 877\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.927862823009491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0107852378860116\n",
      "        model: {}\n",
      "        policy_loss: -0.01690642535686493\n",
      "        total_loss: 0.00041165531729348004\n",
      "        vf_explained_var: 0.14751915633678436\n",
      "        vf_loss: 0.015161036513745785\n",
      "  num_agent_steps_sampled: 1220000\n",
      "  num_agent_steps_trained: 1220000\n",
      "  num_steps_sampled: 1220000\n",
      "  num_steps_trained: 1220000\n",
      "iterations_since_restore: 305\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.569696969696967\n",
      "  ram_util_percent: 44.230303030303034\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917565751176072\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7478819063439874\n",
      "  mean_inference_ms: 0.8505712657970739\n",
      "  mean_raw_obs_processing_ms: 0.08838976139098285\n",
      "time_since_restore: 1644.9027745723724\n",
      "time_this_iter_s: 5.780108690261841\n",
      "time_total_s: 1644.9027745723724\n",
      "timers:\n",
      "  learn_throughput: 2133.702\n",
      "  learn_time_ms: 1874.676\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1118.8\n",
      "  sample_time_ms: 3575.259\n",
      "  update_time_ms: 1.748\n",
      "timestamp: 1633512478\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1220000\n",
      "training_iteration: 305\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:305 starting ! -----------------\n",
      "agent_timesteps_total: 1224000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-28-22\n",
      "done: false\n",
      "episode_len_mean: 709.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.91\n",
      "episode_reward_mean: 0.39922900000000333\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 880\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9902704954147339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009936320595443249\n",
      "        model: {}\n",
      "        policy_loss: -0.023835744708776474\n",
      "        total_loss: 0.0030157878063619137\n",
      "        vf_explained_var: 0.13891419768333435\n",
      "        vf_loss: 0.02486426942050457\n",
      "  num_agent_steps_sampled: 1224000\n",
      "  num_agent_steps_trained: 1224000\n",
      "  num_steps_sampled: 1224000\n",
      "  num_steps_trained: 1224000\n",
      "iterations_since_restore: 306\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.726470588235294\n",
      "  ram_util_percent: 44.26176470588235\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917421201645424\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7479260345629265\n",
      "  mean_inference_ms: 0.850567230127771\n",
      "  mean_raw_obs_processing_ms: 0.08838357081497637\n",
      "time_since_restore: 1650.2889754772186\n",
      "time_this_iter_s: 5.386200904846191\n",
      "time_total_s: 1650.2889754772186\n",
      "timers:\n",
      "  learn_throughput: 2133.227\n",
      "  learn_time_ms: 1875.093\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1125.784\n",
      "  sample_time_ms: 3553.08\n",
      "  update_time_ms: 1.648\n",
      "timestamp: 1633512502\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1224000\n",
      "training_iteration: 306\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:306 starting ! -----------------\n",
      "agent_timesteps_total: 1228000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-28-37\n",
      "done: false\n",
      "episode_len_mean: 721.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.91\n",
      "episode_reward_mean: 0.29928300000000324\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 882\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0277116298675537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012440314516425133\n",
      "        model: {}\n",
      "        policy_loss: -0.019193025305867195\n",
      "        total_loss: -0.003949944395571947\n",
      "        vf_explained_var: 0.01958329789340496\n",
      "        vf_loss: 0.012755024246871471\n",
      "  num_agent_steps_sampled: 1228000\n",
      "  num_agent_steps_trained: 1228000\n",
      "  num_steps_sampled: 1228000\n",
      "  num_steps_trained: 1228000\n",
      "iterations_since_restore: 307\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.58571428571429\n",
      "  ram_util_percent: 44.171428571428585\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917267803054408\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7479493029871503\n",
      "  mean_inference_ms: 0.850564879128819\n",
      "  mean_raw_obs_processing_ms: 0.0883791357207025\n",
      "time_since_restore: 1655.4546537399292\n",
      "time_this_iter_s: 5.165678262710571\n",
      "time_total_s: 1655.4546537399292\n",
      "timers:\n",
      "  learn_throughput: 2133.783\n",
      "  learn_time_ms: 1874.605\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.483\n",
      "  sample_time_ms: 3532.06\n",
      "  update_time_ms: 1.648\n",
      "timestamp: 1633512517\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1228000\n",
      "training_iteration: 307\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:307 starting ! -----------------\n",
      "agent_timesteps_total: 1232000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-28-51\n",
      "done: false\n",
      "episode_len_mean: 726.31\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.91\n",
      "episode_reward_mean: 0.24809100000000325\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 884\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9594952464103699\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011948484927415848\n",
      "        model: {}\n",
      "        policy_loss: -0.02104676142334938\n",
      "        total_loss: -0.0119216563180089\n",
      "        vf_explained_var: 0.1010277196764946\n",
      "        vf_loss: 0.006735407281666994\n",
      "  num_agent_steps_sampled: 1232000\n",
      "  num_agent_steps_trained: 1232000\n",
      "  num_steps_sampled: 1232000\n",
      "  num_steps_trained: 1232000\n",
      "iterations_since_restore: 308\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.92\n",
      "  ram_util_percent: 44.18000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917152911965299\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7479745779800366\n",
      "  mean_inference_ms: 0.8505667576057258\n",
      "  mean_raw_obs_processing_ms: 0.08837470121465549\n",
      "time_since_restore: 1660.9588387012482\n",
      "time_this_iter_s: 5.50418496131897\n",
      "time_total_s: 1660.9588387012482\n",
      "timers:\n",
      "  learn_throughput: 2132.951\n",
      "  learn_time_ms: 1875.337\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.314\n",
      "  sample_time_ms: 3545.111\n",
      "  update_time_ms: 1.648\n",
      "timestamp: 1633512531\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1232000\n",
      "training_iteration: 308\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:308 starting ! -----------------\n",
      "agent_timesteps_total: 1236000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-29-12\n",
      "done: false\n",
      "episode_len_mean: 732.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.91\n",
      "episode_reward_mean: 0.1970745000000032\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 887\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9974971413612366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019192615523934364\n",
      "        model: {}\n",
      "        policy_loss: -0.022651171311736107\n",
      "        total_loss: -0.007876405492424965\n",
      "        vf_explained_var: 0.3946731388568878\n",
      "        vf_loss: 0.010936247184872627\n",
      "  num_agent_steps_sampled: 1236000\n",
      "  num_agent_steps_trained: 1236000\n",
      "  num_steps_sampled: 1236000\n",
      "  num_steps_trained: 1236000\n",
      "iterations_since_restore: 309\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.113793103448273\n",
      "  ram_util_percent: 44.12758620689653\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916951845985604\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7480111717371907\n",
      "  mean_inference_ms: 0.8505684304021357\n",
      "  mean_raw_obs_processing_ms: 0.08836921672452196\n",
      "time_since_restore: 1666.29576253891\n",
      "time_this_iter_s: 5.336923837661743\n",
      "time_total_s: 1666.29576253891\n",
      "timers:\n",
      "  learn_throughput: 2130.852\n",
      "  learn_time_ms: 1877.183\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.94\n",
      "  sample_time_ms: 3540.01\n",
      "  update_time_ms: 1.549\n",
      "timestamp: 1633512552\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1236000\n",
      "training_iteration: 309\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:309 starting ! -----------------\n",
      "agent_timesteps_total: 1240000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-29-25\n",
      "done: false\n",
      "episode_len_mean: 732.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.91\n",
      "episode_reward_mean: 0.19651550000000328\n",
      "episode_reward_min: -2.2521999999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 890\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9240865707397461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008243490010499954\n",
      "        model: {}\n",
      "        policy_loss: -0.014749625697731972\n",
      "        total_loss: -0.005596283823251724\n",
      "        vf_explained_var: 0.2853514552116394\n",
      "        vf_loss: 0.007504642475396395\n",
      "  num_agent_steps_sampled: 1240000\n",
      "  num_agent_steps_trained: 1240000\n",
      "  num_steps_sampled: 1240000\n",
      "  num_steps_trained: 1240000\n",
      "iterations_since_restore: 310\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.76111111111111\n",
      "  ram_util_percent: 44.09444444444445\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916788013439252\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7480443192295075\n",
      "  mean_inference_ms: 0.850570397973987\n",
      "  mean_raw_obs_processing_ms: 0.08836428709715159\n",
      "time_since_restore: 1671.618034362793\n",
      "time_this_iter_s: 5.322271823883057\n",
      "time_total_s: 1671.618034362793\n",
      "timers:\n",
      "  learn_throughput: 2132.293\n",
      "  learn_time_ms: 1875.915\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.545\n",
      "  sample_time_ms: 3519.437\n",
      "  update_time_ms: 1.648\n",
      "timestamp: 1633512565\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1240000\n",
      "training_iteration: 310\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:310 starting ! -----------------\n",
      "agent_timesteps_total: 1244000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-29-40\n",
      "done: false\n",
      "episode_len_mean: 728.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.25843050000000334\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 894\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8922392725944519\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006075539160519838\n",
      "        model: {}\n",
      "        policy_loss: -0.015992075204849243\n",
      "        total_loss: 0.006522668991237879\n",
      "        vf_explained_var: 0.31123775243759155\n",
      "        vf_loss: 0.021299632266163826\n",
      "  num_agent_steps_sampled: 1244000\n",
      "  num_agent_steps_trained: 1244000\n",
      "  num_steps_sampled: 1244000\n",
      "  num_steps_trained: 1244000\n",
      "iterations_since_restore: 311\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.919047619047618\n",
      "  ram_util_percent: 44.07142857142858\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916602591839339\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7480783936773235\n",
      "  mean_inference_ms: 0.8505715989897783\n",
      "  mean_raw_obs_processing_ms: 0.08836110701642713\n",
      "time_since_restore: 1676.9273886680603\n",
      "time_this_iter_s: 5.309354305267334\n",
      "time_total_s: 1676.9273886680603\n",
      "timers:\n",
      "  learn_throughput: 2133.63\n",
      "  learn_time_ms: 1874.739\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.265\n",
      "  sample_time_ms: 3520.304\n",
      "  update_time_ms: 1.748\n",
      "timestamp: 1633512580\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1244000\n",
      "training_iteration: 311\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:311 starting ! -----------------\n",
      "agent_timesteps_total: 1248000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-29-59\n",
      "done: false\n",
      "episode_len_mean: 738.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.21847900000000334\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 896\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.894213855266571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008482621982693672\n",
      "        model: {}\n",
      "        policy_loss: -0.016956545412540436\n",
      "        total_loss: 0.028452206403017044\n",
      "        vf_explained_var: 0.01172762643545866\n",
      "        vf_loss: 0.043712224811315536\n",
      "  num_agent_steps_sampled: 1248000\n",
      "  num_agent_steps_trained: 1248000\n",
      "  num_steps_sampled: 1248000\n",
      "  num_steps_trained: 1248000\n",
      "iterations_since_restore: 312\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.28461538461539\n",
      "  ram_util_percent: 44.0923076923077\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049165601980808035\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7480959558773571\n",
      "  mean_inference_ms: 0.850570972474095\n",
      "  mean_raw_obs_processing_ms: 0.08836001480042736\n",
      "time_since_restore: 1682.3165938854218\n",
      "time_this_iter_s: 5.38920521736145\n",
      "time_total_s: 1682.3165938854218\n",
      "timers:\n",
      "  learn_throughput: 2135.962\n",
      "  learn_time_ms: 1872.692\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.95\n",
      "  sample_time_ms: 3508.925\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633512599\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1248000\n",
      "training_iteration: 312\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:312 starting ! -----------------\n",
      "agent_timesteps_total: 1252000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-30-23\n",
      "done: false\n",
      "episode_len_mean: 740.43\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.3190150000000032\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 899\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9226546883583069\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011111948639154434\n",
      "        model: {}\n",
      "        policy_loss: -0.02337111532688141\n",
      "        total_loss: -0.009224520064890385\n",
      "        vf_explained_var: 0.3609611392021179\n",
      "        vf_loss: 0.011924199759960175\n",
      "  num_agent_steps_sampled: 1252000\n",
      "  num_agent_steps_trained: 1252000\n",
      "  num_steps_sampled: 1252000\n",
      "  num_steps_trained: 1252000\n",
      "iterations_since_restore: 313\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.438235294117646\n",
      "  ram_util_percent: 44.05\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049165424397351734\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7481238131608201\n",
      "  mean_inference_ms: 0.8505717525096789\n",
      "  mean_raw_obs_processing_ms: 0.08835875991298414\n",
      "time_since_restore: 1687.7639236450195\n",
      "time_this_iter_s: 5.447329759597778\n",
      "time_total_s: 1687.7639236450195\n",
      "timers:\n",
      "  learn_throughput: 2135.726\n",
      "  learn_time_ms: 1872.899\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.295\n",
      "  sample_time_ms: 3514.027\n",
      "  update_time_ms: 1.795\n",
      "timestamp: 1633512623\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1252000\n",
      "training_iteration: 313\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:313 starting ! -----------------\n",
      "agent_timesteps_total: 1256000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-30-37\n",
      "done: false\n",
      "episode_len_mean: 749.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.26876500000000325\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 901\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6985042691230774\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008946836926043034\n",
      "        model: {}\n",
      "        policy_loss: -0.016148291528224945\n",
      "        total_loss: -0.0036657380405813456\n",
      "        vf_explained_var: 0.002341659041121602\n",
      "        vf_loss: 0.010693185962736607\n",
      "  num_agent_steps_sampled: 1256000\n",
      "  num_agent_steps_trained: 1256000\n",
      "  num_steps_sampled: 1256000\n",
      "  num_steps_trained: 1256000\n",
      "iterations_since_restore: 314\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.415\n",
      "  ram_util_percent: 44.04\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916508255860388\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7481458365300595\n",
      "  mean_inference_ms: 0.8505716780432525\n",
      "  mean_raw_obs_processing_ms: 0.0883580988166177\n",
      "time_since_restore: 1693.2785987854004\n",
      "time_this_iter_s: 5.514675140380859\n",
      "time_total_s: 1693.2785987854004\n",
      "timers:\n",
      "  learn_throughput: 2126.197\n",
      "  learn_time_ms: 1881.293\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.112\n",
      "  sample_time_ms: 3526.987\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633512637\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1256000\n",
      "training_iteration: 314\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:314 starting ! -----------------\n",
      "agent_timesteps_total: 1260000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-30-52\n",
      "done: false\n",
      "episode_len_mean: 758.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.16787100000000343\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 903\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9285793900489807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011588049121201038\n",
      "        model: {}\n",
      "        policy_loss: -0.016524799168109894\n",
      "        total_loss: 0.006926348898559809\n",
      "        vf_explained_var: 0.19613447785377502\n",
      "        vf_loss: 0.021133538335561752\n",
      "  num_agent_steps_sampled: 1260000\n",
      "  num_agent_steps_trained: 1260000\n",
      "  num_steps_sampled: 1260000\n",
      "  num_steps_trained: 1260000\n",
      "iterations_since_restore: 315\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.523809523809526\n",
      "  ram_util_percent: 44.07619047619048\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049165125524447365\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7481703652044134\n",
      "  mean_inference_ms: 0.8505728238879755\n",
      "  mean_raw_obs_processing_ms: 0.08835714653976884\n",
      "time_since_restore: 1698.818294286728\n",
      "time_this_iter_s: 5.539695501327515\n",
      "time_total_s: 1698.818294286728\n",
      "timers:\n",
      "  learn_throughput: 2136.71\n",
      "  learn_time_ms: 1872.037\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.869\n",
      "  sample_time_ms: 3512.257\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633512652\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1260000\n",
      "training_iteration: 315\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:315 starting ! -----------------\n",
      "agent_timesteps_total: 1264000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-31-06\n",
      "done: false\n",
      "episode_len_mean: 753.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.15782900000000324\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 907\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9946664571762085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01144097838550806\n",
      "        model: {}\n",
      "        policy_loss: -0.026933657005429268\n",
      "        total_loss: -0.00153116206638515\n",
      "        vf_explained_var: -0.06427808851003647\n",
      "        vf_loss: 0.023114297538995743\n",
      "  num_agent_steps_sampled: 1264000\n",
      "  num_agent_steps_trained: 1264000\n",
      "  num_steps_sampled: 1264000\n",
      "  num_steps_trained: 1264000\n",
      "iterations_since_restore: 316\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.045000000000005\n",
      "  ram_util_percent: 44.055\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916410586898978\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7482212811359139\n",
      "  mean_inference_ms: 0.8505698392814185\n",
      "  mean_raw_obs_processing_ms: 0.08835637882073791\n",
      "time_since_restore: 1704.3331274986267\n",
      "time_this_iter_s: 5.514833211898804\n",
      "time_total_s: 1704.3331274986267\n",
      "timers:\n",
      "  learn_throughput: 2138.701\n",
      "  learn_time_ms: 1870.295\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.175\n",
      "  sample_time_ms: 3526.791\n",
      "  update_time_ms: 1.805\n",
      "timestamp: 1633512666\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1264000\n",
      "training_iteration: 316\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:316 starting ! -----------------\n",
      "agent_timesteps_total: 1268000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-31-29\n",
      "done: false\n",
      "episode_len_mean: 753.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.10795550000000322\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 909\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9929002523422241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01088825985789299\n",
      "        model: {}\n",
      "        policy_loss: -0.02056075632572174\n",
      "        total_loss: 0.006792230531573296\n",
      "        vf_explained_var: 0.30511385202407837\n",
      "        vf_loss: 0.025175336748361588\n",
      "  num_agent_steps_sampled: 1268000\n",
      "  num_agent_steps_trained: 1268000\n",
      "  num_steps_sampled: 1268000\n",
      "  num_steps_trained: 1268000\n",
      "iterations_since_restore: 317\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.233333333333334\n",
      "  ram_util_percent: 44.06666666666666\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049163375067285256\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7482485907794869\n",
      "  mean_inference_ms: 0.8505681947206297\n",
      "  mean_raw_obs_processing_ms: 0.08835677289676172\n",
      "time_since_restore: 1709.6835329532623\n",
      "time_this_iter_s: 5.35040545463562\n",
      "time_total_s: 1709.6835329532623\n",
      "timers:\n",
      "  learn_throughput: 2140.085\n",
      "  learn_time_ms: 1869.085\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.875\n",
      "  sample_time_ms: 3546.493\n",
      "  update_time_ms: 1.805\n",
      "timestamp: 1633512689\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1268000\n",
      "training_iteration: 317\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:317 starting ! -----------------\n",
      "agent_timesteps_total: 1272000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-31-53\n",
      "done: false\n",
      "episode_len_mean: 749.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.10790900000000317\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 912\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.94756019115448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01014939695596695\n",
      "        model: {}\n",
      "        policy_loss: -0.02383812516927719\n",
      "        total_loss: 0.004236942157149315\n",
      "        vf_explained_var: -0.12732909619808197\n",
      "        vf_loss: 0.026045190170407295\n",
      "  num_agent_steps_sampled: 1272000\n",
      "  num_agent_steps_trained: 1272000\n",
      "  num_steps_sampled: 1272000\n",
      "  num_steps_trained: 1272000\n",
      "iterations_since_restore: 318\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.527272727272727\n",
      "  ram_util_percent: 44.07272727272727\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916342296362932\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7482900060983431\n",
      "  mean_inference_ms: 0.8505641203766326\n",
      "  mean_raw_obs_processing_ms: 0.08835788634989546\n",
      "time_since_restore: 1714.9676597118378\n",
      "time_this_iter_s: 5.2841267585754395\n",
      "time_total_s: 1714.9676597118378\n",
      "timers:\n",
      "  learn_throughput: 2140.605\n",
      "  learn_time_ms: 1868.631\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.768\n",
      "  sample_time_ms: 3524.949\n",
      "  update_time_ms: 1.78\n",
      "timestamp: 1633512713\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1272000\n",
      "training_iteration: 318\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:318 starting ! -----------------\n",
      "agent_timesteps_total: 1276000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-32-06\n",
      "done: false\n",
      "episode_len_mean: 754.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.04796500000000312\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 915\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0653882026672363\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018885230645537376\n",
      "        model: {}\n",
      "        policy_loss: -0.0196568351238966\n",
      "        total_loss: 0.0006637106416746974\n",
      "        vf_explained_var: 0.12685303390026093\n",
      "        vf_loss: 0.016543501988053322\n",
      "  num_agent_steps_sampled: 1276000\n",
      "  num_agent_steps_trained: 1276000\n",
      "  num_steps_sampled: 1276000\n",
      "  num_steps_trained: 1276000\n",
      "iterations_since_restore: 319\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.610526315789468\n",
      "  ram_util_percent: 44.05263157894737\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916430138735399\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7483276409807496\n",
      "  mean_inference_ms: 0.8505617089972869\n",
      "  mean_raw_obs_processing_ms: 0.08835881466646421\n",
      "time_since_restore: 1720.290459394455\n",
      "time_this_iter_s: 5.3227996826171875\n",
      "time_total_s: 1720.290459394455\n",
      "timers:\n",
      "  learn_throughput: 2137.146\n",
      "  learn_time_ms: 1871.655\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.267\n",
      "  sample_time_ms: 3520.3\n",
      "  update_time_ms: 1.88\n",
      "timestamp: 1633512726\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1276000\n",
      "training_iteration: 319\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:319 starting ! -----------------\n",
      "agent_timesteps_total: 1280000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-32-23\n",
      "done: false\n",
      "episode_len_mean: 749.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.04788950000000315\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 918\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.866029679775238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010121328756213188\n",
      "        model: {}\n",
      "        policy_loss: -0.026027334854006767\n",
      "        total_loss: -0.007434056606143713\n",
      "        vf_explained_var: 0.17397500574588776\n",
      "        vf_loss: 0.016569016501307487\n",
      "  num_agent_steps_sampled: 1280000\n",
      "  num_agent_steps_trained: 1280000\n",
      "  num_steps_sampled: 1280000\n",
      "  num_steps_trained: 1280000\n",
      "iterations_since_restore: 320\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.945833333333336\n",
      "  ram_util_percent: 44.1\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916527220393051\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7483671363214449\n",
      "  mean_inference_ms: 0.8505602481989392\n",
      "  mean_raw_obs_processing_ms: 0.088360531627106\n",
      "time_since_restore: 1725.7322177886963\n",
      "time_this_iter_s: 5.441758394241333\n",
      "time_total_s: 1725.7322177886963\n",
      "timers:\n",
      "  learn_throughput: 2134.607\n",
      "  learn_time_ms: 1873.881\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.117\n",
      "  sample_time_ms: 3530.086\n",
      "  update_time_ms: 1.78\n",
      "timestamp: 1633512743\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1280000\n",
      "training_iteration: 320\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:320 starting ! -----------------\n",
      "agent_timesteps_total: 1284000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-32-42\n",
      "done: false\n",
      "episode_len_mean: 746.97\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.09906500000000308\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 921\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9226502776145935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013201835565268993\n",
      "        model: {}\n",
      "        policy_loss: -0.026115747168660164\n",
      "        total_loss: -0.009923763573169708\n",
      "        vf_explained_var: 0.25564491748809814\n",
      "        vf_loss: 0.013551619835197926\n",
      "  num_agent_steps_sampled: 1284000\n",
      "  num_agent_steps_trained: 1284000\n",
      "  num_steps_sampled: 1284000\n",
      "  num_steps_trained: 1284000\n",
      "iterations_since_restore: 321\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.664\n",
      "  ram_util_percent: 44.06799999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049166455888866825\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7484067022350852\n",
      "  mean_inference_ms: 0.8505571197951168\n",
      "  mean_raw_obs_processing_ms: 0.0883621965216355\n",
      "time_since_restore: 1731.0993762016296\n",
      "time_this_iter_s: 5.36715841293335\n",
      "time_total_s: 1731.0993762016296\n",
      "timers:\n",
      "  learn_throughput: 2133.448\n",
      "  learn_time_ms: 1874.899\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.608\n",
      "  sample_time_ms: 3534.794\n",
      "  update_time_ms: 1.78\n",
      "timestamp: 1633512762\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1284000\n",
      "training_iteration: 321\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:321 starting ! -----------------\n",
      "agent_timesteps_total: 1288000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-32-55\n",
      "done: false\n",
      "episode_len_mean: 745.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.09968850000000304\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 923\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8741340637207031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008124358020722866\n",
      "        model: {}\n",
      "        policy_loss: -0.018440939486026764\n",
      "        total_loss: -0.005435997154563665\n",
      "        vf_explained_var: 0.12113337218761444\n",
      "        vf_loss: 0.011380072683095932\n",
      "  num_agent_steps_sampled: 1288000\n",
      "  num_agent_steps_trained: 1288000\n",
      "  num_steps_sampled: 1288000\n",
      "  num_steps_trained: 1288000\n",
      "iterations_since_restore: 322\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.989473684210523\n",
      "  ram_util_percent: 44.09473684210527\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04916690631881858\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7484316666095364\n",
      "  mean_inference_ms: 0.8505553796410652\n",
      "  mean_raw_obs_processing_ms: 0.08836281653674231\n",
      "time_since_restore: 1736.4054555892944\n",
      "time_this_iter_s: 5.306079387664795\n",
      "time_total_s: 1736.4054555892944\n",
      "timers:\n",
      "  learn_throughput: 2133.803\n",
      "  learn_time_ms: 1874.587\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.17\n",
      "  sample_time_ms: 3526.808\n",
      "  update_time_ms: 1.68\n",
      "timestamp: 1633512775\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1288000\n",
      "training_iteration: 322\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:322 starting ! -----------------\n",
      "agent_timesteps_total: 1292000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-33-19\n",
      "done: false\n",
      "episode_len_mean: 748.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: 0.04029650000000298\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 926\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9846987128257751\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010193333961069584\n",
      "        model: {}\n",
      "        policy_loss: -0.024620791897177696\n",
      "        total_loss: -0.00706917280331254\n",
      "        vf_explained_var: 0.02580305002629757\n",
      "        vf_loss: 0.015512950718402863\n",
      "  num_agent_steps_sampled: 1292000\n",
      "  num_agent_steps_trained: 1292000\n",
      "  num_steps_sampled: 1292000\n",
      "  num_steps_trained: 1292000\n",
      "iterations_since_restore: 323\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.43030303030303\n",
      "  ram_util_percent: 44.06969696969697\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0491675434538075\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7484719679996483\n",
      "  mean_inference_ms: 0.8505543343642508\n",
      "  mean_raw_obs_processing_ms: 0.08836376124674462\n",
      "time_since_restore: 1741.8992068767548\n",
      "time_this_iter_s: 5.493751287460327\n",
      "time_total_s: 1741.8992068767548\n",
      "timers:\n",
      "  learn_throughput: 2128.586\n",
      "  learn_time_ms: 1879.182\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.157\n",
      "  sample_time_ms: 3526.849\n",
      "  update_time_ms: 1.68\n",
      "timestamp: 1633512799\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1292000\n",
      "training_iteration: 323\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:323 starting ! -----------------\n",
      "agent_timesteps_total: 1296000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-33-43\n",
      "done: false\n",
      "episode_len_mean: 748.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9118\n",
      "episode_reward_mean: -0.020056999999997053\n",
      "episode_reward_min: -2.2193500000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 929\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0175981521606445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01380823366343975\n",
      "        model: {}\n",
      "        policy_loss: -0.02397540956735611\n",
      "        total_loss: 0.004196109715849161\n",
      "        vf_explained_var: 0.03334067016839981\n",
      "        vf_loss: 0.025409866124391556\n",
      "  num_agent_steps_sampled: 1296000\n",
      "  num_agent_steps_trained: 1296000\n",
      "  num_steps_sampled: 1296000\n",
      "  num_steps_trained: 1296000\n",
      "iterations_since_restore: 324\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.697142857142858\n",
      "  ram_util_percent: 44.06857142857142\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049167949812592766\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7485113975039692\n",
      "  mean_inference_ms: 0.8505545826350118\n",
      "  mean_raw_obs_processing_ms: 0.08836387661873792\n",
      "time_since_restore: 1747.2589242458344\n",
      "time_this_iter_s: 5.35971736907959\n",
      "time_total_s: 1747.2589242458344\n",
      "timers:\n",
      "  learn_throughput: 2136.749\n",
      "  learn_time_ms: 1872.003\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.804\n",
      "  sample_time_ms: 3518.637\n",
      "  update_time_ms: 1.68\n",
      "timestamp: 1633512823\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1296000\n",
      "training_iteration: 324\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:324 starting ! -----------------\n",
      "agent_timesteps_total: 1300000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-34-06\n",
      "done: false\n",
      "episode_len_mean: 736.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.09222150000000283\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 933\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9934411644935608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008043997921049595\n",
      "        model: {}\n",
      "        policy_loss: -0.02302652597427368\n",
      "        total_loss: -0.006568072829395533\n",
      "        vf_explained_var: 0.05399097874760628\n",
      "        vf_loss: 0.014849653467535973\n",
      "  num_agent_steps_sampled: 1300000\n",
      "  num_agent_steps_trained: 1300000\n",
      "  num_steps_sampled: 1300000\n",
      "  num_steps_trained: 1300000\n",
      "iterations_since_restore: 325\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.848484848484848\n",
      "  ram_util_percent: 44.10000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049169252429152766\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7485629004765247\n",
      "  mean_inference_ms: 0.8505564720404749\n",
      "  mean_raw_obs_processing_ms: 0.08836523214669074\n",
      "time_since_restore: 1752.705097436905\n",
      "time_this_iter_s: 5.446173191070557\n",
      "time_total_s: 1752.705097436905\n",
      "timers:\n",
      "  learn_throughput: 2142.679\n",
      "  learn_time_ms: 1866.822\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.143\n",
      "  sample_time_ms: 3514.496\n",
      "  update_time_ms: 1.78\n",
      "timestamp: 1633512846\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1300000\n",
      "training_iteration: 325\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:325 starting ! -----------------\n",
      "agent_timesteps_total: 1304000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-34-25\n",
      "done: false\n",
      "episode_len_mean: 735.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.19275550000000283\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 936\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9780256152153015\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013880361802875996\n",
      "        model: {}\n",
      "        policy_loss: -0.019379014149308205\n",
      "        total_loss: 0.0030135244596749544\n",
      "        vf_explained_var: 0.30734580755233765\n",
      "        vf_loss: 0.019616467878222466\n",
      "  num_agent_steps_sampled: 1304000\n",
      "  num_agent_steps_trained: 1304000\n",
      "  num_steps_sampled: 1304000\n",
      "  num_steps_trained: 1304000\n",
      "iterations_since_restore: 326\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.415384615384614\n",
      "  ram_util_percent: 44.092307692307685\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917056824144919\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.748601956603727\n",
      "  mean_inference_ms: 0.850561124876035\n",
      "  mean_raw_obs_processing_ms: 0.08836661337046645\n",
      "time_since_restore: 1758.1060676574707\n",
      "time_this_iter_s: 5.400970220565796\n",
      "time_total_s: 1758.1060676574707\n",
      "timers:\n",
      "  learn_throughput: 2142.941\n",
      "  learn_time_ms: 1866.594\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.765\n",
      "  sample_time_ms: 3503.347\n",
      "  update_time_ms: 1.77\n",
      "timestamp: 1633512865\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1304000\n",
      "training_iteration: 326\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:326 starting ! -----------------\n",
      "agent_timesteps_total: 1308000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-34-49\n",
      "done: false\n",
      "episode_len_mean: 735.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.2031220000000028\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 938\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9834077954292297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01063835434615612\n",
      "        model: {}\n",
      "        policy_loss: -0.023769492283463478\n",
      "        total_loss: -0.0012089913943782449\n",
      "        vf_explained_var: 0.3031051456928253\n",
      "        vf_loss: 0.020432835444808006\n",
      "  num_agent_steps_sampled: 1308000\n",
      "  num_agent_steps_trained: 1308000\n",
      "  num_steps_sampled: 1308000\n",
      "  num_steps_trained: 1308000\n",
      "iterations_since_restore: 327\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.472727272727273\n",
      "  ram_util_percent: 44.10303030303031\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917143858235742\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7486282602006432\n",
      "  mean_inference_ms: 0.8505649210708435\n",
      "  mean_raw_obs_processing_ms: 0.08836706917476712\n",
      "time_since_restore: 1763.6483063697815\n",
      "time_this_iter_s: 5.542238712310791\n",
      "time_total_s: 1763.6483063697815\n",
      "timers:\n",
      "  learn_throughput: 2121.597\n",
      "  learn_time_ms: 1885.372\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.711\n",
      "  sample_time_ms: 3503.515\n",
      "  update_time_ms: 1.77\n",
      "timestamp: 1633512889\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1308000\n",
      "training_iteration: 327\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:327 starting ! -----------------\n",
      "agent_timesteps_total: 1312000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-35-07\n",
      "done: false\n",
      "episode_len_mean: 735.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.20303050000000283\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 941\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9911369681358337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011999615468084812\n",
      "        model: {}\n",
      "        policy_loss: -0.02601742371916771\n",
      "        total_loss: -0.007650729734450579\n",
      "        vf_explained_var: 0.32721492648124695\n",
      "        vf_loss: 0.01596676930785179\n",
      "  num_agent_steps_sampled: 1312000\n",
      "  num_agent_steps_trained: 1312000\n",
      "  num_steps_sampled: 1312000\n",
      "  num_steps_trained: 1312000\n",
      "iterations_since_restore: 328\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.896153846153844\n",
      "  ram_util_percent: 44.12692307692306\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917147103788576\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7486642631390708\n",
      "  mean_inference_ms: 0.8505715968793061\n",
      "  mean_raw_obs_processing_ms: 0.08836739194338845\n",
      "time_since_restore: 1768.9731044769287\n",
      "time_this_iter_s: 5.324798107147217\n",
      "time_total_s: 1768.9731044769287\n",
      "timers:\n",
      "  learn_throughput: 2122.586\n",
      "  learn_time_ms: 1884.494\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.096\n",
      "  sample_time_ms: 3508.475\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633512907\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1312000\n",
      "training_iteration: 328\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:328 starting ! -----------------\n",
      "agent_timesteps_total: 1316000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-35-27\n",
      "done: false\n",
      "episode_len_mean: 730.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.2639180000000028\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 945\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0650745630264282\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011302596889436245\n",
      "        model: {}\n",
      "        policy_loss: -0.023141171783208847\n",
      "        total_loss: 0.006901867222040892\n",
      "        vf_explained_var: 0.30806609988212585\n",
      "        vf_loss: 0.027782518416643143\n",
      "  num_agent_steps_sampled: 1316000\n",
      "  num_agent_steps_trained: 1316000\n",
      "  num_steps_sampled: 1316000\n",
      "  num_steps_trained: 1316000\n",
      "iterations_since_restore: 329\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.074074074074073\n",
      "  ram_util_percent: 44.114814814814814\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917143986716772\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7487072044144836\n",
      "  mean_inference_ms: 0.850583433826535\n",
      "  mean_raw_obs_processing_ms: 0.08836812873503842\n",
      "time_since_restore: 1774.3721764087677\n",
      "time_this_iter_s: 5.399071931838989\n",
      "time_total_s: 1774.3721764087677\n",
      "timers:\n",
      "  learn_throughput: 2126.507\n",
      "  learn_time_ms: 1881.019\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.473\n",
      "  sample_time_ms: 3519.661\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633512927\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1316000\n",
      "training_iteration: 329\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:329 starting ! -----------------\n",
      "agent_timesteps_total: 1320000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-35-49\n",
      "done: false\n",
      "episode_len_mean: 726.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.26387300000000286\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 948\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0289599895477295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015809830278158188\n",
      "        model: {}\n",
      "        policy_loss: -0.045415692031383514\n",
      "        total_loss: -0.015769008547067642\n",
      "        vf_explained_var: 0.3156588077545166\n",
      "        vf_loss: 0.02648472972214222\n",
      "  num_agent_steps_sampled: 1320000\n",
      "  num_agent_steps_trained: 1320000\n",
      "  num_steps_sampled: 1320000\n",
      "  num_steps_trained: 1320000\n",
      "iterations_since_restore: 330\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.309375000000003\n",
      "  ram_util_percent: 44.131249999999994\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917167548510486\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7487471774234774\n",
      "  mean_inference_ms: 0.8505918156610348\n",
      "  mean_raw_obs_processing_ms: 0.08836962206893417\n",
      "time_since_restore: 1779.9445078372955\n",
      "time_this_iter_s: 5.572331428527832\n",
      "time_total_s: 1779.9445078372955\n",
      "timers:\n",
      "  learn_throughput: 2116.428\n",
      "  learn_time_ms: 1889.977\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.166\n",
      "  sample_time_ms: 3523.715\n",
      "  update_time_ms: 1.795\n",
      "timestamp: 1633512949\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1320000\n",
      "training_iteration: 330\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:330 starting ! -----------------\n",
      "agent_timesteps_total: 1324000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-36-13\n",
      "done: false\n",
      "episode_len_mean: 726.97\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.2733730000000029\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 950\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0777276754379272\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012933915480971336\n",
      "        model: {}\n",
      "        policy_loss: -0.027204163372516632\n",
      "        total_loss: -0.017102664336562157\n",
      "        vf_explained_var: 0.10398470610380173\n",
      "        vf_loss: 0.007514712400734425\n",
      "  num_agent_steps_sampled: 1324000\n",
      "  num_agent_steps_trained: 1324000\n",
      "  num_steps_sampled: 1324000\n",
      "  num_steps_trained: 1324000\n",
      "iterations_since_restore: 331\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.347058823529412\n",
      "  ram_util_percent: 44.12647058823529\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917192177130866\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7487725297996195\n",
      "  mean_inference_ms: 0.8505974530847068\n",
      "  mean_raw_obs_processing_ms: 0.08837095404692348\n",
      "time_since_restore: 1785.4427444934845\n",
      "time_this_iter_s: 5.498236656188965\n",
      "time_total_s: 1785.4427444934845\n",
      "timers:\n",
      "  learn_throughput: 2099.773\n",
      "  learn_time_ms: 1904.968\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.783\n",
      "  sample_time_ms: 3521.798\n",
      "  update_time_ms: 1.795\n",
      "timestamp: 1633512973\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1324000\n",
      "training_iteration: 331\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:331 starting ! -----------------\n",
      "agent_timesteps_total: 1328000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-36-37\n",
      "done: false\n",
      "episode_len_mean: 732.59\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.21219050000000295\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 952\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0121349096298218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01432359591126442\n",
      "        model: {}\n",
      "        policy_loss: -0.02108604647219181\n",
      "        total_loss: 0.0065575591288506985\n",
      "        vf_explained_var: 0.37241262197494507\n",
      "        vf_loss: 0.024778874590992928\n",
      "  num_agent_steps_sampled: 1328000\n",
      "  num_agent_steps_trained: 1328000\n",
      "  num_steps_sampled: 1328000\n",
      "  num_steps_trained: 1328000\n",
      "iterations_since_restore: 332\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.66969696969697\n",
      "  ram_util_percent: 44.11212121212121\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049172516886398225\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7487939163541975\n",
      "  mean_inference_ms: 0.8506021902851351\n",
      "  mean_raw_obs_processing_ms: 0.08837250866039457\n",
      "time_since_restore: 1790.8541476726532\n",
      "time_this_iter_s: 5.411403179168701\n",
      "time_total_s: 1790.8541476726532\n",
      "timers:\n",
      "  learn_throughput: 2099.335\n",
      "  learn_time_ms: 1905.365\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.56\n",
      "  sample_time_ms: 3531.82\n",
      "  update_time_ms: 1.886\n",
      "timestamp: 1633512997\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1328000\n",
      "training_iteration: 332\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:332 starting ! -----------------\n",
      "agent_timesteps_total: 1332000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-36-49\n",
      "done: false\n",
      "episode_len_mean: 732.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.27361400000000297\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 955\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.118369221687317\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010537267662584782\n",
      "        model: {}\n",
      "        policy_loss: -0.025657523423433304\n",
      "        total_loss: -0.009619968943297863\n",
      "        vf_explained_var: 0.21294046938419342\n",
      "        vf_loss: 0.013930099084973335\n",
      "  num_agent_steps_sampled: 1332000\n",
      "  num_agent_steps_trained: 1332000\n",
      "  num_steps_sampled: 1332000\n",
      "  num_steps_trained: 1332000\n",
      "iterations_since_restore: 333\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.966666666666665\n",
      "  ram_util_percent: 44.11666666666667\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917314542163889\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7488213921406475\n",
      "  mean_inference_ms: 0.8506109847693648\n",
      "  mean_raw_obs_processing_ms: 0.08837462977961737\n",
      "time_since_restore: 1796.3611280918121\n",
      "time_this_iter_s: 5.5069804191589355\n",
      "time_total_s: 1796.3611280918121\n",
      "timers:\n",
      "  learn_throughput: 2096.49\n",
      "  learn_time_ms: 1907.951\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.018\n",
      "  sample_time_ms: 3530.394\n",
      "  update_time_ms: 1.885\n",
      "timestamp: 1633513009\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1332000\n",
      "training_iteration: 333\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:333 starting ! -----------------\n",
      "agent_timesteps_total: 1336000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-37-03\n",
      "done: false\n",
      "episode_len_mean: 720.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.3140885000000028\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 958\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1322697401046753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012198901735246181\n",
      "        model: {}\n",
      "        policy_loss: -0.023828117176890373\n",
      "        total_loss: 0.015627559274435043\n",
      "        vf_explained_var: -0.01391902007162571\n",
      "        vf_loss: 0.0370158925652504\n",
      "  num_agent_steps_sampled: 1336000\n",
      "  num_agent_steps_trained: 1336000\n",
      "  num_steps_sampled: 1336000\n",
      "  num_steps_trained: 1336000\n",
      "iterations_since_restore: 334\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.888888888888893\n",
      "  ram_util_percent: 44.12777777777778\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049174797429275996\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7488452992352529\n",
      "  mean_inference_ms: 0.8506225163225872\n",
      "  mean_raw_obs_processing_ms: 0.08837649485929473\n",
      "time_since_restore: 1801.7345271110535\n",
      "time_this_iter_s: 5.373399019241333\n",
      "time_total_s: 1801.7345271110535\n",
      "timers:\n",
      "  learn_throughput: 2097.632\n",
      "  learn_time_ms: 1906.912\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.272\n",
      "  sample_time_ms: 3532.721\n",
      "  update_time_ms: 1.886\n",
      "timestamp: 1633513023\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1336000\n",
      "training_iteration: 334\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:334 starting ! -----------------\n",
      "agent_timesteps_total: 1340000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-37-17\n",
      "done: false\n",
      "episode_len_mean: 724.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.26360850000000274\n",
      "episode_reward_min: -2.2116500000000014\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 961\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9661055207252502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009401758201420307\n",
      "        model: {}\n",
      "        policy_loss: -0.020071180537343025\n",
      "        total_loss: 0.006175502203404903\n",
      "        vf_explained_var: 0.1222657561302185\n",
      "        vf_loss: 0.024366334080696106\n",
      "  num_agent_steps_sampled: 1340000\n",
      "  num_agent_steps_trained: 1340000\n",
      "  num_steps_sampled: 1340000\n",
      "  num_steps_trained: 1340000\n",
      "iterations_since_restore: 335\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.552380952380954\n",
      "  ram_util_percent: 44.10000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917634197905793\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7488671321762267\n",
      "  mean_inference_ms: 0.850635666109496\n",
      "  mean_raw_obs_processing_ms: 0.08837811242353044\n",
      "time_since_restore: 1807.2655766010284\n",
      "time_this_iter_s: 5.531049489974976\n",
      "time_total_s: 1807.2655766010284\n",
      "timers:\n",
      "  learn_throughput: 2079.402\n",
      "  learn_time_ms: 1923.63\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.928\n",
      "  sample_time_ms: 3524.452\n",
      "  update_time_ms: 1.786\n",
      "timestamp: 1633513037\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1340000\n",
      "training_iteration: 335\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:335 starting ! -----------------\n",
      "agent_timesteps_total: 1344000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-37-40\n",
      "done: false\n",
      "episode_len_mean: 722.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.2644415000000027\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 964\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9995517134666443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01223970390856266\n",
      "        model: {}\n",
      "        policy_loss: -0.03365206718444824\n",
      "        total_loss: -0.00784225482493639\n",
      "        vf_explained_var: 0.0572686642408371\n",
      "        vf_loss: 0.023361874744296074\n",
      "  num_agent_steps_sampled: 1344000\n",
      "  num_agent_steps_trained: 1344000\n",
      "  num_steps_sampled: 1344000\n",
      "  num_steps_trained: 1344000\n",
      "iterations_since_restore: 336\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.454838709677418\n",
      "  ram_util_percent: 44.109677419354824\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04917784094645545\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.748887555333985\n",
      "  mean_inference_ms: 0.8506467065187997\n",
      "  mean_raw_obs_processing_ms: 0.08838065262304731\n",
      "time_since_restore: 1812.719919204712\n",
      "time_this_iter_s: 5.454342603683472\n",
      "time_total_s: 1812.719919204712\n",
      "timers:\n",
      "  learn_throughput: 2078.024\n",
      "  learn_time_ms: 1924.905\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.593\n",
      "  sample_time_ms: 3528.604\n",
      "  update_time_ms: 1.786\n",
      "timestamp: 1633513060\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1344000\n",
      "training_iteration: 336\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:336 starting ! -----------------\n",
      "agent_timesteps_total: 1348000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-37-53\n",
      "done: false\n",
      "episode_len_mean: 725.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.2138425000000027\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 967\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0503116846084595\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011235777288675308\n",
      "        model: {}\n",
      "        policy_loss: -0.025059031322598457\n",
      "        total_loss: -0.006958927493542433\n",
      "        vf_explained_var: 0.06464648991823196\n",
      "        vf_loss: 0.015852946788072586\n",
      "  num_agent_steps_sampled: 1348000\n",
      "  num_agent_steps_trained: 1348000\n",
      "  num_steps_sampled: 1348000\n",
      "  num_steps_trained: 1348000\n",
      "iterations_since_restore: 337\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.0\n",
      "  ram_util_percent: 44.10526315789474\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918000243317833\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489052127985848\n",
      "  mean_inference_ms: 0.8506579102285444\n",
      "  mean_raw_obs_processing_ms: 0.08838378347037837\n",
      "time_since_restore: 1818.0860826969147\n",
      "time_this_iter_s: 5.366163492202759\n",
      "time_total_s: 1818.0860826969147\n",
      "timers:\n",
      "  learn_throughput: 2096.835\n",
      "  learn_time_ms: 1907.637\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.622\n",
      "  sample_time_ms: 3528.514\n",
      "  update_time_ms: 1.788\n",
      "timestamp: 1633513073\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1348000\n",
      "training_iteration: 337\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:337 starting ! -----------------\n",
      "agent_timesteps_total: 1352000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-38-12\n",
      "done: false\n",
      "episode_len_mean: 722.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.20422550000000267\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 969\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1954236030578613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01199562568217516\n",
      "        model: {}\n",
      "        policy_loss: -0.022035526111721992\n",
      "        total_loss: 0.012920469976961613\n",
      "        vf_explained_var: -0.06201787665486336\n",
      "        vf_loss: 0.03255687281489372\n",
      "  num_agent_steps_sampled: 1352000\n",
      "  num_agent_steps_trained: 1352000\n",
      "  num_steps_sampled: 1352000\n",
      "  num_steps_trained: 1352000\n",
      "iterations_since_restore: 338\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.58846153846154\n",
      "  ram_util_percent: 44.13076923076923\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918155845386332\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.748914538555415\n",
      "  mean_inference_ms: 0.8506635757308872\n",
      "  mean_raw_obs_processing_ms: 0.08838565551637856\n",
      "time_since_restore: 1823.4451994895935\n",
      "time_this_iter_s: 5.359116792678833\n",
      "time_total_s: 1823.4451994895935\n",
      "timers:\n",
      "  learn_throughput: 2094.725\n",
      "  learn_time_ms: 1909.559\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.12\n",
      "  sample_time_ms: 3530.078\n",
      "  update_time_ms: 1.887\n",
      "timestamp: 1633513092\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1352000\n",
      "training_iteration: 338\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:338 starting ! -----------------\n",
      "agent_timesteps_total: 1356000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-38-26\n",
      "done: false\n",
      "episode_len_mean: 724.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.1937895000000026\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 971\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1264312267303467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013278833590447903\n",
      "        model: {}\n",
      "        policy_loss: -0.022589638829231262\n",
      "        total_loss: -0.00535115459933877\n",
      "        vf_explained_var: 0.14293833076953888\n",
      "        vf_loss: 0.014582719653844833\n",
      "  num_agent_steps_sampled: 1356000\n",
      "  num_agent_steps_trained: 1356000\n",
      "  num_steps_sampled: 1356000\n",
      "  num_steps_trained: 1356000\n",
      "iterations_since_restore: 339\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.56190476190476\n",
      "  ram_util_percent: 44.14285714285715\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918312375861898\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489246867887053\n",
      "  mean_inference_ms: 0.8506697415289559\n",
      "  mean_raw_obs_processing_ms: 0.0883878054668459\n",
      "time_since_restore: 1828.8814826011658\n",
      "time_this_iter_s: 5.436283111572266\n",
      "time_total_s: 1828.8814826011658\n",
      "timers:\n",
      "  learn_throughput: 2094.678\n",
      "  learn_time_ms: 1909.602\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.939\n",
      "  sample_time_ms: 3533.758\n",
      "  update_time_ms: 1.887\n",
      "timestamp: 1633513106\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1356000\n",
      "training_iteration: 339\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:339 starting ! -----------------\n",
      "agent_timesteps_total: 1360000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-38-50\n",
      "done: false\n",
      "episode_len_mean: 726.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.13410750000000254\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 974\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.129150390625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012411084957420826\n",
      "        model: {}\n",
      "        policy_loss: -0.028318731114268303\n",
      "        total_loss: -0.000999728566966951\n",
      "        vf_explained_var: 0.18552184104919434\n",
      "        vf_loss: 0.02483677677810192\n",
      "  num_agent_steps_sampled: 1360000\n",
      "  num_agent_steps_trained: 1360000\n",
      "  num_steps_sampled: 1360000\n",
      "  num_steps_trained: 1360000\n",
      "iterations_since_restore: 340\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.418181818181818\n",
      "  ram_util_percent: 44.16060606060606\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049185248376055625\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489429567557534\n",
      "  mean_inference_ms: 0.8506807137173371\n",
      "  mean_raw_obs_processing_ms: 0.08839158386389298\n",
      "time_since_restore: 1834.438852071762\n",
      "time_this_iter_s: 5.5573694705963135\n",
      "time_total_s: 1834.438852071762\n",
      "timers:\n",
      "  learn_throughput: 2094.889\n",
      "  learn_time_ms: 1909.409\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.392\n",
      "  sample_time_ms: 3532.346\n",
      "  update_time_ms: 1.887\n",
      "timestamp: 1633513130\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1360000\n",
      "training_iteration: 340\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:340 starting ! -----------------\n",
      "agent_timesteps_total: 1364000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-39-04\n",
      "done: false\n",
      "episode_len_mean: 722.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.13486100000000248\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 977\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9721528887748718\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014047066681087017\n",
      "        model: {}\n",
      "        policy_loss: -0.03356276825070381\n",
      "        total_loss: -0.01617853157222271\n",
      "        vf_explained_var: 0.16919270157814026\n",
      "        vf_loss: 0.014574822038412094\n",
      "  num_agent_steps_sampled: 1364000\n",
      "  num_agent_steps_trained: 1364000\n",
      "  num_steps_sampled: 1364000\n",
      "  num_steps_trained: 1364000\n",
      "iterations_since_restore: 341\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.226315789473684\n",
      "  ram_util_percent: 44.16842105263159\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049186951752889715\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489572196749685\n",
      "  mean_inference_ms: 0.8506871501793161\n",
      "  mean_raw_obs_processing_ms: 0.0883948056748372\n",
      "time_since_restore: 1839.7606551647186\n",
      "time_this_iter_s: 5.321803092956543\n",
      "time_total_s: 1839.7606551647186\n",
      "timers:\n",
      "  learn_throughput: 2112.406\n",
      "  learn_time_ms: 1893.576\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.961\n",
      "  sample_time_ms: 3530.572\n",
      "  update_time_ms: 1.887\n",
      "timestamp: 1633513144\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1364000\n",
      "training_iteration: 341\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:341 starting ! -----------------\n",
      "agent_timesteps_total: 1368000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-39-23\n",
      "done: false\n",
      "episode_len_mean: 719.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.1850910000000024\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 980\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9573315978050232\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009387572295963764\n",
      "        model: {}\n",
      "        policy_loss: -0.018734421581029892\n",
      "        total_loss: 0.0013321932638064027\n",
      "        vf_explained_var: 0.08703221380710602\n",
      "        vf_loss: 0.018189098685979843\n",
      "  num_agent_steps_sampled: 1368000\n",
      "  num_agent_steps_trained: 1368000\n",
      "  num_steps_sampled: 1368000\n",
      "  num_steps_trained: 1368000\n",
      "iterations_since_restore: 342\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.474999999999998\n",
      "  ram_util_percent: 44.12857142857142\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918739944195306\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489660165838912\n",
      "  mean_inference_ms: 0.8506921855478521\n",
      "  mean_raw_obs_processing_ms: 0.08839766468655648\n",
      "time_since_restore: 1845.0364661216736\n",
      "time_this_iter_s: 5.275810956954956\n",
      "time_total_s: 1845.0364661216736\n",
      "timers:\n",
      "  learn_throughput: 2113.884\n",
      "  learn_time_ms: 1892.252\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.885\n",
      "  sample_time_ms: 3518.386\n",
      "  update_time_ms: 1.797\n",
      "timestamp: 1633513163\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1368000\n",
      "training_iteration: 342\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:342 starting ! -----------------\n",
      "agent_timesteps_total: 1372000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-39-37\n",
      "done: false\n",
      "episode_len_mean: 711.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.2352760000000024\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 983\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0091266632080078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011364622972905636\n",
      "        model: {}\n",
      "        policy_loss: -0.02555941231548786\n",
      "        total_loss: -0.004082543775439262\n",
      "        vf_explained_var: 0.2021351158618927\n",
      "        vf_loss: 0.019203949719667435\n",
      "  num_agent_steps_sampled: 1372000\n",
      "  num_agent_steps_trained: 1372000\n",
      "  num_steps_sampled: 1372000\n",
      "  num_steps_trained: 1372000\n",
      "iterations_since_restore: 343\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.515000000000004\n",
      "  ram_util_percent: 44.18500000000002\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04918929080676086\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489835319799103\n",
      "  mean_inference_ms: 0.8506938886549097\n",
      "  mean_raw_obs_processing_ms: 0.08840137505559507\n",
      "time_since_restore: 1850.4271488189697\n",
      "time_this_iter_s: 5.390682697296143\n",
      "time_total_s: 1850.4271488189697\n",
      "timers:\n",
      "  learn_throughput: 2120.547\n",
      "  learn_time_ms: 1886.306\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.696\n",
      "  sample_time_ms: 3512.79\n",
      "  update_time_ms: 1.797\n",
      "timestamp: 1633513177\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1372000\n",
      "training_iteration: 343\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:343 starting ! -----------------\n",
      "agent_timesteps_total: 1376000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-40-01\n",
      "done: false\n",
      "episode_len_mean: 707.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.2357845000000024\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 985\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0193967819213867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01049854513257742\n",
      "        model: {}\n",
      "        policy_loss: -0.016552601009607315\n",
      "        total_loss: -0.0028755292296409607\n",
      "        vf_explained_var: 0.1556236743927002\n",
      "        vf_loss: 0.011577368713915348\n",
      "  num_agent_steps_sampled: 1376000\n",
      "  num_agent_steps_trained: 1376000\n",
      "  num_steps_sampled: 1376000\n",
      "  num_steps_trained: 1376000\n",
      "iterations_since_restore: 344\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.006060606060608\n",
      "  ram_util_percent: 44.157575757575756\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049190787708269686\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489918706369362\n",
      "  mean_inference_ms: 0.8506934207866544\n",
      "  mean_raw_obs_processing_ms: 0.08840364227999521\n",
      "time_since_restore: 1855.7035818099976\n",
      "time_this_iter_s: 5.276432991027832\n",
      "time_total_s: 1855.7035818099976\n",
      "timers:\n",
      "  learn_throughput: 2119.742\n",
      "  learn_time_ms: 1887.022\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.132\n",
      "  sample_time_ms: 3502.222\n",
      "  update_time_ms: 1.797\n",
      "timestamp: 1633513201\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1376000\n",
      "training_iteration: 344\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:344 starting ! -----------------\n",
      "agent_timesteps_total: 1380000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-40-17\n",
      "done: false\n",
      "episode_len_mean: 715.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.18580250000000237\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 987\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1078051328659058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01713947393000126\n",
      "        model: {}\n",
      "        policy_loss: -0.02784952148795128\n",
      "        total_loss: -0.014159307815134525\n",
      "        vf_explained_var: 0.19857949018478394\n",
      "        vf_loss: 0.010262317024171352\n",
      "  num_agent_steps_sampled: 1380000\n",
      "  num_agent_steps_trained: 1380000\n",
      "  num_steps_sampled: 1380000\n",
      "  num_steps_trained: 1380000\n",
      "iterations_since_restore: 345\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.977272727272727\n",
      "  ram_util_percent: 44.168181818181836\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0491925679350212\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7489995530927416\n",
      "  mean_inference_ms: 0.8506948960594007\n",
      "  mean_raw_obs_processing_ms: 0.08840532354620124\n",
      "time_since_restore: 1861.1515040397644\n",
      "time_this_iter_s: 5.447922229766846\n",
      "time_total_s: 1861.1515040397644\n",
      "timers:\n",
      "  learn_throughput: 2130.996\n",
      "  learn_time_ms: 1877.057\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.603\n",
      "  sample_time_ms: 3503.844\n",
      "  update_time_ms: 1.797\n",
      "timestamp: 1633513217\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1380000\n",
      "training_iteration: 345\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:345 starting ! -----------------\n",
      "agent_timesteps_total: 1384000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-40-40\n",
      "done: false\n",
      "episode_len_mean: 719.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.18618800000000232\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 990\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9456949830055237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013772363774478436\n",
      "        model: {}\n",
      "        policy_loss: -0.023626552894711494\n",
      "        total_loss: 0.003119367640465498\n",
      "        vf_explained_var: 0.5011782050132751\n",
      "        vf_loss: 0.02399144321680069\n",
      "  num_agent_steps_sampled: 1384000\n",
      "  num_agent_steps_trained: 1384000\n",
      "  num_steps_sampled: 1384000\n",
      "  num_steps_trained: 1384000\n",
      "iterations_since_restore: 346\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 33.90909090909091\n",
      "  ram_util_percent: 44.33030303030303\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049194514978978775\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490125179332533\n",
      "  mean_inference_ms: 0.8506965753748225\n",
      "  mean_raw_obs_processing_ms: 0.08840802626818156\n",
      "time_since_restore: 1866.4787380695343\n",
      "time_this_iter_s: 5.3272340297698975\n",
      "time_total_s: 1866.4787380695343\n",
      "timers:\n",
      "  learn_throughput: 2133.166\n",
      "  learn_time_ms: 1875.147\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1145.144\n",
      "  sample_time_ms: 3493.011\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633513240\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1384000\n",
      "training_iteration: 346\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:346 starting ! -----------------\n",
      "agent_timesteps_total: 1388000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-40-53\n",
      "done: false\n",
      "episode_len_mean: 722.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.1352395000000023\n",
      "episode_reward_min: -2.1951500000000026\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 993\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1037014722824097\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010161654092371464\n",
      "        model: {}\n",
      "        policy_loss: -0.025364290922880173\n",
      "        total_loss: -0.007162095513194799\n",
      "        vf_explained_var: 0.525187611579895\n",
      "        vf_loss: 0.016169866546988487\n",
      "  num_agent_steps_sampled: 1388000\n",
      "  num_agent_steps_trained: 1388000\n",
      "  num_steps_sampled: 1388000\n",
      "  num_steps_trained: 1388000\n",
      "iterations_since_restore: 347\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.52777777777778\n",
      "  ram_util_percent: 44.22222222222223\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04919714905695809\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749027704852048\n",
      "  mean_inference_ms: 0.8506997630390286\n",
      "  mean_raw_obs_processing_ms: 0.08840945394572303\n",
      "time_since_restore: 1871.8757021427155\n",
      "time_this_iter_s: 5.396964073181152\n",
      "time_total_s: 1871.8757021427155\n",
      "timers:\n",
      "  learn_throughput: 2132.07\n",
      "  learn_time_ms: 1876.111\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.454\n",
      "  sample_time_ms: 3495.118\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633513253\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1388000\n",
      "training_iteration: 347\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:347 starting ! -----------------\n",
      "agent_timesteps_total: 1392000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-41-16\n",
      "done: false\n",
      "episode_len_mean: 720.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.06417400000000235\n",
      "episode_reward_min: -2.195849999999999\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 996\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9791898131370544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01032770611345768\n",
      "        model: {}\n",
      "        policy_loss: -0.02220751903951168\n",
      "        total_loss: 0.0040549286641180515\n",
      "        vf_explained_var: 0.08370713144540787\n",
      "        vf_loss: 0.024196911603212357\n",
      "  num_agent_steps_sampled: 1392000\n",
      "  num_agent_steps_trained: 1392000\n",
      "  num_steps_sampled: 1392000\n",
      "  num_steps_trained: 1392000\n",
      "iterations_since_restore: 348\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.171875\n",
      "  ram_util_percent: 44.1875\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04919939197544482\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490439153024555\n",
      "  mean_inference_ms: 0.8507012125895049\n",
      "  mean_raw_obs_processing_ms: 0.08841175818856811\n",
      "time_since_restore: 1877.3090603351593\n",
      "time_this_iter_s: 5.433358192443848\n",
      "time_total_s: 1877.3090603351593\n",
      "timers:\n",
      "  learn_throughput: 2127.52\n",
      "  learn_time_ms: 1880.124\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.354\n",
      "  sample_time_ms: 3498.478\n",
      "  update_time_ms: 1.696\n",
      "timestamp: 1633513276\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1392000\n",
      "training_iteration: 348\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:348 starting ! -----------------\n",
      "agent_timesteps_total: 1396000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-41-40\n",
      "done: false\n",
      "episode_len_mean: 721.89\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.013644000000002418\n",
      "episode_reward_min: -2.195849999999999\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 998\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0204447507858276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011601468548178673\n",
      "        model: {}\n",
      "        policy_loss: -0.029064035043120384\n",
      "        total_loss: -0.00551318284124136\n",
      "        vf_explained_var: 0.19771134853363037\n",
      "        vf_loss: 0.021230556070804596\n",
      "  num_agent_steps_sampled: 1396000\n",
      "  num_agent_steps_trained: 1396000\n",
      "  num_steps_sampled: 1396000\n",
      "  num_steps_trained: 1396000\n",
      "iterations_since_restore: 349\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.491176470588236\n",
      "  ram_util_percent: 44.13823529411764\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920068545055873\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490537238230255\n",
      "  mean_inference_ms: 0.8507019427483072\n",
      "  mean_raw_obs_processing_ms: 0.0884130069617677\n",
      "time_since_restore: 1882.6983573436737\n",
      "time_this_iter_s: 5.389297008514404\n",
      "time_total_s: 1882.6983573436737\n",
      "timers:\n",
      "  learn_throughput: 2127.451\n",
      "  learn_time_ms: 1880.184\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.898\n",
      "  sample_time_ms: 3493.76\n",
      "  update_time_ms: 1.774\n",
      "timestamp: 1633513300\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1396000\n",
      "training_iteration: 349\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:349 starting ! -----------------\n",
      "agent_timesteps_total: 1400000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-41-54\n",
      "done: false\n",
      "episode_len_mean: 714.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: -0.006548999999997656\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1001\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9712451100349426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009654982015490532\n",
      "        model: {}\n",
      "        policy_loss: -0.017460839822888374\n",
      "        total_loss: -0.005598782561719418\n",
      "        vf_explained_var: -0.19358336925506592\n",
      "        vf_loss: 0.00993106048554182\n",
      "  num_agent_steps_sampled: 1400000\n",
      "  num_agent_steps_trained: 1400000\n",
      "  num_steps_sampled: 1400000\n",
      "  num_steps_trained: 1400000\n",
      "iterations_since_restore: 350\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.235\n",
      "  ram_util_percent: 44.475\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920376231031888\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490766729698614\n",
      "  mean_inference_ms: 0.8507069545693992\n",
      "  mean_raw_obs_processing_ms: 0.08841602647448797\n",
      "time_since_restore: 1888.6163375377655\n",
      "time_this_iter_s: 5.917980194091797\n",
      "time_total_s: 1888.6163375377655\n",
      "timers:\n",
      "  learn_throughput: 2120.443\n",
      "  learn_time_ms: 1886.398\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.143\n",
      "  sample_time_ms: 3523.785\n",
      "  update_time_ms: 1.773\n",
      "timestamp: 1633513314\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1400000\n",
      "training_iteration: 350\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:350 starting ! -----------------\n",
      "agent_timesteps_total: 1404000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-42-08\n",
      "done: false\n",
      "episode_len_mean: 711.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: -0.01689799999999772\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1004\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9795917272567749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009158072993159294\n",
      "        model: {}\n",
      "        policy_loss: -0.021839383989572525\n",
      "        total_loss: -0.007906002923846245\n",
      "        vf_explained_var: 0.149654358625412\n",
      "        vf_loss: 0.012101765722036362\n",
      "  num_agent_steps_sampled: 1404000\n",
      "  num_agent_steps_trained: 1404000\n",
      "  num_steps_sampled: 1404000\n",
      "  num_steps_trained: 1404000\n",
      "iterations_since_restore: 351\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.82631578947368\n",
      "  ram_util_percent: 44.557894736842115\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920624757437654\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7490915696350227\n",
      "  mean_inference_ms: 0.8507140201720803\n",
      "  mean_raw_obs_processing_ms: 0.08841883542130866\n",
      "time_since_restore: 1894.0954594612122\n",
      "time_this_iter_s: 5.479121923446655\n",
      "time_total_s: 1894.0954594612122\n",
      "timers:\n",
      "  learn_throughput: 2113.919\n",
      "  learn_time_ms: 1892.22\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.977\n",
      "  sample_time_ms: 3533.64\n",
      "  update_time_ms: 1.774\n",
      "timestamp: 1633513328\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1404000\n",
      "training_iteration: 351\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:351 starting ! -----------------\n",
      "agent_timesteps_total: 1408000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-42-20\n",
      "done: false\n",
      "episode_len_mean: 715.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: -0.07549199999999767\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1007\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0475425720214844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012436315417289734\n",
      "        model: {}\n",
      "        policy_loss: -0.026689516380429268\n",
      "        total_loss: -0.012640286237001419\n",
      "        vf_explained_var: 0.16565418243408203\n",
      "        vf_loss: 0.011561970226466656\n",
      "  num_agent_steps_sampled: 1408000\n",
      "  num_agent_steps_trained: 1408000\n",
      "  num_steps_sampled: 1408000\n",
      "  num_steps_trained: 1408000\n",
      "iterations_since_restore: 352\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.417647058823526\n",
      "  ram_util_percent: 44.617647058823536\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04920876597917506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491028921775813\n",
      "  mean_inference_ms: 0.8507204146824344\n",
      "  mean_raw_obs_processing_ms: 0.08842098576858132\n",
      "time_since_restore: 1899.5444746017456\n",
      "time_this_iter_s: 5.449015140533447\n",
      "time_total_s: 1899.5444746017456\n",
      "timers:\n",
      "  learn_throughput: 2104.182\n",
      "  learn_time_ms: 1900.976\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.235\n",
      "  sample_time_ms: 3542.221\n",
      "  update_time_ms: 1.873\n",
      "timestamp: 1633513340\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1408000\n",
      "training_iteration: 352\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:352 starting ! -----------------\n",
      "agent_timesteps_total: 1412000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-42-42\n",
      "done: false\n",
      "episode_len_mean: 708.69\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.024656000000002294\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1010\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9204857349395752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012095930986106396\n",
      "        model: {}\n",
      "        policy_loss: -0.04251789301633835\n",
      "        total_loss: -0.026760714128613472\n",
      "        vf_explained_var: 0.2935335338115692\n",
      "        vf_loss: 0.013337994925677776\n",
      "  num_agent_steps_sampled: 1412000\n",
      "  num_agent_steps_trained: 1412000\n",
      "  num_steps_sampled: 1412000\n",
      "  num_steps_trained: 1412000\n",
      "iterations_since_restore: 353\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.65\n",
      "  ram_util_percent: 44.599999999999994\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921092722628027\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491137021171429\n",
      "  mean_inference_ms: 0.8507274738227986\n",
      "  mean_raw_obs_processing_ms: 0.08842303469368114\n",
      "time_since_restore: 1904.8510856628418\n",
      "time_this_iter_s: 5.306611061096191\n",
      "time_total_s: 1904.8510856628418\n",
      "timers:\n",
      "  learn_throughput: 2108.061\n",
      "  learn_time_ms: 1897.478\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.81\n",
      "  sample_time_ms: 3537.287\n",
      "  update_time_ms: 1.873\n",
      "timestamp: 1633513362\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1412000\n",
      "training_iteration: 353\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:353 starting ! -----------------\n",
      "agent_timesteps_total: 1416000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-43-07\n",
      "done: false\n",
      "episode_len_mean: 709.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.07317000000000227\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1014\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9845137596130371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009551139548420906\n",
      "        model: {}\n",
      "        policy_loss: -0.025709684938192368\n",
      "        total_loss: -0.0032669221982359886\n",
      "        vf_explained_var: 0.1759418398141861\n",
      "        vf_loss: 0.02053254283964634\n",
      "  num_agent_steps_sampled: 1416000\n",
      "  num_agent_steps_trained: 1416000\n",
      "  num_steps_sampled: 1416000\n",
      "  num_steps_trained: 1416000\n",
      "iterations_since_restore: 354\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.63235294117647\n",
      "  ram_util_percent: 44.59999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492128631190235\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491360739645492\n",
      "  mean_inference_ms: 0.850736909415335\n",
      "  mean_raw_obs_processing_ms: 0.08842700384978353\n",
      "time_since_restore: 1910.2744789123535\n",
      "time_this_iter_s: 5.423393249511719\n",
      "time_total_s: 1910.2744789123535\n",
      "timers:\n",
      "  learn_throughput: 2106.272\n",
      "  learn_time_ms: 1899.09\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.564\n",
      "  sample_time_ms: 3550.618\n",
      "  update_time_ms: 1.773\n",
      "timestamp: 1633513387\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1416000\n",
      "training_iteration: 354\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:354 starting ! -----------------\n",
      "agent_timesteps_total: 1420000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-43-31\n",
      "done: false\n",
      "episode_len_mean: 703.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.06416200000000222\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1017\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9648237228393555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012517601251602173\n",
      "        model: {}\n",
      "        policy_loss: -0.029789408668875694\n",
      "        total_loss: -0.011979280039668083\n",
      "        vf_explained_var: 0.2597844898700714\n",
      "        vf_loss: 0.015306605957448483\n",
      "  num_agent_steps_sampled: 1420000\n",
      "  num_agent_steps_trained: 1420000\n",
      "  num_steps_sampled: 1420000\n",
      "  num_steps_trained: 1420000\n",
      "iterations_since_restore: 355\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.538235294117648\n",
      "  ram_util_percent: 44.58529411764705\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049214844499049615\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491527285654206\n",
      "  mean_inference_ms: 0.8507429549983228\n",
      "  mean_raw_obs_processing_ms: 0.08842961598144966\n",
      "time_since_restore: 1915.6169905662537\n",
      "time_this_iter_s: 5.3425116539001465\n",
      "time_total_s: 1915.6169905662537\n",
      "timers:\n",
      "  learn_throughput: 2114.223\n",
      "  learn_time_ms: 1891.948\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.598\n",
      "  sample_time_ms: 3547.364\n",
      "  update_time_ms: 1.773\n",
      "timestamp: 1633513411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1420000\n",
      "training_iteration: 355\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:355 starting ! -----------------\n",
      "agent_timesteps_total: 1424000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-43-53\n",
      "done: false\n",
      "episode_len_mean: 700.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.054123000000002225\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1021\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.068932294845581\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014858436770737171\n",
      "        model: {}\n",
      "        policy_loss: -0.02398030087351799\n",
      "        total_loss: -0.008202978409826756\n",
      "        vf_explained_var: 0.4011589586734772\n",
      "        vf_loss: 0.012805632315576077\n",
      "  num_agent_steps_sampled: 1424000\n",
      "  num_agent_steps_trained: 1424000\n",
      "  num_steps_sampled: 1424000\n",
      "  num_steps_trained: 1424000\n",
      "iterations_since_restore: 356\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.500000000000004\n",
      "  ram_util_percent: 44.587096774193526\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049217207697867235\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491744449420892\n",
      "  mean_inference_ms: 0.8507524068950036\n",
      "  mean_raw_obs_processing_ms: 0.08843252400190585\n",
      "time_since_restore: 1921.0112628936768\n",
      "time_this_iter_s: 5.394272327423096\n",
      "time_total_s: 1921.0112628936768\n",
      "timers:\n",
      "  learn_throughput: 2112.129\n",
      "  learn_time_ms: 1893.824\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.036\n",
      "  sample_time_ms: 3552.283\n",
      "  update_time_ms: 1.882\n",
      "timestamp: 1633513433\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1424000\n",
      "training_iteration: 356\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:356 starting ! -----------------\n",
      "agent_timesteps_total: 1428000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-44-05\n",
      "done: false\n",
      "episode_len_mean: 694.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.04406950000000218\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1024\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0774683952331543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01128428429365158\n",
      "        model: {}\n",
      "        policy_loss: -0.025208469480276108\n",
      "        total_loss: 0.0010663545690476894\n",
      "        vf_explained_var: 0.09876218438148499\n",
      "        vf_loss: 0.024017969146370888\n",
      "  num_agent_steps_sampled: 1428000\n",
      "  num_agent_steps_trained: 1428000\n",
      "  num_steps_sampled: 1428000\n",
      "  num_steps_trained: 1428000\n",
      "iterations_since_restore: 357\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.50588235294118\n",
      "  ram_util_percent: 44.57058823529412\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921999125526521\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7491945932301585\n",
      "  mean_inference_ms: 0.8507571595706283\n",
      "  mean_raw_obs_processing_ms: 0.08843515124286457\n",
      "time_since_restore: 1926.404231786728\n",
      "time_this_iter_s: 5.3929688930511475\n",
      "time_total_s: 1926.404231786728\n",
      "timers:\n",
      "  learn_throughput: 2113.275\n",
      "  learn_time_ms: 1892.796\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1125.818\n",
      "  sample_time_ms: 3552.973\n",
      "  update_time_ms: 1.882\n",
      "timestamp: 1633513445\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1428000\n",
      "training_iteration: 357\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:357 starting ! -----------------\n",
      "agent_timesteps_total: 1432000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-44-24\n",
      "done: false\n",
      "episode_len_mean: 694.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.05342800000000223\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1026\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8769582509994507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009118309244513512\n",
      "        model: {}\n",
      "        policy_loss: -0.015174268744885921\n",
      "        total_loss: -0.00575505243614316\n",
      "        vf_explained_var: 0.32932403683662415\n",
      "        vf_loss: 0.007595555391162634\n",
      "  num_agent_steps_sampled: 1432000\n",
      "  num_agent_steps_trained: 1432000\n",
      "  num_steps_sampled: 1432000\n",
      "  num_steps_trained: 1432000\n",
      "iterations_since_restore: 358\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.761538461538464\n",
      "  ram_util_percent: 44.58076923076923\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922188154580159\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7492047618459438\n",
      "  mean_inference_ms: 0.8507605533763044\n",
      "  mean_raw_obs_processing_ms: 0.08843712509759864\n",
      "time_since_restore: 1931.720942735672\n",
      "time_this_iter_s: 5.316710948944092\n",
      "time_total_s: 1931.720942735672\n",
      "timers:\n",
      "  learn_throughput: 2120.191\n",
      "  learn_time_ms: 1886.623\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.56\n",
      "  sample_time_ms: 3547.482\n",
      "  update_time_ms: 1.882\n",
      "timestamp: 1633513464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1432000\n",
      "training_iteration: 358\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:358 starting ! -----------------\n",
      "agent_timesteps_total: 1436000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-44-48\n",
      "done: false\n",
      "episode_len_mean: 689.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9193\n",
      "episode_reward_mean: 0.11436150000000221\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 1031\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0803176164627075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009789821691811085\n",
      "        model: {}\n",
      "        policy_loss: -0.024184545502066612\n",
      "        total_loss: -0.0033544304315000772\n",
      "        vf_explained_var: 0.13637453317642212\n",
      "        vf_loss: 0.018872147426009178\n",
      "  num_agent_steps_sampled: 1436000\n",
      "  num_agent_steps_trained: 1436000\n",
      "  num_steps_sampled: 1436000\n",
      "  num_steps_trained: 1436000\n",
      "iterations_since_restore: 359\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.639999999999997\n",
      "  ram_util_percent: 44.597142857142856\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492267810474881\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749229492705382\n",
      "  mean_inference_ms: 0.8507725929090256\n",
      "  mean_raw_obs_processing_ms: 0.08844247714517035\n",
      "time_since_restore: 1937.3115355968475\n",
      "time_this_iter_s: 5.590592861175537\n",
      "time_total_s: 1937.3115355968475\n",
      "timers:\n",
      "  learn_throughput: 2103.623\n",
      "  learn_time_ms: 1901.481\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1125.888\n",
      "  sample_time_ms: 3552.752\n",
      "  update_time_ms: 1.805\n",
      "timestamp: 1633513488\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1436000\n",
      "training_iteration: 359\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:359 starting ! -----------------\n",
      "agent_timesteps_total: 1440000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-45-12\n",
      "done: false\n",
      "episode_len_mean: 690.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9158\n",
      "episode_reward_mean: 0.11368150000000227\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1034\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0053068399429321\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016095846891403198\n",
      "        model: {}\n",
      "        policy_loss: -0.018737010657787323\n",
      "        total_loss: 0.00405014818534255\n",
      "        vf_explained_var: 0.14931437373161316\n",
      "        vf_loss: 0.01956799440085888\n",
      "  num_agent_steps_sampled: 1440000\n",
      "  num_agent_steps_trained: 1440000\n",
      "  num_steps_sampled: 1440000\n",
      "  num_steps_trained: 1440000\n",
      "iterations_since_restore: 360\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.275757575757574\n",
      "  ram_util_percent: 44.584848484848486\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049229054214732136\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749244522571492\n",
      "  mean_inference_ms: 0.8507783484610135\n",
      "  mean_raw_obs_processing_ms: 0.08844554350004152\n",
      "time_since_restore: 1942.6794815063477\n",
      "time_this_iter_s: 5.367945909500122\n",
      "time_total_s: 1942.6794815063477\n",
      "timers:\n",
      "  learn_throughput: 2122.237\n",
      "  learn_time_ms: 1884.804\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.173\n",
      "  sample_time_ms: 3514.405\n",
      "  update_time_ms: 1.805\n",
      "timestamp: 1633513512\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1440000\n",
      "training_iteration: 360\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:360 starting ! -----------------\n",
      "agent_timesteps_total: 1444000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-45-26\n",
      "done: false\n",
      "episode_len_mean: 678.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9158\n",
      "episode_reward_mean: 0.1646135000000022\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 1039\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.106347918510437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009590290486812592\n",
      "        model: {}\n",
      "        policy_loss: -0.025506481528282166\n",
      "        total_loss: -0.0016668555326759815\n",
      "        vf_explained_var: 0.40984493494033813\n",
      "        vf_loss: 0.021921567618846893\n",
      "  num_agent_steps_sampled: 1444000\n",
      "  num_agent_steps_trained: 1444000\n",
      "  num_steps_sampled: 1444000\n",
      "  num_steps_trained: 1444000\n",
      "iterations_since_restore: 361\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.785000000000004\n",
      "  ram_util_percent: 44.61\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923271351533387\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7492670605587008\n",
      "  mean_inference_ms: 0.8507856762546708\n",
      "  mean_raw_obs_processing_ms: 0.08845063573738812\n",
      "time_since_restore: 1947.980840921402\n",
      "time_this_iter_s: 5.301359415054321\n",
      "time_total_s: 1947.980840921402\n",
      "timers:\n",
      "  learn_throughput: 2129.132\n",
      "  learn_time_ms: 1878.699\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.934\n",
      "  sample_time_ms: 3502.828\n",
      "  update_time_ms: 1.804\n",
      "timestamp: 1633513526\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1444000\n",
      "training_iteration: 361\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:361 starting ! -----------------\n",
      "agent_timesteps_total: 1448000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-45-43\n",
      "done: false\n",
      "episode_len_mean: 675.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9158\n",
      "episode_reward_mean: 0.2152235000000022\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1042\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9760639071464539\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013489448465406895\n",
      "        model: {}\n",
      "        policy_loss: -0.03060903586447239\n",
      "        total_loss: 0.004346617963165045\n",
      "        vf_explained_var: 0.5008948445320129\n",
      "        vf_loss: 0.03225776180624962\n",
      "  num_agent_steps_sampled: 1448000\n",
      "  num_agent_steps_trained: 1448000\n",
      "  num_steps_sampled: 1448000\n",
      "  num_steps_trained: 1448000\n",
      "iterations_since_restore: 362\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.96666666666667\n",
      "  ram_util_percent: 44.59166666666667\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923580343139638\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7492815715218712\n",
      "  mean_inference_ms: 0.8507881102497872\n",
      "  mean_raw_obs_processing_ms: 0.08845353878266166\n",
      "time_since_restore: 1953.3095109462738\n",
      "time_this_iter_s: 5.328670024871826\n",
      "time_total_s: 1953.3095109462738\n",
      "timers:\n",
      "  learn_throughput: 2139.111\n",
      "  learn_time_ms: 1869.936\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.997\n",
      "  sample_time_ms: 3499.572\n",
      "  update_time_ms: 1.804\n",
      "timestamp: 1633513543\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1448000\n",
      "training_iteration: 362\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:362 starting ! -----------------\n",
      "agent_timesteps_total: 1452000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-45-56\n",
      "done: false\n",
      "episode_len_mean: 673.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.26543300000000214\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1046\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9799127578735352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011676962487399578\n",
      "        model: {}\n",
      "        policy_loss: -0.028105895966291428\n",
      "        total_loss: -0.015741538256406784\n",
      "        vf_explained_var: 0.6557708382606506\n",
      "        vf_loss: 0.010028968565165997\n",
      "  num_agent_steps_sampled: 1452000\n",
      "  num_agent_steps_trained: 1452000\n",
      "  num_steps_sampled: 1452000\n",
      "  num_steps_trained: 1452000\n",
      "iterations_since_restore: 363\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.827777777777776\n",
      "  ram_util_percent: 44.62222222222223\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492391014764784\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493016901838798\n",
      "  mean_inference_ms: 0.8507892126792275\n",
      "  mean_raw_obs_processing_ms: 0.0884582949493617\n",
      "time_since_restore: 1958.755116224289\n",
      "time_this_iter_s: 5.445605278015137\n",
      "time_total_s: 1958.755116224289\n",
      "timers:\n",
      "  learn_throughput: 2131.602\n",
      "  learn_time_ms: 1876.523\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.599\n",
      "  sample_time_ms: 3506.928\n",
      "  update_time_ms: 1.804\n",
      "timestamp: 1633513556\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1452000\n",
      "training_iteration: 363\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:363 starting ! -----------------\n",
      "agent_timesteps_total: 1456000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-46-12\n",
      "done: false\n",
      "episode_len_mean: 665.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.316121500000002\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1049\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8831031322479248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012882798910140991\n",
      "        model: {}\n",
      "        policy_loss: -0.03179573267698288\n",
      "        total_loss: -0.016948480159044266\n",
      "        vf_explained_var: 0.32104337215423584\n",
      "        vf_loss: 0.012270687147974968\n",
      "  num_agent_steps_sampled: 1456000\n",
      "  num_agent_steps_trained: 1456000\n",
      "  num_steps_sampled: 1456000\n",
      "  num_steps_trained: 1456000\n",
      "iterations_since_restore: 364\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.734782608695657\n",
      "  ram_util_percent: 44.582608695652176\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924070956844948\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493154579254349\n",
      "  mean_inference_ms: 0.8507900193469274\n",
      "  mean_raw_obs_processing_ms: 0.08846166495371627\n",
      "time_since_restore: 1964.1295683383942\n",
      "time_this_iter_s: 5.374452114105225\n",
      "time_total_s: 1964.1295683383942\n",
      "timers:\n",
      "  learn_throughput: 2135.889\n",
      "  learn_time_ms: 1872.757\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.975\n",
      "  sample_time_ms: 3505.773\n",
      "  update_time_ms: 1.904\n",
      "timestamp: 1633513572\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1456000\n",
      "training_iteration: 364\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:364 starting ! -----------------\n",
      "agent_timesteps_total: 1460000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-46-27\n",
      "done: false\n",
      "episode_len_mean: 657.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.4778950000000019\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1053\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9738075137138367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012224229983985424\n",
      "        model: {}\n",
      "        policy_loss: -0.022186141461133957\n",
      "        total_loss: -0.005303671583533287\n",
      "        vf_explained_var: 0.3955368399620056\n",
      "        vf_loss: 0.014437624253332615\n",
      "  num_agent_steps_sampled: 1460000\n",
      "  num_agent_steps_trained: 1460000\n",
      "  num_steps_sampled: 1460000\n",
      "  num_steps_trained: 1460000\n",
      "iterations_since_restore: 365\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.847619047619045\n",
      "  ram_util_percent: 44.609523809523814\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924239776254215\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493315447654131\n",
      "  mean_inference_ms: 0.8507908102019277\n",
      "  mean_raw_obs_processing_ms: 0.0884656958540167\n",
      "time_since_restore: 1969.5782918930054\n",
      "time_this_iter_s: 5.448723554611206\n",
      "time_total_s: 1969.5782918930054\n",
      "timers:\n",
      "  learn_throughput: 2122.4\n",
      "  learn_time_ms: 1884.659\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.464\n",
      "  sample_time_ms: 3504.271\n",
      "  update_time_ms: 2.004\n",
      "timestamp: 1633513587\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1460000\n",
      "training_iteration: 365\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:365 starting ! -----------------\n",
      "agent_timesteps_total: 1464000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-46-40\n",
      "done: false\n",
      "episode_len_mean: 648.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.5285935000000018\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1057\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0027074813842773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011700115166604519\n",
      "        model: {}\n",
      "        policy_loss: -0.026403483003377914\n",
      "        total_loss: -0.0007879795739427209\n",
      "        vf_explained_var: 0.2121099978685379\n",
      "        vf_loss: 0.023275481536984444\n",
      "  num_agent_steps_sampled: 1464000\n",
      "  num_agent_steps_trained: 1464000\n",
      "  num_steps_sampled: 1464000\n",
      "  num_steps_trained: 1464000\n",
      "iterations_since_restore: 366\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.49473684210526\n",
      "  ram_util_percent: 44.6\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049243587495834754\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493504480877907\n",
      "  mean_inference_ms: 0.8507905051581969\n",
      "  mean_raw_obs_processing_ms: 0.08847012413641353\n",
      "time_since_restore: 1975.0063474178314\n",
      "time_this_iter_s: 5.42805552482605\n",
      "time_total_s: 1975.0063474178314\n",
      "timers:\n",
      "  learn_throughput: 2125.421\n",
      "  learn_time_ms: 1881.98\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.537\n",
      "  sample_time_ms: 3510.198\n",
      "  update_time_ms: 1.996\n",
      "timestamp: 1633513600\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1464000\n",
      "training_iteration: 366\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:366 starting ! -----------------\n",
      "agent_timesteps_total: 1468000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-46-51\n",
      "done: false\n",
      "episode_len_mean: 636.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.6910325000000017\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 1062\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0215365886688232\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012229220010340214\n",
      "        model: {}\n",
      "        policy_loss: -0.02258511632680893\n",
      "        total_loss: -0.001990945776924491\n",
      "        vf_explained_var: 0.5339913964271545\n",
      "        vf_loss: 0.018148327246308327\n",
      "  num_agent_steps_sampled: 1468000\n",
      "  num_agent_steps_trained: 1468000\n",
      "  num_steps_sampled: 1468000\n",
      "  num_steps_trained: 1468000\n",
      "iterations_since_restore: 367\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.75\n",
      "  ram_util_percent: 44.642857142857146\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924587997382997\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493756002129769\n",
      "  mean_inference_ms: 0.8507953198113333\n",
      "  mean_raw_obs_processing_ms: 0.08847681223825642\n",
      "time_since_restore: 1980.5178654193878\n",
      "time_this_iter_s: 5.5115180015563965\n",
      "time_total_s: 1980.5178654193878\n",
      "timers:\n",
      "  learn_throughput: 2125.185\n",
      "  learn_time_ms: 1882.189\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.738\n",
      "  sample_time_ms: 3521.939\n",
      "  update_time_ms: 1.996\n",
      "timestamp: 1633513611\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1468000\n",
      "training_iteration: 367\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:367 starting ! -----------------\n",
      "agent_timesteps_total: 1472000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-47-11\n",
      "done: false\n",
      "episode_len_mean: 635.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.7517525000000017\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1065\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0191535949707031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01869182288646698\n",
      "        model: {}\n",
      "        policy_loss: -0.023530295118689537\n",
      "        total_loss: 0.010874446481466293\n",
      "        vf_explained_var: 0.19856767356395721\n",
      "        vf_loss: 0.030666379258036613\n",
      "  num_agent_steps_sampled: 1472000\n",
      "  num_agent_steps_trained: 1472000\n",
      "  num_steps_sampled: 1472000\n",
      "  num_steps_trained: 1472000\n",
      "iterations_since_restore: 368\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.73103448275862\n",
      "  ram_util_percent: 44.610344827586204\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049247681703094515\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493845396568197\n",
      "  mean_inference_ms: 0.8508008005675136\n",
      "  mean_raw_obs_processing_ms: 0.08847960981537577\n",
      "time_since_restore: 1985.8400399684906\n",
      "time_this_iter_s: 5.322174549102783\n",
      "time_total_s: 1985.8400399684906\n",
      "timers:\n",
      "  learn_throughput: 2125.741\n",
      "  learn_time_ms: 1881.697\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.402\n",
      "  sample_time_ms: 3522.983\n",
      "  update_time_ms: 2.048\n",
      "timestamp: 1633513631\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1472000\n",
      "training_iteration: 368\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:368 starting ! -----------------\n",
      "agent_timesteps_total: 1476000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-47-25\n",
      "done: false\n",
      "episode_len_mean: 641.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.6917150000000016\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1067\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1152238845825195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01425657793879509\n",
      "        model: {}\n",
      "        policy_loss: -0.018109070137143135\n",
      "        total_loss: 0.014502005651593208\n",
      "        vf_explained_var: -0.06737793236970901\n",
      "        vf_loss: 0.029759753495454788\n",
      "  num_agent_steps_sampled: 1476000\n",
      "  num_agent_steps_trained: 1476000\n",
      "  num_steps_sampled: 1476000\n",
      "  num_steps_trained: 1476000\n",
      "iterations_since_restore: 369\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.647368421052636\n",
      "  ram_util_percent: 44.62105263157895\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924853832635809\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7493911428749612\n",
      "  mean_inference_ms: 0.8508040809937168\n",
      "  mean_raw_obs_processing_ms: 0.08848111967876013\n",
      "time_since_restore: 1991.269654750824\n",
      "time_this_iter_s: 5.429614782333374\n",
      "time_total_s: 1991.269654750824\n",
      "timers:\n",
      "  learn_throughput: 2133.846\n",
      "  learn_time_ms: 1874.55\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.283\n",
      "  sample_time_ms: 3514.066\n",
      "  update_time_ms: 2.034\n",
      "timestamp: 1633513645\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1476000\n",
      "training_iteration: 369\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:369 starting ! -----------------\n",
      "agent_timesteps_total: 1480000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-47-41\n",
      "done: false\n",
      "episode_len_mean: 634.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9186999999999985\n",
      "episode_reward_mean: 0.8028210000000017\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1070\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0304864645004272\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010937139391899109\n",
      "        model: {}\n",
      "        policy_loss: -0.02409663051366806\n",
      "        total_loss: -0.005243315827101469\n",
      "        vf_explained_var: 0.3742057681083679\n",
      "        vf_loss: 0.01666589081287384\n",
      "  num_agent_steps_sampled: 1480000\n",
      "  num_agent_steps_trained: 1480000\n",
      "  num_steps_sampled: 1480000\n",
      "  num_steps_trained: 1480000\n",
      "iterations_since_restore: 370\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.740909090909096\n",
      "  ram_util_percent: 44.604545454545466\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924987632325096\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749402012886901\n",
      "  mean_inference_ms: 0.8508081111880383\n",
      "  mean_raw_obs_processing_ms: 0.08848388511575916\n",
      "time_since_restore: 1996.733798980713\n",
      "time_this_iter_s: 5.464144229888916\n",
      "time_total_s: 1996.733798980713\n",
      "timers:\n",
      "  learn_throughput: 2123.147\n",
      "  learn_time_ms: 1883.996\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.223\n",
      "  sample_time_ms: 3514.25\n",
      "  update_time_ms: 2.035\n",
      "timestamp: 1633513661\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1480000\n",
      "training_iteration: 370\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:370 starting ! -----------------\n",
      "agent_timesteps_total: 1484000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-48-04\n",
      "done: false\n",
      "episode_len_mean: 623.71\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.8638055000000017\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1074\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0349748134613037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011393009684979916\n",
      "        model: {}\n",
      "        policy_loss: -0.022856030613183975\n",
      "        total_loss: -0.0036086461041122675\n",
      "        vf_explained_var: 0.3669031858444214\n",
      "        vf_loss: 0.016968781128525734\n",
      "  num_agent_steps_sampled: 1484000\n",
      "  num_agent_steps_trained: 1484000\n",
      "  num_steps_sampled: 1484000\n",
      "  num_steps_trained: 1484000\n",
      "iterations_since_restore: 371\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.8875\n",
      "  ram_util_percent: 44.621875\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049252371726846975\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494120215906512\n",
      "  mean_inference_ms: 0.8508084244881015\n",
      "  mean_raw_obs_processing_ms: 0.08848750947691307\n",
      "time_since_restore: 2002.04714179039\n",
      "time_this_iter_s: 5.313342809677124\n",
      "time_total_s: 2002.04714179039\n",
      "timers:\n",
      "  learn_throughput: 2122.525\n",
      "  learn_time_ms: 1884.548\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.015\n",
      "  sample_time_ms: 3514.893\n",
      "  update_time_ms: 2.035\n",
      "timestamp: 1633513684\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1484000\n",
      "training_iteration: 371\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:371 starting ! -----------------\n",
      "agent_timesteps_total: 1488000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-48-28\n",
      "done: false\n",
      "episode_len_mean: 622.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.8639565000000016\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1077\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0021945238113403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009985275566577911\n",
      "        model: {}\n",
      "        policy_loss: -0.025232693180441856\n",
      "        total_loss: -0.00347444461658597\n",
      "        vf_explained_var: 0.2304040491580963\n",
      "        vf_loss: 0.019761186093091965\n",
      "  num_agent_steps_sampled: 1488000\n",
      "  num_agent_steps_trained: 1488000\n",
      "  num_steps_sampled: 1488000\n",
      "  num_steps_trained: 1488000\n",
      "iterations_since_restore: 372\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.255882352941175\n",
      "  ram_util_percent: 44.6235294117647\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925427644527952\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494214494219089\n",
      "  mean_inference_ms: 0.8508082211153539\n",
      "  mean_raw_obs_processing_ms: 0.08848937582865983\n",
      "time_since_restore: 2007.4820742607117\n",
      "time_this_iter_s: 5.434932470321655\n",
      "time_total_s: 2007.4820742607117\n",
      "timers:\n",
      "  learn_throughput: 2114.784\n",
      "  learn_time_ms: 1891.446\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.763\n",
      "  sample_time_ms: 3518.765\n",
      "  update_time_ms: 2.035\n",
      "timestamp: 1633513708\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1488000\n",
      "training_iteration: 372\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:372 starting ! -----------------\n",
      "agent_timesteps_total: 1492000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-48-42\n",
      "done: false\n",
      "episode_len_mean: 621.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.8034230000000017\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1080\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0127265453338623\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010263791307806969\n",
      "        model: {}\n",
      "        policy_loss: -0.018688751384615898\n",
      "        total_loss: 0.01020000409334898\n",
      "        vf_explained_var: 0.1112830862402916\n",
      "        vf_loss: 0.02683599293231964\n",
      "  num_agent_steps_sampled: 1492000\n",
      "  num_agent_steps_trained: 1492000\n",
      "  num_steps_sampled: 1492000\n",
      "  num_steps_trained: 1492000\n",
      "iterations_since_restore: 373\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.73\n",
      "  ram_util_percent: 44.605000000000004\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049256541595391765\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494349338476302\n",
      "  mean_inference_ms: 0.8508068629900088\n",
      "  mean_raw_obs_processing_ms: 0.08849174368675465\n",
      "time_since_restore: 2012.8425998687744\n",
      "time_this_iter_s: 5.360525608062744\n",
      "time_total_s: 2012.8425998687744\n",
      "timers:\n",
      "  learn_throughput: 2121.137\n",
      "  learn_time_ms: 1885.781\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.703\n",
      "  sample_time_ms: 3515.857\n",
      "  update_time_ms: 2.036\n",
      "timestamp: 1633513722\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1492000\n",
      "training_iteration: 373\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:373 starting ! -----------------\n",
      "agent_timesteps_total: 1496000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-48-59\n",
      "done: false\n",
      "episode_len_mean: 619.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.8036080000000015\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1082\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8848816752433777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009681991301476955\n",
      "        model: {}\n",
      "        policy_loss: -0.017244573682546616\n",
      "        total_loss: -0.004289590287953615\n",
      "        vf_explained_var: 0.24482038617134094\n",
      "        vf_loss: 0.011018583551049232\n",
      "  num_agent_steps_sampled: 1496000\n",
      "  num_agent_steps_trained: 1496000\n",
      "  num_steps_sampled: 1496000\n",
      "  num_steps_trained: 1496000\n",
      "iterations_since_restore: 374\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.820833333333336\n",
      "  ram_util_percent: 44.62916666666666\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049258220037100384\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494416890677839\n",
      "  mean_inference_ms: 0.8508057589942891\n",
      "  mean_raw_obs_processing_ms: 0.08849338462094929\n",
      "time_since_restore: 2018.1210396289825\n",
      "time_this_iter_s: 5.27843976020813\n",
      "time_total_s: 2018.1210396289825\n",
      "timers:\n",
      "  learn_throughput: 2119.176\n",
      "  learn_time_ms: 1887.526\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.374\n",
      "  sample_time_ms: 3504.547\n",
      "  update_time_ms: 1.937\n",
      "timestamp: 1633513739\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1496000\n",
      "training_iteration: 374\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:374 starting ! -----------------\n",
      "agent_timesteps_total: 1500000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-49-12\n",
      "done: false\n",
      "episode_len_mean: 618.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.8538535000000017\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1086\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9324271082878113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014348799362778664\n",
      "        model: {}\n",
      "        policy_loss: -0.030568091198801994\n",
      "        total_loss: -0.004676850512623787\n",
      "        vf_explained_var: 0.31731507182121277\n",
      "        vf_loss: 0.023021480068564415\n",
      "  num_agent_steps_sampled: 1500000\n",
      "  num_agent_steps_trained: 1500000\n",
      "  num_steps_sampled: 1500000\n",
      "  num_steps_trained: 1500000\n",
      "iterations_since_restore: 375\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.17222222222222\n",
      "  ram_util_percent: 44.611111111111114\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926168701717845\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494621449481165\n",
      "  mean_inference_ms: 0.8508025668805197\n",
      "  mean_raw_obs_processing_ms: 0.08849709836761821\n",
      "time_since_restore: 2023.572041273117\n",
      "time_this_iter_s: 5.4510016441345215\n",
      "time_total_s: 2023.572041273117\n",
      "timers:\n",
      "  learn_throughput: 2128.17\n",
      "  learn_time_ms: 1879.549\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.676\n",
      "  sample_time_ms: 3512.851\n",
      "  update_time_ms: 1.837\n",
      "timestamp: 1633513752\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1500000\n",
      "training_iteration: 375\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:375 starting ! -----------------\n",
      "agent_timesteps_total: 1504000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-49-25\n",
      "done: false\n",
      "episode_len_mean: 611.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.9044000000000015\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1089\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9684184193611145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01474765408784151\n",
      "        model: {}\n",
      "        policy_loss: -0.03210223466157913\n",
      "        total_loss: -0.018420018255710602\n",
      "        vf_explained_var: 0.16606688499450684\n",
      "        vf_loss: 0.010732680559158325\n",
      "  num_agent_steps_sampled: 1504000\n",
      "  num_agent_steps_trained: 1504000\n",
      "  num_steps_sampled: 1504000\n",
      "  num_steps_trained: 1504000\n",
      "iterations_since_restore: 376\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.926315789473687\n",
      "  ram_util_percent: 44.64736842105264\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926379086997903\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7494813006449261\n",
      "  mean_inference_ms: 0.8507977109877839\n",
      "  mean_raw_obs_processing_ms: 0.08849920577813807\n",
      "time_since_restore: 2028.8907587528229\n",
      "time_this_iter_s: 5.3187174797058105\n",
      "time_total_s: 2028.8907587528229\n",
      "timers:\n",
      "  learn_throughput: 2127.875\n",
      "  learn_time_ms: 1879.81\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.308\n",
      "  sample_time_ms: 3501.683\n",
      "  update_time_ms: 1.736\n",
      "timestamp: 1633513765\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1504000\n",
      "training_iteration: 376\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:376 starting ! -----------------\n",
      "agent_timesteps_total: 1508000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-49-43\n",
      "done: false\n",
      "episode_len_mean: 612.36\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.9050090000000014\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1092\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8218357563018799\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06461871415376663\n",
      "        model: {}\n",
      "        policy_loss: -0.05256514996290207\n",
      "        total_loss: -0.006919854320585728\n",
      "        vf_explained_var: 0.3258337080478668\n",
      "        vf_loss: 0.03272155672311783\n",
      "  num_agent_steps_sampled: 1508000\n",
      "  num_agent_steps_trained: 1508000\n",
      "  num_steps_sampled: 1508000\n",
      "  num_steps_trained: 1508000\n",
      "iterations_since_restore: 377\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.311999999999998\n",
      "  ram_util_percent: 44.632\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926604687356066\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495006226022005\n",
      "  mean_inference_ms: 0.8507923237368361\n",
      "  mean_raw_obs_processing_ms: 0.08850083477526428\n",
      "time_since_restore: 2034.235250711441\n",
      "time_this_iter_s: 5.344491958618164\n",
      "time_total_s: 2034.235250711441\n",
      "timers:\n",
      "  learn_throughput: 2130.028\n",
      "  learn_time_ms: 1877.909\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1147.456\n",
      "  sample_time_ms: 3485.973\n",
      "  update_time_ms: 1.736\n",
      "timestamp: 1633513783\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1508000\n",
      "training_iteration: 377\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:377 starting ! -----------------\n",
      "agent_timesteps_total: 1512000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-49-58\n",
      "done: false\n",
      "episode_len_mean: 598.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.0259650000000016\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1096\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8866394758224487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007434254512190819\n",
      "        model: {}\n",
      "        policy_loss: -0.019795235246419907\n",
      "        total_loss: 0.013667122460901737\n",
      "        vf_explained_var: 0.24849601089954376\n",
      "        vf_loss: 0.031232083216309547\n",
      "  num_agent_steps_sampled: 1512000\n",
      "  num_agent_steps_trained: 1512000\n",
      "  num_steps_sampled: 1512000\n",
      "  num_steps_trained: 1512000\n",
      "iterations_since_restore: 378\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.37727272727272\n",
      "  ram_util_percent: 44.62272727272728\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926903572614611\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495263756425036\n",
      "  mean_inference_ms: 0.8507850351868745\n",
      "  mean_raw_obs_processing_ms: 0.08850264762735151\n",
      "time_since_restore: 2039.608026266098\n",
      "time_this_iter_s: 5.372775554656982\n",
      "time_total_s: 2039.608026266098\n",
      "timers:\n",
      "  learn_throughput: 2128.836\n",
      "  learn_time_ms: 1878.961\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1146.117\n",
      "  sample_time_ms: 3490.046\n",
      "  update_time_ms: 1.584\n",
      "timestamp: 1633513798\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1512000\n",
      "training_iteration: 378\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:378 starting ! -----------------\n",
      "agent_timesteps_total: 1516000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-50-11\n",
      "done: false\n",
      "episode_len_mean: 595.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 0.9758295000000013\n",
      "episode_reward_min: -2.2079000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1099\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8698282241821289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013909904286265373\n",
      "        model: {}\n",
      "        policy_loss: -0.03672429546713829\n",
      "        total_loss: -0.016989897936582565\n",
      "        vf_explained_var: 0.4341621696949005\n",
      "        vf_loss: 0.01556143257766962\n",
      "  num_agent_steps_sampled: 1516000\n",
      "  num_agent_steps_trained: 1516000\n",
      "  num_steps_sampled: 1516000\n",
      "  num_steps_trained: 1516000\n",
      "iterations_since_restore: 379\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.977777777777778\n",
      "  ram_util_percent: 44.611111111111114\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049271281269523556\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495439212815063\n",
      "  mean_inference_ms: 0.8507800729268519\n",
      "  mean_raw_obs_processing_ms: 0.08850268913095731\n",
      "time_since_restore: 2045.1001346111298\n",
      "time_this_iter_s: 5.492108345031738\n",
      "time_total_s: 2045.1001346111298\n",
      "timers:\n",
      "  learn_throughput: 2135.103\n",
      "  learn_time_ms: 1873.446\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.278\n",
      "  sample_time_ms: 3501.776\n",
      "  update_time_ms: 1.598\n",
      "timestamp: 1633513811\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1516000\n",
      "training_iteration: 379\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:379 starting ! -----------------\n",
      "agent_timesteps_total: 1520000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-50-35\n",
      "done: false\n",
      "episode_len_mean: 595.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.1076325000000016\n",
      "episode_reward_min: -2.2014000000000005\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1102\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8288542032241821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008795845322310925\n",
      "        model: {}\n",
      "        policy_loss: -0.020413188263773918\n",
      "        total_loss: -0.005169407930225134\n",
      "        vf_explained_var: 0.36237332224845886\n",
      "        vf_loss: 0.01260502077639103\n",
      "  num_agent_steps_sampled: 1520000\n",
      "  num_agent_steps_trained: 1520000\n",
      "  num_steps_sampled: 1520000\n",
      "  num_steps_trained: 1520000\n",
      "iterations_since_restore: 380\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.611764705882354\n",
      "  ram_util_percent: 44.599999999999994\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492729351745566\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749552900858325\n",
      "  mean_inference_ms: 0.8507722985926429\n",
      "  mean_raw_obs_processing_ms: 0.08850156849134048\n",
      "time_since_restore: 2050.4706733226776\n",
      "time_this_iter_s: 5.370538711547852\n",
      "time_total_s: 2050.4706733226776\n",
      "timers:\n",
      "  learn_throughput: 2141.923\n",
      "  learn_time_ms: 1867.481\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.44\n",
      "  sample_time_ms: 3498.216\n",
      "  update_time_ms: 1.598\n",
      "timestamp: 1633513835\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1520000\n",
      "training_iteration: 380\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:380 starting ! -----------------\n",
      "agent_timesteps_total: 1524000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-50-50\n",
      "done: false\n",
      "episode_len_mean: 588.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.2179120000000017\n",
      "episode_reward_min: -2.2014000000000005\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1106\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.832496702671051\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007806989364326\n",
      "        model: {}\n",
      "        policy_loss: -0.020149407908320427\n",
      "        total_loss: -0.0009435725514777005\n",
      "        vf_explained_var: 0.25420957803726196\n",
      "        vf_loss: 0.016863739117980003\n",
      "  num_agent_steps_sampled: 1524000\n",
      "  num_agent_steps_trained: 1524000\n",
      "  num_steps_sampled: 1524000\n",
      "  num_steps_trained: 1524000\n",
      "iterations_since_restore: 381\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.059999999999995\n",
      "  ram_util_percent: 44.61\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927566826837761\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.749562855825005\n",
      "  mean_inference_ms: 0.8507637598584209\n",
      "  mean_raw_obs_processing_ms: 0.08849977587089451\n",
      "time_since_restore: 2055.9089410305023\n",
      "time_this_iter_s: 5.438267707824707\n",
      "time_total_s: 2055.9089410305023\n",
      "timers:\n",
      "  learn_throughput: 2138.298\n",
      "  learn_time_ms: 1870.647\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.439\n",
      "  sample_time_ms: 3507.421\n",
      "  update_time_ms: 1.598\n",
      "timestamp: 1633513850\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1524000\n",
      "training_iteration: 381\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:381 starting ! -----------------\n",
      "agent_timesteps_total: 1528000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-51-12\n",
      "done: false\n",
      "episode_len_mean: 585.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.2682985000000015\n",
      "episode_reward_min: -2.2014000000000005\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1110\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8577575087547302\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007919983938336372\n",
      "        model: {}\n",
      "        policy_loss: -0.020699020475149155\n",
      "        total_loss: 0.00381535105407238\n",
      "        vf_explained_var: 0.18244311213493347\n",
      "        vf_loss: 0.022138377651572227\n",
      "  num_agent_steps_sampled: 1528000\n",
      "  num_agent_steps_trained: 1528000\n",
      "  num_steps_sampled: 1528000\n",
      "  num_steps_trained: 1528000\n",
      "iterations_since_restore: 382\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.921875\n",
      "  ram_util_percent: 44.59375\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049279462164935985\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495821174347003\n",
      "  mean_inference_ms: 0.8507551770233198\n",
      "  mean_raw_obs_processing_ms: 0.0884986445543004\n",
      "time_since_restore: 2061.5217440128326\n",
      "time_this_iter_s: 5.612802982330322\n",
      "time_total_s: 2061.5217440128326\n",
      "timers:\n",
      "  learn_throughput: 2137.254\n",
      "  learn_time_ms: 1871.56\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.022\n",
      "  sample_time_ms: 3524.16\n",
      "  update_time_ms: 1.598\n",
      "timestamp: 1633513872\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1528000\n",
      "training_iteration: 382\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:382 starting ! -----------------\n",
      "agent_timesteps_total: 1532000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-51-28\n",
      "done: false\n",
      "episode_len_mean: 578.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.2798035000000016\n",
      "episode_reward_min: -2.1920500000000045\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1113\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9669312834739685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010969939641654491\n",
      "        model: {}\n",
      "        policy_loss: -0.03152403607964516\n",
      "        total_loss: -0.005707948934286833\n",
      "        vf_explained_var: 0.369392454624176\n",
      "        vf_loss: 0.022525113075971603\n",
      "  num_agent_steps_sampled: 1532000\n",
      "  num_agent_steps_trained: 1532000\n",
      "  num_steps_sampled: 1532000\n",
      "  num_steps_trained: 1532000\n",
      "iterations_since_restore: 383\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.55909090909091\n",
      "  ram_util_percent: 44.60000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928243330274569\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7495960509512504\n",
      "  mean_inference_ms: 0.8507487258740373\n",
      "  mean_raw_obs_processing_ms: 0.08849734756257986\n",
      "time_since_restore: 2066.8870055675507\n",
      "time_this_iter_s: 5.365261554718018\n",
      "time_total_s: 2066.8870055675507\n",
      "timers:\n",
      "  learn_throughput: 2137.081\n",
      "  learn_time_ms: 1871.712\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.882\n",
      "  sample_time_ms: 3524.596\n",
      "  update_time_ms: 1.496\n",
      "timestamp: 1633513888\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1532000\n",
      "training_iteration: 383\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:383 starting ! -----------------\n",
      "agent_timesteps_total: 1536000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-51-42\n",
      "done: false\n",
      "episode_len_mean: 590.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.2384960000000016\n",
      "episode_reward_min: -2.1920500000000045\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1116\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8512941002845764\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009074035100638866\n",
      "        model: {}\n",
      "        policy_loss: -0.029015136882662773\n",
      "        total_loss: -0.013222071342170238\n",
      "        vf_explained_var: 0.397088885307312\n",
      "        vf_loss: 0.013070848770439625\n",
      "  num_agent_steps_sampled: 1536000\n",
      "  num_agent_steps_trained: 1536000\n",
      "  num_steps_sampled: 1536000\n",
      "  num_steps_trained: 1536000\n",
      "iterations_since_restore: 384\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.935000000000002\n",
      "  ram_util_percent: 44.6\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928403652542889\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496115100870803\n",
      "  mean_inference_ms: 0.8507443319807486\n",
      "  mean_raw_obs_processing_ms: 0.08849581174458848\n",
      "time_since_restore: 2072.3283698558807\n",
      "time_this_iter_s: 5.441364288330078\n",
      "time_total_s: 2072.3283698558807\n",
      "timers:\n",
      "  learn_throughput: 2133.228\n",
      "  learn_time_ms: 1875.093\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.767\n",
      "  sample_time_ms: 3537.423\n",
      "  update_time_ms: 1.596\n",
      "timestamp: 1633513902\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1536000\n",
      "training_iteration: 384\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:384 starting ! -----------------\n",
      "agent_timesteps_total: 1540000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-51-58\n",
      "done: false\n",
      "episode_len_mean: 589.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.2882325000000017\n",
      "episode_reward_min: -2.1920500000000045\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1119\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8744099736213684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010917289182543755\n",
      "        model: {}\n",
      "        policy_loss: -0.025745129212737083\n",
      "        total_loss: -0.00788099505007267\n",
      "        vf_explained_var: 0.15934966504573822\n",
      "        vf_loss: 0.014588946476578712\n",
      "  num_agent_steps_sampled: 1540000\n",
      "  num_agent_steps_trained: 1540000\n",
      "  num_steps_sampled: 1540000\n",
      "  num_steps_trained: 1540000\n",
      "iterations_since_restore: 385\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.166666666666668\n",
      "  ram_util_percent: 44.5952380952381\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049285832008463704\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496289077306915\n",
      "  mean_inference_ms: 0.8507406326491501\n",
      "  mean_raw_obs_processing_ms: 0.08849436742656462\n",
      "time_since_restore: 2077.7919919490814\n",
      "time_this_iter_s: 5.463622093200684\n",
      "time_total_s: 2077.7919919490814\n",
      "timers:\n",
      "  learn_throughput: 2136.801\n",
      "  learn_time_ms: 1871.957\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.379\n",
      "  sample_time_ms: 3541.77\n",
      "  update_time_ms: 1.596\n",
      "timestamp: 1633513918\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1540000\n",
      "training_iteration: 385\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:385 starting ! -----------------\n",
      "agent_timesteps_total: 1544000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-52-11\n",
      "done: false\n",
      "episode_len_mean: 590.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.2280880000000014\n",
      "episode_reward_min: -2.1920500000000045\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1123\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9710769653320312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010801045224070549\n",
      "        model: {}\n",
      "        policy_loss: -0.024321451783180237\n",
      "        total_loss: -0.0031003537587821484\n",
      "        vf_explained_var: 0.527294397354126\n",
      "        vf_loss: 0.017980780452489853\n",
      "  num_agent_steps_sampled: 1544000\n",
      "  num_agent_steps_trained: 1544000\n",
      "  num_steps_sampled: 1544000\n",
      "  num_steps_trained: 1544000\n",
      "iterations_since_restore: 386\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.794736842105266\n",
      "  ram_util_percent: 44.61578947368421\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928779248411603\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496487348217409\n",
      "  mean_inference_ms: 0.8507374873629218\n",
      "  mean_raw_obs_processing_ms: 0.08849188302771234\n",
      "time_since_restore: 2083.1344525814056\n",
      "time_this_iter_s: 5.342460632324219\n",
      "time_total_s: 2083.1344525814056\n",
      "timers:\n",
      "  learn_throughput: 2134.582\n",
      "  learn_time_ms: 1873.903\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.218\n",
      "  sample_time_ms: 3542.274\n",
      "  update_time_ms: 1.596\n",
      "timestamp: 1633513931\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1544000\n",
      "training_iteration: 386\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:386 starting ! -----------------\n",
      "agent_timesteps_total: 1548000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-52-25\n",
      "done: false\n",
      "episode_len_mean: 587.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.2776780000000012\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1125\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9681649208068848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010359134525060654\n",
      "        model: {}\n",
      "        policy_loss: -0.028208231553435326\n",
      "        total_loss: -0.007467944640666246\n",
      "        vf_explained_var: 0.03287028893828392\n",
      "        vf_loss: 0.01763254776597023\n",
      "  num_agent_steps_sampled: 1548000\n",
      "  num_agent_steps_trained: 1548000\n",
      "  num_steps_sampled: 1548000\n",
      "  num_steps_trained: 1548000\n",
      "iterations_since_restore: 387\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.98\n",
      "  ram_util_percent: 44.585\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928851113638162\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496596591393646\n",
      "  mean_inference_ms: 0.8507365175342504\n",
      "  mean_raw_obs_processing_ms: 0.08849038438473927\n",
      "time_since_restore: 2088.5317866802216\n",
      "time_this_iter_s: 5.397334098815918\n",
      "time_total_s: 2088.5317866802216\n",
      "timers:\n",
      "  learn_throughput: 2132.634\n",
      "  learn_time_ms: 1875.615\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.866\n",
      "  sample_time_ms: 3546.522\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633513945\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1548000\n",
      "training_iteration: 387\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:387 starting ! -----------------\n",
      "agent_timesteps_total: 1552000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-52-38\n",
      "done: false\n",
      "episode_len_mean: 591.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9224500000000004\n",
      "episode_reward_mean: 1.3277125000000012\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1128\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8753026723861694\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009419849142432213\n",
      "        model: {}\n",
      "        policy_loss: -0.0338594950735569\n",
      "        total_loss: -0.020056139677762985\n",
      "        vf_explained_var: 0.25094544887542725\n",
      "        vf_loss: 0.010977393016219139\n",
      "  num_agent_steps_sampled: 1552000\n",
      "  num_agent_steps_trained: 1552000\n",
      "  num_steps_sampled: 1552000\n",
      "  num_steps_trained: 1552000\n",
      "iterations_since_restore: 388\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.144444444444446\n",
      "  ram_util_percent: 44.611111111111114\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049288686258146655\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496750065305499\n",
      "  mean_inference_ms: 0.8507341476228806\n",
      "  mean_raw_obs_processing_ms: 0.08848821367519764\n",
      "time_since_restore: 2093.8583357334137\n",
      "time_this_iter_s: 5.326549053192139\n",
      "time_total_s: 2093.8583357334137\n",
      "timers:\n",
      "  learn_throughput: 2131.484\n",
      "  learn_time_ms: 1876.627\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.666\n",
      "  sample_time_ms: 3540.87\n",
      "  update_time_ms: 1.695\n",
      "timestamp: 1633513958\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1552000\n",
      "training_iteration: 388\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:388 starting ! -----------------\n",
      "agent_timesteps_total: 1556000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-52-52\n",
      "done: false\n",
      "episode_len_mean: 596.41\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 1.2769770000000014\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1132\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8916648030281067\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008129213936626911\n",
      "        model: {}\n",
      "        policy_loss: -0.01864754781126976\n",
      "        total_loss: 0.005831149872392416\n",
      "        vf_explained_var: 0.3069888651371002\n",
      "        vf_loss: 0.022039934992790222\n",
      "  num_agent_steps_sampled: 1556000\n",
      "  num_agent_steps_trained: 1556000\n",
      "  num_steps_sampled: 1556000\n",
      "  num_steps_trained: 1556000\n",
      "iterations_since_restore: 389\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.068421052631578\n",
      "  ram_util_percent: 44.6\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928921281479887\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7496975630893928\n",
      "  mean_inference_ms: 0.8507322953077485\n",
      "  mean_raw_obs_processing_ms: 0.08848645842237964\n",
      "time_since_restore: 2099.3597609996796\n",
      "time_this_iter_s: 5.501425266265869\n",
      "time_total_s: 2099.3597609996796\n",
      "timers:\n",
      "  learn_throughput: 2133.392\n",
      "  learn_time_ms: 1874.948\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.832\n",
      "  sample_time_ms: 3543.487\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633513972\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1556000\n",
      "training_iteration: 389\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:389 starting ! -----------------\n",
      "agent_timesteps_total: 1560000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-53-16\n",
      "done: false\n",
      "episode_len_mean: 594.89\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 1.3372480000000015\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1135\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8536943793296814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012192175723612309\n",
      "        model: {}\n",
      "        policy_loss: -0.021640243008732796\n",
      "        total_loss: -0.005239239428192377\n",
      "        vf_explained_var: 0.21716678142547607\n",
      "        vf_loss: 0.012743351049721241\n",
      "  num_agent_steps_sampled: 1560000\n",
      "  num_agent_steps_trained: 1560000\n",
      "  num_steps_sampled: 1560000\n",
      "  num_steps_trained: 1560000\n",
      "iterations_since_restore: 390\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.902941176470588\n",
      "  ram_util_percent: 44.620588235294115\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928923552704003\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497139022221359\n",
      "  mean_inference_ms: 0.8507320808445853\n",
      "  mean_raw_obs_processing_ms: 0.08848369015488702\n",
      "time_since_restore: 2104.7513608932495\n",
      "time_this_iter_s: 5.391599893569946\n",
      "time_total_s: 2104.7513608932495\n",
      "timers:\n",
      "  learn_throughput: 2129.051\n",
      "  learn_time_ms: 1878.771\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.366\n",
      "  sample_time_ms: 3541.812\n",
      "  update_time_ms: 1.595\n",
      "timestamp: 1633513996\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1560000\n",
      "training_iteration: 390\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:390 starting ! -----------------\n",
      "agent_timesteps_total: 1564000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-53-35\n",
      "done: false\n",
      "episode_len_mean: 605.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 1.2362100000000016\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1137\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9840363264083862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007637825794517994\n",
      "        model: {}\n",
      "        policy_loss: -0.013117647729814053\n",
      "        total_loss: 0.0026864330284297466\n",
      "        vf_explained_var: 0.06338957697153091\n",
      "        vf_loss: 0.013512732461094856\n",
      "  num_agent_steps_sampled: 1564000\n",
      "  num_agent_steps_trained: 1564000\n",
      "  num_steps_sampled: 1564000\n",
      "  num_steps_trained: 1564000\n",
      "iterations_since_restore: 391\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.648148148148145\n",
      "  ram_util_percent: 44.62592592592591\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049289535029295645\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497266218358873\n",
      "  mean_inference_ms: 0.8507321869768956\n",
      "  mean_raw_obs_processing_ms: 0.08848176880062994\n",
      "time_since_restore: 2110.180687904358\n",
      "time_this_iter_s: 5.429327011108398\n",
      "time_total_s: 2110.180687904358\n",
      "timers:\n",
      "  learn_throughput: 2131.749\n",
      "  learn_time_ms: 1876.394\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.861\n",
      "  sample_time_ms: 3543.394\n",
      "  update_time_ms: 1.495\n",
      "timestamp: 1633514015\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1564000\n",
      "training_iteration: 391\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:391 starting ! -----------------\n",
      "agent_timesteps_total: 1568000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-53-48\n",
      "done: false\n",
      "episode_len_mean: 613.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 1.1859010000000014\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1139\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0394400358200073\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009448802098631859\n",
      "        model: {}\n",
      "        policy_loss: -0.018888253718614578\n",
      "        total_loss: -0.001906345714814961\n",
      "        vf_explained_var: 0.16511082649230957\n",
      "        vf_loss: 0.014147265814244747\n",
      "  num_agent_steps_sampled: 1568000\n",
      "  num_agent_steps_trained: 1568000\n",
      "  num_steps_sampled: 1568000\n",
      "  num_steps_trained: 1568000\n",
      "iterations_since_restore: 392\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.489473684210523\n",
      "  ram_util_percent: 44.647368421052626\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049289478805810256\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497397321748247\n",
      "  mean_inference_ms: 0.8507324914876009\n",
      "  mean_raw_obs_processing_ms: 0.08847973174335134\n",
      "time_since_restore: 2115.8683531284332\n",
      "time_this_iter_s: 5.687665224075317\n",
      "time_total_s: 2115.8683531284332\n",
      "timers:\n",
      "  learn_throughput: 2104.982\n",
      "  learn_time_ms: 1900.254\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.111\n",
      "  sample_time_ms: 3526.99\n",
      "  update_time_ms: 1.495\n",
      "timestamp: 1633514028\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1568000\n",
      "training_iteration: 392\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:392 starting ! -----------------\n",
      "agent_timesteps_total: 1572000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-54-07\n",
      "done: false\n",
      "episode_len_mean: 611.69\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 1.0748550000000014\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1143\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8892268538475037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012281093746423721\n",
      "        model: {}\n",
      "        policy_loss: -0.031724102795124054\n",
      "        total_loss: -0.016184749081730843\n",
      "        vf_explained_var: 0.19723619520664215\n",
      "        vf_loss: 0.011855022981762886\n",
      "  num_agent_steps_sampled: 1572000\n",
      "  num_agent_steps_trained: 1572000\n",
      "  num_steps_sampled: 1572000\n",
      "  num_steps_trained: 1572000\n",
      "iterations_since_restore: 393\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.39230769230769\n",
      "  ram_util_percent: 44.63461538461539\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049289096786727084\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497667587191146\n",
      "  mean_inference_ms: 0.8507336698796558\n",
      "  mean_raw_obs_processing_ms: 0.08847668309847347\n",
      "time_since_restore: 2121.2646629810333\n",
      "time_this_iter_s: 5.396309852600098\n",
      "time_total_s: 2121.2646629810333\n",
      "timers:\n",
      "  learn_throughput: 2103.29\n",
      "  learn_time_ms: 1901.783\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.589\n",
      "  sample_time_ms: 3528.617\n",
      "  update_time_ms: 1.496\n",
      "timestamp: 1633514047\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1572000\n",
      "training_iteration: 393\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:393 starting ! -----------------\n",
      "agent_timesteps_total: 1576000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-54-28\n",
      "done: false\n",
      "episode_len_mean: 612.69\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.9047775000000017\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1147\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9252302646636963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010757037438452244\n",
      "        model: {}\n",
      "        policy_loss: -0.028109736740589142\n",
      "        total_loss: -0.0013772359816357493\n",
      "        vf_explained_var: 0.311573326587677\n",
      "        vf_loss: 0.023505395278334618\n",
      "  num_agent_steps_sampled: 1576000\n",
      "  num_agent_steps_trained: 1576000\n",
      "  num_steps_sampled: 1576000\n",
      "  num_steps_trained: 1576000\n",
      "iterations_since_restore: 394\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.136666666666663\n",
      "  ram_util_percent: 44.62666666666665\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928880868075186\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7497932380386922\n",
      "  mean_inference_ms: 0.8507339646141342\n",
      "  mean_raw_obs_processing_ms: 0.08847241629198051\n",
      "time_since_restore: 2126.612347126007\n",
      "time_this_iter_s: 5.347684144973755\n",
      "time_total_s: 2126.612347126007\n",
      "timers:\n",
      "  learn_throughput: 2108.004\n",
      "  learn_time_ms: 1897.53\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.256\n",
      "  sample_time_ms: 3523.435\n",
      "  update_time_ms: 1.396\n",
      "timestamp: 1633514068\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1576000\n",
      "training_iteration: 394\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:394 starting ! -----------------\n",
      "agent_timesteps_total: 1580000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-54-52\n",
      "done: false\n",
      "episode_len_mean: 613.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.9148160000000018\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1150\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9667834639549255\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008738784119486809\n",
      "        model: {}\n",
      "        policy_loss: -0.030533380806446075\n",
      "        total_loss: -0.013883390463888645\n",
      "        vf_explained_var: 0.18631230294704437\n",
      "        vf_loss: 0.014028350822627544\n",
      "  num_agent_steps_sampled: 1580000\n",
      "  num_agent_steps_trained: 1580000\n",
      "  num_steps_sampled: 1580000\n",
      "  num_steps_trained: 1580000\n",
      "iterations_since_restore: 395\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.049999999999997\n",
      "  ram_util_percent: 44.63529411764706\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928884427140593\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498132494857491\n",
      "  mean_inference_ms: 0.8507369373917963\n",
      "  mean_raw_obs_processing_ms: 0.08846930083412242\n",
      "time_since_restore: 2132.2469103336334\n",
      "time_this_iter_s: 5.634563207626343\n",
      "time_total_s: 2132.2469103336334\n",
      "timers:\n",
      "  learn_throughput: 2089.48\n",
      "  learn_time_ms: 1914.352\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1135.187\n",
      "  sample_time_ms: 3523.647\n",
      "  update_time_ms: 1.574\n",
      "timestamp: 1633514092\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1580000\n",
      "training_iteration: 395\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:395 starting ! -----------------\n",
      "agent_timesteps_total: 1584000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-55-05\n",
      "done: false\n",
      "episode_len_mean: 621.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.8135145000000017\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1153\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9653265476226807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009935555048286915\n",
      "        model: {}\n",
      "        policy_loss: -0.0242098867893219\n",
      "        total_loss: 0.005471035838127136\n",
      "        vf_explained_var: -0.03928935527801514\n",
      "        vf_loss: 0.026700260117650032\n",
      "  num_agent_steps_sampled: 1584000\n",
      "  num_agent_steps_trained: 1584000\n",
      "  num_steps_sampled: 1584000\n",
      "  num_steps_trained: 1584000\n",
      "iterations_since_restore: 396\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.655555555555555\n",
      "  ram_util_percent: 44.67222222222223\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928844925752064\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498329016526939\n",
      "  mean_inference_ms: 0.8507398090166893\n",
      "  mean_raw_obs_processing_ms: 0.0884664859790669\n",
      "time_since_restore: 2137.576103448868\n",
      "time_this_iter_s: 5.329193115234375\n",
      "time_total_s: 2137.576103448868\n",
      "timers:\n",
      "  learn_throughput: 2089.781\n",
      "  learn_time_ms: 1914.076\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1135.564\n",
      "  sample_time_ms: 3522.479\n",
      "  update_time_ms: 1.674\n",
      "timestamp: 1633514105\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1584000\n",
      "training_iteration: 396\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:396 starting ! -----------------\n",
      "agent_timesteps_total: 1588000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-55-19\n",
      "done: false\n",
      "episode_len_mean: 626.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.773073500000002\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1155\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.968734860420227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011787695810198784\n",
      "        model: {}\n",
      "        policy_loss: -0.022734807804226875\n",
      "        total_loss: -0.0024931184016168118\n",
      "        vf_explained_var: 0.3934248089790344\n",
      "        vf_loss: 0.016705386340618134\n",
      "  num_agent_steps_sampled: 1588000\n",
      "  num_agent_steps_trained: 1588000\n",
      "  num_steps_sampled: 1588000\n",
      "  num_steps_trained: 1588000\n",
      "iterations_since_restore: 397\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.642105263157895\n",
      "  ram_util_percent: 44.65263157894737\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049288010498811055\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498422388216075\n",
      "  mean_inference_ms: 0.8507416297064774\n",
      "  mean_raw_obs_processing_ms: 0.0884645835880425\n",
      "time_since_restore: 2142.888154745102\n",
      "time_this_iter_s: 5.312051296234131\n",
      "time_total_s: 2142.888154745102\n",
      "timers:\n",
      "  learn_throughput: 2088.797\n",
      "  learn_time_ms: 1914.978\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1138.595\n",
      "  sample_time_ms: 3513.1\n",
      "  update_time_ms: 1.673\n",
      "timestamp: 1633514119\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1588000\n",
      "training_iteration: 397\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:397 starting ! -----------------\n",
      "agent_timesteps_total: 1592000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-55-43\n",
      "done: false\n",
      "episode_len_mean: 633.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.6629020000000022\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1158\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9904582500457764\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012109410017728806\n",
      "        model: {}\n",
      "        policy_loss: -0.030658746138215065\n",
      "        total_loss: -0.013523775152862072\n",
      "        vf_explained_var: 0.15091173350811005\n",
      "        vf_loss: 0.013502147980034351\n",
      "  num_agent_steps_sampled: 1592000\n",
      "  num_agent_steps_trained: 1592000\n",
      "  num_steps_sampled: 1592000\n",
      "  num_steps_trained: 1592000\n",
      "iterations_since_restore: 398\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.258823529411764\n",
      "  ram_util_percent: 44.64705882352941\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928763410705018\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498575972499588\n",
      "  mean_inference_ms: 0.8507444151636565\n",
      "  mean_raw_obs_processing_ms: 0.08846181745559185\n",
      "time_since_restore: 2148.3731620311737\n",
      "time_this_iter_s: 5.485007286071777\n",
      "time_total_s: 2148.3731620311737\n",
      "timers:\n",
      "  learn_throughput: 2089.094\n",
      "  learn_time_ms: 1914.705\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1133.453\n",
      "  sample_time_ms: 3529.038\n",
      "  update_time_ms: 1.773\n",
      "timestamp: 1633514143\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1592000\n",
      "training_iteration: 398\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:398 starting ! -----------------\n",
      "agent_timesteps_total: 1596000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-56-06\n",
      "done: false\n",
      "episode_len_mean: 644.21\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.5617995000000022\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1161\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.997032105922699\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009433398023247719\n",
      "        model: {}\n",
      "        policy_loss: -0.028693463653326035\n",
      "        total_loss: -0.012967977672815323\n",
      "        vf_explained_var: 0.10733658820390701\n",
      "        vf_loss: 0.01289547048509121\n",
      "  num_agent_steps_sampled: 1596000\n",
      "  num_agent_steps_trained: 1596000\n",
      "  num_steps_sampled: 1596000\n",
      "  num_steps_trained: 1596000\n",
      "iterations_since_restore: 399\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.190909090909088\n",
      "  ram_util_percent: 44.660606060606064\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928607911202683\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498683494867207\n",
      "  mean_inference_ms: 0.8507448252879213\n",
      "  mean_raw_obs_processing_ms: 0.08845820946649681\n",
      "time_since_restore: 2153.620257139206\n",
      "time_this_iter_s: 5.247095108032227\n",
      "time_total_s: 2153.620257139206\n",
      "timers:\n",
      "  learn_throughput: 2088.666\n",
      "  learn_time_ms: 1915.098\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1141.831\n",
      "  sample_time_ms: 3503.144\n",
      "  update_time_ms: 1.873\n",
      "timestamp: 1633514166\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1596000\n",
      "training_iteration: 399\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:399 starting ! -----------------\n",
      "agent_timesteps_total: 1600000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-56-31\n",
      "done: false\n",
      "episode_len_mean: 644.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.5112930000000022\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1164\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.974943220615387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007321098819375038\n",
      "        model: {}\n",
      "        policy_loss: -0.019360162317752838\n",
      "        total_loss: 0.0008810935541987419\n",
      "        vf_explained_var: 0.46539947390556335\n",
      "        vf_loss: 0.01804492622613907\n",
      "  num_agent_steps_sampled: 1600000\n",
      "  num_agent_steps_trained: 1600000\n",
      "  num_steps_sampled: 1600000\n",
      "  num_steps_trained: 1600000\n",
      "iterations_since_restore: 400\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.197142857142865\n",
      "  ram_util_percent: 44.65428571428572\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928395166150135\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498824246916507\n",
      "  mean_inference_ms: 0.8507443171024412\n",
      "  mean_raw_obs_processing_ms: 0.08845466055777558\n",
      "time_since_restore: 2159.035353422165\n",
      "time_this_iter_s: 5.415096282958984\n",
      "time_total_s: 2159.035353422165\n",
      "timers:\n",
      "  learn_throughput: 2093.622\n",
      "  learn_time_ms: 1910.564\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1139.538\n",
      "  sample_time_ms: 3510.193\n",
      "  update_time_ms: 1.872\n",
      "timestamp: 1633514191\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1600000\n",
      "training_iteration: 400\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:400 starting ! -----------------\n",
      "agent_timesteps_total: 1604000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-56-41\n",
      "done: false\n",
      "episode_len_mean: 638.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.6221980000000022\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1167\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9058849811553955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010776178911328316\n",
      "        model: {}\n",
      "        policy_loss: -0.02442409284412861\n",
      "        total_loss: -0.009676402434706688\n",
      "        vf_explained_var: 0.381454199552536\n",
      "        vf_loss: 0.011514837853610516\n",
      "  num_agent_steps_sampled: 1604000\n",
      "  num_agent_steps_trained: 1604000\n",
      "  num_steps_sampled: 1604000\n",
      "  num_steps_trained: 1604000\n",
      "iterations_since_restore: 401\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.278571428571432\n",
      "  ram_util_percent: 44.67142857142858\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928122449835248\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7498976874515942\n",
      "  mean_inference_ms: 0.8507422024421633\n",
      "  mean_raw_obs_processing_ms: 0.08845214154331689\n",
      "time_since_restore: 2164.396080970764\n",
      "time_this_iter_s: 5.360727548599243\n",
      "time_total_s: 2164.396080970764\n",
      "timers:\n",
      "  learn_throughput: 2091.877\n",
      "  learn_time_ms: 1912.158\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1142.292\n",
      "  sample_time_ms: 3501.733\n",
      "  update_time_ms: 1.972\n",
      "timestamp: 1633514201\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1604000\n",
      "training_iteration: 401\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:401 starting ! -----------------\n",
      "agent_timesteps_total: 1608000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-57-05\n",
      "done: false\n",
      "episode_len_mean: 637.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.6211075000000023\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1170\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9036218523979187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010990889742970467\n",
      "        model: {}\n",
      "        policy_loss: -0.030023610219359398\n",
      "        total_loss: -0.01469485741108656\n",
      "        vf_explained_var: 0.49731341004371643\n",
      "        vf_loss: 0.012031486257910728\n",
      "  num_agent_steps_sampled: 1608000\n",
      "  num_agent_steps_trained: 1608000\n",
      "  num_steps_sampled: 1608000\n",
      "  num_steps_trained: 1608000\n",
      "iterations_since_restore: 402\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.74848484848485\n",
      "  ram_util_percent: 44.66060606060606\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927852164201986\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499126206520349\n",
      "  mean_inference_ms: 0.8507427484519698\n",
      "  mean_raw_obs_processing_ms: 0.08844961874264382\n",
      "time_since_restore: 2169.8488829135895\n",
      "time_this_iter_s: 5.452801942825317\n",
      "time_total_s: 2169.8488829135895\n",
      "timers:\n",
      "  learn_throughput: 2124.754\n",
      "  learn_time_ms: 1882.571\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1140.303\n",
      "  sample_time_ms: 3507.841\n",
      "  update_time_ms: 1.972\n",
      "timestamp: 1633514225\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1608000\n",
      "training_iteration: 402\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:402 starting ! -----------------\n",
      "agent_timesteps_total: 1612000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-57-24\n",
      "done: false\n",
      "episode_len_mean: 635.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.631340000000002\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1174\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8826018571853638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0075696781277656555\n",
      "        model: {}\n",
      "        policy_loss: -0.020898794755339622\n",
      "        total_loss: 0.007861869409680367\n",
      "        vf_explained_var: 0.24991202354431152\n",
      "        vf_loss: 0.026489749550819397\n",
      "  num_agent_steps_sampled: 1612000\n",
      "  num_agent_steps_trained: 1612000\n",
      "  num_steps_sampled: 1612000\n",
      "  num_steps_trained: 1612000\n",
      "iterations_since_restore: 403\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.557692307692303\n",
      "  ram_util_percent: 44.69230769230769\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492749930753422\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499321612785468\n",
      "  mean_inference_ms: 0.8507447362605117\n",
      "  mean_raw_obs_processing_ms: 0.0884457306585874\n",
      "time_since_restore: 2175.1847121715546\n",
      "time_this_iter_s: 5.335829257965088\n",
      "time_total_s: 2175.1847121715546\n",
      "timers:\n",
      "  learn_throughput: 2125.661\n",
      "  learn_time_ms: 1881.768\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1142.059\n",
      "  sample_time_ms: 3502.445\n",
      "  update_time_ms: 2.073\n",
      "timestamp: 1633514244\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1612000\n",
      "training_iteration: 403\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:403 starting ! -----------------\n",
      "agent_timesteps_total: 1616000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-57-38\n",
      "done: false\n",
      "episode_len_mean: 640.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.5706585000000021\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1177\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8716878294944763\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010110897943377495\n",
      "        model: {}\n",
      "        policy_loss: -0.02341875620186329\n",
      "        total_loss: -0.006125614047050476\n",
      "        vf_explained_var: 0.051214344799518585\n",
      "        vf_loss: 0.01425987295806408\n",
      "  num_agent_steps_sampled: 1616000\n",
      "  num_agent_steps_trained: 1616000\n",
      "  num_steps_sampled: 1616000\n",
      "  num_steps_trained: 1616000\n",
      "iterations_since_restore: 404\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.990476190476194\n",
      "  ram_util_percent: 44.67142857142859\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492725889742414\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499435768105371\n",
      "  mean_inference_ms: 0.8507472489572144\n",
      "  mean_raw_obs_processing_ms: 0.08844340225798632\n",
      "time_since_restore: 2180.524743080139\n",
      "time_this_iter_s: 5.340030908584595\n",
      "time_total_s: 2180.524743080139\n",
      "timers:\n",
      "  learn_throughput: 2123.427\n",
      "  learn_time_ms: 1883.748\n",
      "  load_throughput: 40117685.318\n",
      "  load_time_ms: 0.1\n",
      "  sample_throughput: 1142.908\n",
      "  sample_time_ms: 3499.845\n",
      "  update_time_ms: 2.173\n",
      "timestamp: 1633514258\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1616000\n",
      "training_iteration: 404\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:404 starting ! -----------------\n",
      "agent_timesteps_total: 1620000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-57-52\n",
      "done: false\n",
      "episode_len_mean: 644.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.5199115000000022\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1179\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.965989887714386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010366959497332573\n",
      "        model: {}\n",
      "        policy_loss: -0.025623589754104614\n",
      "        total_loss: -0.00692386319860816\n",
      "        vf_explained_var: 0.007450079079717398\n",
      "        vf_loss: 0.015589635819196701\n",
      "  num_agent_steps_sampled: 1620000\n",
      "  num_agent_steps_trained: 1620000\n",
      "  num_steps_sampled: 1620000\n",
      "  num_steps_trained: 1620000\n",
      "iterations_since_restore: 405\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.525\n",
      "  ram_util_percent: 44.69000000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927122678516982\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499553443372479\n",
      "  mean_inference_ms: 0.8507504937650043\n",
      "  mean_raw_obs_processing_ms: 0.08844149904584184\n",
      "time_since_restore: 2186.152806043625\n",
      "time_this_iter_s: 5.628062963485718\n",
      "time_total_s: 2186.152806043625\n",
      "timers:\n",
      "  learn_throughput: 2138.778\n",
      "  learn_time_ms: 1870.227\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.655\n",
      "  sample_time_ms: 3512.915\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633514272\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1620000\n",
      "training_iteration: 405\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:405 starting ! -----------------\n",
      "agent_timesteps_total: 1624000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-58-07\n",
      "done: false\n",
      "episode_len_mean: 646.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.5195620000000022\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1182\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1304736137390137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015001663938164711\n",
      "        model: {}\n",
      "        policy_loss: -0.029406515881419182\n",
      "        total_loss: -0.010379455983638763\n",
      "        vf_explained_var: -0.1264178454875946\n",
      "        vf_loss: 0.014526561833918095\n",
      "  num_agent_steps_sampled: 1624000\n",
      "  num_agent_steps_trained: 1624000\n",
      "  num_steps_sampled: 1624000\n",
      "  num_steps_trained: 1624000\n",
      "iterations_since_restore: 406\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.857142857142858\n",
      "  ram_util_percent: 44.67619047619049\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926907829751192\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499707611841623\n",
      "  mean_inference_ms: 0.8507569282850553\n",
      "  mean_raw_obs_processing_ms: 0.08843878124075412\n",
      "time_since_restore: 2191.477324485779\n",
      "time_this_iter_s: 5.324518442153931\n",
      "time_total_s: 2191.477324485779\n",
      "timers:\n",
      "  learn_throughput: 2135.071\n",
      "  learn_time_ms: 1873.474\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.894\n",
      "  sample_time_ms: 3509.098\n",
      "  update_time_ms: 1.976\n",
      "timestamp: 1633514287\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1624000\n",
      "training_iteration: 406\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:406 starting ! -----------------\n",
      "agent_timesteps_total: 1628000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-58-24\n",
      "done: false\n",
      "episode_len_mean: 640.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.520202500000002\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1185\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8875280022621155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011845490895211697\n",
      "        model: {}\n",
      "        policy_loss: -0.02836856245994568\n",
      "        total_loss: -0.014605949632823467\n",
      "        vf_explained_var: 0.41020384430885315\n",
      "        vf_loss: 0.010208970867097378\n",
      "  num_agent_steps_sampled: 1628000\n",
      "  num_agent_steps_trained: 1628000\n",
      "  num_steps_sampled: 1628000\n",
      "  num_steps_trained: 1628000\n",
      "iterations_since_restore: 407\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.004347826086956\n",
      "  ram_util_percent: 44.68695652173914\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049267513957536445\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499870393545649\n",
      "  mean_inference_ms: 0.8507649919899872\n",
      "  mean_raw_obs_processing_ms: 0.08843563274448453\n",
      "time_since_restore: 2196.9476256370544\n",
      "time_this_iter_s: 5.470301151275635\n",
      "time_total_s: 2196.9476256370544\n",
      "timers:\n",
      "  learn_throughput: 2135.418\n",
      "  learn_time_ms: 1873.169\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.632\n",
      "  sample_time_ms: 3525.371\n",
      "  update_time_ms: 1.916\n",
      "timestamp: 1633514304\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1628000\n",
      "training_iteration: 407\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:407 starting ! -----------------\n",
      "agent_timesteps_total: 1632000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-58-40\n",
      "done: false\n",
      "episode_len_mean: 650.59\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.41938550000000224\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1187\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.92064368724823\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011379504576325417\n",
      "        model: {}\n",
      "        policy_loss: -0.023017140105366707\n",
      "        total_loss: -0.0016078456537798047\n",
      "        vf_explained_var: -0.01668846420943737\n",
      "        vf_loss: 0.017995445057749748\n",
      "  num_agent_steps_sampled: 1632000\n",
      "  num_agent_steps_trained: 1632000\n",
      "  num_steps_sampled: 1632000\n",
      "  num_steps_trained: 1632000\n",
      "iterations_since_restore: 408\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.082608695652176\n",
      "  ram_util_percent: 44.669565217391316\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926656795505629\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7499990912661179\n",
      "  mean_inference_ms: 0.8507704303177327\n",
      "  mean_raw_obs_processing_ms: 0.08843401920577731\n",
      "time_since_restore: 2202.3843071460724\n",
      "time_this_iter_s: 5.436681509017944\n",
      "time_total_s: 2202.3843071460724\n",
      "timers:\n",
      "  learn_throughput: 2135.219\n",
      "  learn_time_ms: 1873.344\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.183\n",
      "  sample_time_ms: 3520.561\n",
      "  update_time_ms: 1.816\n",
      "timestamp: 1633514320\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1632000\n",
      "training_iteration: 408\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:408 starting ! -----------------\n",
      "agent_timesteps_total: 1636000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-58-54\n",
      "done: false\n",
      "episode_len_mean: 643.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.3580440000000024\n",
      "episode_reward_min: -2.203549999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1190\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9739692807197571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012048108503222466\n",
      "        model: {}\n",
      "        policy_loss: -0.0240822434425354\n",
      "        total_loss: -0.006813297979533672\n",
      "        vf_explained_var: -0.05135094001889229\n",
      "        vf_loss: 0.013654516078531742\n",
      "  num_agent_steps_sampled: 1636000\n",
      "  num_agent_steps_trained: 1636000\n",
      "  num_steps_sampled: 1636000\n",
      "  num_steps_trained: 1636000\n",
      "iterations_since_restore: 409\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.860000000000003\n",
      "  ram_util_percent: 44.720000000000006\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049265497773419736\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7500212401844252\n",
      "  mean_inference_ms: 0.8507792608805319\n",
      "  mean_raw_obs_processing_ms: 0.08843213003661415\n",
      "time_since_restore: 2207.8843138217926\n",
      "time_this_iter_s: 5.500006675720215\n",
      "time_total_s: 2207.8843138217926\n",
      "timers:\n",
      "  learn_throughput: 2136.603\n",
      "  learn_time_ms: 1872.131\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.661\n",
      "  sample_time_ms: 3547.164\n",
      "  update_time_ms: 1.816\n",
      "timestamp: 1633514334\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1636000\n",
      "training_iteration: 409\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:409 starting ! -----------------\n",
      "agent_timesteps_total: 1640000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-59-18\n",
      "done: false\n",
      "episode_len_mean: 643.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.33814750000000227\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 1195\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9403972029685974\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009319829754531384\n",
      "        model: {}\n",
      "        policy_loss: -0.023943593725562096\n",
      "        total_loss: 0.0038427640683948994\n",
      "        vf_explained_var: 0.0111668286845088\n",
      "        vf_loss: 0.02499040775001049\n",
      "  num_agent_steps_sampled: 1640000\n",
      "  num_agent_steps_trained: 1640000\n",
      "  num_steps_sampled: 1640000\n",
      "  num_steps_trained: 1640000\n",
      "iterations_since_restore: 410\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.654545454545456\n",
      "  ram_util_percent: 44.66969696969697\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926335694498394\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.750057809649851\n",
      "  mean_inference_ms: 0.8507945230121959\n",
      "  mean_raw_obs_processing_ms: 0.08842898611477118\n",
      "time_since_restore: 2213.242258787155\n",
      "time_this_iter_s: 5.357944965362549\n",
      "time_total_s: 2213.242258787155\n",
      "timers:\n",
      "  learn_throughput: 2138.686\n",
      "  learn_time_ms: 1870.307\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.937\n",
      "  sample_time_ms: 3543.155\n",
      "  update_time_ms: 1.816\n",
      "timestamp: 1633514358\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1640000\n",
      "training_iteration: 410\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:410 starting ! -----------------\n",
      "agent_timesteps_total: 1644000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_18-59-39\n",
      "done: false\n",
      "episode_len_mean: 648.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.33826350000000216\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1197\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9855207800865173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009100137278437614\n",
      "        model: {}\n",
      "        policy_loss: -0.024412840604782104\n",
      "        total_loss: -0.005309777799993753\n",
      "        vf_explained_var: 0.09420622885227203\n",
      "        vf_loss: 0.01637302152812481\n",
      "  num_agent_steps_sampled: 1644000\n",
      "  num_agent_steps_trained: 1644000\n",
      "  num_steps_sampled: 1644000\n",
      "  num_steps_trained: 1644000\n",
      "iterations_since_restore: 411\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.82413793103449\n",
      "  ram_util_percent: 44.67586206896553\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926203262842124\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7500695412482965\n",
      "  mean_inference_ms: 0.8507999834382665\n",
      "  mean_raw_obs_processing_ms: 0.08842812518782966\n",
      "time_since_restore: 2218.5677976608276\n",
      "time_this_iter_s: 5.325538873672485\n",
      "time_total_s: 2218.5677976608276\n",
      "timers:\n",
      "  learn_throughput: 2135.308\n",
      "  learn_time_ms: 1873.266\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.992\n",
      "  sample_time_ms: 3536.719\n",
      "  update_time_ms: 1.816\n",
      "timestamp: 1633514379\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1644000\n",
      "training_iteration: 411\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:411 starting ! -----------------\n",
      "agent_timesteps_total: 1648000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-00-00\n",
      "done: false\n",
      "episode_len_mean: 650.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.3382255000000023\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1200\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9454300999641418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010698794387280941\n",
      "        model: {}\n",
      "        policy_loss: -0.020951086655259132\n",
      "        total_loss: -0.006021382752805948\n",
      "        vf_explained_var: 0.12756526470184326\n",
      "        vf_loss: 0.011720067821443081\n",
      "  num_agent_steps_sampled: 1648000\n",
      "  num_agent_steps_trained: 1648000\n",
      "  num_steps_sampled: 1648000\n",
      "  num_steps_trained: 1648000\n",
      "iterations_since_restore: 412\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.10322580645162\n",
      "  ram_util_percent: 44.68064516129033\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925981829310754\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7500870835097321\n",
      "  mean_inference_ms: 0.8508073264491365\n",
      "  mean_raw_obs_processing_ms: 0.08842762759677993\n",
      "time_since_restore: 2224.059626340866\n",
      "time_this_iter_s: 5.491828680038452\n",
      "time_total_s: 2224.059626340866\n",
      "timers:\n",
      "  learn_throughput: 2126.405\n",
      "  learn_time_ms: 1881.109\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.242\n",
      "  sample_time_ms: 3532.813\n",
      "  update_time_ms: 1.816\n",
      "timestamp: 1633514400\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1648000\n",
      "training_iteration: 412\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:412 starting ! -----------------\n",
      "agent_timesteps_total: 1652000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-00-25\n",
      "done: false\n",
      "episode_len_mean: 653.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.2879870000000022\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1204\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0386171340942383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012206071056425571\n",
      "        model: {}\n",
      "        policy_loss: -0.029493480920791626\n",
      "        total_loss: 0.013742263428866863\n",
      "        vf_explained_var: -0.06799374520778656\n",
      "        vf_loss: 0.03957391530275345\n",
      "  num_agent_steps_sampled: 1652000\n",
      "  num_agent_steps_trained: 1652000\n",
      "  num_steps_sampled: 1652000\n",
      "  num_steps_trained: 1652000\n",
      "iterations_since_restore: 413\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.426470588235293\n",
      "  ram_util_percent: 44.67352941176471\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925613308613311\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7501140252515728\n",
      "  mean_inference_ms: 0.8508148262852993\n",
      "  mean_raw_obs_processing_ms: 0.08842773927815471\n",
      "time_since_restore: 2229.4655044078827\n",
      "time_this_iter_s: 5.405878067016602\n",
      "time_total_s: 2229.4655044078827\n",
      "timers:\n",
      "  learn_throughput: 2125.982\n",
      "  learn_time_ms: 1881.484\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.08\n",
      "  sample_time_ms: 3539.572\n",
      "  update_time_ms: 1.816\n",
      "timestamp: 1633514425\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1652000\n",
      "training_iteration: 413\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:413 starting ! -----------------\n",
      "agent_timesteps_total: 1656000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-00-42\n",
      "done: false\n",
      "episode_len_mean: 658.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.22754050000000212\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1206\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9094643592834473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010359980165958405\n",
      "        model: {}\n",
      "        policy_loss: -0.028672652319073677\n",
      "        total_loss: -0.007440078072249889\n",
      "        vf_explained_var: -0.13578392565250397\n",
      "        vf_loss: 0.018124587833881378\n",
      "  num_agent_steps_sampled: 1656000\n",
      "  num_agent_steps_trained: 1656000\n",
      "  num_steps_sampled: 1656000\n",
      "  num_steps_trained: 1656000\n",
      "iterations_since_restore: 414\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.82\n",
      "  ram_util_percent: 44.61600000000001\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925438097559875\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7501304663194504\n",
      "  mean_inference_ms: 0.8508168758835819\n",
      "  mean_raw_obs_processing_ms: 0.08842793011472488\n",
      "time_since_restore: 2234.9413843154907\n",
      "time_this_iter_s: 5.475879907608032\n",
      "time_total_s: 2234.9413843154907\n",
      "timers:\n",
      "  learn_throughput: 2125.129\n",
      "  learn_time_ms: 1882.238\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.033\n",
      "  sample_time_ms: 3552.292\n",
      "  update_time_ms: 1.815\n",
      "timestamp: 1633514442\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1656000\n",
      "training_iteration: 414\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:414 starting ! -----------------\n",
      "agent_timesteps_total: 1660000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-00-56\n",
      "done: false\n",
      "episode_len_mean: 666.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.18750650000000232\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1208\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.96939617395401\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010327047668397427\n",
      "        model: {}\n",
      "        policy_loss: -0.02527814358472824\n",
      "        total_loss: -0.014857593923807144\n",
      "        vf_explained_var: 0.25897061824798584\n",
      "        vf_loss: 0.007322434335947037\n",
      "  num_agent_steps_sampled: 1660000\n",
      "  num_agent_steps_trained: 1660000\n",
      "  num_steps_sampled: 1660000\n",
      "  num_steps_trained: 1660000\n",
      "iterations_since_restore: 415\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.957894736842107\n",
      "  ram_util_percent: 44.631578947368425\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925200265153028\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7501424853356616\n",
      "  mean_inference_ms: 0.8508190492627932\n",
      "  mean_raw_obs_processing_ms: 0.08842767383387173\n",
      "time_since_restore: 2240.2789442539215\n",
      "time_this_iter_s: 5.337559938430786\n",
      "time_total_s: 2240.2789442539215\n",
      "timers:\n",
      "  learn_throughput: 2129.147\n",
      "  learn_time_ms: 1878.687\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.167\n",
      "  sample_time_ms: 3526.818\n",
      "  update_time_ms: 1.914\n",
      "timestamp: 1633514456\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1660000\n",
      "training_iteration: 415\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:415 starting ! -----------------\n",
      "agent_timesteps_total: 1664000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-01-18\n",
      "done: false\n",
      "episode_len_mean: 672.39\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.1365905000000024\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1211\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0238502025604248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010953929275274277\n",
      "        model: {}\n",
      "        policy_loss: -0.03417437523603439\n",
      "        total_loss: -0.01790071278810501\n",
      "        vf_explained_var: 0.19095350801944733\n",
      "        vf_loss: 0.01298748143017292\n",
      "  num_agent_steps_sampled: 1664000\n",
      "  num_agent_steps_trained: 1664000\n",
      "  num_steps_sampled: 1664000\n",
      "  num_steps_trained: 1664000\n",
      "iterations_since_restore: 416\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.390322580645154\n",
      "  ram_util_percent: 44.61612903225805\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924844158263438\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7501608988566649\n",
      "  mean_inference_ms: 0.8508234531547031\n",
      "  mean_raw_obs_processing_ms: 0.08842771634267083\n",
      "time_since_restore: 2245.704761505127\n",
      "time_this_iter_s: 5.425817251205444\n",
      "time_total_s: 2245.704761505127\n",
      "timers:\n",
      "  learn_throughput: 2133.206\n",
      "  learn_time_ms: 1875.112\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.708\n",
      "  sample_time_ms: 3540.737\n",
      "  update_time_ms: 1.933\n",
      "timestamp: 1633514478\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1664000\n",
      "training_iteration: 416\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:416 starting ! -----------------\n",
      "agent_timesteps_total: 1668000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-01-42\n",
      "done: false\n",
      "episode_len_mean: 665.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.2274475000000022\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1215\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9113456606864929\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009488457813858986\n",
      "        model: {}\n",
      "        policy_loss: -0.024952556937932968\n",
      "        total_loss: -0.010206745937466621\n",
      "        vf_explained_var: 0.07926604896783829\n",
      "        vf_loss: 0.011899271048605442\n",
      "  num_agent_steps_sampled: 1668000\n",
      "  num_agent_steps_trained: 1668000\n",
      "  num_steps_sampled: 1668000\n",
      "  num_steps_trained: 1668000\n",
      "iterations_since_restore: 417\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.838235294117645\n",
      "  ram_util_percent: 44.60588235294117\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049243723706950446\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7501837436219367\n",
      "  mean_inference_ms: 0.8508289322836334\n",
      "  mean_raw_obs_processing_ms: 0.08842834010186813\n",
      "time_since_restore: 2251.0796291828156\n",
      "time_this_iter_s: 5.374867677688599\n",
      "time_total_s: 2251.0796291828156\n",
      "timers:\n",
      "  learn_throughput: 2131.925\n",
      "  learn_time_ms: 1876.239\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.167\n",
      "  sample_time_ms: 3529.929\n",
      "  update_time_ms: 1.994\n",
      "timestamp: 1633514502\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1668000\n",
      "training_iteration: 417\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:417 starting ! -----------------\n",
      "agent_timesteps_total: 1672000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-02-02\n",
      "done: false\n",
      "episode_len_mean: 670.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.17677300000000226\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1218\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9259944558143616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009731545113027096\n",
      "        model: {}\n",
      "        policy_loss: -0.02516116388142109\n",
      "        total_loss: 0.025577368214726448\n",
      "        vf_explained_var: -0.21406307816505432\n",
      "        vf_loss: 0.04781906679272652\n",
      "  num_agent_steps_sampled: 1672000\n",
      "  num_agent_steps_trained: 1672000\n",
      "  num_steps_sampled: 1672000\n",
      "  num_steps_trained: 1672000\n",
      "iterations_since_restore: 418\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.96296296296296\n",
      "  ram_util_percent: 44.61851851851851\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923990537418077\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7501982607139339\n",
      "  mean_inference_ms: 0.8508307892995003\n",
      "  mean_raw_obs_processing_ms: 0.08842944622268802\n",
      "time_since_restore: 2256.407397031784\n",
      "time_this_iter_s: 5.327767848968506\n",
      "time_total_s: 2256.407397031784\n",
      "timers:\n",
      "  learn_throughput: 2132.703\n",
      "  learn_time_ms: 1875.555\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.469\n",
      "  sample_time_ms: 3519.673\n",
      "  update_time_ms: 1.894\n",
      "timestamp: 1633514522\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1672000\n",
      "training_iteration: 418\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:418 starting ! -----------------\n",
      "agent_timesteps_total: 1676000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-02-15\n",
      "done: false\n",
      "episode_len_mean: 669.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.1970165000000024\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1221\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9317131638526917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014600292779505253\n",
      "        model: {}\n",
      "        policy_loss: -0.029172241687774658\n",
      "        total_loss: -0.012585214339196682\n",
      "        vf_explained_var: 0.26952576637268066\n",
      "        vf_loss: 0.012206939980387688\n",
      "  num_agent_steps_sampled: 1676000\n",
      "  num_agent_steps_trained: 1676000\n",
      "  num_steps_sampled: 1676000\n",
      "  num_steps_trained: 1676000\n",
      "iterations_since_restore: 419\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.25789473684211\n",
      "  ram_util_percent: 44.58947368421053\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049235609692983594\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7502103055166631\n",
      "  mean_inference_ms: 0.8508318226208842\n",
      "  mean_raw_obs_processing_ms: 0.08843165160526717\n",
      "time_since_restore: 2261.7268023490906\n",
      "time_this_iter_s: 5.3194053173065186\n",
      "time_total_s: 2261.7268023490906\n",
      "timers:\n",
      "  learn_throughput: 2131.497\n",
      "  learn_time_ms: 1876.615\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.673\n",
      "  sample_time_ms: 3500.565\n",
      "  update_time_ms: 1.894\n",
      "timestamp: 1633514535\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1676000\n",
      "training_iteration: 419\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:419 starting ! -----------------\n",
      "agent_timesteps_total: 1680000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-02-33\n",
      "done: false\n",
      "episode_len_mean: 677.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.19706750000000237\n",
      "episode_reward_min: -2.207100000000003\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1223\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0277581214904785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012856323271989822\n",
      "        model: {}\n",
      "        policy_loss: -0.026068098843097687\n",
      "        total_loss: -0.015879912301898003\n",
      "        vf_explained_var: 0.41282254457473755\n",
      "        vf_loss: 0.00633128872141242\n",
      "  num_agent_steps_sampled: 1680000\n",
      "  num_agent_steps_trained: 1680000\n",
      "  num_steps_sampled: 1680000\n",
      "  num_steps_trained: 1680000\n",
      "iterations_since_restore: 420\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.25769230769231\n",
      "  ram_util_percent: 44.61923076923077\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923235803995375\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7502177454293062\n",
      "  mean_inference_ms: 0.8508322184597064\n",
      "  mean_raw_obs_processing_ms: 0.08843322623752695\n",
      "time_since_restore: 2267.0570347309113\n",
      "time_this_iter_s: 5.330232381820679\n",
      "time_total_s: 2267.0570347309113\n",
      "timers:\n",
      "  learn_throughput: 2129.245\n",
      "  learn_time_ms: 1878.6\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.205\n",
      "  sample_time_ms: 3495.876\n",
      "  update_time_ms: 1.894\n",
      "timestamp: 1633514553\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1680000\n",
      "training_iteration: 420\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:420 starting ! -----------------\n",
      "agent_timesteps_total: 1684000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-02-52\n",
      "done: false\n",
      "episode_len_mean: 671.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.19726850000000237\n",
      "episode_reward_min: -2.2245500000000025\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1227\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9399458765983582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009832200594246387\n",
      "        model: {}\n",
      "        policy_loss: -0.022859634831547737\n",
      "        total_loss: -0.001544756582006812\n",
      "        vf_explained_var: 0.28958091139793396\n",
      "        vf_loss: 0.01836521551012993\n",
      "  num_agent_steps_sampled: 1684000\n",
      "  num_agent_steps_trained: 1684000\n",
      "  num_steps_sampled: 1684000\n",
      "  num_steps_trained: 1684000\n",
      "iterations_since_restore: 421\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.11111111111111\n",
      "  ram_util_percent: 44.65185185185185\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922731589631739\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7502356459429075\n",
      "  mean_inference_ms: 0.8508336655862118\n",
      "  mean_raw_obs_processing_ms: 0.08843713023647563\n",
      "time_since_restore: 2272.6603050231934\n",
      "time_this_iter_s: 5.6032702922821045\n",
      "time_total_s: 2272.6603050231934\n",
      "timers:\n",
      "  learn_throughput: 2123.025\n",
      "  learn_time_ms: 1884.104\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.992\n",
      "  sample_time_ms: 3518.056\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633514572\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1684000\n",
      "training_iteration: 421\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:421 starting ! -----------------\n",
      "agent_timesteps_total: 1688000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-03-15\n",
      "done: false\n",
      "episode_len_mean: 670.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9433000000000007\n",
      "episode_reward_mean: 0.18704300000000226\n",
      "episode_reward_min: -2.2245500000000025\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1229\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0322151184082031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011469601653516293\n",
      "        model: {}\n",
      "        policy_loss: -0.023332864046096802\n",
      "        total_loss: -0.013268979266285896\n",
      "        vf_explained_var: 0.20836378633975983\n",
      "        vf_loss: 0.00662300456315279\n",
      "  num_agent_steps_sampled: 1688000\n",
      "  num_agent_steps_trained: 1688000\n",
      "  num_steps_sampled: 1688000\n",
      "  num_steps_trained: 1688000\n",
      "iterations_since_restore: 422\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.287096774193547\n",
      "  ram_util_percent: 44.47096774193549\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492251995027347\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7502486983494697\n",
      "  mean_inference_ms: 0.8508336740383271\n",
      "  mean_raw_obs_processing_ms: 0.08843831795698467\n",
      "time_since_restore: 2278.2042319774628\n",
      "time_this_iter_s: 5.543926954269409\n",
      "time_total_s: 2278.2042319774628\n",
      "timers:\n",
      "  learn_throughput: 2131.89\n",
      "  learn_time_ms: 1876.27\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.766\n",
      "  sample_time_ms: 3531.179\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633514595\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1688000\n",
      "training_iteration: 422\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:422 starting ! -----------------\n",
      "agent_timesteps_total: 1692000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-03-31\n",
      "done: false\n",
      "episode_len_mean: 677.23\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.12567150000000224\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1232\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9059665203094482\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010246186517179012\n",
      "        model: {}\n",
      "        policy_loss: -0.04107002168893814\n",
      "        total_loss: -0.03087112307548523\n",
      "        vf_explained_var: 0.25451067090034485\n",
      "        vf_loss: 0.007125043775886297\n",
      "  num_agent_steps_sampled: 1692000\n",
      "  num_agent_steps_trained: 1692000\n",
      "  num_steps_sampled: 1692000\n",
      "  num_steps_trained: 1692000\n",
      "iterations_since_restore: 423\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.8\n",
      "  ram_util_percent: 44.209090909090925\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922185471654163\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7502695438971082\n",
      "  mean_inference_ms: 0.8508324559444371\n",
      "  mean_raw_obs_processing_ms: 0.08844001886974079\n",
      "time_since_restore: 2283.6459698677063\n",
      "time_this_iter_s: 5.44173789024353\n",
      "time_total_s: 2283.6459698677063\n",
      "timers:\n",
      "  learn_throughput: 2132.133\n",
      "  learn_time_ms: 1876.055\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.57\n",
      "  sample_time_ms: 3534.913\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633514611\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1692000\n",
      "training_iteration: 423\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:423 starting ! -----------------\n",
      "agent_timesteps_total: 1696000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-03-55\n",
      "done: false\n",
      "episode_len_mean: 673.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.06519700000000231\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1236\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9840812683105469\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008051363751292229\n",
      "        model: {}\n",
      "        policy_loss: -0.01871965080499649\n",
      "        total_loss: -0.0014424113323912024\n",
      "        vf_explained_var: 0.25308579206466675\n",
      "        vf_loss: 0.014861827716231346\n",
      "  num_agent_steps_sampled: 1696000\n",
      "  num_agent_steps_trained: 1696000\n",
      "  num_steps_sampled: 1696000\n",
      "  num_steps_trained: 1696000\n",
      "iterations_since_restore: 424\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.02285714285714\n",
      "  ram_util_percent: 44.17428571428571\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049218206099540175\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.750301047552552\n",
      "  mean_inference_ms: 0.8508275075253171\n",
      "  mean_raw_obs_processing_ms: 0.08844500287341944\n",
      "time_since_restore: 2289.079184770584\n",
      "time_this_iter_s: 5.433214902877808\n",
      "time_total_s: 2289.079184770584\n",
      "timers:\n",
      "  learn_throughput: 2132.778\n",
      "  learn_time_ms: 1875.488\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.766\n",
      "  sample_time_ms: 3531.18\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633514635\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1696000\n",
      "training_iteration: 424\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:424 starting ! -----------------\n",
      "agent_timesteps_total: 1700000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-04-08\n",
      "done: false\n",
      "episode_len_mean: 667.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.11632750000000225\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1239\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9005734920501709\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010125608183443546\n",
      "        model: {}\n",
      "        policy_loss: -0.027229534462094307\n",
      "        total_loss: -0.013146347366273403\n",
      "        vf_explained_var: 0.01565236784517765\n",
      "        vf_loss: 0.011045506224036217\n",
      "  num_agent_steps_sampled: 1700000\n",
      "  num_agent_steps_trained: 1700000\n",
      "  num_steps_sampled: 1700000\n",
      "  num_steps_trained: 1700000\n",
      "iterations_since_restore: 425\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.28888888888889\n",
      "  ram_util_percent: 44.2\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492160405995474\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7503219716282713\n",
      "  mean_inference_ms: 0.8508230986773303\n",
      "  mean_raw_obs_processing_ms: 0.08844895818087459\n",
      "time_since_restore: 2294.563094139099\n",
      "time_this_iter_s: 5.483909368515015\n",
      "time_total_s: 2294.563094139099\n",
      "timers:\n",
      "  learn_throughput: 2117.016\n",
      "  learn_time_ms: 1889.452\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.596\n",
      "  sample_time_ms: 3531.709\n",
      "  update_time_ms: 1.796\n",
      "timestamp: 1633514648\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1700000\n",
      "training_iteration: 425\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:425 starting ! -----------------\n",
      "agent_timesteps_total: 1704000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-04-32\n",
      "done: false\n",
      "episode_len_mean: 667.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.21691250000000217\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1242\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9291951656341553\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006946704350411892\n",
      "        model: {}\n",
      "        policy_loss: -0.02733270265161991\n",
      "        total_loss: -0.017590949311852455\n",
      "        vf_explained_var: 0.3825155794620514\n",
      "        vf_loss: 0.007657742593437433\n",
      "  num_agent_steps_sampled: 1704000\n",
      "  num_agent_steps_trained: 1704000\n",
      "  num_steps_sampled: 1704000\n",
      "  num_steps_trained: 1704000\n",
      "iterations_since_restore: 426\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.835294117647063\n",
      "  ram_util_percent: 44.10294117647059\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921465430266071\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7503450497152697\n",
      "  mean_inference_ms: 0.8508181044134827\n",
      "  mean_raw_obs_processing_ms: 0.08845310767464057\n",
      "time_since_restore: 2300.020341396332\n",
      "time_this_iter_s: 5.457247257232666\n",
      "time_total_s: 2300.020341396332\n",
      "timers:\n",
      "  learn_throughput: 2117.374\n",
      "  learn_time_ms: 1889.133\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.53\n",
      "  sample_time_ms: 3535.037\n",
      "  update_time_ms: 1.81\n",
      "timestamp: 1633514672\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1704000\n",
      "training_iteration: 426\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:426 starting ! -----------------\n",
      "agent_timesteps_total: 1708000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-04-48\n",
      "done: false\n",
      "episode_len_mean: 673.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.15604400000000215\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1244\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0433250665664673\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011384564451873302\n",
      "        model: {}\n",
      "        policy_loss: -0.014047435484826565\n",
      "        total_loss: 0.009717429988086224\n",
      "        vf_explained_var: 0.060853011906147\n",
      "        vf_loss: 0.020349491387605667\n",
      "  num_agent_steps_sampled: 1708000\n",
      "  num_agent_steps_trained: 1708000\n",
      "  num_steps_sampled: 1708000\n",
      "  num_steps_trained: 1708000\n",
      "iterations_since_restore: 427\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 32.96818181818182\n",
      "  ram_util_percent: 43.78636363636363\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921436107081258\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7503632541099698\n",
      "  mean_inference_ms: 0.8508157296922876\n",
      "  mean_raw_obs_processing_ms: 0.08845633463386665\n",
      "time_since_restore: 2305.7790784835815\n",
      "time_this_iter_s: 5.758737087249756\n",
      "time_total_s: 2305.7790784835815\n",
      "timers:\n",
      "  learn_throughput: 2098.326\n",
      "  learn_time_ms: 1906.282\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1124.754\n",
      "  sample_time_ms: 3556.332\n",
      "  update_time_ms: 1.81\n",
      "timestamp: 1633514688\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1708000\n",
      "training_iteration: 427\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:427 starting ! -----------------\n",
      "agent_timesteps_total: 1712000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-05-04\n",
      "done: false\n",
      "episode_len_mean: 681.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.15580050000000228\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1247\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.965438961982727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018195778131484985\n",
      "        model: {}\n",
      "        policy_loss: -0.03479331359267235\n",
      "        total_loss: -0.016304315999150276\n",
      "        vf_explained_var: 0.3425605893135071\n",
      "        vf_loss: 0.013030261732637882\n",
      "  num_agent_steps_sampled: 1712000\n",
      "  num_agent_steps_trained: 1712000\n",
      "  num_steps_sampled: 1712000\n",
      "  num_steps_trained: 1712000\n",
      "iterations_since_restore: 428\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.14090909090909\n",
      "  ram_util_percent: 43.89999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921449273581322\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7503908587777758\n",
      "  mean_inference_ms: 0.8508132693830119\n",
      "  mean_raw_obs_processing_ms: 0.08846098044974596\n",
      "time_since_restore: 2311.208246946335\n",
      "time_this_iter_s: 5.429168462753296\n",
      "time_total_s: 2311.208246946335\n",
      "timers:\n",
      "  learn_throughput: 2097.407\n",
      "  learn_time_ms: 1907.117\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1121.819\n",
      "  sample_time_ms: 3565.638\n",
      "  update_time_ms: 1.91\n",
      "timestamp: 1633514704\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1712000\n",
      "training_iteration: 428\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:428 starting ! -----------------\n",
      "agent_timesteps_total: 1716000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-05-28\n",
      "done: false\n",
      "episode_len_mean: 677.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.14575100000000224\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1250\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0119965076446533\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01177221443504095\n",
      "        model: {}\n",
      "        policy_loss: -0.03183114156126976\n",
      "        total_loss: -0.005720517132431269\n",
      "        vf_explained_var: 0.12035535275936127\n",
      "        vf_loss: 0.022578956559300423\n",
      "  num_agent_steps_sampled: 1716000\n",
      "  num_agent_steps_trained: 1716000\n",
      "  num_steps_sampled: 1716000\n",
      "  num_steps_trained: 1716000\n",
      "iterations_since_restore: 429\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.28235294117647\n",
      "  ram_util_percent: 43.844117647058816\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921493082431802\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7504179176300465\n",
      "  mean_inference_ms: 0.8508081262401389\n",
      "  mean_raw_obs_processing_ms: 0.08846599764385576\n",
      "time_since_restore: 2316.6260499954224\n",
      "time_this_iter_s: 5.417803049087524\n",
      "time_total_s: 2316.6260499954224\n",
      "timers:\n",
      "  learn_throughput: 2096.044\n",
      "  learn_time_ms: 1908.357\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1119.107\n",
      "  sample_time_ms: 3574.278\n",
      "  update_time_ms: 1.91\n",
      "timestamp: 1633514728\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1716000\n",
      "training_iteration: 429\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:429 starting ! -----------------\n",
      "agent_timesteps_total: 1720000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-05-41\n",
      "done: false\n",
      "episode_len_mean: 670.17\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.1362795000000021\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1254\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9630138874053955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010691284202039242\n",
      "        model: {}\n",
      "        policy_loss: -0.02456541173160076\n",
      "        total_loss: -0.00730821443721652\n",
      "        vf_explained_var: 0.2476738691329956\n",
      "        vf_loss: 0.014049811288714409\n",
      "  num_agent_steps_sampled: 1720000\n",
      "  num_agent_steps_trained: 1720000\n",
      "  num_steps_sampled: 1720000\n",
      "  num_steps_trained: 1720000\n",
      "iterations_since_restore: 430\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.594736842105263\n",
      "  ram_util_percent: 43.82631578947368\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049216260750665625\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7504580507810411\n",
      "  mean_inference_ms: 0.850801719066575\n",
      "  mean_raw_obs_processing_ms: 0.0884717812050598\n",
      "time_since_restore: 2322.1011970043182\n",
      "time_this_iter_s: 5.475147008895874\n",
      "time_total_s: 2322.1011970043182\n",
      "timers:\n",
      "  learn_throughput: 2090.485\n",
      "  learn_time_ms: 1913.432\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1116.172\n",
      "  sample_time_ms: 3583.677\n",
      "  update_time_ms: 1.907\n",
      "timestamp: 1633514741\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1720000\n",
      "training_iteration: 430\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:430 starting ! -----------------\n",
      "agent_timesteps_total: 1724000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-06-00\n",
      "done: false\n",
      "episode_len_mean: 673.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.19554250000000223\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1257\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0157088041305542\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012608752585947514\n",
      "        model: {}\n",
      "        policy_loss: -0.039159681648015976\n",
      "        total_loss: -0.022886211052536964\n",
      "        vf_explained_var: 0.27243542671203613\n",
      "        vf_loss: 0.012490840628743172\n",
      "  num_agent_steps_sampled: 1724000\n",
      "  num_agent_steps_trained: 1724000\n",
      "  num_steps_sampled: 1724000\n",
      "  num_steps_trained: 1724000\n",
      "iterations_since_restore: 431\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.35\n",
      "  ram_util_percent: 43.83846153846154\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921698855717691\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7504875273175875\n",
      "  mean_inference_ms: 0.8507961732471508\n",
      "  mean_raw_obs_processing_ms: 0.08847613168497696\n",
      "time_since_restore: 2327.443228006363\n",
      "time_this_iter_s: 5.342031002044678\n",
      "time_total_s: 2327.443228006363\n",
      "timers:\n",
      "  learn_throughput: 2101.723\n",
      "  learn_time_ms: 1903.201\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1121.099\n",
      "  sample_time_ms: 3567.928\n",
      "  update_time_ms: 1.999\n",
      "timestamp: 1633514760\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1724000\n",
      "training_iteration: 431\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:431 starting ! -----------------\n",
      "agent_timesteps_total: 1728000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-06-14\n",
      "done: false\n",
      "episode_len_mean: 657.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.24618200000000223\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1261\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9846222996711731\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007897193543612957\n",
      "        model: {}\n",
      "        policy_loss: -0.017642393708229065\n",
      "        total_loss: 0.0015218144981190562\n",
      "        vf_explained_var: 0.4154601991176605\n",
      "        vf_loss: 0.016795054078102112\n",
      "  num_agent_steps_sampled: 1728000\n",
      "  num_agent_steps_trained: 1728000\n",
      "  num_steps_sampled: 1728000\n",
      "  num_steps_trained: 1728000\n",
      "iterations_since_restore: 432\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.285\n",
      "  ram_util_percent: 43.79999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04921894293036791\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7505331941685697\n",
      "  mean_inference_ms: 0.8507894109951522\n",
      "  mean_raw_obs_processing_ms: 0.08848175117397272\n",
      "time_since_restore: 2332.9728951454163\n",
      "time_this_iter_s: 5.529667139053345\n",
      "time_total_s: 2332.9728951454163\n",
      "timers:\n",
      "  learn_throughput: 2096.256\n",
      "  learn_time_ms: 1908.164\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1123.11\n",
      "  sample_time_ms: 3561.538\n",
      "  update_time_ms: 1.999\n",
      "timestamp: 1633514774\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1728000\n",
      "training_iteration: 432\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:432 starting ! -----------------\n",
      "agent_timesteps_total: 1732000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-06-26\n",
      "done: false\n",
      "episode_len_mean: 666.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.19536600000000223\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1263\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9967203736305237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017176784574985504\n",
      "        model: {}\n",
      "        policy_loss: -0.03927017003297806\n",
      "        total_loss: -0.021436480805277824\n",
      "        vf_explained_var: -0.044025715440511703\n",
      "        vf_loss: 0.012680654413998127\n",
      "  num_agent_steps_sampled: 1732000\n",
      "  num_agent_steps_trained: 1732000\n",
      "  num_steps_sampled: 1732000\n",
      "  num_steps_trained: 1732000\n",
      "iterations_since_restore: 433\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.15294117647059\n",
      "  ram_util_percent: 43.805882352941175\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922021980964484\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7505541547549304\n",
      "  mean_inference_ms: 0.8507859865420212\n",
      "  mean_raw_obs_processing_ms: 0.0884850138609655\n",
      "time_since_restore: 2338.290280342102\n",
      "time_this_iter_s: 5.317385196685791\n",
      "time_total_s: 2338.290280342102\n",
      "timers:\n",
      "  learn_throughput: 2096.267\n",
      "  learn_time_ms: 1908.154\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.009\n",
      "  sample_time_ms: 3549.218\n",
      "  update_time_ms: 1.997\n",
      "timestamp: 1633514786\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1732000\n",
      "training_iteration: 433\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:433 starting ! -----------------\n",
      "agent_timesteps_total: 1736000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-06-50\n",
      "done: false\n",
      "episode_len_mean: 669.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.1950240000000022\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1266\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9271069169044495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013495481573045254\n",
      "        model: {}\n",
      "        policy_loss: -0.030033549293875694\n",
      "        total_loss: -0.01634073816239834\n",
      "        vf_explained_var: 0.3359636962413788\n",
      "        vf_loss: 0.009644166566431522\n",
      "  num_agent_steps_sampled: 1736000\n",
      "  num_agent_steps_trained: 1736000\n",
      "  num_steps_sampled: 1736000\n",
      "  num_steps_trained: 1736000\n",
      "iterations_since_restore: 434\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.044117647058826\n",
      "  ram_util_percent: 43.805882352941175\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049222985951731156\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7505850736585077\n",
      "  mean_inference_ms: 0.8507833102895114\n",
      "  mean_raw_obs_processing_ms: 0.08848883234254008\n",
      "time_since_restore: 2343.6944546699524\n",
      "time_this_iter_s: 5.404174327850342\n",
      "time_total_s: 2343.6944546699524\n",
      "timers:\n",
      "  learn_throughput: 2097.501\n",
      "  learn_time_ms: 1907.031\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1127.533\n",
      "  sample_time_ms: 3547.567\n",
      "  update_time_ms: 1.997\n",
      "timestamp: 1633514810\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1736000\n",
      "training_iteration: 434\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:434 starting ! -----------------\n",
      "agent_timesteps_total: 1740000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-07-04\n",
      "done: false\n",
      "episode_len_mean: 663.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.93795\n",
      "episode_reward_mean: 0.24613650000000212\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1268\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0041790008544922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015707330778241158\n",
      "        model: {}\n",
      "        policy_loss: -0.03375314921140671\n",
      "        total_loss: -0.0209851935505867\n",
      "        vf_explained_var: -0.09508714824914932\n",
      "        vf_loss: 0.008055760525166988\n",
      "  num_agent_steps_sampled: 1740000\n",
      "  num_agent_steps_trained: 1740000\n",
      "  num_steps_sampled: 1740000\n",
      "  num_steps_trained: 1740000\n",
      "iterations_since_restore: 435\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.157894736842103\n",
      "  ram_util_percent: 43.80526315789473\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922480590789518\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7506059757573789\n",
      "  mean_inference_ms: 0.8507812324057721\n",
      "  mean_raw_obs_processing_ms: 0.08849079386237638\n",
      "time_since_restore: 2349.0842955112457\n",
      "time_this_iter_s: 5.389840841293335\n",
      "time_total_s: 2349.0842955112457\n",
      "timers:\n",
      "  learn_throughput: 2111.443\n",
      "  learn_time_ms: 1894.439\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1126.52\n",
      "  sample_time_ms: 3550.76\n",
      "  update_time_ms: 1.994\n",
      "timestamp: 1633514824\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1740000\n",
      "training_iteration: 435\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:435 starting ! -----------------\n",
      "agent_timesteps_total: 1744000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-07-17\n",
      "done: false\n",
      "episode_len_mean: 669.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9115\n",
      "episode_reward_mean: 0.18538600000000216\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 1273\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8777223825454712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012036032043397427\n",
      "        model: {}\n",
      "        policy_loss: -0.02625945396721363\n",
      "        total_loss: -0.014362511225044727\n",
      "        vf_explained_var: 0.5634160041809082\n",
      "        vf_loss: 0.008286130614578724\n",
      "  num_agent_steps_sampled: 1744000\n",
      "  num_agent_steps_trained: 1744000\n",
      "  num_steps_sampled: 1744000\n",
      "  num_steps_trained: 1744000\n",
      "iterations_since_restore: 436\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.752631578947366\n",
      "  ram_util_percent: 43.78947368421051\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04922842822587489\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7506589000682692\n",
      "  mean_inference_ms: 0.850775177363798\n",
      "  mean_raw_obs_processing_ms: 0.0884957170224357\n",
      "time_since_restore: 2354.6438195705414\n",
      "time_this_iter_s: 5.559524059295654\n",
      "time_total_s: 2354.6438195705414\n",
      "timers:\n",
      "  learn_throughput: 2089.058\n",
      "  learn_time_ms: 1914.738\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1129.662\n",
      "  sample_time_ms: 3540.881\n",
      "  update_time_ms: 1.98\n",
      "timestamp: 1633514837\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1744000\n",
      "training_iteration: 436\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:436 starting ! -----------------\n",
      "agent_timesteps_total: 1748000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-07-32\n",
      "done: false\n",
      "episode_len_mean: 661.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.34676750000000217\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1277\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9702681303024292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010697754099965096\n",
      "        model: {}\n",
      "        policy_loss: -0.025185035541653633\n",
      "        total_loss: -0.005129325669258833\n",
      "        vf_explained_var: 0.6417205929756165\n",
      "        vf_loss: 0.016846386715769768\n",
      "  num_agent_steps_sampled: 1748000\n",
      "  num_agent_steps_trained: 1748000\n",
      "  num_steps_sampled: 1748000\n",
      "  num_steps_trained: 1748000\n",
      "iterations_since_restore: 437\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.70952380952381\n",
      "  ram_util_percent: 43.82857142857141\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492311638934275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7507013657790125\n",
      "  mean_inference_ms: 0.8507701948441092\n",
      "  mean_raw_obs_processing_ms: 0.08849952763790891\n",
      "time_since_restore: 2359.9707515239716\n",
      "time_this_iter_s: 5.326931953430176\n",
      "time_total_s: 2359.9707515239716\n",
      "timers:\n",
      "  learn_throughput: 2108.531\n",
      "  learn_time_ms: 1897.055\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.902\n",
      "  sample_time_ms: 3515.24\n",
      "  update_time_ms: 1.98\n",
      "timestamp: 1633514852\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1748000\n",
      "training_iteration: 437\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:437 starting ! -----------------\n",
      "agent_timesteps_total: 1752000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-07-45\n",
      "done: false\n",
      "episode_len_mean: 666.69\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.3466770000000023\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1279\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0459837913513184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011629410088062286\n",
      "        model: {}\n",
      "        policy_loss: -0.030500663444399834\n",
      "        total_loss: -0.012090533971786499\n",
      "        vf_explained_var: 0.12541119754314423\n",
      "        vf_loss: 0.014921307563781738\n",
      "  num_agent_steps_sampled: 1752000\n",
      "  num_agent_steps_trained: 1752000\n",
      "  num_steps_sampled: 1752000\n",
      "  num_steps_trained: 1752000\n",
      "iterations_since_restore: 438\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.27058823529412\n",
      "  ram_util_percent: 43.811764705882354\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049232271932190985\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7507168834536337\n",
      "  mean_inference_ms: 0.8507669163460528\n",
      "  mean_raw_obs_processing_ms: 0.08850167756879335\n",
      "time_since_restore: 2365.361394405365\n",
      "time_this_iter_s: 5.390642881393433\n",
      "time_total_s: 2365.361394405365\n",
      "timers:\n",
      "  learn_throughput: 2101.283\n",
      "  learn_time_ms: 1903.599\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1141.286\n",
      "  sample_time_ms: 3504.819\n",
      "  update_time_ms: 1.981\n",
      "timestamp: 1633514865\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1752000\n",
      "training_iteration: 438\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:438 starting ! -----------------\n",
      "agent_timesteps_total: 1756000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-08-07\n",
      "done: false\n",
      "episode_len_mean: 663.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.4072920000000023\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1281\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9108864665031433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011354313232004642\n",
      "        model: {}\n",
      "        policy_loss: -0.022372014820575714\n",
      "        total_loss: -0.006368004251271486\n",
      "        vf_explained_var: 0.21098418533802032\n",
      "        vf_loss: 0.012597710825502872\n",
      "  num_agent_steps_sampled: 1756000\n",
      "  num_agent_steps_trained: 1756000\n",
      "  num_steps_sampled: 1756000\n",
      "  num_steps_trained: 1756000\n",
      "iterations_since_restore: 439\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.418750000000003\n",
      "  ram_util_percent: 43.825\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923372419061371\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7507342794492584\n",
      "  mean_inference_ms: 0.8507627773648544\n",
      "  mean_raw_obs_processing_ms: 0.08850358435330946\n",
      "time_since_restore: 2370.7397122383118\n",
      "time_this_iter_s: 5.378317832946777\n",
      "time_total_s: 2370.7397122383118\n",
      "timers:\n",
      "  learn_throughput: 2098.942\n",
      "  learn_time_ms: 1905.722\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.335\n",
      "  sample_time_ms: 3498.538\n",
      "  update_time_ms: 1.981\n",
      "timestamp: 1633514887\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1756000\n",
      "training_iteration: 439\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:439 starting ! -----------------\n",
      "agent_timesteps_total: 1760000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-08-22\n",
      "done: false\n",
      "episode_len_mean: 669.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.4072460000000024\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1283\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9361607432365417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011683071032166481\n",
      "        model: {}\n",
      "        policy_loss: -0.025344030931591988\n",
      "        total_loss: -0.013788278214633465\n",
      "        vf_explained_var: 0.19829507172107697\n",
      "        vf_loss: 0.0080508291721344\n",
      "  num_agent_steps_sampled: 1760000\n",
      "  num_agent_steps_trained: 1760000\n",
      "  num_steps_sampled: 1760000\n",
      "  num_steps_trained: 1760000\n",
      "iterations_since_restore: 440\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.47142857142857\n",
      "  ram_util_percent: 43.8142857142857\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923484824940814\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7507511699109353\n",
      "  mean_inference_ms: 0.8507585970815659\n",
      "  mean_raw_obs_processing_ms: 0.08850544947014498\n",
      "time_since_restore: 2376.286226272583\n",
      "time_this_iter_s: 5.54651403427124\n",
      "time_total_s: 2376.286226272583\n",
      "timers:\n",
      "  learn_throughput: 2088.305\n",
      "  learn_time_ms: 1915.429\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1144.219\n",
      "  sample_time_ms: 3495.834\n",
      "  update_time_ms: 1.985\n",
      "timestamp: 1633514902\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1760000\n",
      "training_iteration: 440\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:440 starting ! -----------------\n",
      "agent_timesteps_total: 1764000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-08-36\n",
      "done: false\n",
      "episode_len_mean: 673.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.3372885000000022\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1286\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0186048746109009\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011246994137763977\n",
      "        model: {}\n",
      "        policy_loss: -0.0248117595911026\n",
      "        total_loss: -0.007170907687395811\n",
      "        vf_explained_var: -0.2326037436723709\n",
      "        vf_loss: 0.014266752637922764\n",
      "  num_agent_steps_sampled: 1764000\n",
      "  num_agent_steps_trained: 1764000\n",
      "  num_steps_sampled: 1764000\n",
      "  num_steps_trained: 1764000\n",
      "iterations_since_restore: 441\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.800000000000004\n",
      "  ram_util_percent: 43.9\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04923650044847819\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7507780758272737\n",
      "  mean_inference_ms: 0.8507543001539156\n",
      "  mean_raw_obs_processing_ms: 0.0885085535326244\n",
      "time_since_restore: 2381.895837545395\n",
      "time_this_iter_s: 5.60961127281189\n",
      "time_total_s: 2381.895837545395\n",
      "timers:\n",
      "  learn_throughput: 2082.477\n",
      "  learn_time_ms: 1920.79\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.313\n",
      "  sample_time_ms: 3517.063\n",
      "  update_time_ms: 1.893\n",
      "timestamp: 1633514916\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1764000\n",
      "training_iteration: 441\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:441 starting ! -----------------\n",
      "agent_timesteps_total: 1768000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-08-53\n",
      "done: false\n",
      "episode_len_mean: 661.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.4282970000000022\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1289\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9931715726852417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008331628516316414\n",
      "        model: {}\n",
      "        policy_loss: -0.02905431017279625\n",
      "        total_loss: 0.005884036421775818\n",
      "        vf_explained_var: 0.02023450657725334\n",
      "        vf_loss: 0.03243885934352875\n",
      "  num_agent_steps_sampled: 1768000\n",
      "  num_agent_steps_trained: 1768000\n",
      "  num_steps_sampled: 1768000\n",
      "  num_steps_trained: 1768000\n",
      "iterations_since_restore: 442\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.495833333333334\n",
      "  ram_util_percent: 43.82916666666666\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049238526886081664\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.750802965527409\n",
      "  mean_inference_ms: 0.8507504656005047\n",
      "  mean_raw_obs_processing_ms: 0.08851195706627227\n",
      "time_since_restore: 2387.384088754654\n",
      "time_this_iter_s: 5.488251209259033\n",
      "time_total_s: 2387.384088754654\n",
      "timers:\n",
      "  learn_throughput: 2088.271\n",
      "  learn_time_ms: 1915.461\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.933\n",
      "  sample_time_ms: 3518.238\n",
      "  update_time_ms: 1.893\n",
      "timestamp: 1633514933\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1768000\n",
      "training_iteration: 442\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:442 starting ! -----------------\n",
      "agent_timesteps_total: 1772000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-09-15\n",
      "done: false\n",
      "episode_len_mean: 667.54\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.5091740000000022\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1293\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9108729958534241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008374237455427647\n",
      "        model: {}\n",
      "        policy_loss: -0.029635341838002205\n",
      "        total_loss: -6.98745907357079e-06\n",
      "        vf_explained_var: -0.027366813272237778\n",
      "        vf_loss: 0.027116086333990097\n",
      "  num_agent_steps_sampled: 1772000\n",
      "  num_agent_steps_trained: 1772000\n",
      "  num_steps_sampled: 1772000\n",
      "  num_steps_trained: 1772000\n",
      "iterations_since_restore: 443\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.532258064516125\n",
      "  ram_util_percent: 43.81290322580644\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924027713011707\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7508330616896812\n",
      "  mean_inference_ms: 0.8507458723644535\n",
      "  mean_raw_obs_processing_ms: 0.08851606146338035\n",
      "time_since_restore: 2392.71014547348\n",
      "time_this_iter_s: 5.326056718826294\n",
      "time_total_s: 2392.71014547348\n",
      "timers:\n",
      "  learn_throughput: 2088.234\n",
      "  learn_time_ms: 1915.494\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.678\n",
      "  sample_time_ms: 3519.026\n",
      "  update_time_ms: 1.793\n",
      "timestamp: 1633514955\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1772000\n",
      "training_iteration: 443\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:443 starting ! -----------------\n",
      "agent_timesteps_total: 1776000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-09-34\n",
      "done: false\n",
      "episode_len_mean: 671.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.45826400000000234\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1296\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9310094118118286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010985413566231728\n",
      "        model: {}\n",
      "        policy_loss: -0.027543410658836365\n",
      "        total_loss: 0.0068834335543215275\n",
      "        vf_explained_var: 0.342752605676651\n",
      "        vf_loss: 0.03113122470676899\n",
      "  num_agent_steps_sampled: 1776000\n",
      "  num_agent_steps_trained: 1776000\n",
      "  num_steps_sampled: 1776000\n",
      "  num_steps_trained: 1776000\n",
      "iterations_since_restore: 444\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.711111111111112\n",
      "  ram_util_percent: 43.8037037037037\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924175677792613\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7508560540154169\n",
      "  mean_inference_ms: 0.8507433522116853\n",
      "  mean_raw_obs_processing_ms: 0.08851905569090436\n",
      "time_since_restore: 2398.086250305176\n",
      "time_this_iter_s: 5.376104831695557\n",
      "time_total_s: 2398.086250305176\n",
      "timers:\n",
      "  learn_throughput: 2089.455\n",
      "  learn_time_ms: 1914.375\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.242\n",
      "  sample_time_ms: 3517.28\n",
      "  update_time_ms: 1.794\n",
      "timestamp: 1633514974\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1776000\n",
      "training_iteration: 444\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:444 starting ! -----------------\n",
      "agent_timesteps_total: 1780000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-09-58\n",
      "done: false\n",
      "episode_len_mean: 667.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.45810750000000233\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1299\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9314990043640137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012546906247735023\n",
      "        model: {}\n",
      "        policy_loss: -0.027977123856544495\n",
      "        total_loss: -0.010170286521315575\n",
      "        vf_explained_var: -0.04177951440215111\n",
      "        vf_loss: 0.014042764902114868\n",
      "  num_agent_steps_sampled: 1780000\n",
      "  num_agent_steps_trained: 1780000\n",
      "  num_steps_sampled: 1780000\n",
      "  num_steps_trained: 1780000\n",
      "iterations_since_restore: 445\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.170588235294115\n",
      "  ram_util_percent: 43.811764705882354\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492432433268766\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.750879073811346\n",
      "  mean_inference_ms: 0.8507418542278502\n",
      "  mean_raw_obs_processing_ms: 0.08852190632506686\n",
      "time_since_restore: 2403.4313633441925\n",
      "time_this_iter_s: 5.345113039016724\n",
      "time_total_s: 2403.4313633441925\n",
      "timers:\n",
      "  learn_throughput: 2091.265\n",
      "  learn_time_ms: 1912.718\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.141\n",
      "  sample_time_ms: 3514.502\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633514998\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1780000\n",
      "training_iteration: 445\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:445 starting ! -----------------\n",
      "agent_timesteps_total: 1784000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-10-22\n",
      "done: false\n",
      "episode_len_mean: 667.09\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.45752050000000233\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1302\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.000773549079895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00953199528157711\n",
      "        model: {}\n",
      "        policy_loss: -0.028478924185037613\n",
      "        total_loss: -0.005739246495068073\n",
      "        vf_explained_var: 0.019312385469675064\n",
      "        vf_loss: 0.019880080595612526\n",
      "  num_agent_steps_sampled: 1784000\n",
      "  num_agent_steps_trained: 1784000\n",
      "  num_steps_sampled: 1784000\n",
      "  num_steps_trained: 1784000\n",
      "iterations_since_restore: 446\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.141176470588235\n",
      "  ram_util_percent: 43.80294117647058\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924545024990172\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.750898819911224\n",
      "  mean_inference_ms: 0.8507421559107717\n",
      "  mean_raw_obs_processing_ms: 0.08852469809632826\n",
      "time_since_restore: 2408.8556876182556\n",
      "time_this_iter_s: 5.42432427406311\n",
      "time_total_s: 2408.8556876182556\n",
      "timers:\n",
      "  learn_throughput: 2107.801\n",
      "  learn_time_ms: 1897.712\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1137.721\n",
      "  sample_time_ms: 3515.802\n",
      "  update_time_ms: 1.697\n",
      "timestamp: 1633515022\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1784000\n",
      "training_iteration: 446\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:446 starting ! -----------------\n",
      "agent_timesteps_total: 1788000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-10-34\n",
      "done: false\n",
      "episode_len_mean: 667.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.3971865000000024\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1305\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8915184736251831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01449541375041008\n",
      "        model: {}\n",
      "        policy_loss: -0.02951432578265667\n",
      "        total_loss: 0.019924689084291458\n",
      "        vf_explained_var: 0.12607988715171814\n",
      "        vf_loss: 0.045090388506650925\n",
      "  num_agent_steps_sampled: 1788000\n",
      "  num_agent_steps_trained: 1788000\n",
      "  num_steps_sampled: 1788000\n",
      "  num_steps_trained: 1788000\n",
      "iterations_since_restore: 447\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.018749999999997\n",
      "  ram_util_percent: 43.8\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04924767963010525\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7509181318904374\n",
      "  mean_inference_ms: 0.8507425654720285\n",
      "  mean_raw_obs_processing_ms: 0.0885268943815499\n",
      "time_since_restore: 2414.2378902435303\n",
      "time_this_iter_s: 5.382202625274658\n",
      "time_total_s: 2414.2378902435303\n",
      "timers:\n",
      "  learn_throughput: 2109.775\n",
      "  learn_time_ms: 1895.937\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1135.328\n",
      "  sample_time_ms: 3523.211\n",
      "  update_time_ms: 1.671\n",
      "timestamp: 1633515034\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1788000\n",
      "training_iteration: 447\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:447 starting ! -----------------\n",
      "agent_timesteps_total: 1792000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-10-57\n",
      "done: false\n",
      "episode_len_mean: 655.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.45817600000000236\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1309\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9560734629631042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008789675310254097\n",
      "        model: {}\n",
      "        policy_loss: -0.030817953869700432\n",
      "        total_loss: -0.016803042963147163\n",
      "        vf_explained_var: 0.04905616492033005\n",
      "        vf_loss: 0.011378005146980286\n",
      "  num_agent_steps_sampled: 1792000\n",
      "  num_agent_steps_trained: 1792000\n",
      "  num_steps_sampled: 1792000\n",
      "  num_steps_trained: 1792000\n",
      "iterations_since_restore: 448\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.59090909090909\n",
      "  ram_util_percent: 43.8\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492503205596568\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.750941720917544\n",
      "  mean_inference_ms: 0.8507442910830285\n",
      "  mean_raw_obs_processing_ms: 0.08852915178666837\n",
      "time_since_restore: 2419.6147010326385\n",
      "time_this_iter_s: 5.376810789108276\n",
      "time_total_s: 2419.6147010326385\n",
      "timers:\n",
      "  learn_throughput: 2118.091\n",
      "  learn_time_ms: 1888.493\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.403\n",
      "  sample_time_ms: 3529.194\n",
      "  update_time_ms: 1.671\n",
      "timestamp: 1633515057\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1792000\n",
      "training_iteration: 448\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:448 starting ! -----------------\n",
      "agent_timesteps_total: 1796000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-11-22\n",
      "done: false\n",
      "episode_len_mean: 656.59\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.45779750000000236\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1312\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9587806463241577\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011189385317265987\n",
      "        model: {}\n",
      "        policy_loss: -0.023126691579818726\n",
      "        total_loss: 0.03522088751196861\n",
      "        vf_explained_var: 0.21197813749313354\n",
      "        vf_loss: 0.054990753531455994\n",
      "  num_agent_steps_sampled: 1796000\n",
      "  num_agent_steps_trained: 1796000\n",
      "  num_steps_sampled: 1796000\n",
      "  num_steps_trained: 1796000\n",
      "iterations_since_restore: 449\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.517647058823528\n",
      "  ram_util_percent: 43.78235294117648\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925321438797469\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7509591542855446\n",
      "  mean_inference_ms: 0.8507439480219943\n",
      "  mean_raw_obs_processing_ms: 0.0885300385729505\n",
      "time_since_restore: 2424.991023540497\n",
      "time_this_iter_s: 5.376322507858276\n",
      "time_total_s: 2424.991023540497\n",
      "timers:\n",
      "  learn_throughput: 2119.738\n",
      "  learn_time_ms: 1887.026\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.007\n",
      "  sample_time_ms: 3530.429\n",
      "  update_time_ms: 1.671\n",
      "timestamp: 1633515082\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1796000\n",
      "training_iteration: 449\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:449 starting ! -----------------\n",
      "agent_timesteps_total: 1800000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-11-46\n",
      "done: false\n",
      "episode_len_mean: 655.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.45830050000000233\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1316\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9493536949157715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01396429818123579\n",
      "        model: {}\n",
      "        policy_loss: -0.03804774209856987\n",
      "        total_loss: -0.014217478223145008\n",
      "        vf_explained_var: 0.20783983170986176\n",
      "        vf_loss: 0.019640974700450897\n",
      "  num_agent_steps_sampled: 1800000\n",
      "  num_agent_steps_trained: 1800000\n",
      "  num_steps_sampled: 1800000\n",
      "  num_steps_trained: 1800000\n",
      "iterations_since_restore: 450\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.035294117647055\n",
      "  ram_util_percent: 43.77058823529412\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04925697803438263\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7509820155324135\n",
      "  mean_inference_ms: 0.850743198428131\n",
      "  mean_raw_obs_processing_ms: 0.08853142092780547\n",
      "time_since_restore: 2430.338011264801\n",
      "time_this_iter_s: 5.346987724304199\n",
      "time_total_s: 2430.338011264801\n",
      "timers:\n",
      "  learn_throughput: 2137.145\n",
      "  learn_time_ms: 1871.656\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.471\n",
      "  sample_time_ms: 3525.873\n",
      "  update_time_ms: 1.671\n",
      "timestamp: 1633515106\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1800000\n",
      "training_iteration: 450\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:450 starting ! -----------------\n",
      "agent_timesteps_total: 1804000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-12-10\n",
      "done: false\n",
      "episode_len_mean: 656.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9139500000000003\n",
      "episode_reward_mean: 0.45860350000000233\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1318\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0514553785324097\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00945884920656681\n",
      "        model: {}\n",
      "        policy_loss: -0.027965663000941277\n",
      "        total_loss: -0.005753252189606428\n",
      "        vf_explained_var: 0.017624225467443466\n",
      "        vf_loss: 0.01937476173043251\n",
      "  num_agent_steps_sampled: 1804000\n",
      "  num_agent_steps_trained: 1804000\n",
      "  num_steps_sampled: 1804000\n",
      "  num_steps_trained: 1804000\n",
      "iterations_since_restore: 451\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.269696969696973\n",
      "  ram_util_percent: 43.769696969696966\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049258905339134226\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.750994451189944\n",
      "  mean_inference_ms: 0.8507433010276344\n",
      "  mean_raw_obs_processing_ms: 0.08853220692940177\n",
      "time_since_restore: 2435.7432446479797\n",
      "time_this_iter_s: 5.405233383178711\n",
      "time_total_s: 2435.7432446479797\n",
      "timers:\n",
      "  learn_throughput: 2143.033\n",
      "  learn_time_ms: 1866.514\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.375\n",
      "  sample_time_ms: 3510.696\n",
      "  update_time_ms: 1.67\n",
      "timestamp: 1633515130\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1804000\n",
      "training_iteration: 451\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:451 starting ! -----------------\n",
      "agent_timesteps_total: 1808000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-12-24\n",
      "done: false\n",
      "episode_len_mean: 658.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5100490000000023\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1321\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0499870777130127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01132415235042572\n",
      "        model: {}\n",
      "        policy_loss: -0.03341503441333771\n",
      "        total_loss: -0.001547096879221499\n",
      "        vf_explained_var: -0.13015897572040558\n",
      "        vf_loss: 0.02847069315612316\n",
      "  num_agent_steps_sampled: 1808000\n",
      "  num_agent_steps_trained: 1808000\n",
      "  num_steps_sampled: 1808000\n",
      "  num_steps_trained: 1808000\n",
      "iterations_since_restore: 452\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.609523809523807\n",
      "  ram_util_percent: 43.75714285714285\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049262543141263784\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7510145553617413\n",
      "  mean_inference_ms: 0.8507441284532892\n",
      "  mean_raw_obs_processing_ms: 0.08853308500734812\n",
      "time_since_restore: 2441.193140745163\n",
      "time_this_iter_s: 5.4498960971832275\n",
      "time_total_s: 2441.193140745163\n",
      "timers:\n",
      "  learn_throughput: 2136.284\n",
      "  learn_time_ms: 1872.41\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1142.585\n",
      "  sample_time_ms: 3500.835\n",
      "  update_time_ms: 1.77\n",
      "timestamp: 1633515144\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1808000\n",
      "training_iteration: 452\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:452 starting ! -----------------\n",
      "agent_timesteps_total: 1812000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-12-40\n",
      "done: false\n",
      "episode_len_mean: 654.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.4997965000000022\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1324\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0369374752044678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008978328667581081\n",
      "        model: {}\n",
      "        policy_loss: -0.02656431868672371\n",
      "        total_loss: -0.003618597751483321\n",
      "        vf_explained_var: 0.0691395029425621\n",
      "        vf_loss: 0.020252227783203125\n",
      "  num_agent_steps_sampled: 1812000\n",
      "  num_agent_steps_trained: 1812000\n",
      "  num_steps_sampled: 1812000\n",
      "  num_steps_trained: 1812000\n",
      "iterations_since_restore: 453\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.734782608695657\n",
      "  ram_util_percent: 43.78260869565216\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04926653601695124\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7510378376090082\n",
      "  mean_inference_ms: 0.8507442753846598\n",
      "  mean_raw_obs_processing_ms: 0.08853429901111134\n",
      "time_since_restore: 2446.65176486969\n",
      "time_this_iter_s: 5.4586241245269775\n",
      "time_total_s: 2446.65176486969\n",
      "timers:\n",
      "  learn_throughput: 2136.257\n",
      "  learn_time_ms: 1872.434\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.295\n",
      "  sample_time_ms: 3514.028\n",
      "  update_time_ms: 1.87\n",
      "timestamp: 1633515160\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1812000\n",
      "training_iteration: 453\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:453 starting ! -----------------\n",
      "agent_timesteps_total: 1816000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-13-05\n",
      "done: false\n",
      "episode_len_mean: 659.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5104540000000023\n",
      "episode_reward_min: -2.230000000000002\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1327\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0264517068862915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011100881733000278\n",
      "        model: {}\n",
      "        policy_loss: -0.024739747866988182\n",
      "        total_loss: -0.009613918140530586\n",
      "        vf_explained_var: 0.30413538217544556\n",
      "        vf_loss: 0.011795570142567158\n",
      "  num_agent_steps_sampled: 1816000\n",
      "  num_agent_steps_trained: 1816000\n",
      "  num_steps_sampled: 1816000\n",
      "  num_steps_trained: 1816000\n",
      "iterations_since_restore: 454\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.620588235294118\n",
      "  ram_util_percent: 43.76764705882353\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049270730474836585\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7510570140956225\n",
      "  mean_inference_ms: 0.8507432187177187\n",
      "  mean_raw_obs_processing_ms: 0.08853524890163726\n",
      "time_since_restore: 2451.9593069553375\n",
      "time_this_iter_s: 5.307542085647583\n",
      "time_total_s: 2451.9593069553375\n",
      "timers:\n",
      "  learn_throughput: 2134.709\n",
      "  learn_time_ms: 1873.792\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.964\n",
      "  sample_time_ms: 3505.807\n",
      "  update_time_ms: 1.77\n",
      "timestamp: 1633515185\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1816000\n",
      "training_iteration: 454\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:454 starting ! -----------------\n",
      "agent_timesteps_total: 1820000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-13-18\n",
      "done: false\n",
      "episode_len_mean: 654.59\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5823645000000023\n",
      "episode_reward_min: -2.1819500000000005\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1330\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.037052035331726\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013631596229970455\n",
      "        model: {}\n",
      "        policy_loss: -0.033576346933841705\n",
      "        total_loss: 0.007977291010320187\n",
      "        vf_explained_var: 0.029474537819623947\n",
      "        vf_loss: 0.03746415674686432\n",
      "  num_agent_steps_sampled: 1820000\n",
      "  num_agent_steps_trained: 1820000\n",
      "  num_steps_sampled: 1820000\n",
      "  num_steps_trained: 1820000\n",
      "iterations_since_restore: 455\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.88888888888889\n",
      "  ram_util_percent: 43.816666666666656\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927442443833787\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.751069540422562\n",
      "  mean_inference_ms: 0.8507430401226267\n",
      "  mean_raw_obs_processing_ms: 0.08853717783201417\n",
      "time_since_restore: 2457.377163171768\n",
      "time_this_iter_s: 5.417856216430664\n",
      "time_total_s: 2457.377163171768\n",
      "timers:\n",
      "  learn_throughput: 2134.266\n",
      "  learn_time_ms: 1874.181\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.763\n",
      "  sample_time_ms: 3512.583\n",
      "  update_time_ms: 1.87\n",
      "timestamp: 1633515198\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1820000\n",
      "training_iteration: 455\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:455 starting ! -----------------\n",
      "agent_timesteps_total: 1824000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-13-32\n",
      "done: false\n",
      "episode_len_mean: 655.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5319905000000024\n",
      "episode_reward_min: -2.1819500000000005\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1333\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.03117036819458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010514600202441216\n",
      "        model: {}\n",
      "        policy_loss: -0.028470611199736595\n",
      "        total_loss: -0.015947911888360977\n",
      "        vf_explained_var: 0.3836665749549866\n",
      "        vf_loss: 0.009368319064378738\n",
      "  num_agent_steps_sampled: 1824000\n",
      "  num_agent_steps_trained: 1824000\n",
      "  num_steps_sampled: 1824000\n",
      "  num_steps_trained: 1824000\n",
      "iterations_since_restore: 456\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.584999999999997\n",
      "  ram_util_percent: 43.769999999999996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04927768301353783\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7510813300156578\n",
      "  mean_inference_ms: 0.8507431151059305\n",
      "  mean_raw_obs_processing_ms: 0.08853815778767492\n",
      "time_since_restore: 2462.828407049179\n",
      "time_this_iter_s: 5.451243877410889\n",
      "time_total_s: 2462.828407049179\n",
      "timers:\n",
      "  learn_throughput: 2131.008\n",
      "  learn_time_ms: 1877.046\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.811\n",
      "  sample_time_ms: 3512.435\n",
      "  update_time_ms: 1.87\n",
      "timestamp: 1633515212\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1824000\n",
      "training_iteration: 456\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:456 starting ! -----------------\n",
      "agent_timesteps_total: 1828000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-13-46\n",
      "done: false\n",
      "episode_len_mean: 660.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5418270000000024\n",
      "episode_reward_min: -2.1819500000000005\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1335\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0095521211624146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011340034194290638\n",
      "        model: {}\n",
      "        policy_loss: -0.030791083350777626\n",
      "        total_loss: -0.019798094406723976\n",
      "        vf_explained_var: 0.3316943645477295\n",
      "        vf_loss: 0.007590974681079388\n",
      "  num_agent_steps_sampled: 1828000\n",
      "  num_agent_steps_trained: 1828000\n",
      "  num_steps_sampled: 1828000\n",
      "  num_steps_trained: 1828000\n",
      "iterations_since_restore: 457\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.126315789473683\n",
      "  ram_util_percent: 43.77894736842104\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049279681116324546\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.751087448151122\n",
      "  mean_inference_ms: 0.8507440466302458\n",
      "  mean_raw_obs_processing_ms: 0.08853899724530466\n",
      "time_since_restore: 2468.2086789608\n",
      "time_this_iter_s: 5.380271911621094\n",
      "time_total_s: 2468.2086789608\n",
      "timers:\n",
      "  learn_throughput: 2129.165\n",
      "  learn_time_ms: 1878.671\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.391\n",
      "  sample_time_ms: 3510.647\n",
      "  update_time_ms: 1.897\n",
      "timestamp: 1633515226\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1828000\n",
      "training_iteration: 457\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:457 starting ! -----------------\n",
      "agent_timesteps_total: 1832000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-14-00\n",
      "done: false\n",
      "episode_len_mean: 656.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5223915000000023\n",
      "episode_reward_min: -2.1819500000000005\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1339\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9224452972412109\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007405063137412071\n",
      "        model: {}\n",
      "        policy_loss: -0.02327020652592182\n",
      "        total_loss: 0.012791785411536694\n",
      "        vf_explained_var: -0.1689629703760147\n",
      "        vf_loss: 0.033840473741292953\n",
      "  num_agent_steps_sampled: 1832000\n",
      "  num_agent_steps_trained: 1832000\n",
      "  num_steps_sampled: 1832000\n",
      "  num_steps_trained: 1832000\n",
      "iterations_since_restore: 458\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.464999999999996\n",
      "  ram_util_percent: 43.82499999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928338862448185\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511035479059487\n",
      "  mean_inference_ms: 0.8507467211047611\n",
      "  mean_raw_obs_processing_ms: 0.08854125023134916\n",
      "time_since_restore: 2473.728681564331\n",
      "time_this_iter_s: 5.520002603530884\n",
      "time_total_s: 2473.728681564331\n",
      "timers:\n",
      "  learn_throughput: 2123.084\n",
      "  learn_time_ms: 1884.052\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1136.443\n",
      "  sample_time_ms: 3519.756\n",
      "  update_time_ms: 1.797\n",
      "timestamp: 1633515240\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1832000\n",
      "training_iteration: 458\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:458 starting ! -----------------\n",
      "agent_timesteps_total: 1836000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-14-13\n",
      "done: false\n",
      "episode_len_mean: 653.65\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5428810000000024\n",
      "episode_reward_min: -2.1625500000000004\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1343\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0665570497512817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011393929831683636\n",
      "        model: {}\n",
      "        policy_loss: -0.031456977128982544\n",
      "        total_loss: -0.015485232695937157\n",
      "        vf_explained_var: 0.21052002906799316\n",
      "        vf_loss: 0.012553561478853226\n",
      "  num_agent_steps_sampled: 1836000\n",
      "  num_agent_steps_trained: 1836000\n",
      "  num_steps_sampled: 1836000\n",
      "  num_steps_trained: 1836000\n",
      "iterations_since_restore: 459\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.784999999999997\n",
      "  ram_util_percent: 43.80499999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928602001869281\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511132121002935\n",
      "  mean_inference_ms: 0.8507476986917536\n",
      "  mean_raw_obs_processing_ms: 0.08854304277314569\n",
      "time_since_restore: 2479.0276141166687\n",
      "time_this_iter_s: 5.2989325523376465\n",
      "time_total_s: 2479.0276141166687\n",
      "timers:\n",
      "  learn_throughput: 2125.715\n",
      "  learn_time_ms: 1881.719\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.158\n",
      "  sample_time_ms: 3514.45\n",
      "  update_time_ms: 1.777\n",
      "timestamp: 1633515253\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1836000\n",
      "training_iteration: 459\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:459 starting ! -----------------\n",
      "agent_timesteps_total: 1840000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-14-38\n",
      "done: false\n",
      "episode_len_mean: 640.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6544725000000022\n",
      "episode_reward_min: -2.1590000000000007\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1346\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9143127202987671\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011636189185082912\n",
      "        model: {}\n",
      "        policy_loss: -0.02030869945883751\n",
      "        total_loss: 0.0026476362254470587\n",
      "        vf_explained_var: 0.33138400316238403\n",
      "        vf_loss: 0.019465474411845207\n",
      "  num_agent_steps_sampled: 1840000\n",
      "  num_agent_steps_trained: 1840000\n",
      "  num_steps_sampled: 1840000\n",
      "  num_steps_trained: 1840000\n",
      "iterations_since_restore: 460\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.147058823529413\n",
      "  ram_util_percent: 43.8\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04928779541696485\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511192152184948\n",
      "  mean_inference_ms: 0.850747687418203\n",
      "  mean_raw_obs_processing_ms: 0.08854420238285053\n",
      "time_since_restore: 2484.5723700523376\n",
      "time_this_iter_s: 5.544755935668945\n",
      "time_total_s: 2484.5723700523376\n",
      "timers:\n",
      "  learn_throughput: 2120.699\n",
      "  learn_time_ms: 1886.171\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.161\n",
      "  sample_time_ms: 3529.95\n",
      "  update_time_ms: 1.777\n",
      "timestamp: 1633515278\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1840000\n",
      "training_iteration: 460\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:460 starting ! -----------------\n",
      "agent_timesteps_total: 1844000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-14-57\n",
      "done: false\n",
      "episode_len_mean: 652.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6632550000000024\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1349\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0122690200805664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014070413075387478\n",
      "        model: {}\n",
      "        policy_loss: -0.026031630113720894\n",
      "        total_loss: -0.010506493970751762\n",
      "        vf_explained_var: 0.289519339799881\n",
      "        vf_loss: 0.01130401249974966\n",
      "  num_agent_steps_sampled: 1844000\n",
      "  num_agent_steps_trained: 1844000\n",
      "  num_steps_sampled: 1844000\n",
      "  num_steps_trained: 1844000\n",
      "iterations_since_restore: 461\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.40740740740741\n",
      "  ram_util_percent: 43.825925925925915\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0492893701045616\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511211798760945\n",
      "  mean_inference_ms: 0.8507464001835795\n",
      "  mean_raw_obs_processing_ms: 0.08854508853369338\n",
      "time_since_restore: 2489.8244438171387\n",
      "time_this_iter_s: 5.252073764801025\n",
      "time_total_s: 2489.8244438171387\n",
      "timers:\n",
      "  learn_throughput: 2119.35\n",
      "  learn_time_ms: 1887.371\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1138.529\n",
      "  sample_time_ms: 3513.304\n",
      "  update_time_ms: 1.876\n",
      "timestamp: 1633515297\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1844000\n",
      "training_iteration: 461\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:461 starting ! -----------------\n",
      "agent_timesteps_total: 1848000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-15-21\n",
      "done: false\n",
      "episode_len_mean: 653.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6130205000000025\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1351\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0530526638031006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012863120064139366\n",
      "        model: {}\n",
      "        policy_loss: -0.030666429549455643\n",
      "        total_loss: -0.02038695104420185\n",
      "        vf_explained_var: 0.2928372323513031\n",
      "        vf_loss: 0.006420540623366833\n",
      "  num_agent_steps_sampled: 1848000\n",
      "  num_agent_steps_trained: 1848000\n",
      "  num_steps_sampled: 1848000\n",
      "  num_steps_trained: 1848000\n",
      "iterations_since_restore: 462\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.13235294117647\n",
      "  ram_util_percent: 43.76470588235294\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049290512220079796\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511215755498596\n",
      "  mean_inference_ms: 0.8507456653849065\n",
      "  mean_raw_obs_processing_ms: 0.08854521986614346\n",
      "time_since_restore: 2495.1822357177734\n",
      "time_this_iter_s: 5.357791900634766\n",
      "time_total_s: 2495.1822357177734\n",
      "timers:\n",
      "  learn_throughput: 2125.581\n",
      "  learn_time_ms: 1881.838\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.708\n",
      "  sample_time_ms: 3509.672\n",
      "  update_time_ms: 1.677\n",
      "timestamp: 1633515321\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1848000\n",
      "training_iteration: 462\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:462 starting ! -----------------\n",
      "agent_timesteps_total: 1852000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-15-46\n",
      "done: false\n",
      "episode_len_mean: 660.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6031305000000023\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1353\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.175461769104004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014810716733336449\n",
      "        model: {}\n",
      "        policy_loss: -0.027666760608553886\n",
      "        total_loss: -0.010292932391166687\n",
      "        vf_explained_var: 0.0438540056347847\n",
      "        vf_loss: 0.012930620461702347\n",
      "  num_agent_steps_sampled: 1852000\n",
      "  num_agent_steps_trained: 1852000\n",
      "  num_steps_sampled: 1852000\n",
      "  num_steps_trained: 1852000\n",
      "iterations_since_restore: 463\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.194117647058825\n",
      "  ram_util_percent: 43.773529411764706\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049291301943300246\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511207690730828\n",
      "  mean_inference_ms: 0.8507449036551182\n",
      "  mean_raw_obs_processing_ms: 0.0885457358958359\n",
      "time_since_restore: 2500.5240144729614\n",
      "time_this_iter_s: 5.341778755187988\n",
      "time_total_s: 2500.5240144729614\n",
      "timers:\n",
      "  learn_throughput: 2125.37\n",
      "  learn_time_ms: 1882.025\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1143.56\n",
      "  sample_time_ms: 3497.849\n",
      "  update_time_ms: 1.678\n",
      "timestamp: 1633515346\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1852000\n",
      "training_iteration: 463\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:463 starting ! -----------------\n",
      "agent_timesteps_total: 1856000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-16-05\n",
      "done: false\n",
      "episode_len_mean: 662.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6641220000000022\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1356\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0477701425552368\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009825808927416801\n",
      "        model: {}\n",
      "        policy_loss: -0.021645912900567055\n",
      "        total_loss: -0.000394748873077333\n",
      "        vf_explained_var: 0.21822723746299744\n",
      "        vf_loss: 0.01830342225730419\n",
      "  num_agent_steps_sampled: 1856000\n",
      "  num_agent_steps_trained: 1856000\n",
      "  num_steps_sampled: 1856000\n",
      "  num_steps_trained: 1856000\n",
      "iterations_since_restore: 464\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.788888888888888\n",
      "  ram_util_percent: 43.8074074074074\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929271060156948\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.751120775431627\n",
      "  mean_inference_ms: 0.8507441439602466\n",
      "  mean_raw_obs_processing_ms: 0.08854696270297105\n",
      "time_since_restore: 2506.0158925056458\n",
      "time_this_iter_s: 5.491878032684326\n",
      "time_total_s: 2506.0158925056458\n",
      "timers:\n",
      "  learn_throughput: 2118.196\n",
      "  learn_time_ms: 1888.399\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1139.656\n",
      "  sample_time_ms: 3509.831\n",
      "  update_time_ms: 1.678\n",
      "timestamp: 1633515365\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1856000\n",
      "training_iteration: 464\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:464 starting ! -----------------\n",
      "agent_timesteps_total: 1860000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-16-29\n",
      "done: false\n",
      "episode_len_mean: 667.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6636420000000024\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1360\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.098742961883545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011270356364548206\n",
      "        model: {}\n",
      "        policy_loss: -0.023882873356342316\n",
      "        total_loss: -0.0022425255738198757\n",
      "        vf_explained_var: 0.4112991392612457\n",
      "        vf_loss: 0.018259242177009583\n",
      "  num_agent_steps_sampled: 1860000\n",
      "  num_agent_steps_trained: 1860000\n",
      "  num_steps_sampled: 1860000\n",
      "  num_steps_trained: 1860000\n",
      "iterations_since_restore: 465\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.376470588235293\n",
      "  ram_util_percent: 43.79705882352942\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929436160450921\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511200242896001\n",
      "  mean_inference_ms: 0.8507427291517328\n",
      "  mean_raw_obs_processing_ms: 0.08854834892826952\n",
      "time_since_restore: 2511.4941170215607\n",
      "time_this_iter_s: 5.478224515914917\n",
      "time_total_s: 2511.4941170215607\n",
      "timers:\n",
      "  learn_throughput: 2109.049\n",
      "  learn_time_ms: 1896.589\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.315\n",
      "  sample_time_ms: 3507.802\n",
      "  update_time_ms: 1.683\n",
      "timestamp: 1633515389\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1860000\n",
      "training_iteration: 465\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:465 starting ! -----------------\n",
      "agent_timesteps_total: 1864000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-16-43\n",
      "done: false\n",
      "episode_len_mean: 663.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.7144060000000023\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1363\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0405025482177734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01550863403826952\n",
      "        model: {}\n",
      "        policy_loss: -0.03338352218270302\n",
      "        total_loss: -0.010600099340081215\n",
      "        vf_explained_var: 0.2509498596191406\n",
      "        vf_loss: 0.0181308314204216\n",
      "  num_agent_steps_sampled: 1864000\n",
      "  num_agent_steps_trained: 1864000\n",
      "  num_steps_sampled: 1864000\n",
      "  num_steps_trained: 1864000\n",
      "iterations_since_restore: 466\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.778947368421054\n",
      "  ram_util_percent: 43.78421052631578\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049295350272120154\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511210945926438\n",
      "  mean_inference_ms: 0.8507418646689254\n",
      "  mean_raw_obs_processing_ms: 0.0885483039571006\n",
      "time_since_restore: 2516.8695719242096\n",
      "time_this_iter_s: 5.375454902648926\n",
      "time_total_s: 2516.8695719242096\n",
      "timers:\n",
      "  learn_throughput: 2116.311\n",
      "  learn_time_ms: 1890.081\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1140.703\n",
      "  sample_time_ms: 3506.611\n",
      "  update_time_ms: 1.683\n",
      "timestamp: 1633515403\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1864000\n",
      "training_iteration: 466\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:466 starting ! -----------------\n",
      "agent_timesteps_total: 1868000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-16-57\n",
      "done: false\n",
      "episode_len_mean: 652.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6645050000000022\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1367\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.111556887626648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010167797096073627\n",
      "        model: {}\n",
      "        policy_loss: -0.030107375234365463\n",
      "        total_loss: -0.012205512262880802\n",
      "        vf_explained_var: 0.5202525854110718\n",
      "        vf_loss: 0.014851522631943226\n",
      "  num_agent_steps_sampled: 1868000\n",
      "  num_agent_steps_trained: 1868000\n",
      "  num_steps_sampled: 1868000\n",
      "  num_steps_trained: 1868000\n",
      "iterations_since_restore: 467\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.925\n",
      "  ram_util_percent: 43.824999999999996\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929705709561496\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511331493122576\n",
      "  mean_inference_ms: 0.8507418176878849\n",
      "  mean_raw_obs_processing_ms: 0.08854909283297518\n",
      "time_since_restore: 2522.8527686595917\n",
      "time_this_iter_s: 5.98319673538208\n",
      "time_total_s: 2522.8527686595917\n",
      "timers:\n",
      "  learn_throughput: 2083.947\n",
      "  learn_time_ms: 1919.434\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1130.737\n",
      "  sample_time_ms: 3537.517\n",
      "  update_time_ms: 1.682\n",
      "timestamp: 1633515417\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1868000\n",
      "training_iteration: 467\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:467 starting ! -----------------\n",
      "agent_timesteps_total: 1872000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-17-21\n",
      "done: false\n",
      "episode_len_mean: 650.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.6248510000000023\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1369\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0483821630477905\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015337626449763775\n",
      "        model: {}\n",
      "        policy_loss: -0.02863774448633194\n",
      "        total_loss: -0.008550036698579788\n",
      "        vf_explained_var: -0.05309423804283142\n",
      "        vf_loss: 0.015486420132219791\n",
      "  num_agent_steps_sampled: 1872000\n",
      "  num_agent_steps_trained: 1872000\n",
      "  num_steps_sampled: 1872000\n",
      "  num_steps_trained: 1872000\n",
      "iterations_since_restore: 468\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.035294117647062\n",
      "  ram_util_percent: 43.89117647058824\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929801386840756\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511384754391899\n",
      "  mean_inference_ms: 0.850742418555222\n",
      "  mean_raw_obs_processing_ms: 0.08855014216577142\n",
      "time_since_restore: 2528.2609264850616\n",
      "time_this_iter_s: 5.408157825469971\n",
      "time_total_s: 2528.2609264850616\n",
      "timers:\n",
      "  learn_throughput: 2088.41\n",
      "  learn_time_ms: 1915.332\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1133.001\n",
      "  sample_time_ms: 3530.447\n",
      "  update_time_ms: 1.771\n",
      "timestamp: 1633515441\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1872000\n",
      "training_iteration: 468\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:468 starting ! -----------------\n",
      "agent_timesteps_total: 1876000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-17-38\n",
      "done: false\n",
      "episode_len_mean: 660.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5749330000000021\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1373\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0190590620040894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00928863137960434\n",
      "        model: {}\n",
      "        policy_loss: -0.021810153499245644\n",
      "        total_loss: 0.009891674853861332\n",
      "        vf_explained_var: -0.039781518280506134\n",
      "        vf_loss: 0.02891523949801922\n",
      "  num_agent_steps_sampled: 1876000\n",
      "  num_agent_steps_trained: 1876000\n",
      "  num_steps_sampled: 1876000\n",
      "  num_steps_trained: 1876000\n",
      "iterations_since_restore: 469\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.995833333333337\n",
      "  ram_util_percent: 43.82916666666667\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04929979925567235\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511468931063952\n",
      "  mean_inference_ms: 0.8507431255962183\n",
      "  mean_raw_obs_processing_ms: 0.08855321495502885\n",
      "time_since_restore: 2533.6648869514465\n",
      "time_this_iter_s: 5.403960466384888\n",
      "time_total_s: 2533.6648869514465\n",
      "timers:\n",
      "  learn_throughput: 2079.929\n",
      "  learn_time_ms: 1923.142\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1132.092\n",
      "  sample_time_ms: 3533.282\n",
      "  update_time_ms: 1.692\n",
      "timestamp: 1633515458\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1876000\n",
      "training_iteration: 469\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:469 starting ! -----------------\n",
      "agent_timesteps_total: 1880000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-17-52\n",
      "done: false\n",
      "episode_len_mean: 672.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.47452150000000226\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1375\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1369051933288574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014874218963086605\n",
      "        model: {}\n",
      "        policy_loss: -0.0265559870749712\n",
      "        total_loss: 0.004212353378534317\n",
      "        vf_explained_var: 0.2100318968296051\n",
      "        vf_loss: 0.026306072250008583\n",
      "  num_agent_steps_sampled: 1880000\n",
      "  num_agent_steps_trained: 1880000\n",
      "  num_steps_sampled: 1880000\n",
      "  num_steps_trained: 1880000\n",
      "iterations_since_restore: 470\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.335\n",
      "  ram_util_percent: 43.88999999999999\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049301099503919546\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511517348743397\n",
      "  mean_inference_ms: 0.8507443309226396\n",
      "  mean_raw_obs_processing_ms: 0.08855483532044164\n",
      "time_since_restore: 2539.090329170227\n",
      "time_this_iter_s: 5.425442218780518\n",
      "time_total_s: 2539.090329170227\n",
      "timers:\n",
      "  learn_throughput: 2084.879\n",
      "  learn_time_ms: 1918.577\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1134.535\n",
      "  sample_time_ms: 3525.674\n",
      "  update_time_ms: 1.592\n",
      "timestamp: 1633515472\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1880000\n",
      "training_iteration: 470\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Evaluation at steps:470 starting ! -----------------\n",
      "agent_timesteps_total: 1884000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-18-16\n",
      "done: false\n",
      "episode_len_mean: 671.39\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.47430550000000227\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 1377\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0871529579162598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01084133516997099\n",
      "        model: {}\n",
      "        policy_loss: -0.03509129211306572\n",
      "        total_loss: -0.015090832486748695\n",
      "        vf_explained_var: 0.06174226105213165\n",
      "        vf_loss: 0.016748061403632164\n",
      "  num_agent_steps_sampled: 1884000\n",
      "  num_agent_steps_trained: 1884000\n",
      "  num_steps_sampled: 1884000\n",
      "  num_steps_trained: 1884000\n",
      "iterations_since_restore: 471\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.33823529411765\n",
      "  ram_util_percent: 43.86176470588236\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.049302255247343094\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511565456481958\n",
      "  mean_inference_ms: 0.8507454481321204\n",
      "  mean_raw_obs_processing_ms: 0.08855613264257865\n",
      "time_since_restore: 2544.443796634674\n",
      "time_this_iter_s: 5.3534674644470215\n",
      "time_total_s: 2544.443796634674\n",
      "timers:\n",
      "  learn_throughput: 2085.725\n",
      "  learn_time_ms: 1917.798\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1131.0\n",
      "  sample_time_ms: 3536.694\n",
      "  update_time_ms: 1.581\n",
      "timestamp: 1633515496\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1884000\n",
      "training_iteration: 471\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:471 starting ! -----------------\n",
      "agent_timesteps_total: 1888000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-18-40\n",
      "done: false\n",
      "episode_len_mean: 663.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.4753535000000021\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1381\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.151949405670166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0117721538990736\n",
      "        model: {}\n",
      "        policy_loss: -0.033039454370737076\n",
      "        total_loss: -0.013534599915146828\n",
      "        vf_explained_var: 0.12468511611223221\n",
      "        vf_loss: 0.015973202884197235\n",
      "  num_agent_steps_sampled: 1888000\n",
      "  num_agent_steps_trained: 1888000\n",
      "  num_steps_sampled: 1888000\n",
      "  num_steps_trained: 1888000\n",
      "iterations_since_restore: 472\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.423529411764704\n",
      "  ram_util_percent: 43.82647058823529\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0493049767968491\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511698405230547\n",
      "  mean_inference_ms: 0.8507483457396795\n",
      "  mean_raw_obs_processing_ms: 0.08855912867161696\n",
      "time_since_restore: 2549.893800973892\n",
      "time_this_iter_s: 5.45000433921814\n",
      "time_total_s: 2549.893800973892\n",
      "timers:\n",
      "  learn_throughput: 2083.431\n",
      "  learn_time_ms: 1919.91\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.738\n",
      "  sample_time_ms: 3543.779\n",
      "  update_time_ms: 1.581\n",
      "timestamp: 1633515520\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1888000\n",
      "training_iteration: 472\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:472 starting ! -----------------\n",
      "agent_timesteps_total: 1892000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-18-56\n",
      "done: false\n",
      "episode_len_mean: 647.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5354010000000022\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 1385\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0647763013839722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012180913239717484\n",
      "        model: {}\n",
      "        policy_loss: -0.025320006534457207\n",
      "        total_loss: -0.007348629646003246\n",
      "        vf_explained_var: 0.14855128526687622\n",
      "        vf_loss: 0.01431710459291935\n",
      "  num_agent_steps_sampled: 1892000\n",
      "  num_agent_steps_trained: 1892000\n",
      "  num_steps_sampled: 1892000\n",
      "  num_steps_trained: 1892000\n",
      "iterations_since_restore: 473\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.928571428571427\n",
      "  ram_util_percent: 43.83333333333332\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04930701963422357\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511841541207019\n",
      "  mean_inference_ms: 0.850749957540387\n",
      "  mean_raw_obs_processing_ms: 0.08856211529556482\n",
      "time_since_restore: 2555.327018260956\n",
      "time_this_iter_s: 5.433217287063599\n",
      "time_total_s: 2555.327018260956\n",
      "timers:\n",
      "  learn_throughput: 2083.978\n",
      "  learn_time_ms: 1919.406\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1125.685\n",
      "  sample_time_ms: 3553.391\n",
      "  update_time_ms: 1.58\n",
      "timestamp: 1633515536\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1892000\n",
      "training_iteration: 473\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:473 starting ! -----------------\n",
      "agent_timesteps_total: 1896000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-06_19-19-09\n",
      "done: false\n",
      "episode_len_mean: 649.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 3.9159000000000006\n",
      "episode_reward_mean: 0.5453870000000022\n",
      "episode_reward_min: -2.2007499999999993\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 1388\n",
      "experiment_id: 0258895a28d64448a57f5261e765e07b\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9745769500732422\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011021283455193043\n",
      "        model: {}\n",
      "        policy_loss: -0.03406557813286781\n",
      "        total_loss: -0.008478418923914433\n",
      "        vf_explained_var: 0.23419883847236633\n",
      "        vf_loss: 0.022280769422650337\n",
      "  num_agent_steps_sampled: 1896000\n",
      "  num_agent_steps_trained: 1896000\n",
      "  num_steps_sampled: 1896000\n",
      "  num_steps_trained: 1896000\n",
      "iterations_since_restore: 474\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.44736842105263\n",
      "  ram_util_percent: 43.80526315789473\n",
      "pid: 1280\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04930822175465108\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.7511909976686774\n",
      "  mean_inference_ms: 0.8507499340561294\n",
      "  mean_raw_obs_processing_ms: 0.08856331512635748\n",
      "time_since_restore: 2560.8272082805634\n",
      "time_this_iter_s: 5.500190019607544\n",
      "time_total_s: 2560.8272082805634\n",
      "timers:\n",
      "  learn_throughput: 2074.271\n",
      "  learn_time_ms: 1928.388\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 1128.273\n",
      "  sample_time_ms: 3545.243\n",
      "  update_time_ms: 1.779\n",
      "timestamp: 1633515549\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1896000\n",
      "training_iteration: 474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training & evaluation\n",
    "record_mode = 1\n",
    "results_dir = os.path.join('./' + PROJECT + '/results/')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "results_file = results_dir + TRIAL + '.pkl'\n",
    "for steps in range(10001):\n",
    "    # Training\n",
    "    results = trainer.train()\n",
    "\n",
    "    # Evaluation\n",
    "    if steps % EVAL_FREQ == 0:\n",
    "        print(f'\\n----------------- Evaluation at steps:{steps} starting ! -----------------')\n",
    "        print(pretty_print(results))\n",
    "        check_point = trainer.save(checkpoint_dir=check_point_dir)\n",
    "        win = 0\n",
    "        for i in range(NUM_EVAL):\n",
    "            # print(f'\\nEvaluation {i}:')\n",
    "            obs = eval_env.reset()\n",
    "            done = False\n",
    "            \n",
    "            step_num = 0\n",
    "            trajectory_length = 100\n",
    "            env_blue_pos = [0]\n",
    "            env_red_pos = [0]\n",
    "            env_mrm_pos = [0]\n",
    "            if record_mode == 0:\n",
    "                file_name = \"test_num\" + str(steps) +str(i)\n",
    "                video = cv2.VideoWriter(file_name+'.mp4',0x00000020,20.0,(eval_env.WINNDOW_SIZE_lon,eval_env.WINDOW_SIZE_lat))\n",
    "\n",
    "            while not done:\n",
    "                action_dict = {}\n",
    "                for j in range(eval_env.blue_num):\n",
    "                    if not eval_env.blue[j].hitpoint == 0:\n",
    "                        action_dict['blue_' + str(j)] = trainer.compute_action(obs['blue_' + str(j)])\n",
    "\n",
    "                obs, rewards, dones, infos = eval_env.step(action_dict)\n",
    "                done = dones[\"__all__\"]\n",
    "                #print(f'rewards:{rewards}')\n",
    "                if record_mode == 0:\n",
    "                    img = eval_env.render_movie(file_name,step_num)\n",
    "                    video.write(img.astype('unit8'))\n",
    "                elif record_mode == 1:\n",
    "                    eval_env.render()\n",
    "                elif record_mode == 2:\n",
    "                    eval_env.render()\n",
    "                    \n",
    "                #env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp = render_env.copy_from_env(eval_env)\n",
    "                \n",
    "                #env_blue_pos.append(env_blue_pos_temp)\n",
    "                #env_red_pos.append(env_red_pos_temp)\n",
    "                #env_mrm_pos.append(env_mrm_pos_temp)\n",
    "                #step_num = step_num + 1\n",
    "                \n",
    "                \n",
    "            #del env_blue_pos[0]\n",
    "            #del env_red_pos[0]\n",
    "            #del env_mrm_pos[0]\n",
    "            \n",
    "            #hist_blue_pos = np.vstack(env_blue_pos)\n",
    "            #hist_red_pos = np.vstack(env_red_pos)\n",
    "            #hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "            \n",
    "            #f = open(results_file,'wb')\n",
    "            #pickle.dump(emv_blue_pos,f)\n",
    "            #pickle.dump(emv_red_pos,f)\n",
    "            #pickle.dump(emv_mrm_pos,f)\n",
    "            #f.close()\n",
    "            \n",
    "            if record_mode == 0:\n",
    "                video.release()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be88191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd83be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
