{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "import numpy as np\n",
    "from environment_rllib import MyEnv\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "config = {\"env\": MyEnv,\"num_gpus\": 0,\"num_workers\": 1, \"num_cpus_per_worker\": 0}\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "trainer = PPOTrainer(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 05:14:40,378\tWARNING deprecation.py:39 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-03_05-14-42\n",
      "done: false\n",
      "episode_len_mean: 1002.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -0.3200573076614543\n",
      "episode_reward_mean: -0.3200573076614543\n",
      "episode_reward_min: -0.3200573076614543\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1\n",
      "experiment_id: 27697d55e8e1438b9c85392168f869b4\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.070223808288574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009243628941476345\n",
      "        model: {}\n",
      "        policy_loss: -0.0030010284390300512\n",
      "        total_loss: -0.0010332720121368766\n",
      "        vf_explained_var: -0.27432334423065186\n",
      "        vf_loss: 0.00011902878031833097\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.472727272727273\n",
      "  ram_util_percent: 43.312727272727294\n",
      "pid: 1728\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06006706481811584\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 17.151504084802998\n",
      "  mean_inference_ms: 1.0447576009053579\n",
      "  mean_raw_obs_processing_ms: 0.10817864726389247\n",
      "time_since_restore: 38.92024612426758\n",
      "time_this_iter_s: 38.92024612426758\n",
      "time_total_s: 38.92024612426758\n",
      "timers:\n",
      "  learn_throughput: 1887.869\n",
      "  learn_time_ms: 2118.791\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 108.712\n",
      "  sample_time_ms: 36794.421\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633205682\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "agent_timesteps_total: 8000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-03_05-15-20\n",
      "done: false\n",
      "episode_len_mean: 626.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.0460133027514418\n",
      "episode_reward_mean: 0.16684235725722726\n",
      "episode_reward_min: -0.4161521295244961\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 6\n",
      "experiment_id: 27697d55e8e1438b9c85392168f869b4\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0662286281585693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0045729828998446465\n",
      "        model: {}\n",
      "        policy_loss: -0.010944475419819355\n",
      "        total_loss: -0.002367445733398199\n",
      "        vf_explained_var: -0.7701210975646973\n",
      "        vf_loss: 0.007662429008632898\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.36481481481481\n",
      "  ram_util_percent: 43.35000000000001\n",
      "pid: 1728\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06142034497735943\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 17.11447489772918\n",
      "  mean_inference_ms: 1.039967629578549\n",
      "  mean_raw_obs_processing_ms: 0.11422604991196361\n",
      "time_since_restore: 77.36164021492004\n",
      "time_this_iter_s: 38.441394090652466\n",
      "time_total_s: 77.36164021492004\n",
      "timers:\n",
      "  learn_throughput: 2031.484\n",
      "  learn_time_ms: 1969.004\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 108.988\n",
      "  sample_time_ms: 36701.255\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633205720\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "custom_metrics: {}\n",
      "date: 2021-10-03_05-15-59\n",
      "done: false\n",
      "episode_len_mean: 633.5555555555555\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.0460133027514418\n",
      "episode_reward_mean: 0.05855815385074939\n",
      "episode_reward_min: -0.4161521295244961\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 9\n",
      "experiment_id: 27697d55e8e1438b9c85392168f869b4\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.046156406402588\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010305476374924183\n",
      "        model: {}\n",
      "        policy_loss: -0.011063403449952602\n",
      "        total_loss: -0.008746681734919548\n",
      "        vf_explained_var: -0.3615330159664154\n",
      "        vf_loss: 0.0012861728901043534\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.0.196\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.223636363636363\n",
      "  ram_util_percent: 43.25636363636362\n",
      "pid: 1728\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06155844304514684\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 17.116610713429438\n",
      "  mean_inference_ms: 1.0406539186827062\n",
      "  mean_raw_obs_processing_ms: 0.11475808810169978\n",
      "time_since_restore: 116.07171249389648\n",
      "time_this_iter_s: 38.71007227897644\n",
      "time_total_s: 116.07171249389648\n",
      "timers:\n",
      "  learn_throughput: 2049.356\n",
      "  learn_time_ms: 1951.832\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 108.904\n",
      "  sample_time_ms: 36729.694\n",
      "  update_time_ms: 1.995\n",
      "timestamp: 1633205759\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "for _ in range(100):\n",
    "    r = trainer.train()\n",
    "    print(pretty_print(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Takumi/ray_results\\\\PPO_Knapsack_2021-10-01_00-09-29xds6vvpc\\\\checkpoint_000002\\\\checkpoint-2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = trainer.save()\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 00:10:11,547\tINFO trainable.py:383 -- Restored on 192.168.0.196 from checkpoint: C:\\Users\\Takumi/ray_results\\PPO_Knapsack_2021-10-01_00-09-29xds6vvpc\\checkpoint_000002\\checkpoint-2\n",
      "2021-10-01 00:10:11,547\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 8.515936851501465, '_episodes_total': 800}\n",
      "\u001b[2m\u001b[36m(pid=12860)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=12860)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=14992)\u001b[0m Windows fatal exception: access violation\n",
      "\u001b[2m\u001b[36m(pid=14992)\u001b[0m \n",
      "2021-10-01 00:13:28,733\tWARNING worker.py:1215 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffeffc77096cc3cea7d17ef79e01000000 Worker ID: ac4b781d5f2215f4aa4b09fbfa56a3ca3b13d398363937efd9314c86 Node ID: 4cb013c27e987b4844903d0375265fed25d8109544c3dd23fd7159f6 Worker IP address: 192.168.0.196 Worker port: 54235 Worker PID: 17600\n",
      "2021-10-01 00:13:28,827\tWARNING worker.py:1215 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff50ac29e090d075b9ff98605001000000 Worker ID: ed050994e95fbfeae68705c9fb8b7bc3e8ef0d525e45152ebe70e1d6 Node ID: 4cb013c27e987b4844903d0375265fed25d8109544c3dd23fd7159f6 Worker IP address: 192.168.0.196 Worker port: 54260 Worker PID: 14444\n",
      "2021-10-01 00:13:28,889\tWARNING worker.py:1215 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff9b3480dbf7d360a4f71025cc01000000 Worker ID: dfceb34f9ca5cc7a4cdad36452f1e4e007cca349031bd7a6b6137c66 Node ID: 4cb013c27e987b4844903d0375265fed25d8109544c3dd23fd7159f6 Worker IP address: 192.168.0.196 Worker port: 54250 Worker PID: 5356\n",
      "2021-10-01 00:13:28,950\tWARNING worker.py:1215 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff0c636bffcd258eb8052cb9401000000 Worker ID: e46d1965335c68b0c02986c9747fdf078d8c6a1e7f53a1e3320b03ab Node ID: 4cb013c27e987b4844903d0375265fed25d8109544c3dd23fd7159f6 Worker IP address: 192.168.0.196 Worker port: 54233 Worker PID: 17280\n",
      "2021-10-01 00:13:30,461\tERROR worker.py:475 -- print_logs: Error while reading from socket: (10054, '既存の接続はリモート ホストに強制的に切断されました。', None, 10054, None)\n",
      "2021-10-01 00:13:30,462\tERROR worker.py:1217 -- listen_error_messages_raylet: Error while reading from socket: (10054, '既存の接続はリモート ホストに強制的に切断されました。', None, 10054, None)\n",
      "2021-10-01 00:13:30,463\tERROR import_thread.py:88 -- ImportThread: Error while reading from socket: (10054, '既存の接続はリモート ホストに強制的に切断されました。', None, 10054, None)\n"
     ]
    }
   ],
   "source": [
    "trainer.restore(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
