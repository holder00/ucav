{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6504bb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T-OHKI-MSI\\miniconda3\\envs\\AI2\\lib\\site-packages\\quaternion\\numba_wrapper.py:23: UserWarning: \n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Could not import from numba, which means that some\n",
      "parts of this code may run MUCH more slowly.  You\n",
      "may wish to install numba.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "  warnings.warn(warning_text)\n",
      "2021-12-29 00:19:11,035\tINFO trainer.py:723 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "2021-12-29 00:19:11,036\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2021-12-29 00:19:11,037\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-12-29 00:19:12,457\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2021-12-29 00:19:17,671\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import Logger, UnifiedLogger, pretty_print\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.examples.models.shared_weights_model import TF2SharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.agents.ppo import ppo, PPOTrainer, PPOTFPolicy\n",
    "from ray.rllib.agents.a3c.a3c_tf_policy import A3CTFPolicy\n",
    "from ray.rllib.agents.a3c import a3c\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from environment_rllib_3d import MyEnv\n",
    "from settings.initial_settings import *\n",
    "from settings.reset_conditions import reset_conditions\n",
    "#from modules.models import MyConv2DModel_v0B_Small_CBAM_1DConv_Share\n",
    "from modules.models import DenseNetModelLarge\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from modules.savers import save_conditions\n",
    "from utility.result_env import render_env\n",
    "from utility.terminate_uavsimproc import teminate_proc\n",
    "from utility.latest_learned_file_path import latest_learned_file_path\n",
    "from utility.save_logs import save_logs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import ctypes\n",
    "import warnings\n",
    "\n",
    "#UCAV.exeが起動している場合、プロセスキルする。\n",
    "teminate_proc.UAVsimprockill(proc_name=\"UCAV.exe\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.filterwarnings('ignore', category=matplotlib.MatplotlibDeprecationWarning)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "PROJECT = \"UCAV\"\n",
    "TRIAL_ID = 2\n",
    "TRIAL = 'test_' + str(TRIAL_ID)\n",
    "EVAL_FREQ = 10\n",
    "CONTINUAL = False\n",
    "NUM_EVAL = 1\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "    timestr = datetime.datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "\n",
    "    def logger_creator(config):\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "ModelCatalog.register_custom_model('my_model', DenseNetModelLarge)\n",
    "\n",
    "# config = {\"env\": MyEnv,\n",
    "#           \"num_workers\": NUM_WORKERS,\n",
    "#           \"num_gpus\": NUM_GPUS,\n",
    "#           \"num_cpus_per_worker\": NUM_CPUS_PER_WORKER,\n",
    "#           \"num_sgd_iter\": NUM_SGD_ITER,\n",
    "#           \"lr\": LEARNING_RATE,\n",
    "#           \"gamma\": GAMMA,  # default=0.99\n",
    "#           \"model\": {\"custom_model\": \"my_model\"}\n",
    "#           # \"framework\": framework\n",
    "#           }  # use tensorflow 2\n",
    "eval_env = MyEnv({})\n",
    "policies = {\n",
    "    #\"blue_1\": PolicySpec(config={\"gamma\": 0.99}),\n",
    "    #\"blue_2\": PolicySpec(config={\"gamma\": 0.95}),\n",
    "    \"blue_0\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "    \"blue_1\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "    #\"blue_0\": (A3CTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "    #\"blue_1\": (A3CTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "}\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    #print(agent_id,episode)\n",
    "    #pol_id = policy_ids[agent_id]\n",
    "\n",
    "    pol_id = agent_id\n",
    "    return pol_id\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "\n",
    "config = {\"env\": MyEnv,\"num_gpus\": 0,\"num_workers\": 0, \"num_cpus_per_worker\": 0,\"num_gpus_per_worker\": 0,\n",
    "          \"create_env_on_driver\": True,\"train_batch_size\": 600*5,\"batch_mode\": \"complete_episodes\",\n",
    "          \"shuffle_sequences\": True, \"gamma\":0.999, \"lr\": 1e-5,\n",
    "          \"clip_actions\":True,\"normalize_actions\":False,\n",
    "          \"observation_space\":eval_env.observation_space,\"action_space\":eval_env.action_space,\n",
    "          \"explore\":True,\n",
    "          \"sgd_minibatch_size\": 200, \"num_sgd_iter\":20,\n",
    "          \"exploration_config\": {\"type\": \"StochasticSampling\",\"random_timesteps\":60000}, #PPO デフォルト\n",
    "          \"model\":{\"fcnet_activation\": \"tanh\",\"fcnet_hiddens\": [256, 256, 256],\"post_fcnet_activation\": \"linear\",},#\"linear\",\"relu\",\"tanh\" \n",
    "          #\"model\":{\"fcnet_hiddens\": [256, 256]},\n",
    "          \"multiagent\": {\"policies\": policies,  \"policy_mapping_fn\": policy_mapping_fn}\n",
    "         }\n",
    "res_name = \"sgd\"+str(config[\"sgd_minibatch_size\"])+\"sgd_num\"+str(config[\"num_sgd_iter\"])+\"lr\"+str(config[\"lr\"])+\"gamma\"+str(config[\"gamma\"])\n",
    "res_name = \"test\"\n",
    "conditions_dir = os.path.join('./' + PROJECT + '/conditions/')\n",
    "\n",
    "if not os.path.exists(conditions_dir):\n",
    "    os.makedirs(conditions_dir)\n",
    "save_conditions(conditions_dir)\n",
    "\n",
    "# PPOTrainer()は、try_import_tfを使うと、なぜかTensorflowのeager modeのエラーになる。\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config,\n",
    "                         logger_creator=custom_log_creator(\n",
    "                             os.path.expanduser(\"./\" + PROJECT + \"/logs\"), TRIAL))\n",
    "\n",
    "if CONTINUAL:\n",
    "    # Continual learning: Need to specify the checkpoint\n",
    "    # model_path = PROJECT + '/checkpoints/' + TRIAL + '/checkpoint_000197/checkpoint-197'\n",
    "    model_path = latest_learned_file_path('./UCAV/checkpoints/test_2/*')\n",
    "    trainer.restore(checkpoint_path=model_path)\n",
    "\n",
    "# models_dir = os.path.join('./' + PROJECT + '/models/')\n",
    "# if not os.path.exists(models_dir):\n",
    "#     os.makedirs(models_dir)\n",
    "# text_name = models_dir + TRIAL + '.txt'\n",
    "# with open(text_name, \"w\") as fp:\n",
    "#     trainer.get_policy().model.base_model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))\n",
    "# png_name = models_dir + TRIAL + '.png'\n",
    "# plot_model(trainer.get_policy().model.base_model, to_file=png_name, show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define checkpoint dir\n",
    "check_point_dir = os.path.join('./' + PROJECT + '/checkpoints/', TRIAL)\n",
    "if not os.path.exists(check_point_dir):\n",
    "    os.makedirs(check_point_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebbfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Training at steps:0 start! -----------------\n",
      "235 blue_0 DOWN\n",
      "361 blue_1 Shoot at red_1 launch distance : 99575.63696507293 True True\n",
      "363 blue_1 Shoot at red_0 launch distance : 99708.92274014397 True True\n",
      "467 blue_1 DOWN\n",
      "DOWN LOSE\n",
      "blue_0 True True 467 0.0 -10.118\n",
      "blue_1 True True 467 -10.0 -9.574353501999997\n",
      "295 blue_0 Shoot at red_1 launch distance : 99518.16876329668 True True\n",
      "Same tgt shoot\n",
      "317 blue_0 Shoot at red_1 launch distance : 98435.97444532157 True True\n",
      "364 blue_1 DOWN\n",
      "486 red_0 Shoot at blue_0\n",
      "606 red_0 Splash :blue_0\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False True 606 -9.999 -9.909013958\n",
      "blue_1 False True 606 0.0 -10.173\n",
      "316 blue_0 Shoot at red_0 launch distance : 97357.220759428 True True\n",
      "Same tgt shoot\n",
      "318 blue_0 Shoot at red_0 launch distance : 96994.39491537643 True True\n",
      "348 blue_1 DOWN\n",
      "545 red_1 Shoot at blue_0\n",
      "662 red_1 Splash :blue_0\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False True 662 -9.999 -8.196682158000007\n",
      "blue_1 False True 662 0.0 -10.145\n",
      "199 blue_0 DOWN\n",
      "387 blue_1 Shoot at red_0 launch distance : 99962.0360186806 True True\n",
      "Same tgt shoot\n",
      "389 blue_1 Shoot at red_0 launch distance : 99552.09587447168 True True\n",
      "488 blue_1 DOWN\n",
      "DOWN LOSE\n",
      "blue_0 True True 488 0.0 -10.098\n",
      "blue_1 True True 488 -10.0 -11.535056876000002\n",
      "362 blue_1 Shoot at red_1 launch distance : 99382.40612905285 True True\n",
      "Same tgt shoot\n",
      "365 blue_1 Shoot at red_1 launch distance : 98285.82592113677 True True\n",
      "374 blue_0 Shoot at red_0 launch distance : 99802.11578418566 True True\n",
      "Same tgt shoot\n",
      "380 blue_0 Shoot at red_0 launch distance : 98496.52474072372 True True\n",
      "663 blue_0 DOWN\n",
      "730 blue_1 DOWN\n",
      "DOWN LOSE\n",
      "blue_0 True True 730 0.0 -11.52305540199999\n",
      "blue_1 True True 730 -10.0 -11.220932449999987\n",
      "329 blue_0 Shoot at red_0 launch distance : 99230.29076849468 True True\n",
      "Same tgt shoot\n",
      "330 blue_0 Shoot at red_0 launch distance : 98958.38686538902 True True\n",
      "367 blue_0 DOWN\n",
      "388 blue_1 Shoot at red_1 launch distance : 93904.69056442282 True True\n",
      "Same tgt shoot\n",
      "390 blue_1 Shoot at red_1 launch distance : 93278.99949077498 True True\n",
      "596 blue_1 DOWN\n",
      "DOWN LOSE\n",
      "blue_0 True True 596 0.0 -11.151825874000004\n",
      "blue_1 True True 596 -10.0 -6.957125311999993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 00:27:12,052\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 7098\n",
      "custom_metrics: {}\n",
      "date: 2021-12-29_00-27-16\n",
      "done: false\n",
      "episode_len_mean: 591.5\n",
      "episode_media: {}\n",
      "episode_reward_max: -18.108951185999942\n",
      "episode_reward_mean: -20.10034092199997\n",
      "episode_reward_min: -22.743987851999968\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 6\n",
      "experiment_id: afe033145d9a4282970609e6b69c87fc\n",
      "hostname: MSI\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 9.999999747378752e-06\n",
      "        entropy: 5.622260093688965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00026400090428069234\n",
      "        model: {}\n",
      "        policy_loss: 0.30267173051834106\n",
      "        total_loss: 47.190155029296875\n",
      "        vf_explained_var: -0.058177296072244644\n",
      "        vf_loss: 46.88743209838867\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 9.999999747378752e-06\n",
      "        entropy: 5.62200403213501\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00039592411485500634\n",
      "        model: {}\n",
      "        policy_loss: 0.2621181309223175\n",
      "        total_loss: 47.776824951171875\n",
      "        vf_explained_var: -0.0031816293485462666\n",
      "        vf_loss: 47.514625549316406\n",
      "  num_agent_steps_sampled: 7098\n",
      "  num_agent_steps_trained: 7098\n",
      "  num_steps_sampled: 3549\n",
      "  num_steps_trained: 3549\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.83844984802431\n",
      "  ram_util_percent: 64.59012158054712\n",
      "pid: 2308\n",
      "policy_reward_max:\n",
      "  blue_0: -8.196682158000007\n",
      "  blue_1: -6.957125311999993\n",
      "policy_reward_mean:\n",
      "  blue_0: -10.166096232\n",
      "  blue_1: -9.934244689999996\n",
      "policy_reward_min:\n",
      "  blue_0: -11.52305540199999\n",
      "  blue_1: -11.535056876000002\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.49628969649193994\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 122.49914122299411\n",
      "  mean_inference_ms: 3.5602390262442576\n",
      "  mean_raw_obs_processing_ms: 6.218563805163747\n",
      "time_since_restore: 475.7711572647095\n",
      "time_this_iter_s: 475.7711572647095\n",
      "time_total_s: 475.7711572647095\n",
      "timers:\n",
      "  learn_throughput: 821.972\n",
      "  learn_time_ms: 4317.667\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_throughput: 7.481\n",
      "  sample_time_ms: 474381.177\n",
      "timestamp: 1640705236\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3549\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:0 starting ! -----------------\n",
      "291 red_1 Shoot at blue_0\n",
      "316 red_0 Shoot at blue_0\n",
      "369 red_1 Splash :blue_0\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "493 red_1 Shoot at blue_1\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "Too Close Finish\n",
      "504 red_1 Splash :blue_1\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False True 504 0.01 1.8169999999999933\n",
      "blue_1 False True 504 -9.989 -7.33800000000001\n",
      "\n",
      "----------------- Training at steps:1 start! -----------------\n"
     ]
    }
   ],
   "source": [
    "#def getkey(key):\n",
    "    # return 111\n",
    "#    return(bool(ctypes.windll.user32.GetAsyncKeyState(key) & 0x8000))\n",
    "# Training & evaluation\n",
    "\n",
    "record_mode = 0\n",
    "results_dir = os.path.join('./' + PROJECT + '/results/')\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "results_file = results_dir + TRIAL + '.pkl'\n",
    "for steps in range(10001):\n",
    "    # Training\n",
    "    print(f'\\n----------------- Training at steps:{steps} start! -----------------')\n",
    "    eval_env.reset()\n",
    "    results = trainer.train()\n",
    "    save_logs(res_name,results,steps,CONTINUAL)\n",
    "    print(pretty_print(results))\n",
    "    #check_point = trainer.save(checkpoint_dir=check_point_dir)\n",
    "    # Evaluation\n",
    "    if steps % EVAL_FREQ == 0:\n",
    "        print(f'\\n----------------- Evaluation at steps:{steps} starting ! -----------------')\n",
    "        #print(pretty_print(results))\n",
    "        check_point = trainer.save(checkpoint_dir=check_point_dir)\n",
    "        win = 0\n",
    "        for i in range(NUM_EVAL):\n",
    "            # print(f'\\nEvaluation {i}:')\n",
    "            obs = eval_env.reset()\n",
    "            done = False\n",
    "            \n",
    "            step_num = 0\n",
    "            fig = plt.figure(1,figsize=(8.0, 6.0))\n",
    "            ESC = 0x1B          # ESCキーの仮想キーコード\n",
    "            trajectory_length = 100\n",
    "            env_blue_pos = [0]\n",
    "            env_red_pos = [0]\n",
    "            env_mrm_pos = [0]\n",
    "            if record_mode == 0:\n",
    "                file_name = \"test_num\" + str(steps) +str(i)\n",
    "                video = cv2.VideoWriter(file_name+'.mp4',0x00000020,20.0,(800,600))\n",
    "\n",
    "            while True:\n",
    "                action_dict = {}\n",
    "                for j in range(eval_env.blue_num):\n",
    "                    #if not eval_env.blue[j].hitpoint == 0:\n",
    "                    #action_dict['blue_' + str(j)] = trainer.compute_action(obs['blue_' + str(j)])\n",
    "                    action_dict['blue_' + str(j)] = trainer.compute_single_action(obs['blue_' + str(j)],policy_id='blue_' + str(j),\n",
    "                                                                       clip_action=True,explore=False)\n",
    "                obs, rewards, dones, infos = eval_env.step(action_dict)\n",
    "                env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp= render_env.copy_from_env(eval_env)\n",
    "                env_blue_pos.append(env_blue_pos_temp)\n",
    "                env_red_pos.append(env_red_pos_temp)\n",
    "                env_mrm_pos.append(env_mrm_pos_temp)\n",
    "                if step_num == 0:\n",
    "                    del env_blue_pos[0]\n",
    "                    del env_red_pos[0]\n",
    "                    del env_mrm_pos[0]\n",
    "\n",
    "                hist_blue_pos = np.vstack(env_blue_pos)\n",
    "                hist_red_pos = np.vstack(env_red_pos)\n",
    "                hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "                plt.clf()\n",
    "                render_env.rend_3d(eval_env,hist_blue_pos,\"b\",1)\n",
    "                render_env.rend_3d(eval_env,hist_red_pos,\"r\",1)\n",
    "                render_env.rend_3d(eval_env,hist_mrm_pos,\"k\",1)\n",
    "                fig.canvas.draw()\n",
    "                plt.pause(.01)\n",
    "                if record_mode == 0:\n",
    "                    img = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)\n",
    "                    # cv2.imshow('test', img)\n",
    "                    # cv2.waitKey(1)\n",
    "                    # cv2.destroyAllWindows()\n",
    "                    video.write(img.astype('uint8'))\n",
    "\n",
    "                \n",
    "                step_num = step_num + 1\n",
    "                \n",
    "                done = dones[\"__all__\"]\n",
    "                #print(f'rewards:{rewards}')\n",
    "                #if record_mode == 0:\n",
    "                #    img = eval_env.render_movie(file_name,step_num)\n",
    "                #    video.write(img.astype('unit8'))\n",
    "                #elif record_mode == 1:\n",
    "                #    eval_env.render()\n",
    "                #elif record_mode == 2:\n",
    "                #    eval_env.render()\n",
    "                    \n",
    "                #env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp = render_env.copy_from_env(eval_env)\n",
    "                \n",
    "                #env_blue_pos.append(env_blue_pos_temp)\n",
    "                #env_red_pos.append(env_red_pos_temp)\n",
    "                #env_mrm_pos.append(env_mrm_pos_temp)\n",
    "                #step_num = step_num + 1\n",
    "                # エピソードの終了処理\n",
    "                if dones['__all__']:\n",
    "                    # print(f'all done at {env.steps}')\n",
    "                    break\n",
    "                \n",
    "            #del env_blue_pos[0]\n",
    "            #del env_red_pos[0]\n",
    "            #del env_mrm_pos[0]\n",
    "            \n",
    "            #hist_blue_pos = np.vstack(env_blue_pos)\n",
    "            #hist_red_pos = np.vstack(env_red_pos)\n",
    "            #hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "            \n",
    "            #f = open(results_file,'wb')\n",
    "            #pickle.dump(emv_blue_pos,f)\n",
    "            #pickle.dump(emv_red_pos,f)\n",
    "            #pickle.dump(emv_mrm_pos,f)\n",
    "            #f.close()\n",
    "            \n",
    "            if record_mode == 0:\n",
    "                video.release()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bfaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
