{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee02754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T-OHKI-MSI\\miniconda3\\envs\\AI2\\lib\\site-packages\\quaternion\\numba_wrapper.py:23: UserWarning: \n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Could not import from numba, which means that some\n",
      "parts of this code may run MUCH more slowly.  You\n",
      "may wish to install numba.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "  warnings.warn(warning_text)\n",
      "2021-12-27 17:42:53,778\tINFO trainer.py:723 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "2021-12-27 17:42:53,779\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2021-12-27 17:42:53,779\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-12-27 17:42:54,965\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2021-12-27 17:42:59,010\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import Logger, UnifiedLogger, pretty_print\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.examples.models.shared_weights_model import TF2SharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.agents.ppo import ppo, PPOTrainer, PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from environment_rllib_3d import MyEnv\n",
    "from settings.initial_settings import *\n",
    "from settings.reset_conditions import reset_conditions\n",
    "#from modules.models import MyConv2DModel_v0B_Small_CBAM_1DConv_Share\n",
    "from modules.models import DenseNetModelLarge\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from modules.savers import save_conditions\n",
    "from utility.result_env import render_env\n",
    "from utility.terminate_uavsimproc import teminate_proc\n",
    "from utility.latest_learned_file_path import latest_learned_file_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import ctypes\n",
    "import warnings\n",
    "\n",
    "#UCAV.exeが起動している場合、プロセスキルする。\n",
    "teminate_proc.UAVsimprockill(proc_name=\"UCAV.exe\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.filterwarnings('ignore', category=matplotlib.MatplotlibDeprecationWarning)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "PROJECT = \"UCAV\"\n",
    "TRIAL_ID = 2\n",
    "TRIAL = 'test_' + str(TRIAL_ID)\n",
    "EVAL_FREQ = 10\n",
    "NUM_EVAL = 1\n",
    "CONTINUAL = False\n",
    "\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "    timestr = datetime.datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "\n",
    "    def logger_creator(config):\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "ModelCatalog.register_custom_model('my_model', DenseNetModelLarge)\n",
    "\n",
    "# config = {\"env\": MyEnv,\n",
    "#           \"num_workers\": NUM_WORKERS,\n",
    "#           \"num_gpus\": NUM_GPUS,\n",
    "#           \"num_cpus_per_worker\": NUM_CPUS_PER_WORKER,\n",
    "#           \"num_sgd_iter\": NUM_SGD_ITER,\n",
    "#           \"lr\": LEARNING_RATE,\n",
    "#           \"gamma\": GAMMA,  # default=0.99\n",
    "#           \"model\": {\"custom_model\": \"my_model\"}\n",
    "#           # \"framework\": framework\n",
    "#           }  # use tensorflow 2\n",
    "eval_env = MyEnv({})\n",
    "policies = {\n",
    "    #\"blue_1\": PolicySpec(config={\"gamma\": 0.99}),\n",
    "    #\"blue_2\": PolicySpec(config={\"gamma\": 0.95}),\n",
    "    \"blue_0\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "    \"blue_1\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "}\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    #print(agent_id,episode)\n",
    "    #pol_id = policy_ids[agent_id]\n",
    "\n",
    "    pol_id = agent_id\n",
    "    return pol_id\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "\n",
    "config = {\"env\": MyEnv,\"num_gpus\": 0,\"num_workers\": 0, \"num_cpus_per_worker\": 0,\"num_gpus_per_worker\": 0,\n",
    "          \"create_env_on_driver\": True,\"train_batch_size\": 600*5,\"batch_mode\": \"complete_episodes\",\n",
    "          \"shuffle_sequences\": True, \"gamma\":0.999, \"lr\": 1e-5,\n",
    "          \"clip_actions\":True,\"normalize_actions\":False,\n",
    "          \"observation_space\":eval_env.observation_space,\"action_space\":eval_env.action_space,\n",
    "          \"explore\":True,\n",
    "          \"sgd_minibatch_size\": 200, \"num_sgd_iter\":20,\n",
    "          \"exploration_config\": {\"type\": \"StochasticSampling\",\"random_timesteps\":60000}, #PPO デフォルト\n",
    "          \"model\":{\"fcnet_activation\": \"tanh\",\"fcnet_hiddens\": [256, 256, 256],\"post_fcnet_activation\": \"linear\",},#\"linear\",\"relu\",\"tanh\" \n",
    "          #\"model\":{\"fcnet_hiddens\": [256, 256]},\n",
    "          \"multiagent\": {\"policies\": policies,  \"policy_mapping_fn\": policy_mapping_fn}\n",
    "         }\n",
    "res_name = \"sgd\"+str(config[\"sgd_minibatch_size\"])+\"sgd_num\"+str(config[\"num_sgd_iter\"])+\"lr\"+str(config[\"lr\"])+\"gamma\"+str(config[\"gamma\"])\n",
    "res_name = \"test\"\n",
    "\n",
    "conditions_dir = os.path.join('./' + PROJECT + '/conditions/')\n",
    "\n",
    "if not os.path.exists(conditions_dir):\n",
    "    os.makedirs(conditions_dir)\n",
    "save_conditions(conditions_dir)\n",
    "\n",
    "# PPOTrainer()は、try_import_tfを使うと、なぜかTensorflowのeager modeのエラーになる。\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config,\n",
    "                         logger_creator=custom_log_creator(\n",
    "                             os.path.expanduser(\"./\" + PROJECT + \"/logs\"), TRIAL))\n",
    "\n",
    "if CONTINUAL:\n",
    "    # Continual learning: Need to specify the checkpoint\n",
    "    # model_path = PROJECT + '/checkpoints/' + TRIAL + '/checkpoint_000197/checkpoint-197'\n",
    "    model_path = latest_learned_file_path('./UCAV/checkpoints/test_2/*')\n",
    "    trainer.restore(checkpoint_path=model_path)\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "eval_env = MyEnv({})\n",
    "record_mode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383e321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 17:43:02,718\tWARNING deprecation.py:46 -- DeprecationWarning: `compute_action` has been deprecated. Use `compute_single_action` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] [-0.] [0.]\n",
      "[0.] [-0.] [0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [-0.] [-0.]\n",
      "[0.] [-0.] [-0.]\n",
      "[0.] [-0.] [-0.]\n",
      "[-0.] [-0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[0.] [0.] [0.]\n",
      "[-0.] [-0.] [-0.]\n",
      "[-0.] [-0.] [0.]\n",
      "[-0.] [-0.] [-0.]\n",
      "[0.] [-0.] [0.]\n",
      "[-0.] [-0.] [0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n",
      "[0.] [0.] [-0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EVAL):\n",
    "    # print(f'\\nEvaluation {i}:')\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "\n",
    "    step_num = 0\n",
    "    fig = plt.figure(1,figsize=(8.0, 6.0))\n",
    "    ESC = 0x1B          # ESCキーの仮想キーコード\n",
    "    trajectory_length = 100\n",
    "    env_blue_pos = [0]\n",
    "    env_red_pos = [0]\n",
    "    env_mrm_pos = [0]\n",
    "    if record_mode == 0:\n",
    "        file_name = \"test_num\" +str(i)\n",
    "        video = cv2.VideoWriter(file_name+'.mp4',0x00000020,20.0,(800,600))\n",
    "\n",
    "    while True:\n",
    "        action_dict = {}\n",
    "        for j in range(eval_env.blue_num):\n",
    "            if not eval_env.blue[j].hitpoint == 0:\n",
    "                #action_dict['blue_' + str(j)] = trainer.compute_action(obs['blue_' + str(j)])\n",
    "                action_dict['blue_' + str(j)] = trainer.compute_single_action(obs['blue_' + str(j)],policy_id='blue_' + str(j),\n",
    "                                                                   clip_action=True,explore=False)\n",
    "   \n",
    "        obs, rewards, dones, infos = eval_env.step(action_dict)\n",
    "        #for j in range(eval_env.blue_num): \n",
    "\n",
    "        print(action_dict['blue_' + str(j)][\"vector_psi_x\"],action_dict['blue_' + str(j)][\"vector_gam_x\"],\n",
    "              action_dict['blue_' + str(j)][\"velocity\"])\n",
    "            \n",
    "            #print('blue_' + str(j),eval_env.blue[j].V, np.rad2deg(eval_env.blue[j].psi),  np.rad2deg(eval_env.blue[j].gam),\n",
    "            #     eval_env.blue[j].V_ref,np.rad2deg(eval_env.blue[j].psi_ref),  np.rad2deg(eval_env.blue[j].gam_ref))\n",
    "        env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp= render_env.copy_from_env(eval_env)\n",
    "        env_blue_pos.append(env_blue_pos_temp)\n",
    "        env_red_pos.append(env_red_pos_temp)\n",
    "        env_mrm_pos.append(env_mrm_pos_temp)\n",
    "        if step_num == 0:\n",
    "            del env_blue_pos[0]\n",
    "            del env_red_pos[0]\n",
    "            del env_mrm_pos[0]\n",
    "\n",
    "        hist_blue_pos = np.vstack(env_blue_pos)\n",
    "        hist_red_pos = np.vstack(env_red_pos)\n",
    "        hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "        plt.clf()\n",
    "        render_env.rend_3d(eval_env,hist_blue_pos,\"b\",1)\n",
    "        render_env.rend_3d(eval_env,hist_red_pos,\"r\",1)\n",
    "        render_env.rend_3d(eval_env,hist_mrm_pos,\"k\",1)\n",
    "        plt.pause(.01)\n",
    "        step_num = step_num + 1\n",
    "\n",
    "        done = dones[\"__all__\"]\n",
    "        if record_mode == 0:\n",
    "            img = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)\n",
    "            # cv2.imshow('test', img)\n",
    "            # cv2.waitKey(1)\n",
    "            # cv2.destroyAllWindows()\n",
    "            video.write(img.astype('uint8'))\n",
    "        elif record_mode == 1:\n",
    "            env.render()\n",
    "        elif record_mode == 2:\n",
    "            True #将来のため処理予約\n",
    "\n",
    "        #env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp = render_env.copy_from_env(eval_env)\n",
    "\n",
    "        #env_blue_pos.append(env_blue_pos_temp)\n",
    "        #env_red_pos.append(env_red_pos_temp)\n",
    "        #env_mrm_pos.append(env_mrm_pos_temp)\n",
    "        #step_num = step_num + 1\n",
    "        # エピソードの終了処理\n",
    "        if dones['__all__']:\n",
    "            # print(f'all done at {env.steps}')\n",
    "            if record_mode == 0:\n",
    "                video.release()\n",
    "            break\n",
    "\n",
    "    #del env_blue_pos[0]\n",
    "    #del env_red_pos[0]\n",
    "    #del env_mrm_pos[0]\n",
    "\n",
    "    #hist_blue_pos = np.vstack(env_blue_pos)\n",
    "    #hist_red_pos = np.vstack(env_red_pos)\n",
    "    #hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "\n",
    "    #f = open(results_file,'wb')\n",
    "    #f = open(\"log.pkl\",\"wb\")\n",
    "    #pickle.dump(emv_blue_pos,f)\n",
    "    #pickle.dump(emv_blue_pos,f)\n",
    "    #pickle.dump(emv_red_pos,f)\n",
    "    #pickle.dump(emv_mrm_pos,f)\n",
    "    #f.close()\n",
    "\n",
    "    if record_mode == 0:\n",
    "        video.release()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b07aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
