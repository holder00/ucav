{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6504bb36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 01:49:46,253\tINFO trainer.py:723 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "2021-12-09 01:49:46,255\tWARNING ppo.py:151 -- `train_batch_size` (256) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 256.\n",
      "2021-12-09 01:49:46,255\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2021-12-09 01:49:46,256\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-12-09 01:49:47,283\tWARNING deprecation.py:46 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2021-12-09 01:49:51,336\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import Logger, UnifiedLogger, pretty_print\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.examples.models.shared_weights_model import TF2SharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.agents.ppo import ppo, PPOTrainer, PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from environment_rllib_3d import MyEnv\n",
    "from settings.initial_settings import *\n",
    "from settings.reset_conditions import reset_conditions\n",
    "#from modules.models import MyConv2DModel_v0B_Small_CBAM_1DConv_Share\n",
    "from modules.models import DenseNetModelLarge\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from modules.savers import save_conditions\n",
    "from utility.result_env import render_env\n",
    "from utility.terminate_uavsimproc import teminate_proc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import ctypes\n",
    "import warnings\n",
    "\n",
    "#UCAV.exeが起動している場合、プロセスキルする。\n",
    "teminate_proc.UAVsimprockill(proc_name=\"UCAV.exe\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.filterwarnings('ignore', category=matplotlib.MatplotlibDeprecationWarning)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "PROJECT = \"UCAV\"\n",
    "TRIAL_ID = 2\n",
    "TRIAL = 'test_' + str(TRIAL_ID)\n",
    "EVAL_FREQ = 10\n",
    "CONTINUAL = False\n",
    "\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "    timestr = datetime.datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "\n",
    "    def logger_creator(config):\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "ModelCatalog.register_custom_model('my_model', DenseNetModelLarge)\n",
    "\n",
    "# config = {\"env\": MyEnv,\n",
    "#           \"num_workers\": NUM_WORKERS,\n",
    "#           \"num_gpus\": NUM_GPUS,\n",
    "#           \"num_cpus_per_worker\": NUM_CPUS_PER_WORKER,\n",
    "#           \"num_sgd_iter\": NUM_SGD_ITER,\n",
    "#           \"lr\": LEARNING_RATE,\n",
    "#           \"gamma\": GAMMA,  # default=0.99\n",
    "#           \"model\": {\"custom_model\": \"my_model\"}\n",
    "#           # \"framework\": framework\n",
    "#           }  # use tensorflow 2\n",
    "eval_env = MyEnv({})\n",
    "policies = {\n",
    "    #\"blue_1\": PolicySpec(config={\"gamma\": 0.99}),\n",
    "    #\"blue_2\": PolicySpec(config={\"gamma\": 0.95}),\n",
    "    \"blue_0\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "    \"blue_1\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "}\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    #print(agent_id,episode)\n",
    "    #pol_id = policy_ids[agent_id]\n",
    "\n",
    "    pol_id = agent_id\n",
    "    return pol_id\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "\n",
    "config = {\"env\": MyEnv,\"num_gpus\": 0,\"num_workers\": 0, \"num_cpus_per_worker\": 0,\"num_gpus_per_worker\": 0,\n",
    "          \"create_env_on_driver\": True,\"train_batch_size\": 256,\"batch_mode\": \"complete_episodes\",\n",
    "          \"multiagent\": {\"policies\": policies,  \"policy_mapping_fn\": policy_mapping_fn}\n",
    "         }\n",
    "conditions_dir = os.path.join('./' + PROJECT + '/conditions/')\n",
    "\n",
    "if not os.path.exists(conditions_dir):\n",
    "    os.makedirs(conditions_dir)\n",
    "save_conditions(conditions_dir)\n",
    "\n",
    "# PPOTrainer()は、try_import_tfを使うと、なぜかTensorflowのeager modeのエラーになる。\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config,\n",
    "                         logger_creator=custom_log_creator(\n",
    "                             os.path.expanduser(\"./\" + PROJECT + \"/logs\"), TRIAL))\n",
    "\n",
    "if CONTINUAL:\n",
    "    # Continual learning: Need to specify the checkpoint\n",
    "    model_path = PROJECT + '/checkpoints/' + TRIAL + '/checkpoint_000039/checkpoint-39'\n",
    "    trainer.restore(checkpoint_path=model_path)\n",
    "\n",
    "# models_dir = os.path.join('./' + PROJECT + '/models/')\n",
    "# if not os.path.exists(models_dir):\n",
    "#     os.makedirs(models_dir)\n",
    "# text_name = models_dir + TRIAL + '.txt'\n",
    "# with open(text_name, \"w\") as fp:\n",
    "#     trainer.get_policy().model.base_model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))\n",
    "# png_name = models_dir + TRIAL + '.png'\n",
    "# plot_model(trainer.get_policy().model.base_model, to_file=png_name, show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define checkpoint dir\n",
    "check_point_dir = os.path.join('./' + PROJECT + '/checkpoints/', TRIAL)\n",
    "if not os.path.exists(check_point_dir):\n",
    "    os.makedirs(check_point_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebbfb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------- Training at steps:0 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 200 -2.0 -1.0049999999999992\n",
      "blue_1 True True 200 0.005 1.0000000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 85 0.005 0.42500000000000027\n",
      "blue_1 True True 85 -2.0 -1.5799999999999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 01:50:59,633\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 570\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-51-01\n",
      "done: false\n",
      "episode_len_mean: 142.5\n",
      "episode_media: {}\n",
      "episode_reward_max: -0.005000000000020437\n",
      "episode_reward_mean: -0.58000000000001\n",
      "episode_reward_min: -1.1549999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 2\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.571272850036621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021246980875730515\n",
      "        model: {}\n",
      "        policy_loss: 0.020580817013978958\n",
      "        total_loss: 0.4572502672672272\n",
      "        vf_explained_var: 0.13450315594673157\n",
      "        vf_loss: 0.43242010474205017\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.505207061767578\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01910955086350441\n",
      "        model: {}\n",
      "        policy_loss: -0.29683917760849\n",
      "        total_loss: -0.035331740975379944\n",
      "        vf_explained_var: 0.09403850883245468\n",
      "        vf_loss: 0.2576855421066284\n",
      "  num_agent_steps_sampled: 570\n",
      "  num_agent_steps_trained: 570\n",
      "  num_steps_sampled: 285\n",
      "  num_steps_trained: 285\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.60206185567011\n",
      "  ram_util_percent: 38.97113402061856\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.42500000000000027\n",
      "  blue_1: 1.0000000000000007\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.2899999999999995\n",
      "  blue_1: -0.2899999999999995\n",
      "policy_reward_min:\n",
      "  blue_0: -1.0049999999999992\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6579777577540258\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 173.87494007190625\n",
      "  mean_inference_ms: 4.161433740095659\n",
      "  mean_raw_obs_processing_ms: 40.17162239634907\n",
      "time_since_restore: 64.3513822555542\n",
      "time_this_iter_s: 64.3513822555542\n",
      "time_total_s: 64.3513822555542\n",
      "timers:\n",
      "  learn_throughput: 165.671\n",
      "  learn_time_ms: 1720.277\n",
      "  load_throughput: 277994.567\n",
      "  load_time_ms: 1.025\n",
      "  sample_throughput: 4.173\n",
      "  sample_time_ms: 68297.696\n",
      "timestamp: 1638982261\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 285\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:0 starting ! -----------------\n",
      "agent_timesteps_total: 570\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-51-01\n",
      "done: false\n",
      "episode_len_mean: 142.5\n",
      "episode_media: {}\n",
      "episode_reward_max: -0.005000000000020437\n",
      "episode_reward_mean: -0.58000000000001\n",
      "episode_reward_min: -1.1549999999999994\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 2\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.571272850036621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021246980875730515\n",
      "        model: {}\n",
      "        policy_loss: 0.020580817013978958\n",
      "        total_loss: 0.4572502672672272\n",
      "        vf_explained_var: 0.13450315594673157\n",
      "        vf_loss: 0.43242010474205017\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.505207061767578\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01910955086350441\n",
      "        model: {}\n",
      "        policy_loss: -0.29683917760849\n",
      "        total_loss: -0.035331740975379944\n",
      "        vf_explained_var: 0.09403850883245468\n",
      "        vf_loss: 0.2576855421066284\n",
      "  num_agent_steps_sampled: 570\n",
      "  num_agent_steps_trained: 570\n",
      "  num_steps_sampled: 285\n",
      "  num_steps_trained: 285\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.60206185567011\n",
      "  ram_util_percent: 38.97113402061856\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.42500000000000027\n",
      "  blue_1: 1.0000000000000007\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.2899999999999995\n",
      "  blue_1: -0.2899999999999995\n",
      "policy_reward_min:\n",
      "  blue_0: -1.0049999999999992\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6579777577540258\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 173.87494007190625\n",
      "  mean_inference_ms: 4.161433740095659\n",
      "  mean_raw_obs_processing_ms: 40.17162239634907\n",
      "time_since_restore: 64.3513822555542\n",
      "time_this_iter_s: 64.3513822555542\n",
      "time_total_s: 64.3513822555542\n",
      "timers:\n",
      "  learn_throughput: 165.671\n",
      "  learn_time_ms: 1720.277\n",
      "  load_throughput: 277994.567\n",
      "  load_time_ms: 1.025\n",
      "  sample_throughput: 4.173\n",
      "  sample_time_ms: 68297.696\n",
      "timestamp: 1638982261\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 285\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:1 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 132 0.005 0.6600000000000005\n",
      "blue_1 True True 132 -2.0 -1.3449999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 42 -2.0 -1.795\n",
      "blue_1 True True 42 0.005 0.2100000000000001\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 359 -2.0 -0.21000000000001617\n",
      "blue_1 True True 359 0.004 2.0299999999999896\n",
      "agent_timesteps_total: 1636\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-52-50\n",
      "done: false\n",
      "episode_len_mean: 163.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.322000000000011\n",
      "episode_reward_min: -1.585\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 5\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.489062309265137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012630393728613853\n",
      "        model: {}\n",
      "        policy_loss: -0.14632421731948853\n",
      "        total_loss: 0.2482147216796875\n",
      "        vf_explained_var: 0.06285616010427475\n",
      "        vf_loss: 0.39074984192848206\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.49873161315918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015337771736085415\n",
      "        model: {}\n",
      "        policy_loss: -0.04289749637246132\n",
      "        total_loss: 0.3478289246559143\n",
      "        vf_explained_var: -0.19405265152454376\n",
      "        vf_loss: 0.38765886425971985\n",
      "  num_agent_steps_sampled: 1636\n",
      "  num_agent_steps_trained: 1636\n",
      "  num_steps_sampled: 818\n",
      "  num_steps_trained: 818\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 71.70738255033558\n",
      "  ram_util_percent: 39.104697986577186\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.6600000000000005\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.38500000000000295\n",
      "  blue_1: 0.06299999999999825\n",
      "policy_reward_min:\n",
      "  blue_0: -1.795\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6932985691320805\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 166.42433256059735\n",
      "  mean_inference_ms: 3.989924393691026\n",
      "  mean_raw_obs_processing_ms: 37.281434281127204\n",
      "time_since_restore: 167.59878635406494\n",
      "time_this_iter_s: 103.24740409851074\n",
      "time_total_s: 167.59878635406494\n",
      "timers:\n",
      "  learn_throughput: 314.318\n",
      "  learn_time_ms: 1301.231\n",
      "  load_throughput: 404353.644\n",
      "  load_time_ms: 1.011\n",
      "  sample_throughput: 4.591\n",
      "  sample_time_ms: 89096.059\n",
      "timestamp: 1638982370\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 818\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:2 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 93 0.005 0.4650000000000003\n",
      "blue_1 True True 93 -2.0 -1.5399999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 98 0.005 0.4900000000000003\n",
      "blue_1 True True 98 -2.0 -1.5149999999999997\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 176 0.005 0.8800000000000007\n",
      "blue_1 True True 176 -2.0 -1.1249999999999993\n",
      "agent_timesteps_total: 2370\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-54-05\n",
      "done: false\n",
      "episode_len_mean: 148.125\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.49437500000000856\n",
      "episode_reward_min: -1.585\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 8\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.34201431274414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016781751066446304\n",
      "        model: {}\n",
      "        policy_loss: 0.1344641000032425\n",
      "        total_loss: 0.18044313788414001\n",
      "        vf_explained_var: 0.3325442671775818\n",
      "        vf_loss: 0.04094448313117027\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.43451976776123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016137665137648582\n",
      "        model: {}\n",
      "        policy_loss: -0.18654099106788635\n",
      "        total_loss: 0.456393301486969\n",
      "        vf_explained_var: -0.014236992225050926\n",
      "        vf_loss: 0.6397067308425903\n",
      "  num_agent_steps_sampled: 2370\n",
      "  num_agent_steps_trained: 2370\n",
      "  num_steps_sampled: 1185\n",
      "  num_steps_trained: 1185\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.323076923076925\n",
      "  ram_util_percent: 39.13653846153847\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.8800000000000007\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.011250000000001648\n",
      "  blue_1: -0.4831250000000009\n",
      "policy_reward_min:\n",
      "  blue_0: -1.795\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.7004951300676924\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 161.7543943735069\n",
      "  mean_inference_ms: 3.898531353188309\n",
      "  mean_raw_obs_processing_ms: 37.8245067978699\n",
      "time_since_restore: 236.95631313323975\n",
      "time_this_iter_s: 69.3575267791748\n",
      "time_total_s: 236.95631313323975\n",
      "timers:\n",
      "  learn_throughput: 385.617\n",
      "  learn_time_ms: 1024.332\n",
      "  load_throughput: 392315.908\n",
      "  load_time_ms: 1.007\n",
      "  sample_throughput: 4.671\n",
      "  sample_time_ms: 84564.974\n",
      "timestamp: 1638982445\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1185\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:3 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 141 0.005 0.7050000000000005\n",
      "blue_1 True True 141 -2.0 -1.2999999999999994\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 40 -2.0 -1.805\n",
      "blue_1 True True 40 0.005 0.2000000000000001\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 165 0.005 0.8250000000000006\n",
      "blue_1 True True 165 -2.0 -1.1799999999999993\n",
      "agent_timesteps_total: 3062\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-55-26\n",
      "done: false\n",
      "episode_len_mean: 139.1818181818182\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.5918181818181899\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 11\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.396687507629395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015333007089793682\n",
      "        model: {}\n",
      "        policy_loss: 0.042596373707056046\n",
      "        total_loss: 0.5205609798431396\n",
      "        vf_explained_var: 0.33660653233528137\n",
      "        vf_loss: 0.4733646810054779\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.270456314086914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017704982310533524\n",
      "        model: {}\n",
      "        policy_loss: -0.32023635506629944\n",
      "        total_loss: 0.032120268791913986\n",
      "        vf_explained_var: -0.028853150084614754\n",
      "        vf_loss: 0.3488156199455261\n",
      "  num_agent_steps_sampled: 3062\n",
      "  num_agent_steps_trained: 3062\n",
      "  num_steps_sampled: 1531\n",
      "  num_steps_trained: 1531\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 62.02272727272727\n",
      "  ram_util_percent: 39.08363636363638\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.8800000000000007\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.03318181818181927\n",
      "  blue_1: -0.558636363636364\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.697607021715239\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 160.10770104084725\n",
      "  mean_inference_ms: 3.8336384269781374\n",
      "  mean_raw_obs_processing_ms: 38.7201356860235\n",
      "time_since_restore: 311.8199393749237\n",
      "time_this_iter_s: 74.86362624168396\n",
      "time_total_s: 311.8199393749237\n",
      "timers:\n",
      "  learn_throughput: 429.943\n",
      "  learn_time_ms: 890.235\n",
      "  load_throughput: 506865.532\n",
      "  load_time_ms: 0.755\n",
      "  sample_throughput: 4.58\n",
      "  sample_time_ms: 83564.475\n",
      "timestamp: 1638982526\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1531\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:4 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 43 -2.0 -1.7899999999999998\n",
      "blue_1 True True 43 0.005 0.2150000000000001\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 161 -2.0 -1.1999999999999993\n",
      "blue_1 True True 161 0.005 0.8050000000000006\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 201 -2.0 -0.9999999999999993\n",
      "blue_1 True True 201 0.005 1.0050000000000006\n",
      "agent_timesteps_total: 3872\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-56-53\n",
      "done: false\n",
      "episode_len_mean: 138.28571428571428\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.6053571428571515\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 14\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.277592658996582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019701611250638962\n",
      "        model: {}\n",
      "        policy_loss: -0.15503135323524475\n",
      "        total_loss: 0.2449677288532257\n",
      "        vf_explained_var: 0.23964375257492065\n",
      "        vf_loss: 0.39408865571022034\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.386094093322754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018297169357538223\n",
      "        model: {}\n",
      "        policy_loss: -0.11720636487007141\n",
      "        total_loss: 0.04194482043385506\n",
      "        vf_explained_var: 0.52735435962677\n",
      "        vf_loss: 0.15549175441265106\n",
      "  num_agent_steps_sampled: 3872\n",
      "  num_agent_steps_trained: 3872\n",
      "  num_steps_sampled: 1936\n",
      "  num_steps_trained: 1936\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.775\n",
      "  ram_util_percent: 39.069166666666675\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.8800000000000007\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.3110714285714293\n",
      "  blue_1: -0.29428571428571454\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6944135765497655\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 159.09178726038968\n",
      "  mean_inference_ms: 3.7838771441609027\n",
      "  mean_raw_obs_processing_ms: 39.286058360821336\n",
      "time_since_restore: 393.7377197742462\n",
      "time_this_iter_s: 81.91778039932251\n",
      "time_total_s: 393.7377197742462\n",
      "timers:\n",
      "  learn_throughput: 448.922\n",
      "  learn_time_ms: 862.512\n",
      "  load_throughput: 640948.184\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 4.592\n",
      "  sample_time_ms: 84313.585\n",
      "timestamp: 1638982613\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 1936\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:5 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 110 -2.0 -1.4549999999999996\n",
      "blue_1 True True 110 0.005 0.5500000000000004\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 138 -2.0 -1.3149999999999995\n",
      "blue_1 True True 138 0.005 0.6900000000000005\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 147 0.005 0.7350000000000005\n",
      "blue_1 True True 147 -2.0 -1.2699999999999996\n",
      "agent_timesteps_total: 4662\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-58-15\n",
      "done: false\n",
      "episode_len_mean: 137.11764705882354\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.6200000000000082\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 17\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.29921817779541\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015279192477464676\n",
      "        model: {}\n",
      "        policy_loss: -0.034662507474422455\n",
      "        total_loss: 0.38736334443092346\n",
      "        vf_explained_var: 0.3469267189502716\n",
      "        vf_loss: 0.41744211316108704\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.38670825958252\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016709301620721817\n",
      "        model: {}\n",
      "        policy_loss: -0.13171380758285522\n",
      "        total_loss: 0.1055375412106514\n",
      "        vf_explained_var: 0.04402787238359451\n",
      "        vf_loss: 0.2339095026254654\n",
      "  num_agent_steps_sampled: 4662\n",
      "  num_agent_steps_trained: 4662\n",
      "  num_steps_sampled: 2331\n",
      "  num_steps_trained: 2331\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 64.22123893805309\n",
      "  ram_util_percent: 39.119469026548686\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.8800000000000007\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.375882352941177\n",
      "  blue_1: -0.24411764705882377\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6926709139936154\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 158.05880774967417\n",
      "  mean_inference_ms: 3.74868782503565\n",
      "  mean_raw_obs_processing_ms: 39.7038168673339\n",
      "time_since_restore: 469.4808394908905\n",
      "time_this_iter_s: 75.74311971664429\n",
      "time_total_s: 469.4808394908905\n",
      "timers:\n",
      "  learn_throughput: 469.635\n",
      "  learn_time_ms: 827.238\n",
      "  load_throughput: 771720.153\n",
      "  load_time_ms: 0.503\n",
      "  sample_throughput: 4.633\n",
      "  sample_time_ms: 83850.514\n",
      "timestamp: 1638982695\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2331\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:6 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 135 -2.0 -1.3299999999999996\n",
      "blue_1 True True 135 0.005 0.6750000000000005\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 96 0.005 0.4800000000000003\n",
      "blue_1 True True 96 -2.0 -1.5249999999999997\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 242 -1.998 -0.7720000000000036\n",
      "blue_1 True True 242 0.005 1.2099999999999962\n",
      "agent_timesteps_total: 5608\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_01-59-46\n",
      "done: false\n",
      "episode_len_mean: 140.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.5901000000000086\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 20\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.437850952148438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01497672125697136\n",
      "        model: {}\n",
      "        policy_loss: -0.2831854224205017\n",
      "        total_loss: -0.0764390304684639\n",
      "        vf_explained_var: 0.39256489276885986\n",
      "        vf_loss: 0.2022533416748047\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.569469451904297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03206106647849083\n",
      "        model: {}\n",
      "        policy_loss: 0.014210168272256851\n",
      "        total_loss: 0.48013216257095337\n",
      "        vf_explained_var: -0.25904545187950134\n",
      "        vf_loss: 0.45950978994369507\n",
      "  num_agent_steps_sampled: 5608\n",
      "  num_agent_steps_trained: 5608\n",
      "  num_steps_sampled: 2804\n",
      "  num_steps_trained: 2804\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.3216\n",
      "  ram_util_percent: 39.04800000000001\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.8800000000000007\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.4006000000000006\n",
      "  blue_1: -0.1895000000000003\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6913149492434931\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 156.9724395169187\n",
      "  mean_inference_ms: 3.7168470947901433\n",
      "  mean_raw_obs_processing_ms: 39.86669348205621\n",
      "time_since_restore: 554.9453899860382\n",
      "time_this_iter_s: 85.4645504951477\n",
      "time_total_s: 554.9453899860382\n",
      "timers:\n",
      "  learn_throughput: 505.216\n",
      "  learn_time_ms: 792.872\n",
      "  load_throughput: 928315.448\n",
      "  load_time_ms: 0.432\n",
      "  sample_throughput: 4.718\n",
      "  sample_time_ms: 84896.561\n",
      "timestamp: 1638982786\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 2804\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:7 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 145 0.005 0.7250000000000005\n",
      "blue_1 True True 145 -2.0 -1.2799999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 134 0.005 0.6700000000000005\n",
      "blue_1 True True 134 -2.0 -1.3349999999999995\n",
      "agent_timesteps_total: 6166\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-00-43\n",
      "done: false\n",
      "episode_len_mean: 140.13636363636363\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.5919090909090994\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 22\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.220480918884277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015160517767071724\n",
      "        model: {}\n",
      "        policy_loss: 0.019901134073734283\n",
      "        total_loss: 0.11650995165109634\n",
      "        vf_explained_var: -0.9608033299446106\n",
      "        vf_loss: 0.09206067025661469\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.34385871887207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016249997541308403\n",
      "        model: {}\n",
      "        policy_loss: -0.21095436811447144\n",
      "        total_loss: 0.12628008425235748\n",
      "        vf_explained_var: 0.32257091999053955\n",
      "        vf_loss: 0.3323593735694885\n",
      "  num_agent_steps_sampled: 6166\n",
      "  num_agent_steps_trained: 6166\n",
      "  num_steps_sampled: 3083\n",
      "  num_steps_trained: 3083\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.583544303797474\n",
      "  ram_util_percent: 39.025316455696206\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.8800000000000007\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.3007727272727278\n",
      "  blue_1: -0.29113636363636386\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.689493305108284\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 156.31352976398375\n",
      "  mean_inference_ms: 3.6961381656013446\n",
      "  mean_raw_obs_processing_ms: 39.94736848824641\n",
      "time_since_restore: 606.3115236759186\n",
      "time_this_iter_s: 51.36613368988037\n",
      "time_total_s: 606.3115236759186\n",
      "timers:\n",
      "  learn_throughput: 519.127\n",
      "  learn_time_ms: 742.352\n",
      "  load_throughput: 767329.648\n",
      "  load_time_ms: 0.502\n",
      "  sample_throughput: 4.732\n",
      "  sample_time_ms: 81435.491\n",
      "timestamp: 1638982843\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3083\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:8 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 121 0.005 0.6050000000000004\n",
      "blue_1 True True 121 -2.0 -1.3999999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 186 0.005 0.9300000000000007\n",
      "blue_1 True True 186 -2.0 -1.0749999999999993\n",
      "agent_timesteps_total: 6780\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-01-41\n",
      "done: false\n",
      "episode_len_mean: 141.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.5817500000000088\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 24\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.330434799194336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0171060748398304\n",
      "        model: {}\n",
      "        policy_loss: 0.0353730209171772\n",
      "        total_loss: 0.07704035937786102\n",
      "        vf_explained_var: -0.1972438395023346\n",
      "        vf_loss: 0.036535535007715225\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.266175270080566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017453085631132126\n",
      "        model: {}\n",
      "        policy_loss: -0.2663003206253052\n",
      "        total_loss: -0.022252310067415237\n",
      "        vf_explained_var: 0.4277615547180176\n",
      "        vf_loss: 0.23881207406520844\n",
      "  num_agent_steps_sampled: 6780\n",
      "  num_agent_steps_trained: 6780\n",
      "  num_steps_sampled: 3390\n",
      "  num_steps_trained: 3390\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 9\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.01874999999999\n",
      "  ram_util_percent: 38.957499999999996\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 0.9300000000000007\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.21175000000000044\n",
      "  blue_1: -0.37000000000000016\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.687703871530085\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 155.61001179120774\n",
      "  mean_inference_ms: 3.6754037962813917\n",
      "  mean_raw_obs_processing_ms: 39.98464984520934\n",
      "time_since_restore: 658.811003446579\n",
      "time_this_iter_s: 52.4994797706604\n",
      "time_total_s: 658.811003446579\n",
      "timers:\n",
      "  learn_throughput: 535.101\n",
      "  learn_time_ms: 703.918\n",
      "  load_throughput: 843739.055\n",
      "  load_time_ms: 0.446\n",
      "  sample_throughput: 4.777\n",
      "  sample_time_ms: 78842.347\n",
      "timestamp: 1638982901\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3390\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:9 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 274 0.005 1.3699999999999928\n",
      "blue_1 True True 274 -2.0 -0.5740000000000067\n",
      "agent_timesteps_total: 7328\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-02-29\n",
      "done: false\n",
      "episode_len_mean: 146.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.5266400000000097\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 25\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.462729454040527\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012954026460647583\n",
      "        model: {}\n",
      "        policy_loss: -0.17876583337783813\n",
      "        total_loss: -0.1591177135705948\n",
      "        vf_explained_var: -0.9859381318092346\n",
      "        vf_loss: 0.015761878341436386\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.486557960510254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0155925452709198\n",
      "        model: {}\n",
      "        policy_loss: -0.23594436049461365\n",
      "        total_loss: -0.09800796210765839\n",
      "        vf_explained_var: 0.6104968786239624\n",
      "        vf_loss: 0.133258655667305\n",
      "  num_agent_steps_sampled: 7328\n",
      "  num_agent_steps_trained: 7328\n",
      "  num_steps_sampled: 3664\n",
      "  num_steps_trained: 3664\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.11060606060605\n",
      "  ram_util_percent: 38.21515151515151\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.1484800000000007\n",
      "  blue_1: -0.37816000000000044\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6866821069531507\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 155.24540037033952\n",
      "  mean_inference_ms: 3.6655226257806897\n",
      "  mean_raw_obs_processing_ms: 39.94260768175421\n",
      "time_since_restore: 701.3932747840881\n",
      "time_this_iter_s: 42.582271337509155\n",
      "time_total_s: 701.3932747840881\n",
      "timers:\n",
      "  learn_throughput: 547.415\n",
      "  learn_time_ms: 669.327\n",
      "  load_throughput: 730796.988\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 4.835\n",
      "  sample_time_ms: 75784.807\n",
      "timestamp: 1638982949\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3664\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:10 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 136 -2.0 -1.3249999999999995\n",
      "blue_1 True True 136 0.005 0.6800000000000005\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 69 -2.0 -1.6599999999999997\n",
      "blue_1 True True 69 0.005 0.3450000000000002\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 62 -2.0 -1.6949999999999998\n",
      "blue_1 True True 62 0.005 0.31000000000000016\n",
      "agent_timesteps_total: 7862\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-03-29\n",
      "done: false\n",
      "episode_len_mean: 140.39285714285714\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.5896785714285803\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 28\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.425164222717285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017509035766124725\n",
      "        model: {}\n",
      "        policy_loss: -0.12010315805673599\n",
      "        total_loss: 0.8964297771453857\n",
      "        vf_explained_var: 0.20716466009616852\n",
      "        vf_loss: 1.0112802982330322\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.224420547485352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.031029710546135902\n",
      "        model: {}\n",
      "        policy_loss: -0.08968811482191086\n",
      "        total_loss: 0.09032133966684341\n",
      "        vf_explained_var: -0.668390691280365\n",
      "        vf_loss: 0.1707005351781845\n",
      "  num_agent_steps_sampled: 7862\n",
      "  num_agent_steps_trained: 7862\n",
      "  num_steps_sampled: 3931\n",
      "  num_steps_trained: 3931\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 11\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.42738095238095\n",
      "  ram_util_percent: 36.94880952380952\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.2997142857142863\n",
      "  blue_1: -0.28996428571428606\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6835041733158123\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 154.23382383013333\n",
      "  mean_inference_ms: 3.635785055489819\n",
      "  mean_raw_obs_processing_ms: 40.006888446801746\n",
      "time_since_restore: 755.8013331890106\n",
      "time_this_iter_s: 54.408058404922485\n",
      "time_total_s: 755.8013331890106\n",
      "timers:\n",
      "  learn_throughput: 682.77\n",
      "  learn_time_ms: 534.001\n",
      "  load_throughput: 731380.381\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 4.865\n",
      "  sample_time_ms: 74950.804\n",
      "timestamp: 1638983009\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3931\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:10 starting ! -----------------\n",
      "agent_timesteps_total: 7862\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-03-29\n",
      "done: false\n",
      "episode_len_mean: 140.39285714285714\n",
      "episode_media: {}\n",
      "episode_reward_max: 1.819999999999971\n",
      "episode_reward_mean: -0.5896785714285803\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 28\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.425164222717285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017509035766124725\n",
      "        model: {}\n",
      "        policy_loss: -0.12010315805673599\n",
      "        total_loss: 0.8964297771453857\n",
      "        vf_explained_var: 0.20716466009616852\n",
      "        vf_loss: 1.0112802982330322\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.224420547485352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.031029710546135902\n",
      "        model: {}\n",
      "        policy_loss: -0.08968811482191086\n",
      "        total_loss: 0.09032133966684341\n",
      "        vf_explained_var: -0.668390691280365\n",
      "        vf_loss: 0.1707005351781845\n",
      "  num_agent_steps_sampled: 7862\n",
      "  num_agent_steps_trained: 7862\n",
      "  num_steps_sampled: 3931\n",
      "  num_steps_trained: 3931\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 11\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.42738095238095\n",
      "  ram_util_percent: 36.94880952380952\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.0299999999999896\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.2997142857142863\n",
      "  blue_1: -0.28996428571428606\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6835041733158123\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 154.23382383013333\n",
      "  mean_inference_ms: 3.635785055489819\n",
      "  mean_raw_obs_processing_ms: 40.006888446801746\n",
      "time_since_restore: 755.8013331890106\n",
      "time_this_iter_s: 54.408058404922485\n",
      "time_total_s: 755.8013331890106\n",
      "timers:\n",
      "  learn_throughput: 682.77\n",
      "  learn_time_ms: 534.001\n",
      "  load_throughput: 731380.381\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 4.865\n",
      "  sample_time_ms: 74950.804\n",
      "timestamp: 1638983009\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 3931\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:11 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 102 -2.0 -1.4949999999999997\n",
      "blue_1 True True 102 0.005 0.5100000000000003\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 431 -2.0 0.14999999999997637\n",
      "blue_1 True True 431 0.004 2.4319999999999977\n",
      "agent_timesteps_total: 8928\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-04-52\n",
      "done: false\n",
      "episode_len_mean: 148.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.49713333333334414\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 30\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.303421020507812\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0203426294028759\n",
      "        model: {}\n",
      "        policy_loss: -0.15432271361351013\n",
      "        total_loss: 0.34549203515052795\n",
      "        vf_explained_var: -0.16296476125717163\n",
      "        vf_loss: 0.49371203780174255\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.465100288391113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022250927984714508\n",
      "        model: {}\n",
      "        policy_loss: -0.13585110008716583\n",
      "        total_loss: -0.09263309091329575\n",
      "        vf_explained_var: -0.5673071146011353\n",
      "        vf_loss: 0.033205095678567886\n",
      "  num_agent_steps_sampled: 8928\n",
      "  num_agent_steps_trained: 8928\n",
      "  num_steps_sampled: 4464\n",
      "  num_steps_trained: 4464\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 12\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.756637168141594\n",
      "  ram_util_percent: 36.9858407079646\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.32456666666666806\n",
      "  blue_1: -0.17256666666666706\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6810370173373542\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 153.45537823078206\n",
      "  mean_inference_ms: 3.614237663466744\n",
      "  mean_raw_obs_processing_ms: 39.89000341266889\n",
      "time_since_restore: 832.9119675159454\n",
      "time_this_iter_s: 77.11063432693481\n",
      "time_total_s: 832.9119675159454\n",
      "timers:\n",
      "  learn_throughput: 703.74\n",
      "  learn_time_ms: 518.089\n",
      "  load_throughput: 731205.527\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 5.05\n",
      "  sample_time_ms: 72197.103\n",
      "timestamp: 1638983092\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4464\n",
      "training_iteration: 12\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:12 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 213 -2.0 -0.9400000000000006\n",
      "blue_1 True True 213 0.005 1.0649999999999993\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 88 0.005 0.4400000000000003\n",
      "blue_1 True True 88 -2.0 -1.5649999999999997\n",
      "agent_timesteps_total: 9530\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-05-45\n",
      "done: false\n",
      "episode_len_mean: 148.90625\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.4973125000000108\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 32\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.3696870803833\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02663837932050228\n",
      "        model: {}\n",
      "        policy_loss: 0.03653357923030853\n",
      "        total_loss: 0.4013146162033081\n",
      "        vf_explained_var: 0.19583365321159363\n",
      "        vf_loss: 0.35279372334480286\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.415024757385254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011099844239652157\n",
      "        model: {}\n",
      "        policy_loss: -0.3832659125328064\n",
      "        total_loss: -0.17518819868564606\n",
      "        vf_explained_var: 0.19011135399341583\n",
      "        vf_loss: 0.20058530569076538\n",
      "  num_agent_steps_sampled: 9530\n",
      "  num_agent_steps_trained: 9530\n",
      "  num_steps_sampled: 4765\n",
      "  num_steps_trained: 4765\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 13\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.267999999999994\n",
      "  ram_util_percent: 36.97866666666666\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.3199062500000012\n",
      "  blue_1: -0.17740625000000038\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6786277972862709\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 152.67647093876116\n",
      "  mean_inference_ms: 3.593111928452029\n",
      "  mean_raw_obs_processing_ms: 39.78321167730941\n",
      "time_since_restore: 880.9045121669769\n",
      "time_this_iter_s: 47.992544651031494\n",
      "time_total_s: 880.9045121669769\n",
      "timers:\n",
      "  learn_throughput: 705.508\n",
      "  learn_time_ms: 507.436\n",
      "  load_throughput: 718072.226\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 5.111\n",
      "  sample_time_ms: 70042.401\n",
      "timestamp: 1638983145\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4765\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:13 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 88 -2.0 -1.5649999999999997\n",
      "blue_1 True True 88 0.005 0.4400000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 161 0.005 0.8050000000000006\n",
      "blue_1 True True 161 -2.0 -1.1999999999999993\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 139 0.005 0.6950000000000005\n",
      "blue_1 True True 139 -2.0 -1.3099999999999996\n",
      "agent_timesteps_total: 10306\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-06-53\n",
      "done: false\n",
      "episode_len_mean: 147.22857142857143\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.5156857142857248\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 35\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.244721412658691\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01834145374596119\n",
      "        model: {}\n",
      "        policy_loss: -0.07655520737171173\n",
      "        total_loss: 0.31676456332206726\n",
      "        vf_explained_var: 0.472747802734375\n",
      "        vf_loss: 0.3809392750263214\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.458284378051758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01868116669356823\n",
      "        model: {}\n",
      "        policy_loss: -0.10161545872688293\n",
      "        total_loss: 0.2836710214614868\n",
      "        vf_explained_var: 0.20438461005687714\n",
      "        vf_loss: 0.37267667055130005\n",
      "  num_agent_steps_sampled: 10306\n",
      "  num_agent_steps_trained: 10306\n",
      "  num_steps_sampled: 5153\n",
      "  num_steps_trained: 5153\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 14\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.635106382978734\n",
      "  ram_util_percent: 36.977659574468085\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.29434285714285824\n",
      "  blue_1: -0.22134285714285745\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6750992911280387\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 151.48876179341838\n",
      "  mean_inference_ms: 3.5630004682254555\n",
      "  mean_raw_obs_processing_ms: 39.67882141103171\n",
      "time_since_restore: 943.1081600189209\n",
      "time_this_iter_s: 62.20364785194397\n",
      "time_total_s: 943.1081600189209\n",
      "timers:\n",
      "  learn_throughput: 701.006\n",
      "  learn_time_ms: 516.686\n",
      "  load_throughput: 605490.996\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 5.269\n",
      "  sample_time_ms: 68742.694\n",
      "timestamp: 1638983213\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5153\n",
      "training_iteration: 14\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:14 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 120 0.005 0.6000000000000004\n",
      "blue_1 True True 120 -2.0 -1.4049999999999996\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 142 -2.0 -1.2949999999999995\n",
      "blue_1 True True 142 0.005 0.7100000000000005\n",
      "agent_timesteps_total: 10830\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-07-46\n",
      "done: false\n",
      "episode_len_mean: 146.35135135135135\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.5253783783783886\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 37\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.813232421875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011288088746368885\n",
      "        model: {}\n",
      "        policy_loss: -0.11483291536569595\n",
      "        total_loss: 0.2541795074939728\n",
      "        vf_explained_var: -0.2940928637981415\n",
      "        vf_loss: 0.3613929748535156\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.589580535888672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013757484033703804\n",
      "        model: {}\n",
      "        policy_loss: -0.058244798332452774\n",
      "        total_loss: 0.4299105405807495\n",
      "        vf_explained_var: -0.3142895996570587\n",
      "        vf_loss: 0.4788689911365509\n",
      "  num_agent_steps_sampled: 10830\n",
      "  num_agent_steps_trained: 10830\n",
      "  num_steps_sampled: 5415\n",
      "  num_steps_trained: 5415\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 15\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.531506849315065\n",
      "  ram_util_percent: 36.979452054794514\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.29721621621621724\n",
      "  blue_1: -0.2281621621621624\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6729011289961119\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 150.78767412168352\n",
      "  mean_inference_ms: 3.5440499547824724\n",
      "  mean_raw_obs_processing_ms: 39.629013374222616\n",
      "time_since_restore: 990.1942067146301\n",
      "time_this_iter_s: 47.08604669570923\n",
      "time_total_s: 990.1942067146301\n",
      "timers:\n",
      "  learn_throughput: 728.904\n",
      "  learn_time_ms: 477.292\n",
      "  load_throughput: 581585.636\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 5.328\n",
      "  sample_time_ms: 65298.548\n",
      "timestamp: 1638983266\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5415\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:15 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 103 0.005 0.5150000000000003\n",
      "blue_1 True True 103 -2.0 -1.4899999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 231 -2.0 -0.8500000000000025\n",
      "blue_1 True True 231 0.005 1.1549999999999974\n",
      "agent_timesteps_total: 11498\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-08-45\n",
      "done: false\n",
      "episode_len_mean: 147.4102564102564\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.515615384615395\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 39\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.546721458435059\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010646376758813858\n",
      "        model: {}\n",
      "        policy_loss: -0.5290195941925049\n",
      "        total_loss: -0.4588930904865265\n",
      "        vf_explained_var: -0.07255817949771881\n",
      "        vf_loss: 0.06294018775224686\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.657519340515137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014684680849313736\n",
      "        model: {}\n",
      "        policy_loss: 0.07515990734100342\n",
      "        total_loss: 0.4915594756603241\n",
      "        vf_explained_var: -0.22586578130722046\n",
      "        vf_loss: 0.4064873456954956\n",
      "  num_agent_steps_sampled: 11498\n",
      "  num_agent_steps_trained: 11498\n",
      "  num_steps_sampled: 5749\n",
      "  num_steps_trained: 5749\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 16\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.930864197530866\n",
      "  ram_util_percent: 36.98765432098765\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.3699999999999928\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.29056410256410364\n",
      "  blue_1: -0.22505128205128236\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6707285857405086\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 150.11054930694974\n",
      "  mean_inference_ms: 3.525655512882553\n",
      "  mean_raw_obs_processing_ms: 39.56874099892349\n",
      "time_since_restore: 1043.7604548931122\n",
      "time_this_iter_s: 53.566248178482056\n",
      "time_total_s: 1043.7604548931122\n",
      "timers:\n",
      "  learn_throughput: 761.24\n",
      "  learn_time_ms: 449.004\n",
      "  load_throughput: 487889.024\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 5.421\n",
      "  sample_time_ms: 63056.548\n",
      "timestamp: 1638983325\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 5749\n",
      "training_iteration: 16\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:16 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 392 0.004 1.94799999999999\n",
      "blue_1 True True 392 -2.0 -0.04500000000001969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 12282\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-09-36\n",
      "done: false\n",
      "episode_len_mean: 153.525\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.45515000000001116\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 40\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.603569030761719\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016534652560949326\n",
      "        model: {}\n",
      "        policy_loss: -0.11024714261293411\n",
      "        total_loss: -0.036456190049648285\n",
      "        vf_explained_var: -0.8656741380691528\n",
      "        vf_loss: 0.0626300722360611\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.618038177490234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01488566491752863\n",
      "        model: {}\n",
      "        policy_loss: -0.12417888641357422\n",
      "        total_loss: 0.3011630177497864\n",
      "        vf_explained_var: -0.5752673745155334\n",
      "        vf_loss: 0.4152941107749939\n",
      "  num_agent_steps_sampled: 12282\n",
      "  num_agent_steps_trained: 12282\n",
      "  num_steps_sampled: 6141\n",
      "  num_steps_trained: 6141\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 17\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.74714285714286\n",
      "  ram_util_percent: 37.0\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.23460000000000125\n",
      "  blue_1: -0.22055000000000075\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6695985706047786\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 149.73130605959284\n",
      "  mean_inference_ms: 3.5165393663651217\n",
      "  mean_raw_obs_processing_ms: 39.50293785388455\n",
      "time_since_restore: 1089.0774221420288\n",
      "time_this_iter_s: 45.316967248916626\n",
      "time_total_s: 1089.0774221420288\n",
      "timers:\n",
      "  learn_throughput: 751.251\n",
      "  learn_time_ms: 444.193\n",
      "  load_throughput: 476326.996\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 5.655\n",
      "  sample_time_ms: 59008.966\n",
      "timestamp: 1638983376\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6141\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:17 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 101 -2.0 -1.4999999999999996\n",
      "blue_1 True True 101 0.005 0.5050000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 205 0.005 1.0669999999999993\n",
      "blue_1 True True 205 -2.0 -0.9799999999999998\n",
      "agent_timesteps_total: 12894\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-10-31\n",
      "done: false\n",
      "episode_len_mean: 153.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.45509523809524927\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 42\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.30941390991211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016910355538129807\n",
      "        model: {}\n",
      "        policy_loss: 0.004812602419406176\n",
      "        total_loss: 0.6393492221832275\n",
      "        vf_explained_var: 0.4635172188282013\n",
      "        vf_loss: 0.6231220960617065\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.214312553405762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010529905557632446\n",
      "        model: {}\n",
      "        policy_loss: -0.4018421471118927\n",
      "        total_loss: -0.3007456958293915\n",
      "        vf_explained_var: -0.2958696186542511\n",
      "        vf_loss: 0.09398873895406723\n",
      "  num_agent_steps_sampled: 12894\n",
      "  num_agent_steps_trained: 12894\n",
      "  num_steps_sampled: 6447\n",
      "  num_steps_trained: 6447\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 18\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.740259740259745\n",
      "  ram_util_percent: 36.97532467532467\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.23373809523809644\n",
      "  blue_1: -0.22135714285714356\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6674121870110578\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 148.99395235887067\n",
      "  mean_inference_ms: 3.4987929836129092\n",
      "  mean_raw_obs_processing_ms: 39.37966581816803\n",
      "time_since_restore: 1138.4714086055756\n",
      "time_this_iter_s: 49.39398646354675\n",
      "time_total_s: 1138.4714086055756\n",
      "timers:\n",
      "  learn_throughput: 759.133\n",
      "  learn_time_ms: 443.137\n",
      "  load_throughput: 480164.664\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 5.721\n",
      "  sample_time_ms: 58802.827\n",
      "timestamp: 1638983431\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6447\n",
      "training_iteration: 18\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:18 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 107 0.005 0.5350000000000004\n",
      "blue_1 True True 107 -2.0 -1.4699999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 120 0.005 0.6000000000000004\n",
      "blue_1 True True 120 -2.0 -1.4049999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 195 0.005 0.9750000000000008\n",
      "blue_1 True True 195 -2.0 -1.0299999999999994\n",
      "agent_timesteps_total: 13738\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-11-42\n",
      "done: false\n",
      "episode_len_mean: 152.64444444444445\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.46464444444445546\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 45\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.190215110778809\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012992454692721367\n",
      "        model: {}\n",
      "        policy_loss: 0.08367619663476944\n",
      "        total_loss: 0.12322621792554855\n",
      "        vf_explained_var: -0.27368035912513733\n",
      "        vf_loss: 0.030780144035816193\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.168960571289062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011952548287808895\n",
      "        model: {}\n",
      "        policy_loss: -0.21945135295391083\n",
      "        total_loss: 0.012077919207513332\n",
      "        vf_explained_var: 0.5614736676216125\n",
      "        vf_loss: 0.22346128523349762\n",
      "  num_agent_steps_sampled: 13738\n",
      "  num_agent_steps_trained: 13738\n",
      "  num_steps_sampled: 6869\n",
      "  num_steps_trained: 6869\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 19\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.5888888888889\n",
      "  ram_util_percent: 36.97676767676767\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.17126666666666776\n",
      "  blue_1: -0.29337777777777835\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6642660696015297\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 147.92098386725525\n",
      "  mean_inference_ms: 3.473529041216568\n",
      "  mean_raw_obs_processing_ms: 39.227262812794635\n",
      "time_since_restore: 1204.6134157180786\n",
      "time_this_iter_s: 66.14200711250305\n",
      "time_total_s: 1204.6134157180786\n",
      "timers:\n",
      "  learn_throughput: 761.686\n",
      "  learn_time_ms: 456.75\n",
      "  load_throughput: 496579.33\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 5.784\n",
      "  sample_time_ms: 60148.299\n",
      "timestamp: 1638983502\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 6869\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:19 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 404 -1.998 0.352999999999992\n",
      "blue_1 True True 404 0.005 2.0979999999999803\n",
      "agent_timesteps_total: 14546\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-12-35\n",
      "done: false\n",
      "episode_len_mean: 158.1086956521739\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.4012608695652294\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 46\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.265438079833984\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012916975654661655\n",
      "        model: {}\n",
      "        policy_loss: -0.2097528725862503\n",
      "        total_loss: -0.05872686579823494\n",
      "        vf_explained_var: -0.23511815071105957\n",
      "        vf_loss: 0.14230704307556152\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.435859680175781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020333191379904747\n",
      "        model: {}\n",
      "        policy_loss: -0.16534702479839325\n",
      "        total_loss: 0.2224445939064026\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.37406671047210693\n",
      "  num_agent_steps_sampled: 14546\n",
      "  num_agent_steps_trained: 14546\n",
      "  num_steps_sampled: 7273\n",
      "  num_steps_trained: 7273\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 20\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.321917808219176\n",
      "  ram_util_percent: 36.99315068493151\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.15986956521739257\n",
      "  blue_1: -0.24139130434782707\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6632363391714403\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 147.5505335471035\n",
      "  mean_inference_ms: 3.465435582274428\n",
      "  mean_raw_obs_processing_ms: 39.15335144637374\n",
      "time_since_restore: 1251.2813742160797\n",
      "time_this_iter_s: 46.6679584980011\n",
      "time_total_s: 1251.2813742160797\n",
      "timers:\n",
      "  learn_throughput: 760.742\n",
      "  learn_time_ms: 474.405\n",
      "  load_throughput: 516470.816\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 5.961\n",
      "  sample_time_ms: 60543.411\n",
      "timestamp: 1638983555\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7273\n",
      "training_iteration: 20\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:20 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 137 -2.0 -1.3199999999999994\n",
      "blue_1 True True 137 0.005 0.6850000000000005\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 387 0.005 1.9349999999999807\n",
      "blue_1 True True 387 -2.0 -0.08400000000001762\n",
      "agent_timesteps_total: 15594\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-13-52\n",
      "done: false\n",
      "episode_len_mean: 162.4375\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.35920833333334606\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 48\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.734068870544434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01578766293823719\n",
      "        model: {}\n",
      "        policy_loss: -0.06452318280935287\n",
      "        total_loss: 0.41196775436401367\n",
      "        vf_explained_var: -0.02040175534784794\n",
      "        vf_loss: 0.46583423018455505\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.548569679260254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008616399951279163\n",
      "        model: {}\n",
      "        policy_loss: -0.1295279711484909\n",
      "        total_loss: 0.1511421799659729\n",
      "        vf_explained_var: 0.06539540737867355\n",
      "        vf_loss: 0.2719460427761078\n",
      "  num_agent_steps_sampled: 15594\n",
      "  num_agent_steps_trained: 15594\n",
      "  num_steps_sampled: 7797\n",
      "  num_steps_trained: 7797\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 21\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.58301886792452\n",
      "  ram_util_percent: 36.99150943396226\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.14039583333333486\n",
      "  blue_1: -0.21881250000000138\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.661049432872774\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 146.79937274624373\n",
      "  mean_inference_ms: 3.449277962417043\n",
      "  mean_raw_obs_processing_ms: 38.974750042014854\n",
      "time_since_restore: 1323.0666539669037\n",
      "time_this_iter_s: 71.78527975082397\n",
      "time_total_s: 1323.0666539669037\n",
      "timers:\n",
      "  learn_throughput: 749.668\n",
      "  learn_time_ms: 515.695\n",
      "  load_throughput: 553135.912\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 6.21\n",
      "  sample_time_ms: 62255.706\n",
      "timestamp: 1638983632\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7797\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:20 starting ! -----------------\n",
      "agent_timesteps_total: 15594\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-13-52\n",
      "done: false\n",
      "episode_len_mean: 162.4375\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.35920833333334606\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 48\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.734068870544434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01578766293823719\n",
      "        model: {}\n",
      "        policy_loss: -0.06452318280935287\n",
      "        total_loss: 0.41196775436401367\n",
      "        vf_explained_var: -0.02040175534784794\n",
      "        vf_loss: 0.46583423018455505\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.548569679260254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008616399951279163\n",
      "        model: {}\n",
      "        policy_loss: -0.1295279711484909\n",
      "        total_loss: 0.1511421799659729\n",
      "        vf_explained_var: 0.06539540737867355\n",
      "        vf_loss: 0.2719460427761078\n",
      "  num_agent_steps_sampled: 15594\n",
      "  num_agent_steps_trained: 15594\n",
      "  num_steps_sampled: 7797\n",
      "  num_steps_trained: 7797\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 21\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.58301886792452\n",
      "  ram_util_percent: 36.99150943396226\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.14039583333333486\n",
      "  blue_1: -0.21881250000000138\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5799999999999996\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.661049432872774\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 146.79937274624373\n",
      "  mean_inference_ms: 3.449277962417043\n",
      "  mean_raw_obs_processing_ms: 38.974750042014854\n",
      "time_since_restore: 1323.0666539669037\n",
      "time_this_iter_s: 71.78527975082397\n",
      "time_total_s: 1323.0666539669037\n",
      "timers:\n",
      "  learn_throughput: 749.668\n",
      "  learn_time_ms: 515.695\n",
      "  load_throughput: 553135.912\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 6.21\n",
      "  sample_time_ms: 62255.706\n",
      "timestamp: 1638983632\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 7797\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:21 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 83 0.005 0.41500000000000026\n",
      "blue_1 True True 83 -2.0 -1.5899999999999999\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 200 0.005 1.0000000000000007\n",
      "blue_1 True True 200 -2.0 -1.0049999999999992\n",
      "agent_timesteps_total: 16160\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-14-48\n",
      "done: false\n",
      "episode_len_mean: 161.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.36844000000001265\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 50\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.43166446685791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013226641342043877\n",
      "        model: {}\n",
      "        policy_loss: -0.22664375603199005\n",
      "        total_loss: -0.19568021595478058\n",
      "        vf_explained_var: 0.6380185484886169\n",
      "        vf_loss: 0.022035544738173485\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.137214660644531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01078676339238882\n",
      "        model: {}\n",
      "        policy_loss: -0.22680360078811646\n",
      "        total_loss: 0.19596727192401886\n",
      "        vf_explained_var: 0.3361210525035858\n",
      "        vf_loss: 0.41184931993484497\n",
      "  num_agent_steps_sampled: 16160\n",
      "  num_agent_steps_trained: 16160\n",
      "  num_steps_sampled: 8080\n",
      "  num_steps_trained: 8080\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 22\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.94871794871796\n",
      "  ram_util_percent: 36.99743589743589\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.10648000000000145\n",
      "  blue_1: -0.2619600000000013\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5899999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6589323416613218\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 146.11461935688482\n",
      "  mean_inference_ms: 3.433996735528313\n",
      "  mean_raw_obs_processing_ms: 38.81689492712133\n",
      "time_since_restore: 1373.4688165187836\n",
      "time_this_iter_s: 50.40216255187988\n",
      "time_total_s: 1373.4688165187836\n",
      "timers:\n",
      "  learn_throughput: 754.607\n",
      "  learn_time_ms: 479.19\n",
      "  load_throughput: 515555.213\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.06\n",
      "  sample_time_ms: 59668.448\n",
      "timestamp: 1638983688\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 8080\n",
      "training_iteration: 22\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:22 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 168 0.005 0.8400000000000006\n",
      "blue_1 True True 168 -2.0 -1.1649999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 197 0.005 0.9850000000000008\n",
      "blue_1 True True 197 -2.0 -1.0339999999999994\n",
      "agent_timesteps_total: 16890\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-15-45\n",
      "done: false\n",
      "episode_len_mean: 162.40384615384616\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.3614615384615512\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 52\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.373424530029297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01570185460150242\n",
      "        model: {}\n",
      "        policy_loss: -0.442752480506897\n",
      "        total_loss: -0.4278503954410553\n",
      "        vf_explained_var: 0.6737633943557739\n",
      "        vf_loss: 0.004303325898945332\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.337594985961914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016092447564005852\n",
      "        model: {}\n",
      "        policy_loss: -0.37417274713516235\n",
      "        total_loss: -0.22154928743839264\n",
      "        vf_explained_var: 0.6813175082206726\n",
      "        vf_loss: 0.13632987439632416\n",
      "  num_agent_steps_sampled: 16890\n",
      "  num_agent_steps_trained: 16890\n",
      "  num_steps_sampled: 8445\n",
      "  num_steps_trained: 8445\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 23\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.88987341772152\n",
      "  ram_util_percent: 37.392405063291136\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.06728846153846291\n",
      "  blue_1: -0.29417307692307815\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.5899999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6568847257064209\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 145.4412642761654\n",
      "  mean_inference_ms: 3.4195389879264564\n",
      "  mean_raw_obs_processing_ms: 38.66427770722003\n",
      "time_since_restore: 1424.6122107505798\n",
      "time_this_iter_s: 51.143394231796265\n",
      "time_total_s: 1424.6122107505798\n",
      "timers:\n",
      "  learn_throughput: 768.918\n",
      "  learn_time_ms: 478.594\n",
      "  load_throughput: 611603.547\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 6.139\n",
      "  sample_time_ms: 59944.293\n",
      "timestamp: 1638983745\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 8445\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:23 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 92 0.005 0.4600000000000003\n",
      "blue_1 True True 92 -2.0 -1.5449999999999997\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 69 0.005 0.3450000000000002\n",
      "blue_1 True True 69 -2.0 -1.6599999999999997\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 171 0.005 0.8550000000000006\n",
      "blue_1 True True 171 -2.0 -1.1499999999999995\n",
      "agent_timesteps_total: 17554\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-16-48\n",
      "done: false\n",
      "episode_len_mean: 159.5818181818182\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.39074545454546694\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 55\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.186394691467285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023703692480921745\n",
      "        model: {}\n",
      "        policy_loss: -0.2188088446855545\n",
      "        total_loss: -0.1905389130115509\n",
      "        vf_explained_var: 0.6000088453292847\n",
      "        vf_loss: 0.01226992066949606\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.939918041229248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00824602972716093\n",
      "        model: {}\n",
      "        policy_loss: -0.2875770032405853\n",
      "        total_loss: -0.12750187516212463\n",
      "        vf_explained_var: 0.6668657064437866\n",
      "        vf_loss: 0.15172597765922546\n",
      "  num_agent_steps_sampled: 17554\n",
      "  num_agent_steps_trained: 17554\n",
      "  num_steps_sampled: 8777\n",
      "  num_steps_trained: 8777\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 24\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.37386363636363\n",
      "  ram_util_percent: 37.392045454545446\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.03343636363636491\n",
      "  blue_1: -0.357309090909092\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.6599999999999997\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6540266651543539\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 144.50510508210505\n",
      "  mean_inference_ms: 3.399101208365235\n",
      "  mean_raw_obs_processing_ms: 38.488039935479684\n",
      "time_since_restore: 1482.59970164299\n",
      "time_this_iter_s: 57.98749089241028\n",
      "time_total_s: 1482.59970164299\n",
      "timers:\n",
      "  learn_throughput: 790.149\n",
      "  learn_time_ms: 458.648\n",
      "  load_throughput: 602153.377\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 6.086\n",
      "  sample_time_ms: 59544.16\n",
      "timestamp: 1638983808\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 8777\n",
      "training_iteration: 24\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:24 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 96 -2.0 -1.5249999999999997\n",
      "blue_1 True True 96 0.005 0.4800000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 140 0.005 0.7000000000000005\n",
      "blue_1 True True 140 -2.0 -1.3049999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 107 -2.0 -1.4699999999999998\n",
      "blue_1 True True 107 0.005 0.5350000000000004\n",
      "agent_timesteps_total: 18240\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-17-59\n",
      "done: false\n",
      "episode_len_mean: 157.24137931034483\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.4151034482758739\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 58\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.235904693603516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013962700963020325\n",
      "        model: {}\n",
      "        policy_loss: -0.34606507420539856\n",
      "        total_loss: 0.26962873339653015\n",
      "        vf_explained_var: 0.035411518067121506\n",
      "        vf_loss: 0.6015565991401672\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.641108989715576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013651171699166298\n",
      "        model: {}\n",
      "        policy_loss: 0.3287993371486664\n",
      "        total_loss: 0.5041958689689636\n",
      "        vf_explained_var: -0.11761517822742462\n",
      "        vf_loss: 0.16157473623752594\n",
      "  num_agent_steps_sampled: 18240\n",
      "  num_agent_steps_trained: 18240\n",
      "  num_steps_sampled: 9120\n",
      "  num_steps_trained: 9120\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 25\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.88787878787878\n",
      "  ram_util_percent: 37.37676767676768\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.07127586206896672\n",
      "  blue_1: -0.3438275862068976\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.6599999999999997\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6513914991592022\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 143.68419473284757\n",
      "  mean_inference_ms: 3.3800019290833556\n",
      "  mean_raw_obs_processing_ms: 38.35583957035046\n",
      "time_since_restore: 1548.0259284973145\n",
      "time_this_iter_s: 65.42622685432434\n",
      "time_total_s: 1548.0259284973145\n",
      "timers:\n",
      "  learn_throughput: 805.823\n",
      "  learn_time_ms: 459.778\n",
      "  load_throughput: 615612.103\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 6.038\n",
      "  sample_time_ms: 61359.97\n",
      "timestamp: 1638983879\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 9120\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:25 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 163 0.005 0.8150000000000006\n",
      "blue_1 True True 163 -2.0 -1.1899999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 48 -2.0 -1.765\n",
      "blue_1 True True 48 0.005 0.24000000000000013\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 74 -2.0 -1.6349999999999998\n",
      "blue_1 True True 74 0.005 0.3700000000000002\n",
      "agent_timesteps_total: 18810\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-19-01\n",
      "done: false\n",
      "episode_len_mean: 154.18032786885246\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.581999999999922\n",
      "episode_reward_mean: -0.44657377049181474\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 61\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.431756973266602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015631208196282387\n",
      "        model: {}\n",
      "        policy_loss: -0.24263812601566315\n",
      "        total_loss: 0.2841806709766388\n",
      "        vf_explained_var: -0.39254361391067505\n",
      "        vf_loss: 0.510992169380188\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.21367359161377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01267229113727808\n",
      "        model: {}\n",
      "        policy_loss: 0.06756316125392914\n",
      "        total_loss: 0.20683708786964417\n",
      "        vf_explained_var: 0.6639786958694458\n",
      "        vf_loss: 0.12644322216510773\n",
      "  num_agent_steps_sampled: 18810\n",
      "  num_agent_steps_trained: 18810\n",
      "  num_steps_sampled: 9405\n",
      "  num_steps_trained: 9405\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 26\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.883529411764705\n",
      "  ram_util_percent: 37.43411764705883\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.94799999999999\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.11014754098360768\n",
      "  blue_1: -0.3364262295081977\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.6599999999999997\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.648945761586987\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 142.95286341001398\n",
      "  mean_inference_ms: 3.362612078997681\n",
      "  mean_raw_obs_processing_ms: 38.27053621311142\n",
      "time_since_restore: 1604.4588387012482\n",
      "time_this_iter_s: 56.432910203933716\n",
      "time_total_s: 1604.4588387012482\n",
      "timers:\n",
      "  learn_throughput: 795.34\n",
      "  learn_time_ms: 459.678\n",
      "  load_throughput: 610323.4\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 5.93\n",
      "  sample_time_ms: 61648.578\n",
      "timestamp: 1638983941\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 9405\n",
      "training_iteration: 26\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:26 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.026999999999959\n",
      "agent_timesteps_total: 20010\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-20-19\n",
      "done: false\n",
      "episode_len_mean: 161.3709677419355\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.37441935483872263\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 62\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.678204536437988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006921911612153053\n",
      "        model: {}\n",
      "        policy_loss: -0.3925185799598694\n",
      "        total_loss: -0.3176053464412689\n",
      "        vf_explained_var: -0.647697389125824\n",
      "        vf_loss: 0.06790483742952347\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.254459381103516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010327337309718132\n",
      "        model: {}\n",
      "        policy_loss: -0.3009629547595978\n",
      "        total_loss: -0.08445458114147186\n",
      "        vf_explained_var: -0.9995990991592407\n",
      "        vf_loss: 0.20605196058750153\n",
      "  num_agent_steps_sampled: 20010\n",
      "  num_agent_steps_trained: 20010\n",
      "  num_steps_sampled: 10005\n",
      "  num_steps_trained: 10005\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 27\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.87358490566038\n",
      "  ram_util_percent: 37.46415094339623\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.07611290322580823\n",
      "  blue_1: -0.2983064516129049\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.6599999999999997\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6481494140131697\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 142.702134620049\n",
      "  mean_inference_ms: 3.356923901734239\n",
      "  mean_raw_obs_processing_ms: 38.21796422972969\n",
      "time_since_restore: 1676.22438955307\n",
      "time_this_iter_s: 71.7655508518219\n",
      "time_total_s: 1676.22438955307\n",
      "timers:\n",
      "  learn_throughput: 813.753\n",
      "  learn_time_ms: 474.837\n",
      "  load_throughput: 552981.802\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 6.011\n",
      "  sample_time_ms: 64280.232\n",
      "timestamp: 1638984019\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 10005\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:27 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 107 -2.0 -1.4699999999999998\n",
      "blue_1 True True 107 0.005 0.5350000000000004\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 129 -2.0 -1.3599999999999994\n",
      "blue_1 True True 129 0.005 0.6450000000000005\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 65 0.005 0.3250000000000002\n",
      "blue_1 True True 65 -2.0 -1.6799999999999997\n",
      "agent_timesteps_total: 20612\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-21-25\n",
      "done: false\n",
      "episode_len_mean: 158.55384615384617\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4033692307692432\n",
      "episode_reward_min: -1.605\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 65\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.433828353881836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019071033224463463\n",
      "        model: {}\n",
      "        policy_loss: 0.1235930472612381\n",
      "        total_loss: 1.0642259120941162\n",
      "        vf_explained_var: -0.030538860708475113\n",
      "        vf_loss: 0.9213234782218933\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.599371910095215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008474042639136314\n",
      "        model: {}\n",
      "        policy_loss: -0.3976689875125885\n",
      "        total_loss: -0.2019587904214859\n",
      "        vf_explained_var: -0.570634126663208\n",
      "        vf_loss: 0.18713025748729706\n",
      "  num_agent_steps_sampled: 20612\n",
      "  num_agent_steps_trained: 20612\n",
      "  num_steps_sampled: 10306\n",
      "  num_steps_trained: 10306\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 28\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.738461538461536\n",
      "  ram_util_percent: 37.417582417582416\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.11113846153846323\n",
      "  blue_1: -0.29223076923077085\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.6799999999999997\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6458183924621272\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 142.01539128045235\n",
      "  mean_inference_ms: 3.3405634202870442\n",
      "  mean_raw_obs_processing_ms: 38.09709684558956\n",
      "time_since_restore: 1736.8098170757294\n",
      "time_this_iter_s: 60.5854275226593\n",
      "time_total_s: 1736.8098170757294\n",
      "timers:\n",
      "  learn_throughput: 815.133\n",
      "  learn_time_ms: 473.42\n",
      "  load_throughput: 552303.936\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 5.9\n",
      "  sample_time_ms: 65411.138\n",
      "timestamp: 1638984085\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 10306\n",
      "training_iteration: 28\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:28 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 29 0.005 0.14500000000000005\n",
      "blue_1 True True 29 -2.0 -1.8599999999999999\n",
      "blue_0DOWN\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 107 -2.0 -1.4699999999999998\n",
      "blue_1 True True 107 -2.0 -1.4699999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 215 -2.0 -0.9300000000000008\n",
      "blue_1 True True 215 0.005 1.074999999999999\n",
      "agent_timesteps_total: 21314\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-22-36\n",
      "done: false\n",
      "episode_len_mean: 156.72058823529412\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.45189705882354164\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 68\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.481910705566406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011006140150129795\n",
      "        model: {}\n",
      "        policy_loss: -0.3598683774471283\n",
      "        total_loss: -0.015537838451564312\n",
      "        vf_explained_var: 0.19612489640712738\n",
      "        vf_loss: 0.3331867754459381\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.286442756652832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015838388353586197\n",
      "        model: {}\n",
      "        policy_loss: 0.14942437410354614\n",
      "        total_loss: 0.820769727230072\n",
      "        vf_explained_var: 0.13489480316638947\n",
      "        vf_loss: 0.6553089618682861\n",
      "  num_agent_steps_sampled: 21314\n",
      "  num_agent_steps_trained: 21314\n",
      "  num_steps_sampled: 10657\n",
      "  num_steps_trained: 10657\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 29\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.571717171717175\n",
      "  ram_util_percent: 37.418181818181814\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.13939705882353104\n",
      "  blue_1: -0.3125000000000015\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6435637644780122\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 141.39935776622363\n",
      "  mean_inference_ms: 3.325218563392107\n",
      "  mean_raw_obs_processing_ms: 38.00433198866906\n",
      "time_since_restore: 1802.3221158981323\n",
      "time_this_iter_s: 65.51229882240295\n",
      "time_total_s: 1802.3221158981323\n",
      "timers:\n",
      "  learn_throughput: 830.218\n",
      "  learn_time_ms: 456.266\n",
      "  load_throughput: 542142.345\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 5.795\n",
      "  sample_time_ms: 65363.943\n",
      "timestamp: 1638984156\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 10657\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:29 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 115 0.005 0.5750000000000004\n",
      "blue_1 True True 115 -2.0 -1.4299999999999997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 262 0.005 1.309999999999994\n",
      "blue_1 True True 262 -1.998 -0.45200000000000173\n",
      "agent_timesteps_total: 22068\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-23-38\n",
      "done: false\n",
      "episode_len_mean: 157.62857142857143\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4389428571428694\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 70\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.419123649597168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01901080645620823\n",
      "        model: {}\n",
      "        policy_loss: -0.20868441462516785\n",
      "        total_loss: 0.12868963181972504\n",
      "        vf_explained_var: 0.40314796566963196\n",
      "        vf_loss: 0.3181256651878357\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.4932861328125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012347306124866009\n",
      "        model: {}\n",
      "        policy_loss: -0.3004280626773834\n",
      "        total_loss: 0.15416160225868225\n",
      "        vf_explained_var: -0.3048640787601471\n",
      "        vf_loss: 0.4420880973339081\n",
      "  num_agent_steps_sampled: 22068\n",
      "  num_agent_steps_trained: 22068\n",
      "  num_steps_sampled: 11034\n",
      "  num_steps_trained: 11034\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 30\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.208139534883706\n",
      "  ram_util_percent: 37.4093023255814\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.10848571428571593\n",
      "  blue_1: -0.33045714285714434\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6420909877675302\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 141.00552965089867\n",
      "  mean_inference_ms: 3.3155302441324395\n",
      "  mean_raw_obs_processing_ms: 37.940892170803956\n",
      "time_since_restore: 1858.6397774219513\n",
      "time_this_iter_s: 56.31766152381897\n",
      "time_total_s: 1858.6397774219513\n",
      "timers:\n",
      "  learn_throughput: 857.476\n",
      "  learn_time_ms: 438.613\n",
      "  load_throughput: 536794.411\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 5.67\n",
      "  sample_time_ms: 66332.165\n",
      "timestamp: 1638984218\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 11034\n",
      "training_iteration: 30\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:30 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 196 -2.0 -1.0249999999999992\n",
      "blue_1 True True 196 0.005 0.9800000000000008\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 259 -2.0 -0.7100000000000055\n",
      "blue_1 True True 259 0.005 1.2949999999999944\n",
      "agent_timesteps_total: 22978\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-24-46\n",
      "done: false\n",
      "episode_len_mean: 159.56944444444446\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.41925000000001267\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 72\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.517144203186035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016644222661852837\n",
      "        model: {}\n",
      "        policy_loss: -0.30738621950149536\n",
      "        total_loss: 0.034232135862112045\n",
      "        vf_explained_var: 0.34797677397727966\n",
      "        vf_loss: 0.32476606965065\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.33603286743164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013364119455218315\n",
      "        model: {}\n",
      "        policy_loss: -0.2657742500305176\n",
      "        total_loss: -0.1729646623134613\n",
      "        vf_explained_var: -0.8508213758468628\n",
      "        vf_loss: 0.07927843928337097\n",
      "  num_agent_steps_sampled: 22978\n",
      "  num_agent_steps_trained: 22978\n",
      "  num_steps_sampled: 11489\n",
      "  num_steps_trained: 11489\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 31\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.975531914893615\n",
      "  ram_util_percent: 37.43936170212766\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.1295694444444461\n",
      "  blue_1: -0.28968055555555705\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6407095710536734\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 140.61238205889535\n",
      "  mean_inference_ms: 3.306090157856483\n",
      "  mean_raw_obs_processing_ms: 37.868867799231424\n",
      "time_since_restore: 1921.2665770053864\n",
      "time_this_iter_s: 62.62679958343506\n",
      "time_total_s: 1921.2665770053864\n",
      "timers:\n",
      "  learn_throughput: 890.767\n",
      "  learn_time_ms: 414.474\n",
      "  load_throughput: 526802.87\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 5.643\n",
      "  sample_time_ms: 65422.956\n",
      "timestamp: 1638984286\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 11489\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:30 starting ! -----------------\n",
      "agent_timesteps_total: 22978\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-24-46\n",
      "done: false\n",
      "episode_len_mean: 159.56944444444446\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.41925000000001267\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 72\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.517144203186035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016644222661852837\n",
      "        model: {}\n",
      "        policy_loss: -0.30738621950149536\n",
      "        total_loss: 0.034232135862112045\n",
      "        vf_explained_var: 0.34797677397727966\n",
      "        vf_loss: 0.32476606965065\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.33603286743164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013364119455218315\n",
      "        model: {}\n",
      "        policy_loss: -0.2657742500305176\n",
      "        total_loss: -0.1729646623134613\n",
      "        vf_explained_var: -0.8508213758468628\n",
      "        vf_loss: 0.07927843928337097\n",
      "  num_agent_steps_sampled: 22978\n",
      "  num_agent_steps_trained: 22978\n",
      "  num_steps_sampled: 11489\n",
      "  num_steps_trained: 11489\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 31\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.975531914893615\n",
      "  ram_util_percent: 37.43936170212766\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.1295694444444461\n",
      "  blue_1: -0.28968055555555705\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6407095710536734\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 140.61238205889535\n",
      "  mean_inference_ms: 3.306090157856483\n",
      "  mean_raw_obs_processing_ms: 37.868867799231424\n",
      "time_since_restore: 1921.2665770053864\n",
      "time_this_iter_s: 62.62679958343506\n",
      "time_total_s: 1921.2665770053864\n",
      "timers:\n",
      "  learn_throughput: 890.767\n",
      "  learn_time_ms: 414.474\n",
      "  load_throughput: 526802.87\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 5.643\n",
      "  sample_time_ms: 65422.956\n",
      "timestamp: 1638984286\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 11489\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:31 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 103 -2.0 -1.4899999999999998\n",
      "blue_1 True True 103 0.005 0.5150000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 176 0.005 0.8800000000000007\n",
      "blue_1 True True 176 -2.0 -1.1249999999999993\n",
      "agent_timesteps_total: 23536\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-25-37\n",
      "done: false\n",
      "episode_len_mean: 159.02702702702703\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.424405405405418\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 74\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.260869979858398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013645772822201252\n",
      "        model: {}\n",
      "        policy_loss: -0.029441216960549355\n",
      "        total_loss: 0.44232621788978577\n",
      "        vf_explained_var: 0.2552935779094696\n",
      "        vf_loss: 0.4579510986804962\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.501669883728027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008825506083667278\n",
      "        model: {}\n",
      "        policy_loss: -0.25709494948387146\n",
      "        total_loss: -0.09244373440742493\n",
      "        vf_explained_var: -0.10109683126211166\n",
      "        vf_loss: 0.15571537613868713\n",
      "  num_agent_steps_sampled: 23536\n",
      "  num_agent_steps_trained: 23536\n",
      "  num_steps_sampled: 11768\n",
      "  num_steps_trained: 11768\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 32\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.56619718309859\n",
      "  ram_util_percent: 37.43661971830986\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.13431081081081242\n",
      "  blue_1: -0.2900945945945961\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6394057667486819\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 140.235754474066\n",
      "  mean_inference_ms: 3.2970049655378983\n",
      "  mean_raw_obs_processing_ms: 37.8035819695149\n",
      "time_since_restore: 1967.0003399848938\n",
      "time_this_iter_s: 45.733762979507446\n",
      "time_total_s: 1967.0003399848938\n",
      "timers:\n",
      "  learn_throughput: 888.086\n",
      "  learn_time_ms: 415.275\n",
      "  load_throughput: 528208.747\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 5.681\n",
      "  sample_time_ms: 64922.307\n",
      "timestamp: 1638984337\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 11768\n",
      "training_iteration: 32\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:32 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 274 0.005 1.3699999999999928\n",
      "blue_1 True True 274 -2.0 -0.6350000000000071\n",
      "agent_timesteps_total: 24084\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-26-20\n",
      "done: false\n",
      "episode_len_mean: 160.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.40894666666667956\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 75\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.467756271362305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012324532493948936\n",
      "        model: {}\n",
      "        policy_loss: -0.06814851611852646\n",
      "        total_loss: 0.09138258546590805\n",
      "        vf_explained_var: -0.9845232367515564\n",
      "        vf_loss: 0.14705252647399902\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.153644561767578\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015102664940059185\n",
      "        model: {}\n",
      "        policy_loss: -0.22173859179019928\n",
      "        total_loss: 0.027980748564004898\n",
      "        vf_explained_var: 0.017280273139476776\n",
      "        vf_loss: 0.23442789912223816\n",
      "  num_agent_steps_sampled: 24084\n",
      "  num_agent_steps_trained: 24084\n",
      "  num_steps_sampled: 12042\n",
      "  num_steps_trained: 12042\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 33\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.523728813559316\n",
      "  ram_util_percent: 37.4135593220339\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.11425333333333502\n",
      "  blue_1: -0.2946933333333349\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6387746723793281\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 140.05017739349339\n",
      "  mean_inference_ms: 3.292572962072975\n",
      "  mean_raw_obs_processing_ms: 37.76771432108473\n",
      "time_since_restore: 2004.275444984436\n",
      "time_this_iter_s: 37.275104999542236\n",
      "time_total_s: 2004.275444984436\n",
      "timers:\n",
      "  learn_throughput: 865.14\n",
      "  learn_time_ms: 415.771\n",
      "  load_throughput: 450732.298\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 5.661\n",
      "  sample_time_ms: 63536.658\n",
      "timestamp: 1638984380\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 12042\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:33 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 44 0.005 0.2200000000000001\n",
      "blue_1 True True 44 -2.0 -1.785\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 154 0.005 0.7700000000000006\n",
      "blue_1 True True 154 -2.0 -1.2349999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 202 0.005 1.0100000000000005\n",
      "blue_1 True True 202 -2.0 -0.9949999999999994\n",
      "agent_timesteps_total: 24884\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-27-35\n",
      "done: false\n",
      "episode_len_mean: 159.51282051282053\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4190512820512948\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 78\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.3837308883667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015118931420147419\n",
      "        model: {}\n",
      "        policy_loss: -0.07659237086772919\n",
      "        total_loss: -0.03857309743762016\n",
      "        vf_explained_var: 0.13639861345291138\n",
      "        vf_loss: 0.022711340337991714\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.225756645202637\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013454838655889034\n",
      "        model: {}\n",
      "        policy_loss: -0.14589978754520416\n",
      "        total_loss: 0.17387525737285614\n",
      "        vf_explained_var: 0.15524394810199738\n",
      "        vf_loss: 0.30615201592445374\n",
      "  num_agent_steps_sampled: 24884\n",
      "  num_agent_steps_trained: 24884\n",
      "  num_steps_sampled: 12442\n",
      "  num_steps_trained: 12442\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 34\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.232692307692304\n",
      "  ram_util_percent: 37.596153846153854\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.08421794871795032\n",
      "  blue_1: -0.33483333333333487\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6369155352121655\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 139.521496796705\n",
      "  mean_inference_ms: 3.27986929330897\n",
      "  mean_raw_obs_processing_ms: 37.67401327338201\n",
      "time_since_restore: 2073.309403657913\n",
      "time_this_iter_s: 69.03395867347717\n",
      "time_total_s: 2073.309403657913\n",
      "timers:\n",
      "  learn_throughput: 850.683\n",
      "  learn_time_ms: 430.83\n",
      "  load_throughput: 459280.674\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 5.671\n",
      "  sample_time_ms: 64622.791\n",
      "timestamp: 1638984455\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 12442\n",
      "training_iteration: 34\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:34 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 152 -2.0 -1.2449999999999994\n",
      "blue_1 True True 152 0.005 0.7600000000000006\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 87 0.005 0.4350000000000003\n",
      "blue_1 True True 87 -2.0 -1.5699999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 202 0.005 1.0100000000000005\n",
      "blue_1 True True 202 -2.0 -0.9949999999999994\n",
      "agent_timesteps_total: 25766\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-28-49\n",
      "done: false\n",
      "episode_len_mean: 159.0493827160494\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.42334567901235826\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 81\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.653471946716309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014400109648704529\n",
      "        model: {}\n",
      "        policy_loss: 0.004231810104101896\n",
      "        total_loss: 0.41761788725852966\n",
      "        vf_explained_var: -0.16450496017932892\n",
      "        vf_loss: 0.39880600571632385\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.414652824401855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010924283415079117\n",
      "        model: {}\n",
      "        policy_loss: -0.2536658048629761\n",
      "        total_loss: 0.12300639599561691\n",
      "        vf_explained_var: -0.101924367249012\n",
      "        vf_loss: 0.36561134457588196\n",
      "  num_agent_steps_sampled: 25766\n",
      "  num_agent_steps_trained: 25766\n",
      "  num_steps_sampled: 12883\n",
      "  num_steps_trained: 12883\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 35\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.869902912621356\n",
      "  ram_util_percent: 37.5\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.07862962962963116\n",
      "  blue_1: -0.34471604938271744\n",
      "policy_reward_min:\n",
      "  blue_0: -1.805\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6351566828025037\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 139.01653770149588\n",
      "  mean_inference_ms: 3.267877709159129\n",
      "  mean_raw_obs_processing_ms: 37.590683297648944\n",
      "time_since_restore: 2142.4247086048126\n",
      "time_this_iter_s: 69.11530494689941\n",
      "time_total_s: 2142.4247086048126\n",
      "timers:\n",
      "  learn_throughput: 841.107\n",
      "  learn_time_ms: 447.387\n",
      "  load_throughput: 471561.576\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 5.79\n",
      "  sample_time_ms: 64988.368\n",
      "timestamp: 1638984529\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 12883\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:35 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 103 -2.0 -1.4899999999999998\n",
      "blue_1 True True 103 0.005 0.5150000000000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 35 -2.0 -1.8299999999999998\n",
      "blue_1 True True 35 0.005 0.17500000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 379 0.005 1.8949999999999816\n",
      "blue_1 True True 379 -2.0 -0.08200000000001828\n",
      "agent_timesteps_total: 26800\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-30-17\n",
      "done: false\n",
      "episode_len_mean: 159.52380952380952\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4179523809523938\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 84\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.7182035446167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0142704788595438\n",
      "        model: {}\n",
      "        policy_loss: -0.07828971743583679\n",
      "        total_loss: 0.5306165814399719\n",
      "        vf_explained_var: -0.4397851824760437\n",
      "        vf_loss: 0.5944573879241943\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.501819610595703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013547818176448345\n",
      "        model: {}\n",
      "        policy_loss: -0.10773275792598724\n",
      "        total_loss: 0.19299229979515076\n",
      "        vf_explained_var: -0.13946743309497833\n",
      "        vf_loss: 0.2870078980922699\n",
      "  num_agent_steps_sampled: 26800\n",
      "  num_agent_steps_trained: 26800\n",
      "  num_steps_sampled: 13400\n",
      "  num_steps_trained: 13400\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 36\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.63966942148761\n",
      "  ram_util_percent: 37.48099173553719\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.09278571428571598\n",
      "  blue_1: -0.3251666666666682\n",
      "policy_reward_min:\n",
      "  blue_0: -1.8299999999999998\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6334184119700423\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 138.54092694150435\n",
      "  mean_inference_ms: 3.2565130955574793\n",
      "  mean_raw_obs_processing_ms: 37.5091510762712\n",
      "time_since_restore: 2224.145038843155\n",
      "time_this_iter_s: 81.72033023834229\n",
      "time_total_s: 2224.145038843155\n",
      "timers:\n",
      "  learn_throughput: 833.672\n",
      "  learn_time_ms: 479.205\n",
      "  load_throughput: 572002.611\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 5.919\n",
      "  sample_time_ms: 67499.027\n",
      "timestamp: 1638984617\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 13400\n",
      "training_iteration: 36\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:36 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 46 -2.0 -1.775\n",
      "blue_1 True True 46 0.005 0.23000000000000012\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 138 0.005 0.6900000000000005\n",
      "blue_1 True True 138 -2.0 -1.3149999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 109 0.005 0.5450000000000004\n",
      "blue_1 True True 109 -2.0 -1.4599999999999995\n",
      "agent_timesteps_total: 27386\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-31-21\n",
      "done: false\n",
      "episode_len_mean: 157.39080459770116\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4390000000000125\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 87\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.387792587280273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012846983037889004\n",
      "        model: {}\n",
      "        policy_loss: -0.03690866753458977\n",
      "        total_loss: 0.4360024333000183\n",
      "        vf_explained_var: 0.3016919493675232\n",
      "        vf_loss: 0.4599035084247589\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.8562445640563965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0188855342566967\n",
      "        model: {}\n",
      "        policy_loss: -0.23726268112659454\n",
      "        total_loss: 0.07472105324268341\n",
      "        vf_explained_var: 0.10530772060155869\n",
      "        vf_loss: 0.2928621172904968\n",
      "  num_agent_steps_sampled: 27386\n",
      "  num_agent_steps_trained: 27386\n",
      "  num_steps_sampled: 13693\n",
      "  num_steps_trained: 13693\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 37\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.83370786516854\n",
      "  ram_util_percent: 37.465168539325845\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.09579310344827749\n",
      "  blue_1: -0.34320689655172565\n",
      "policy_reward_min:\n",
      "  blue_0: -1.8299999999999998\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6318269001462626\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 138.109197875513\n",
      "  mean_inference_ms: 3.245725037563296\n",
      "  mean_raw_obs_processing_ms: 37.44901974756769\n",
      "time_since_restore: 2283.317233324051\n",
      "time_this_iter_s: 59.172194480895996\n",
      "time_total_s: 2283.317233324051\n",
      "timers:\n",
      "  learn_throughput: 826.14\n",
      "  learn_time_ms: 446.413\n",
      "  load_throughput: 528118.578\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 5.563\n",
      "  sample_time_ms: 66299.911\n",
      "timestamp: 1638984681\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 13693\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:37 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 213 0.005 1.0649999999999993\n",
      "blue_1 True True 213 -2.0 -0.9400000000000006\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 267 0.005 1.3349999999999935\n",
      "blue_1 True True 267 -2.001 -0.7050000000000025\n",
      "agent_timesteps_total: 28346\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-32-32\n",
      "done: false\n",
      "episode_len_mean: 159.24719101123594\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.42065168539327125\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 89\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.691693305969238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010104967281222343\n",
      "        model: {}\n",
      "        policy_loss: -0.2600524127483368\n",
      "        total_loss: -0.18792423605918884\n",
      "        vf_explained_var: 0.10272937268018723\n",
      "        vf_loss: 0.061896905303001404\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.276726722717285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01527023408561945\n",
      "        model: {}\n",
      "        policy_loss: -0.2900082767009735\n",
      "        total_loss: 0.06678806990385056\n",
      "        vf_explained_var: 0.1028166264295578\n",
      "        vf_loss: 0.341335266828537\n",
      "  num_agent_steps_sampled: 28346\n",
      "  num_agent_steps_trained: 28346\n",
      "  num_steps_sampled: 14173\n",
      "  num_steps_trained: 14173\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 38\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.06907216494846\n",
      "  ram_util_percent: 37.473195876288656\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.06667415730337245\n",
      "  blue_1: -0.3539775280898891\n",
      "policy_reward_min:\n",
      "  blue_0: -1.8299999999999998\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6308071334565591\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 137.82295583402654\n",
      "  mean_inference_ms: 3.238787281132208\n",
      "  mean_raw_obs_processing_ms: 37.40185325998064\n",
      "time_since_restore: 2348.017439365387\n",
      "time_this_iter_s: 64.70020604133606\n",
      "time_total_s: 2348.017439365387\n",
      "timers:\n",
      "  learn_throughput: 833.28\n",
      "  learn_time_ms: 464.07\n",
      "  load_throughput: 553732.326\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 5.801\n",
      "  sample_time_ms: 66662.89\n",
      "timestamp: 1638984752\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 14173\n",
      "training_iteration: 38\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:38 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 340 -2.0 -0.30500000000001415\n",
      "blue_1 True True 340 0.005 1.6999999999999857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 29026\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-33-19\n",
      "done: false\n",
      "episode_len_mean: 161.25555555555556\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.400477777777791\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 90\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.314188957214355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007322967518121004\n",
      "        model: {}\n",
      "        policy_loss: -0.5401609539985657\n",
      "        total_loss: -0.465212881565094\n",
      "        vf_explained_var: 0.29244616627693176\n",
      "        vf_loss: 0.06753359735012054\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.579607009887695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00891738012433052\n",
      "        model: {}\n",
      "        policy_loss: -0.47366803884506226\n",
      "        total_loss: -0.2668244540691376\n",
      "        vf_explained_var: -0.6044830083847046\n",
      "        vf_loss: 0.1978147178888321\n",
      "  num_agent_steps_sampled: 29026\n",
      "  num_agent_steps_trained: 29026\n",
      "  num_steps_sampled: 14513\n",
      "  num_steps_trained: 14513\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 39\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.12121212121212\n",
      "  ram_util_percent: 37.471212121212126\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.06932222222222402\n",
      "  blue_1: -0.33115555555555715\n",
      "policy_reward_min:\n",
      "  blue_0: -1.8299999999999998\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6303105009160118\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 137.6785596667352\n",
      "  mean_inference_ms: 3.2353911057158937\n",
      "  mean_raw_obs_processing_ms: 37.37420316308048\n",
      "time_since_restore: 2389.926873922348\n",
      "time_this_iter_s: 41.90943455696106\n",
      "time_total_s: 2389.926873922348\n",
      "timers:\n",
      "  learn_throughput: 830.953\n",
      "  learn_time_ms: 464.046\n",
      "  load_throughput: 552157.189\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 5.995\n",
      "  sample_time_ms: 64319.539\n",
      "timestamp: 1638984799\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 14513\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:39 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 26 -2.0 -1.875\n",
      "blue_1 True True 26 0.005 0.13000000000000003\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 189 -2.0 -1.0599999999999992\n",
      "blue_1 True True 189 0.005 0.9450000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 249 0.005 1.2449999999999954\n",
      "blue_1 True True 249 -2.0 -0.7460000000000029\n",
      "agent_timesteps_total: 29954\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-34-40\n",
      "done: false\n",
      "episode_len_mean: 161.04301075268816\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.40219354838711\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 93\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.599746704101562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012384527362883091\n",
      "        model: {}\n",
      "        policy_loss: 0.06361251324415207\n",
      "        total_loss: 0.598774790763855\n",
      "        vf_explained_var: -0.1541110724210739\n",
      "        vf_loss: 0.52262282371521\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.267848014831543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007899670861661434\n",
      "        model: {}\n",
      "        policy_loss: -0.4182893633842468\n",
      "        total_loss: -0.3306538760662079\n",
      "        vf_explained_var: -0.5204359889030457\n",
      "        vf_loss: 0.0796370878815651\n",
      "  num_agent_steps_sampled: 29954\n",
      "  num_agent_steps_trained: 29954\n",
      "  num_steps_sampled: 14977\n",
      "  num_steps_trained: 14977\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 40\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.824324324324316\n",
      "  ram_util_percent: 37.46396396396396\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.0852580645161308\n",
      "  blue_1: -0.3169354838709693\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6288402603079347\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 137.26036110782692\n",
      "  mean_inference_ms: 3.225474427446505\n",
      "  mean_raw_obs_processing_ms: 37.295879739462336\n",
      "time_since_restore: 2464.9349098205566\n",
      "time_this_iter_s: 75.00803589820862\n",
      "time_total_s: 2464.9349098205566\n",
      "timers:\n",
      "  learn_throughput: 816.2\n",
      "  learn_time_ms: 483.092\n",
      "  load_throughput: 564653.647\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 5.959\n",
      "  sample_time_ms: 66166.362\n",
      "timestamp: 1638984880\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 14977\n",
      "training_iteration: 40\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:40 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 266 0.005 1.3299999999999936\n",
      "blue_1 True True 266 -2.0 -0.6750000000000063\n",
      "agent_timesteps_total: 30486\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-35-20\n",
      "done: false\n",
      "episode_len_mean: 162.1595744680851\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3909468085106518\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 94\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.197490692138672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015466680750250816\n",
      "        model: {}\n",
      "        policy_loss: -0.1405397206544876\n",
      "        total_loss: -0.05983930826187134\n",
      "        vf_explained_var: -0.5434804558753967\n",
      "        vf_loss: 0.06504041701555252\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.468384265899658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015125039033591747\n",
      "        model: {}\n",
      "        policy_loss: -0.1677522510290146\n",
      "        total_loss: 0.158499613404274\n",
      "        vf_explained_var: -0.20495246350765228\n",
      "        vf_loss: 0.3109377920627594\n",
      "  num_agent_steps_sampled: 30486\n",
      "  num_agent_steps_trained: 30486\n",
      "  num_steps_sampled: 15243\n",
      "  num_steps_trained: 15243\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 41\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.92857142857143\n",
      "  ram_util_percent: 37.473214285714285\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.07020212765957627\n",
      "  blue_1: -0.3207446808510655\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.628367580468793\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 137.1231843154775\n",
      "  mean_inference_ms: 3.2222852676876506\n",
      "  mean_raw_obs_processing_ms: 37.26832166562896\n",
      "time_since_restore: 2499.6772005558014\n",
      "time_this_iter_s: 34.74229073524475\n",
      "time_total_s: 2499.6772005558014\n",
      "timers:\n",
      "  learn_throughput: 807.065\n",
      "  learn_time_ms: 465.142\n",
      "  load_throughput: 539392.868\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 5.919\n",
      "  sample_time_ms: 63418.753\n",
      "timestamp: 1638984920\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 15243\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:40 starting ! -----------------\n",
      "agent_timesteps_total: 30486\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-35-20\n",
      "done: false\n",
      "episode_len_mean: 162.1595744680851\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3909468085106518\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 94\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.197490692138672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015466680750250816\n",
      "        model: {}\n",
      "        policy_loss: -0.1405397206544876\n",
      "        total_loss: -0.05983930826187134\n",
      "        vf_explained_var: -0.5434804558753967\n",
      "        vf_loss: 0.06504041701555252\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.468384265899658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015125039033591747\n",
      "        model: {}\n",
      "        policy_loss: -0.1677522510290146\n",
      "        total_loss: 0.158499613404274\n",
      "        vf_explained_var: -0.20495246350765228\n",
      "        vf_loss: 0.3109377920627594\n",
      "  num_agent_steps_sampled: 30486\n",
      "  num_agent_steps_trained: 30486\n",
      "  num_steps_sampled: 15243\n",
      "  num_steps_trained: 15243\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 41\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.92857142857143\n",
      "  ram_util_percent: 37.473214285714285\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.07020212765957627\n",
      "  blue_1: -0.3207446808510655\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.628367580468793\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 137.1231843154775\n",
      "  mean_inference_ms: 3.2222852676876506\n",
      "  mean_raw_obs_processing_ms: 37.26832166562896\n",
      "time_since_restore: 2499.6772005558014\n",
      "time_this_iter_s: 34.74229073524475\n",
      "time_total_s: 2499.6772005558014\n",
      "timers:\n",
      "  learn_throughput: 807.065\n",
      "  learn_time_ms: 465.142\n",
      "  load_throughput: 539392.868\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 5.919\n",
      "  sample_time_ms: 63418.753\n",
      "timestamp: 1638984920\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 15243\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:41 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 79 0.005 0.39500000000000024\n",
      "blue_1 True True 79 -2.0 -1.6099999999999999\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 78 0.005 0.39000000000000024\n",
      "blue_1 True True 78 -2.0 -1.6149999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 154 0.005 0.7700000000000006\n",
      "blue_1 True True 154 -2.0 -1.2349999999999994\n",
      "agent_timesteps_total: 31108\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-36-27\n",
      "done: false\n",
      "episode_len_mean: 160.35051546391753\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.40880412371135344\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 97\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.647622108459473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01534600555896759\n",
      "        model: {}\n",
      "        policy_loss: -0.13274747133255005\n",
      "        total_loss: -0.10006888210773468\n",
      "        vf_explained_var: -0.4952336847782135\n",
      "        vf_loss: 0.01714075729250908\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.615100383758545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01858806423842907\n",
      "        model: {}\n",
      "        policy_loss: -0.23246219754219055\n",
      "        total_loss: 0.19395524263381958\n",
      "        vf_explained_var: 0.41974490880966187\n",
      "        vf_loss: 0.40759700536727905\n",
      "  num_agent_steps_sampled: 31108\n",
      "  num_agent_steps_trained: 31108\n",
      "  num_steps_sampled: 15554\n",
      "  num_steps_trained: 15554\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 42\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.34086021505376\n",
      "  ram_util_percent: 37.462365591397855\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.05200000000000173\n",
      "  blue_1: -0.35680412371134174\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6269818144709653\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 136.73732183645905\n",
      "  mean_inference_ms: 3.212998724911725\n",
      "  mean_raw_obs_processing_ms: 37.20072006946945\n",
      "time_since_restore: 2560.7870657444\n",
      "time_this_iter_s: 61.10986518859863\n",
      "time_total_s: 2560.7870657444\n",
      "timers:\n",
      "  learn_throughput: 813.077\n",
      "  learn_time_ms: 465.638\n",
      "  load_throughput: 543934.882\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 5.83\n",
      "  sample_time_ms: 64938.733\n",
      "timestamp: 1638984987\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 15554\n",
      "training_iteration: 42\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:42 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 148 0.005 0.7400000000000005\n",
      "blue_1 True True 148 -2.0 -1.2649999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 167 0.005 0.8350000000000006\n",
      "blue_1 True True 167 -2.0 -1.1699999999999995\n",
      "agent_timesteps_total: 31738\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-37-21\n",
      "done: false\n",
      "episode_len_mean: 160.2929292929293\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4092323232323365\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 99\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.643850326538086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018809733912348747\n",
      "        model: {}\n",
      "        policy_loss: -0.058390237390995026\n",
      "        total_loss: -0.029395831748843193\n",
      "        vf_explained_var: 0.5535755157470703\n",
      "        vf_loss: 0.009949544444680214\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.540727615356445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012049025855958462\n",
      "        model: {}\n",
      "        policy_loss: -0.3055571913719177\n",
      "        total_loss: -0.11669865995645523\n",
      "        vf_explained_var: 0.5139338374137878\n",
      "        vf_loss: 0.17665883898735046\n",
      "  num_agent_steps_sampled: 31738\n",
      "  num_agent_steps_trained: 31738\n",
      "  num_steps_sampled: 15869\n",
      "  num_steps_trained: 15869\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 43\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.50263157894737\n",
      "  ram_util_percent: 37.472368421052636\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.03504040404040573\n",
      "  blue_1: -0.37419191919192074\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6261004727290105\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 136.48916273511114\n",
      "  mean_inference_ms: 3.207050960273415\n",
      "  mean_raw_obs_processing_ms: 37.15811484827408\n",
      "time_since_restore: 2609.586621761322\n",
      "time_this_iter_s: 48.799556016922\n",
      "time_total_s: 2609.586621761322\n",
      "timers:\n",
      "  learn_throughput: 822.406\n",
      "  learn_time_ms: 465.342\n",
      "  load_throughput: 549881.861\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 5.791\n",
      "  sample_time_ms: 66086.611\n",
      "timestamp: 1638985041\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 15869\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:43 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 101 -2.0 -1.4999999999999996\n",
      "blue_1 True True 101 0.005 0.5050000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 187 0.005 0.9350000000000007\n",
      "blue_1 True True 187 -2.0 -1.0699999999999994\n",
      "agent_timesteps_total: 32314\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-38-14\n",
      "done: false\n",
      "episode_len_mean: 159.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.416390000000013\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 101\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.652271270751953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01771542988717556\n",
      "        model: {}\n",
      "        policy_loss: -0.02489680051803589\n",
      "        total_loss: 0.6095655560493469\n",
      "        vf_explained_var: 0.2403169721364975\n",
      "        vf_loss: 0.6165253520011902\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.930906295776367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012438277713954449\n",
      "        model: {}\n",
      "        policy_loss: -0.33412107825279236\n",
      "        total_loss: -0.2136673629283905\n",
      "        vf_explained_var: -0.20902560651302338\n",
      "        vf_loss: 0.10785994678735733\n",
      "  num_agent_steps_sampled: 32314\n",
      "  num_agent_steps_trained: 32314\n",
      "  num_steps_sampled: 16157\n",
      "  num_steps_trained: 16157\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 44\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.52328767123287\n",
      "  ram_util_percent: 37.46712328767123\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.030290000000001677\n",
      "  blue_1: -0.38610000000000155\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6248978804418633\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 135.8730982578846\n",
      "  mean_inference_ms: 3.1917034696000752\n",
      "  mean_raw_obs_processing_ms: 37.08795839725471\n",
      "time_since_restore: 2656.7093653678894\n",
      "time_this_iter_s: 47.12274360656738\n",
      "time_total_s: 2656.7093653678894\n",
      "timers:\n",
      "  learn_throughput: 828.512\n",
      "  learn_time_ms: 448.394\n",
      "  load_throughput: 623074.191\n",
      "  load_time_ms: 0.596\n",
      "  sample_throughput: 5.813\n",
      "  sample_time_ms: 63909.251\n",
      "timestamp: 1638985094\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 16157\n",
      "training_iteration: 44\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:44 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 172 0.005 0.8600000000000007\n",
      "blue_1 True True 172 -2.0 -1.1449999999999994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 94 0.005 0.4700000000000003\n",
      "blue_1 True True 94 -2.0 -1.5349999999999997\n",
      "agent_timesteps_total: 32846\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-39-06\n",
      "done: false\n",
      "episode_len_mean: 160.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.41149000000001307\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 103\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.59394359588623\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01484588347375393\n",
      "        model: {}\n",
      "        policy_loss: -0.10387764126062393\n",
      "        total_loss: -0.0011894300114363432\n",
      "        vf_explained_var: -0.18023373186588287\n",
      "        vf_loss: 0.08765673637390137\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.010453224182129\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01591266319155693\n",
      "        model: {}\n",
      "        policy_loss: -0.14924383163452148\n",
      "        total_loss: 0.05578434467315674\n",
      "        vf_explained_var: 0.845943033695221\n",
      "        vf_loss: 0.1889166235923767\n",
      "  num_agent_steps_sampled: 32846\n",
      "  num_agent_steps_trained: 32846\n",
      "  num_steps_sampled: 16423\n",
      "  num_steps_trained: 16423\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 45\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.346575342465755\n",
      "  ram_util_percent: 37.47123287671232\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.027840000000001676\n",
      "  blue_1: -0.3836500000000014\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.62277059487856\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 135.0093678976781\n",
      "  mean_inference_ms: 3.169609969421274\n",
      "  mean_raw_obs_processing_ms: 37.03795303265509\n",
      "time_since_restore: 2703.784341096878\n",
      "time_this_iter_s: 47.07497572898865\n",
      "time_total_s: 2703.784341096878\n",
      "timers:\n",
      "  learn_throughput: 818.989\n",
      "  learn_time_ms: 432.24\n",
      "  load_throughput: 510656.079\n",
      "  load_time_ms: 0.693\n",
      "  sample_throughput: 5.737\n",
      "  sample_time_ms: 61702.606\n",
      "timestamp: 1638985146\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 16423\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:45 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 264 0.005 1.3199999999999938\n",
      "blue_1 True True 264 -2.0 -0.685000000000006\n",
      "agent_timesteps_total: 33374\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-39-41\n",
      "done: false\n",
      "episode_len_mean: 162.28\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3892900000000134\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 104\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.698153495788574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01548018679022789\n",
      "        model: {}\n",
      "        policy_loss: -0.1401078850030899\n",
      "        total_loss: -0.1156139150261879\n",
      "        vf_explained_var: -0.47353076934814453\n",
      "        vf_loss: 0.00882030837237835\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.992747783660889\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011855321936309338\n",
      "        model: {}\n",
      "        policy_loss: -0.145766019821167\n",
      "        total_loss: -0.013709674589335918\n",
      "        vf_explained_var: 0.44234201312065125\n",
      "        vf_loss: 0.12005282938480377\n",
      "  num_agent_steps_sampled: 33374\n",
      "  num_agent_steps_trained: 33374\n",
      "  num_steps_sampled: 16687\n",
      "  num_steps_trained: 16687\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 46\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.30816326530613\n",
      "  ram_util_percent: 37.46326530612245\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.0033099999999982622\n",
      "  blue_1: -0.39260000000000156\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6214222963697297\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 134.63349570927863\n",
      "  mean_inference_ms: 3.159973779031457\n",
      "  mean_raw_obs_processing_ms: 37.03484035893812\n",
      "time_since_restore: 2733.405123949051\n",
      "time_this_iter_s: 29.62078285217285\n",
      "time_total_s: 2733.405123949051\n",
      "timers:\n",
      "  learn_throughput: 819.652\n",
      "  learn_time_ms: 401.024\n",
      "  load_throughput: 414524.708\n",
      "  load_time_ms: 0.793\n",
      "  sample_throughput: 5.816\n",
      "  sample_time_ms: 56512.898\n",
      "timestamp: 1638985181\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 16687\n",
      "training_iteration: 46\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:46 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 163 0.005 0.8150000000000006\n",
      "blue_1 True True 163 -2.0 -1.1899999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 136 -2.0 -1.3249999999999995\n",
      "blue_1 True True 136 0.005 0.6800000000000005\n",
      "agent_timesteps_total: 33972\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-40-32\n",
      "done: false\n",
      "episode_len_mean: 160.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4069400000000134\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 106\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.46125316619873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01277189888060093\n",
      "        model: {}\n",
      "        policy_loss: -0.36384785175323486\n",
      "        total_loss: -0.1139574944972992\n",
      "        vf_explained_var: 0.007487596943974495\n",
      "        vf_loss: 0.2369588315486908\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.759685039520264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01401080098003149\n",
      "        model: {}\n",
      "        policy_loss: 0.15271976590156555\n",
      "        total_loss: 0.4382576048374176\n",
      "        vf_explained_var: 0.7155901193618774\n",
      "        vf_loss: 0.2713519036769867\n",
      "  num_agent_steps_sampled: 33972\n",
      "  num_agent_steps_trained: 33972\n",
      "  num_steps_sampled: 16986\n",
      "  num_steps_trained: 16986\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 47\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.12714285714286\n",
      "  ram_util_percent: 37.46714285714286\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.004340000000001576\n",
      "  blue_1: -0.40260000000000146\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6187569590797652\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 133.95136499447625\n",
      "  mean_inference_ms: 3.141965708318505\n",
      "  mean_raw_obs_processing_ms: 36.99569020717826\n",
      "time_since_restore: 2778.576358318329\n",
      "time_this_iter_s: 45.171234369277954\n",
      "time_total_s: 2778.576358318329\n",
      "timers:\n",
      "  learn_throughput: 813.702\n",
      "  learn_time_ms: 404.694\n",
      "  load_throughput: 415218.948\n",
      "  load_time_ms: 0.793\n",
      "  sample_throughput: 5.979\n",
      "  sample_time_ms: 55080.482\n",
      "timestamp: 1638985232\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 16986\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:47 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 91 0.005 0.4550000000000003\n",
      "blue_1 True True 91 -2.0 -1.5499999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 155 -2.0 -1.2299999999999995\n",
      "blue_1 True True 155 0.005 0.7750000000000006\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 175 -2.0 -1.1299999999999994\n",
      "blue_1 True True 175 0.005 0.8750000000000007\n",
      "agent_timesteps_total: 34814\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-41-45\n",
      "done: false\n",
      "episode_len_mean: 160.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.4063400000000134\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 109\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.438923835754395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013464119285345078\n",
      "        model: {}\n",
      "        policy_loss: -0.2255532145500183\n",
      "        total_loss: 0.10845974832773209\n",
      "        vf_explained_var: 0.08275583386421204\n",
      "        vf_loss: 0.3203805387020111\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.054316997528076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016483837738633156\n",
      "        model: {}\n",
      "        policy_loss: 0.01856437511742115\n",
      "        total_loss: 0.5040995478630066\n",
      "        vf_explained_var: -0.5618200302124023\n",
      "        vf_loss: 0.4688452482223511\n",
      "  num_agent_steps_sampled: 34814\n",
      "  num_agent_steps_trained: 34814\n",
      "  num_steps_sampled: 17407\n",
      "  num_steps_trained: 17407\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 48\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.166000000000004\n",
      "  ram_util_percent: 37.485\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.04414000000000158\n",
      "  blue_1: -0.3622000000000014\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6150752456706524\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 133.0160355041734\n",
      "  mean_inference_ms: 3.1176299980768736\n",
      "  mean_raw_obs_processing_ms: 36.866039356823784\n",
      "time_since_restore: 2845.3286831378937\n",
      "time_this_iter_s: 66.75232481956482\n",
      "time_total_s: 2845.3286831378937\n",
      "timers:\n",
      "  learn_throughput: 798.14\n",
      "  learn_time_ms: 405.192\n",
      "  load_throughput: 466434.412\n",
      "  load_time_ms: 0.693\n",
      "  sample_throughput: 5.849\n",
      "  sample_time_ms: 55289.515\n",
      "timestamp: 1638985305\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 17407\n",
      "training_iteration: 48\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:48 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 171 0.005 0.8550000000000006\n",
      "blue_1 True True 171 -2.0 -1.1499999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 87 0.005 0.4350000000000003\n",
      "blue_1 True True 87 -2.0 -1.5699999999999998\n",
      "agent_timesteps_total: 35330\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-42-31\n",
      "done: false\n",
      "episode_len_mean: 161.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.40104000000001333\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 111\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.499217987060547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014788059517741203\n",
      "        model: {}\n",
      "        policy_loss: -0.08298910409212112\n",
      "        total_loss: 0.07937650382518768\n",
      "        vf_explained_var: 0.2549959719181061\n",
      "        vf_loss: 0.14739269018173218\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.29904842376709\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015305140055716038\n",
      "        model: {}\n",
      "        policy_loss: -0.09648118913173676\n",
      "        total_loss: 0.39204141497612\n",
      "        vf_explained_var: 0.3062807619571686\n",
      "        vf_loss: 0.47302621603012085\n",
      "  num_agent_steps_sampled: 35330\n",
      "  num_agent_steps_trained: 35330\n",
      "  num_steps_sampled: 17665\n",
      "  num_steps_trained: 17665\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 49\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.34307692307693\n",
      "  ram_util_percent: 37.490769230769224\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.021440000000001583\n",
      "  blue_1: -0.3796000000000015\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6129302280742632\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 132.36546789818422\n",
      "  mean_inference_ms: 3.102524228845348\n",
      "  mean_raw_obs_processing_ms: 36.75020084251676\n",
      "time_since_restore: 2886.233060359955\n",
      "time_this_iter_s: 40.90437722206116\n",
      "time_total_s: 2886.233060359955\n",
      "timers:\n",
      "  learn_throughput: 778.239\n",
      "  learn_time_ms: 405.017\n",
      "  load_throughput: 397452.01\n",
      "  load_time_ms: 0.793\n",
      "  sample_throughput: 5.711\n",
      "  sample_time_ms: 55187.404\n",
      "timestamp: 1638985351\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 17665\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:49 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 197 -2.0 -1.0199999999999991\n",
      "blue_1 True True 197 0.005 0.9850000000000008\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 187 -2.0 -1.0699999999999994\n",
      "blue_1 True True 187 0.005 0.9350000000000007\n",
      "agent_timesteps_total: 36098\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-43-33\n",
      "done: false\n",
      "episode_len_mean: 163.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3830400000000136\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 113\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.761551856994629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016115956008434296\n",
      "        model: {}\n",
      "        policy_loss: -0.08035387098789215\n",
      "        total_loss: 0.2583652138710022\n",
      "        vf_explained_var: 0.4829302132129669\n",
      "        vf_loss: 0.32240164279937744\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.943048000335693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016842110082507133\n",
      "        model: {}\n",
      "        policy_loss: -0.07080348581075668\n",
      "        total_loss: 0.2823524475097656\n",
      "        vf_explained_var: 0.0009595361188985407\n",
      "        vf_loss: 0.33610329031944275\n",
      "  num_agent_steps_sampled: 36098\n",
      "  num_agent_steps_trained: 36098\n",
      "  num_steps_sampled: 18049\n",
      "  num_steps_trained: 18049\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 50\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.24166666666667\n",
      "  ram_util_percent: 37.49404761904762\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.012440000000001566\n",
      "  blue_1: -0.37060000000000143\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6109166888505425\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 131.71718748472324\n",
      "  mean_inference_ms: 3.088552583487019\n",
      "  mean_raw_obs_processing_ms: 36.62671481135022\n",
      "time_since_restore: 2942.329241037369\n",
      "time_this_iter_s: 56.09618067741394\n",
      "time_total_s: 2942.329241037369\n",
      "timers:\n",
      "  learn_throughput: 761.299\n",
      "  learn_time_ms: 403.521\n",
      "  load_throughput: 385995.084\n",
      "  load_time_ms: 0.796\n",
      "  sample_throughput: 5.764\n",
      "  sample_time_ms: 53299.327\n",
      "timestamp: 1638985413\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 18049\n",
      "training_iteration: 50\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:50 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 120 0.005 0.6000000000000004\n",
      "blue_1 True True 120 -2.0 -1.4049999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 87 0.005 0.4350000000000003\n",
      "blue_1 True True 87 -2.0 -1.5699999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 103 -2.0 -1.4899999999999998\n",
      "blue_1 True True 103 0.005 0.5150000000000003\n",
      "agent_timesteps_total: 36718\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-44-40\n",
      "done: false\n",
      "episode_len_mean: 161.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3969400000000134\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 116\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.54440689086914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010613961145281792\n",
      "        model: {}\n",
      "        policy_loss: -0.3670949935913086\n",
      "        total_loss: -0.05494120344519615\n",
      "        vf_explained_var: -0.9453890919685364\n",
      "        vf_loss: 0.301407128572464\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.576526165008545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013338164426386356\n",
      "        model: {}\n",
      "        policy_loss: 0.1970038115978241\n",
      "        total_loss: 0.9912222027778625\n",
      "        vf_explained_var: 0.34382492303848267\n",
      "        vf_loss: 0.7807134985923767\n",
      "  num_agent_steps_sampled: 36718\n",
      "  num_agent_steps_trained: 36718\n",
      "  num_steps_sampled: 18359\n",
      "  num_steps_trained: 18359\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 51\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.34574468085106\n",
      "  ram_util_percent: 37.49787234042553\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.020709999999998403\n",
      "  blue_1: -0.4176500000000014\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6078126183122617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 130.79676176144068\n",
      "  mean_inference_ms: 3.0678814576083355\n",
      "  mean_raw_obs_processing_ms: 36.44500347061532\n",
      "time_since_restore: 3004.2304151058197\n",
      "time_this_iter_s: 61.90117406845093\n",
      "time_total_s: 3004.2304151058197\n",
      "timers:\n",
      "  learn_throughput: 772.778\n",
      "  learn_time_ms: 403.221\n",
      "  load_throughput: 390774.444\n",
      "  load_time_ms: 0.797\n",
      "  sample_throughput: 5.563\n",
      "  sample_time_ms: 56012.282\n",
      "timestamp: 1638985480\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 18359\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:50 starting ! -----------------\n",
      "agent_timesteps_total: 36718\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-44-40\n",
      "done: false\n",
      "episode_len_mean: 161.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3969400000000134\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 116\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.54440689086914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010613961145281792\n",
      "        model: {}\n",
      "        policy_loss: -0.3670949935913086\n",
      "        total_loss: -0.05494120344519615\n",
      "        vf_explained_var: -0.9453890919685364\n",
      "        vf_loss: 0.301407128572464\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.576526165008545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013338164426386356\n",
      "        model: {}\n",
      "        policy_loss: 0.1970038115978241\n",
      "        total_loss: 0.9912222027778625\n",
      "        vf_explained_var: 0.34382492303848267\n",
      "        vf_loss: 0.7807134985923767\n",
      "  num_agent_steps_sampled: 36718\n",
      "  num_agent_steps_trained: 36718\n",
      "  num_steps_sampled: 18359\n",
      "  num_steps_trained: 18359\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 51\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.34574468085106\n",
      "  ram_util_percent: 37.49787234042553\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 1.9999999999999583\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.020709999999998403\n",
      "  blue_1: -0.4176500000000014\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6078126183122617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 130.79676176144068\n",
      "  mean_inference_ms: 3.0678814576083355\n",
      "  mean_raw_obs_processing_ms: 36.44500347061532\n",
      "time_since_restore: 3004.2304151058197\n",
      "time_this_iter_s: 61.90117406845093\n",
      "time_total_s: 3004.2304151058197\n",
      "timers:\n",
      "  learn_throughput: 772.778\n",
      "  learn_time_ms: 403.221\n",
      "  load_throughput: 390774.444\n",
      "  load_time_ms: 0.797\n",
      "  sample_throughput: 5.563\n",
      "  sample_time_ms: 56012.282\n",
      "timestamp: 1638985480\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 18359\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:51 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 176 -2.0 -1.1249999999999993\n",
      "blue_1 True True 176 0.005 0.8800000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 438 0.005 2.1899999999999755\n",
      "blue_1 True True 438 -2.001 0.29399999999998805\n",
      "agent_timesteps_total: 37946\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-46-04\n",
      "done: false\n",
      "episode_len_mean: 165.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3626500000000143\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 118\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.974084854125977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01594521850347519\n",
      "        model: {}\n",
      "        policy_loss: -0.013280787505209446\n",
      "        total_loss: 0.42634615302085876\n",
      "        vf_explained_var: -0.34951403737068176\n",
      "        vf_loss: 0.42348241806030273\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.80201244354248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006456979550421238\n",
      "        model: {}\n",
      "        policy_loss: -0.4149544835090637\n",
      "        total_loss: -0.1733262985944748\n",
      "        vf_explained_var: 0.12153350561857224\n",
      "        vf_loss: 0.2350904494524002\n",
      "  num_agent_steps_sampled: 37946\n",
      "  num_agent_steps_trained: 37946\n",
      "  num_steps_sampled: 18973\n",
      "  num_steps_trained: 18973\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 52\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.55\n",
      "  ram_util_percent: 37.539655172413795\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.03730999999999816\n",
      "  blue_1: -0.3999600000000016\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6057428931322498\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 130.20982399165737\n",
      "  mean_inference_ms: 3.0546450661784563\n",
      "  mean_raw_obs_processing_ms: 36.31956561421353\n",
      "time_since_restore: 3082.9613876342773\n",
      "time_this_iter_s: 78.73097252845764\n",
      "time_total_s: 3082.9613876342773\n",
      "timers:\n",
      "  learn_throughput: 783.752\n",
      "  learn_time_ms: 436.235\n",
      "  load_throughput: 490116.729\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 5.921\n",
      "  sample_time_ms: 57740.498\n",
      "timestamp: 1638985564\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 18973\n",
      "training_iteration: 52\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:52 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 274 0.005 1.3699999999999928\n",
      "blue_1 True True 274 -2.0 -0.6350000000000071\n",
      "agent_timesteps_total: 38494\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-46-47\n",
      "done: false\n",
      "episode_len_mean: 166.85\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3448500000000147\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 119\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.629962921142578\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012777447700500488\n",
      "        model: {}\n",
      "        policy_loss: -0.20096847414970398\n",
      "        total_loss: -0.15738481283187866\n",
      "        vf_explained_var: -0.6973819732666016\n",
      "        vf_loss: 0.03064650669693947\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.314865112304688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015329133719205856\n",
      "        model: {}\n",
      "        policy_loss: -0.227240651845932\n",
      "        total_loss: 0.08456623554229736\n",
      "        vf_explained_var: 0.1177857294678688\n",
      "        vf_loss: 0.29628613591194153\n",
      "  num_agent_steps_sampled: 38494\n",
      "  num_agent_steps_trained: 38494\n",
      "  num_steps_sampled: 19247\n",
      "  num_steps_trained: 19247\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 53\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.26271186440679\n",
      "  ram_util_percent: 37.496610169491525\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.04620999999999808\n",
      "  blue_1: -0.3910600000000016\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.604702977274716\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 129.92672365551348\n",
      "  mean_inference_ms: 3.048232010395699\n",
      "  mean_raw_obs_processing_ms: 36.25912531679773\n",
      "time_since_restore: 3120.104777097702\n",
      "time_this_iter_s: 37.14338946342468\n",
      "time_total_s: 3120.104777097702\n",
      "timers:\n",
      "  learn_throughput: 774.412\n",
      "  learn_time_ms: 436.202\n",
      "  load_throughput: 486316.981\n",
      "  load_time_ms: 0.695\n",
      "  sample_throughput: 5.967\n",
      "  sample_time_ms: 56612.628\n",
      "timestamp: 1638985607\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 19247\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:53 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 56 -2.0 -1.7249999999999999\n",
      "blue_1 True True 56 0.005 0.28000000000000014\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 67 0.005 0.3350000000000002\n",
      "blue_1 True True 67 -2.0 -1.67\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 155 0.005 0.7750000000000006\n",
      "blue_1 True True 155 -2.0 -1.2299999999999995\n",
      "agent_timesteps_total: 39050\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-47-48\n",
      "done: false\n",
      "episode_len_mean: 164.42\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.3693800000000143\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 122\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.431360244750977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016197483986616135\n",
      "        model: {}\n",
      "        policy_loss: -0.04650966450572014\n",
      "        total_loss: 0.551216185092926\n",
      "        vf_explained_var: 0.15444637835025787\n",
      "        vf_loss: 0.5813259482383728\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.5649027824401855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015390723943710327\n",
      "        model: {}\n",
      "        policy_loss: -0.19987311959266663\n",
      "        total_loss: 0.3187461793422699\n",
      "        vf_explained_var: 0.27443012595176697\n",
      "        vf_loss: 0.5030362010002136\n",
      "  num_agent_steps_sampled: 39050\n",
      "  num_agent_steps_trained: 39050\n",
      "  num_steps_sampled: 19525\n",
      "  num_steps_trained: 19525\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 54\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.29529411764706\n",
      "  ram_util_percent: 37.48823529411764\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.03382999999999813\n",
      "  blue_1: -0.4032100000000016\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6017957298278658\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 129.10494938421013\n",
      "  mean_inference_ms: 3.0298926664315458\n",
      "  mean_raw_obs_processing_ms: 36.08917033100635\n",
      "time_since_restore: 3175.7428319454193\n",
      "time_this_iter_s: 55.638054847717285\n",
      "time_total_s: 3175.7428319454193\n",
      "timers:\n",
      "  learn_throughput: 771.76\n",
      "  learn_time_ms: 436.405\n",
      "  load_throughput: 423987.51\n",
      "  load_time_ms: 0.794\n",
      "  sample_throughput: 5.861\n",
      "  sample_time_ms: 57465.144\n",
      "timestamp: 1638985668\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 19525\n",
      "training_iteration: 54\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:54 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 254 -2.0 -0.735000000000005\n",
      "blue_1 True True 254 0.005 1.269999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 34 -2.0 -1.835\n",
      "blue_1 True True 34 0.005 0.17000000000000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 39626\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-48-39\n",
      "done: false\n",
      "episode_len_mean: 164.23\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.026999999999897\n",
      "episode_reward_mean: -0.37128000000001443\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 124\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.569992065429688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013209039345383644\n",
      "        model: {}\n",
      "        policy_loss: -0.2599874436855316\n",
      "        total_loss: 0.1690317690372467\n",
      "        vf_explained_var: 0.12981559336185455\n",
      "        vf_loss: 0.4156450629234314\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.23942756652832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012867530807852745\n",
      "        model: {}\n",
      "        policy_loss: -0.3161011338233948\n",
      "        total_loss: 0.10030661523342133\n",
      "        vf_explained_var: -0.35325103998184204\n",
      "        vf_loss: 0.4033793807029724\n",
      "  num_agent_steps_sampled: 39626\n",
      "  num_agent_steps_trained: 39626\n",
      "  num_steps_sampled: 19813\n",
      "  num_steps_trained: 19813\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 55\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.530985915492955\n",
      "  ram_util_percent: 37.4943661971831\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.00722000000000192\n",
      "  blue_1: -0.3640600000000017\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5999959318314103\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 128.59924287686454\n",
      "  mean_inference_ms: 3.018780922549611\n",
      "  mean_raw_obs_processing_ms: 35.98434156516909\n",
      "time_since_restore: 3221.365756034851\n",
      "time_this_iter_s: 45.62292408943176\n",
      "time_total_s: 3221.365756034851\n",
      "timers:\n",
      "  learn_throughput: 779.657\n",
      "  learn_time_ms: 434.807\n",
      "  load_throughput: 425441.805\n",
      "  load_time_ms: 0.797\n",
      "  sample_throughput: 5.914\n",
      "  sample_time_ms: 57320.383\n",
      "timestamp: 1638985719\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 19813\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:55 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 183 -2.0 -1.0899999999999994\n",
      "blue_1 True True 183 0.005 0.9150000000000007\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 2.145999999999957\n",
      "blue_1 False False 600 -0.995 1.9999999999999583\n",
      "agent_timesteps_total: 41192\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-50-17\n",
      "done: false\n",
      "episode_len_mean: 167.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.3330800000000151\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 126\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.662703514099121\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014105149544775486\n",
      "        model: {}\n",
      "        policy_loss: -0.10378699749708176\n",
      "        total_loss: 0.209610253572464\n",
      "        vf_explained_var: -0.40356582403182983\n",
      "        vf_loss: 0.2991158068180084\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.987559795379639\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012062006630003452\n",
      "        model: {}\n",
      "        policy_loss: -0.1337025761604309\n",
      "        total_loss: -0.07941056787967682\n",
      "        vf_explained_var: -0.44213616847991943\n",
      "        vf_loss: 0.042079221457242966\n",
      "  num_agent_steps_sampled: 41192\n",
      "  num_agent_steps_trained: 41192\n",
      "  num_steps_sampled: 20596\n",
      "  num_steps_trained: 20596\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 56\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.50820895522388\n",
      "  ram_util_percent: 37.51343283582089\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.0028899999999977277\n",
      "  blue_1: -0.3359700000000021\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5983590843483426\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 128.10971458154285\n",
      "  mean_inference_ms: 3.0083600426536385\n",
      "  mean_raw_obs_processing_ms: 35.87707610766272\n",
      "time_since_restore: 3313.6375658512115\n",
      "time_this_iter_s: 92.27180981636047\n",
      "time_total_s: 3313.6375658512115\n",
      "timers:\n",
      "  learn_throughput: 788.83\n",
      "  learn_time_ms: 495.544\n",
      "  load_throughput: 492402.749\n",
      "  load_time_ms: 0.794\n",
      "  sample_throughput: 6.154\n",
      "  sample_time_ms: 63518.048\n",
      "timestamp: 1638985817\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 20596\n",
      "training_iteration: 56\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:56 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 268 -2.0 -0.6650000000000065\n",
      "blue_1 True True 268 0.005 1.3399999999999934\n",
      "agent_timesteps_total: 41728\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-50-54\n",
      "done: false\n",
      "episode_len_mean: 169.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.31318000000001545\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 127\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.479757308959961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016975192353129387\n",
      "        model: {}\n",
      "        policy_loss: -0.17761503159999847\n",
      "        total_loss: 0.11222939938306808\n",
      "        vf_explained_var: 0.4749665856361389\n",
      "        vf_loss: 0.2726570665836334\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.211838722229004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022943994030356407\n",
      "        model: {}\n",
      "        policy_loss: -0.161116361618042\n",
      "        total_loss: -0.11956454813480377\n",
      "        vf_explained_var: -0.4114396870136261\n",
      "        vf_loss: 0.01832100749015808\n",
      "  num_agent_steps_sampled: 41728\n",
      "  num_agent_steps_trained: 41728\n",
      "  num_steps_sampled: 20864\n",
      "  num_steps_trained: 20864\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 57\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.47254901960783\n",
      "  ram_util_percent: 37.51176470588235\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.01283999999999766\n",
      "  blue_1: -0.3260200000000022\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5975691000484818\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 127.86433871340512\n",
      "  mean_inference_ms: 3.0033378639891826\n",
      "  mean_raw_obs_processing_ms: 35.81369610331299\n",
      "time_since_restore: 3344.6947112083435\n",
      "time_this_iter_s: 31.057145357131958\n",
      "time_total_s: 3344.6947112083435\n",
      "timers:\n",
      "  learn_throughput: 788.445\n",
      "  learn_time_ms: 491.854\n",
      "  load_throughput: 558702.673\n",
      "  load_time_ms: 0.694\n",
      "  sample_throughput: 6.237\n",
      "  sample_time_ms: 62172.563\n",
      "timestamp: 1638985854\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 20864\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:57 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 337 0.005 1.7139999999999924\n",
      "blue_1 True True 337 -2.0 -0.32000000000001383\n",
      "agent_timesteps_total: 42402\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-51-36\n",
      "done: false\n",
      "episode_len_mean: 172.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.2853900000000158\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 128\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.275689125061035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013318746350705624\n",
      "        model: {}\n",
      "        policy_loss: -0.2917560935020447\n",
      "        total_loss: -0.1674019992351532\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.11086884140968323\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.601669788360596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010062378831207752\n",
      "        model: {}\n",
      "        policy_loss: -0.5375993847846985\n",
      "        total_loss: -0.4255097806453705\n",
      "        vf_explained_var: 0.32218948006629944\n",
      "        vf_loss: 0.09680738300085068\n",
      "  num_agent_steps_sampled: 42402\n",
      "  num_agent_steps_trained: 42402\n",
      "  num_steps_sampled: 21201\n",
      "  num_steps_trained: 21201\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 58\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.25254237288135\n",
      "  ram_util_percent: 37.498305084745766\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.4319999999999977\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.04692999999999758\n",
      "  blue_1: -0.3323200000000023\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.596782731255078\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 127.61368025395603\n",
      "  mean_inference_ms: 2.9983001436986387\n",
      "  mean_raw_obs_processing_ms: 35.747578904983264\n",
      "time_since_restore: 3381.593910217285\n",
      "time_this_iter_s: 36.89919900894165\n",
      "time_total_s: 3381.593910217285\n",
      "timers:\n",
      "  learn_throughput: 801.43\n",
      "  learn_time_ms: 473.404\n",
      "  load_throughput: 477916.61\n",
      "  load_time_ms: 0.794\n",
      "  sample_throughput: 6.409\n",
      "  sample_time_ms: 59200.833\n",
      "timestamp: 1638985896\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 21201\n",
      "training_iteration: 58\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:58 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 162 0.005 0.8100000000000006\n",
      "blue_1 True True 162 -2.0 -1.1949999999999994\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 93 -2.0 -1.5399999999999996\n",
      "blue_1 True True 93 0.005 0.4650000000000003\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 123 -2.0 -1.3899999999999997\n",
      "blue_1 True True 123 0.005 0.6150000000000004\n",
      "agent_timesteps_total: 43158\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-52-50\n",
      "done: false\n",
      "episode_len_mean: 169.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.32496000000001496\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 131\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.937071323394775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01536607462912798\n",
      "        model: {}\n",
      "        policy_loss: -0.3995071351528168\n",
      "        total_loss: 0.3193712532520294\n",
      "        vf_explained_var: -0.42647895216941833\n",
      "        vf_loss: 0.703320324420929\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.102320671081543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019734462723135948\n",
      "        model: {}\n",
      "        policy_loss: 0.24754565954208374\n",
      "        total_loss: 0.6532326340675354\n",
      "        vf_explained_var: 0.3642958700656891\n",
      "        vf_loss: 0.37571531534194946\n",
      "  num_agent_steps_sampled: 43158\n",
      "  num_agent_steps_trained: 43158\n",
      "  num_steps_sampled: 21579\n",
      "  num_steps_trained: 21579\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 59\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.57647058823529\n",
      "  ram_util_percent: 37.49313725490196\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.0979999999999803\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.04857999999999778\n",
      "  blue_1: -0.3735400000000022\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5947625367154379\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 126.981087978279\n",
      "  mean_inference_ms: 2.9857332907437972\n",
      "  mean_raw_obs_processing_ms: 35.623922031813734\n",
      "time_since_restore: 3450.0327825546265\n",
      "time_this_iter_s: 68.43887233734131\n",
      "time_total_s: 3450.0327825546265\n",
      "timers:\n",
      "  learn_throughput: 827.306\n",
      "  learn_time_ms: 473.102\n",
      "  load_throughput: 493017.775\n",
      "  load_time_ms: 0.794\n",
      "  sample_throughput: 6.319\n",
      "  sample_time_ms: 61938.234\n",
      "timestamp: 1638985970\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 21579\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:59 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 193 0.005 0.9650000000000007\n",
      "blue_1 True True 193 -2.0 -1.0399999999999991\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 143 0.005 0.7150000000000005\n",
      "blue_1 True True 143 -2.0 -1.2899999999999996\n",
      "agent_timesteps_total: 43830\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-53-46\n",
      "done: false\n",
      "episode_len_mean: 170.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.3089600000000153\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 133\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.2147798538208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02067728526890278\n",
      "        model: {}\n",
      "        policy_loss: 0.0301776435226202\n",
      "        total_loss: 0.13056853413581848\n",
      "        vf_explained_var: -0.016270730644464493\n",
      "        vf_loss: 0.07945514470338821\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.375969409942627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012320836074650288\n",
      "        model: {}\n",
      "        policy_loss: -0.2881789207458496\n",
      "        total_loss: -0.09900116175413132\n",
      "        vf_explained_var: 0.5781793594360352\n",
      "        vf_loss: 0.17046545445919037\n",
      "  num_agent_steps_sampled: 43830\n",
      "  num_agent_steps_trained: 43830\n",
      "  num_steps_sampled: 21915\n",
      "  num_steps_trained: 21915\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 60\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.151948051948054\n",
      "  ram_util_percent: 37.4961038961039\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.0979999999999803\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.0766299999999978\n",
      "  blue_1: -0.38559000000000215\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5935124837584168\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 126.59873371003934\n",
      "  mean_inference_ms: 2.9781434547848114\n",
      "  mean_raw_obs_processing_ms: 35.53833295692793\n",
      "time_since_restore: 3499.782489299774\n",
      "time_this_iter_s: 49.749706745147705\n",
      "time_total_s: 3499.782489299774\n",
      "timers:\n",
      "  learn_throughput: 848.703\n",
      "  learn_time_ms: 455.518\n",
      "  load_throughput: 488673.958\n",
      "  load_time_ms: 0.791\n",
      "  sample_throughput: 6.305\n",
      "  sample_time_ms: 61319.667\n",
      "timestamp: 1638986026\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 21915\n",
      "training_iteration: 60\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:60 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 155 0.005 0.7750000000000006\n",
      "blue_1 True True 155 -2.0 -1.2299999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 276 0.005 1.3799999999999926\n",
      "blue_1 True True 276 -2.0 -0.6250000000000073\n",
      "agent_timesteps_total: 44692\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-54-49\n",
      "done: false\n",
      "episode_len_mean: 171.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.29586000000001555\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 135\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.829657554626465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012538129463791847\n",
      "        model: {}\n",
      "        policy_loss: -0.09786354005336761\n",
      "        total_loss: -0.030405346304178238\n",
      "        vf_explained_var: -0.572882890701294\n",
      "        vf_loss: 0.0484158992767334\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.639370918273926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01581205613911152\n",
      "        model: {}\n",
      "        policy_loss: -0.21613602340221405\n",
      "        total_loss: 0.18131040036678314\n",
      "        vf_explained_var: 0.24234554171562195\n",
      "        vf_loss: 0.37343186140060425\n",
      "  num_agent_steps_sampled: 44692\n",
      "  num_agent_steps_trained: 44692\n",
      "  num_steps_sampled: 22346\n",
      "  num_steps_trained: 22346\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 61\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.137499999999996\n",
      "  ram_util_percent: 37.50454545454545\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.0979999999999803\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.0831799999999977\n",
      "  blue_1: -0.3790400000000023\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5923229125250049\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 126.23157238249205\n",
      "  mean_inference_ms: 2.97083506662273\n",
      "  mean_raw_obs_processing_ms: 35.44586862776126\n",
      "time_since_restore: 3557.836923599243\n",
      "time_this_iter_s: 58.054434299468994\n",
      "time_total_s: 3557.836923599243\n",
      "timers:\n",
      "  learn_throughput: 838.022\n",
      "  learn_time_ms: 475.763\n",
      "  load_throughput: 576207.362\n",
      "  load_time_ms: 0.692\n",
      "  sample_throughput: 6.547\n",
      "  sample_time_ms: 60897.682\n",
      "timestamp: 1638986089\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 22346\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:60 starting ! -----------------\n",
      "agent_timesteps_total: 44692\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-54-49\n",
      "done: false\n",
      "episode_len_mean: 171.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.29586000000001555\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 135\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.829657554626465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012538129463791847\n",
      "        model: {}\n",
      "        policy_loss: -0.09786354005336761\n",
      "        total_loss: -0.030405346304178238\n",
      "        vf_explained_var: -0.572882890701294\n",
      "        vf_loss: 0.0484158992767334\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.639370918273926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01581205613911152\n",
      "        model: {}\n",
      "        policy_loss: -0.21613602340221405\n",
      "        total_loss: 0.18131040036678314\n",
      "        vf_explained_var: 0.24234554171562195\n",
      "        vf_loss: 0.37343186140060425\n",
      "  num_agent_steps_sampled: 44692\n",
      "  num_agent_steps_trained: 44692\n",
      "  num_steps_sampled: 22346\n",
      "  num_steps_trained: 22346\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 61\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.137499999999996\n",
      "  ram_util_percent: 37.50454545454545\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.0979999999999803\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.0831799999999977\n",
      "  blue_1: -0.3790400000000023\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5923229125250049\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 126.23157238249205\n",
      "  mean_inference_ms: 2.97083506662273\n",
      "  mean_raw_obs_processing_ms: 35.44586862776126\n",
      "time_since_restore: 3557.836923599243\n",
      "time_this_iter_s: 58.054434299468994\n",
      "time_total_s: 3557.836923599243\n",
      "timers:\n",
      "  learn_throughput: 838.022\n",
      "  learn_time_ms: 475.763\n",
      "  load_throughput: 576207.362\n",
      "  load_time_ms: 0.692\n",
      "  sample_throughput: 6.547\n",
      "  sample_time_ms: 60897.682\n",
      "timestamp: 1638986089\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 22346\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:61 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 120 -2.0 -1.4049999999999996\n",
      "blue_1 True True 120 0.005 0.6000000000000004\n",
      "LOSE\n",
      "blue_0 False True 449 -0.995 1.2649999999999744\n",
      "blue_1 False True 449 -0.993 1.579999999999993\n",
      "agent_timesteps_total: 45830\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-56-13\n",
      "done: false\n",
      "episode_len_mean: 175.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.26156000000001617\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 137\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.785264015197754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013143614865839481\n",
      "        model: {}\n",
      "        policy_loss: -0.1856577843427658\n",
      "        total_loss: 0.17618057131767273\n",
      "        vf_explained_var: -0.45084697008132935\n",
      "        vf_loss: 0.3418765366077423\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.835076332092285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01173965260386467\n",
      "        model: {}\n",
      "        policy_loss: -0.29009976983070374\n",
      "        total_loss: -0.11542219668626785\n",
      "        vf_explained_var: -0.40634140372276306\n",
      "        vf_loss: 0.1568479835987091\n",
      "  num_agent_steps_sampled: 45830\n",
      "  num_agent_steps_trained: 45830\n",
      "  num_steps_sampled: 22915\n",
      "  num_steps_trained: 22915\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 62\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.70350877192984\n",
      "  ram_util_percent: 37.4877192982456\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.0979999999999803\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08872999999999742\n",
      "  blue_1: -0.3502900000000024\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5911958587886736\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 125.86641200902486\n",
      "  mean_inference_ms: 2.9640310196288464\n",
      "  mean_raw_obs_processing_ms: 35.34257464923505\n",
      "time_since_restore: 3635.641087770462\n",
      "time_this_iter_s: 77.80416417121887\n",
      "time_total_s: 3635.641087770462\n",
      "timers:\n",
      "  learn_throughput: 831.177\n",
      "  learn_time_ms: 474.267\n",
      "  load_throughput: 497935.442\n",
      "  load_time_ms: 0.792\n",
      "  sample_throughput: 6.481\n",
      "  sample_time_ms: 60826.214\n",
      "timestamp: 1638986173\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 22915\n",
      "training_iteration: 62\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:62 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 216 -2.0 -0.9250000000000009\n",
      "blue_1 True True 216 0.005 1.079999999999999\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 482 -2.0 0.40499999999997094\n",
      "blue_1 True True 482 0.005 2.437999999999971\n",
      "agent_timesteps_total: 47226\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-57-46\n",
      "done: false\n",
      "episode_len_mean: 178.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.22488000000001696\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 139\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.337653160095215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0091544883325696\n",
      "        model: {}\n",
      "        policy_loss: -0.22313222289085388\n",
      "        total_loss: 0.046015117317438126\n",
      "        vf_explained_var: -0.16278237104415894\n",
      "        vf_loss: 0.2552439272403717\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.438838005065918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008386322297155857\n",
      "        model: {}\n",
      "        policy_loss: -0.1940925568342209\n",
      "        total_loss: -0.16234850883483887\n",
      "        vf_explained_var: -0.43799906969070435\n",
      "        vf_loss: 0.01900731772184372\n",
      "  num_agent_steps_sampled: 47226\n",
      "  num_agent_steps_trained: 47226\n",
      "  num_steps_sampled: 23613\n",
      "  num_steps_trained: 23613\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 63\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.15390624999999\n",
      "  ram_util_percent: 37.400781249999994\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08687999999999717\n",
      "  blue_1: -0.3117600000000027\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5901141519759362\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 125.51053024202979\n",
      "  mean_inference_ms: 2.9577114158806093\n",
      "  mean_raw_obs_processing_ms: 35.235172047624346\n",
      "time_since_restore: 3723.0349004268646\n",
      "time_this_iter_s: 87.39381265640259\n",
      "time_total_s: 3723.0349004268646\n",
      "timers:\n",
      "  learn_throughput: 836.601\n",
      "  learn_time_ms: 521.873\n",
      "  load_throughput: 549424.88\n",
      "  load_time_ms: 0.795\n",
      "  sample_throughput: 6.636\n",
      "  sample_time_ms: 65796.531\n",
      "timestamp: 1638986266\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 23613\n",
      "training_iteration: 63\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:63 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 86 -2.0 -1.5749999999999997\n",
      "blue_1 True True 86 0.005 0.43000000000000027\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 94 -2.0 -1.5349999999999997\n",
      "blue_1 True True 94 0.005 0.4700000000000003\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 287 -2.0 -0.5700000000000085\n",
      "blue_1 True True 287 0.007 1.5149999999999935\n",
      "agent_timesteps_total: 48160\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-59-08\n",
      "done: false\n",
      "episode_len_mean: 176.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.24748000000001658\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 142\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.60159683227539\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012085679918527603\n",
      "        model: {}\n",
      "        policy_loss: -0.22963112592697144\n",
      "        total_loss: 0.19592712819576263\n",
      "        vf_explained_var: 0.12474693357944489\n",
      "        vf_loss: 0.40720313787460327\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.153650283813477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013295352458953857\n",
      "        model: {}\n",
      "        policy_loss: -0.09891477972269058\n",
      "        total_loss: -0.061320722103118896\n",
      "        vf_explained_var: 0.08903403580188751\n",
      "        vf_loss: 0.01740172691643238\n",
      "  num_agent_steps_sampled: 48160\n",
      "  num_agent_steps_trained: 48160\n",
      "  num_steps_sampled: 24080\n",
      "  num_steps_trained: 24080\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 64\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.93157894736843\n",
      "  ram_util_percent: 37.48157894736842\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.1899999999999755\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.034929999999997206\n",
      "  blue_1: -0.2824100000000026\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5886639284592082\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 125.07215419125227\n",
      "  mean_inference_ms: 2.9492372398388373\n",
      "  mean_raw_obs_processing_ms: 35.12160243097816\n",
      "time_since_restore: 3799.6194937229156\n",
      "time_this_iter_s: 76.58459329605103\n",
      "time_total_s: 3799.6194937229156\n",
      "timers:\n",
      "  learn_throughput: 842.411\n",
      "  learn_time_ms: 540.71\n",
      "  load_throughput: 573174.569\n",
      "  load_time_ms: 0.795\n",
      "  sample_throughput: 6.706\n",
      "  sample_time_ms: 67921.287\n",
      "timestamp: 1638986348\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 24080\n",
      "training_iteration: 64\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:64 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 451 0.005 2.254999999999974\n",
      "blue_1 True True 451 -2.001 0.2049999999999792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 49062\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_02-59-58\n",
      "done: false\n",
      "episode_len_mean: 179.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.21353000000001743\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 143\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.675944328308105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008343678899109364\n",
      "        model: {}\n",
      "        policy_loss: -0.3055329918861389\n",
      "        total_loss: -0.20895807445049286\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.0839029848575592\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.068110466003418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006518301088362932\n",
      "        model: {}\n",
      "        policy_loss: -0.38562291860580444\n",
      "        total_loss: -0.2650417685508728\n",
      "        vf_explained_var: -0.11256594955921173\n",
      "        vf_loss: 0.11068148165941238\n",
      "  num_agent_steps_sampled: 49062\n",
      "  num_agent_steps_trained: 49062\n",
      "  num_steps_sampled: 24531\n",
      "  num_steps_trained: 24531\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 65\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.697142857142865\n",
      "  ram_util_percent: 37.48428571428572\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.05212999999999693\n",
      "  blue_1: -0.26566000000000267\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5882203140730704\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 124.93489509640557\n",
      "  mean_inference_ms: 2.9467012360821747\n",
      "  mean_raw_obs_processing_ms: 35.078317198523976\n",
      "time_since_restore: 3844.622287273407\n",
      "time_this_iter_s: 45.00279355049133\n",
      "time_total_s: 3844.622287273407\n",
      "timers:\n",
      "  learn_throughput: 843.46\n",
      "  learn_time_ms: 559.362\n",
      "  load_throughput: 593436.282\n",
      "  load_time_ms: 0.795\n",
      "  sample_throughput: 6.953\n",
      "  sample_time_ms: 67857.998\n",
      "timestamp: 1638986398\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 24531\n",
      "training_iteration: 65\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:65 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 278 -1.998 -0.5590000000000037\n",
      "blue_1 True True 278 0.005 1.3899999999999924\n",
      "agent_timesteps_total: 49618\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-00-35\n",
      "done: false\n",
      "episode_len_mean: 181.35\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.19717000000001764\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 144\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.166088104248047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013482486829161644\n",
      "        model: {}\n",
      "        policy_loss: -0.2492649406194687\n",
      "        total_loss: 0.14246965944766998\n",
      "        vf_explained_var: -0.07139306515455246\n",
      "        vf_loss: 0.37125808000564575\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.20534086227417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011517183855175972\n",
      "        model: {}\n",
      "        policy_loss: -0.20809154212474823\n",
      "        total_loss: -0.17649860680103302\n",
      "        vf_explained_var: -0.42665359377861023\n",
      "        vf_loss: 0.014101252891123295\n",
      "  num_agent_steps_sampled: 49618\n",
      "  num_agent_steps_trained: 49618\n",
      "  num_steps_sampled: 24809\n",
      "  num_steps_trained: 24809\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 66\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.188\n",
      "  ram_util_percent: 37.472\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.040539999999996905\n",
      "  blue_1: -0.23771000000000278\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5877801075938877\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 124.79412034981812\n",
      "  mean_inference_ms: 2.9441548164265123\n",
      "  mean_raw_obs_processing_ms: 35.03364161321205\n",
      "time_since_restore: 3875.9954545497894\n",
      "time_this_iter_s: 31.373167276382446\n",
      "time_total_s: 3875.9954545497894\n",
      "timers:\n",
      "  learn_throughput: 846.533\n",
      "  learn_time_ms: 497.677\n",
      "  load_throughput: 528016.577\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 6.812\n",
      "  sample_time_ms: 61849.744\n",
      "timestamp: 1638986435\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 24809\n",
      "training_iteration: 66\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:66 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 195 -2.0 -1.0299999999999994\n",
      "blue_1 True True 195 0.005 0.9750000000000008\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 148 -2.0 -1.2649999999999995\n",
      "blue_1 True True 148 0.005 0.7400000000000005\n",
      "agent_timesteps_total: 50304\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-01-32\n",
      "done: false\n",
      "episode_len_mean: 178.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.2269300000000172\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 146\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.2531099319458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013461574912071228\n",
      "        model: {}\n",
      "        policy_loss: -0.3569471538066864\n",
      "        total_loss: -0.06307883560657501\n",
      "        vf_explained_var: 0.3354734480381012\n",
      "        vf_loss: 0.27342361211776733\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.549912929534912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010784757323563099\n",
      "        model: {}\n",
      "        policy_loss: -0.39383241534233093\n",
      "        total_loss: -0.3699600398540497\n",
      "        vf_explained_var: 0.18724313378334045\n",
      "        vf_loss: 0.007493012584745884\n",
      "  num_agent_steps_sampled: 50304\n",
      "  num_agent_steps_trained: 50304\n",
      "  num_steps_sampled: 25152\n",
      "  num_steps_trained: 25152\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 67\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.06202531645569\n",
      "  ram_util_percent: 37.47341772151899\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.004309999999996983\n",
      "  blue_1: -0.23124000000000255\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5869359317199796\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 124.53074016034486\n",
      "  mean_inference_ms: 2.9392350587471525\n",
      "  mean_raw_obs_processing_ms: 34.95694873192224\n",
      "time_since_restore: 3926.9320271015167\n",
      "time_this_iter_s: 50.936572551727295\n",
      "time_total_s: 3926.9320271015167\n",
      "timers:\n",
      "  learn_throughput: 861.09\n",
      "  learn_time_ms: 497.973\n",
      "  load_throughput: 537416.349\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 6.724\n",
      "  sample_time_ms: 63773.916\n",
      "timestamp: 1638986492\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 25152\n",
      "training_iteration: 67\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:67 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 84 0.005 0.42000000000000026\n",
      "blue_1 True True 84 -2.0 -1.5849999999999997\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 240 -2.0 -0.8050000000000035\n",
      "blue_1 True True 240 0.005 1.1999999999999964\n",
      "agent_timesteps_total: 50952\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-02-29\n",
      "done: false\n",
      "episode_len_mean: 176.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.24679000000001686\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 148\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.958946704864502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009006889536976814\n",
      "        model: {}\n",
      "        policy_loss: -0.4940074682235718\n",
      "        total_loss: -0.331773579120636\n",
      "        vf_explained_var: -0.22196224331855774\n",
      "        vf_loss: 0.14855466783046722\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.980535507202148\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017972031608223915\n",
      "        model: {}\n",
      "        policy_loss: -0.016745006665587425\n",
      "        total_loss: 0.689261794090271\n",
      "        vf_explained_var: 0.4119507372379303\n",
      "        vf_loss: 0.6787118315696716\n",
      "  num_agent_steps_sampled: 50952\n",
      "  num_agent_steps_trained: 50952\n",
      "  num_steps_sampled: 25476\n",
      "  num_steps_trained: 25476\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 68\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.36666666666667\n",
      "  ram_util_percent: 37.467948717948715\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.005690000000002847\n",
      "  blue_1: -0.24110000000000242\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5862508894233707\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 124.31501654740889\n",
      "  mean_inference_ms: 2.9349105867127308\n",
      "  mean_raw_obs_processing_ms: 34.912592033499905\n",
      "time_since_restore: 3978.362727880478\n",
      "time_this_iter_s: 51.43070077896118\n",
      "time_total_s: 3978.362727880478\n",
      "timers:\n",
      "  learn_throughput: 858.824\n",
      "  learn_time_ms: 497.774\n",
      "  load_throughput: 537731.282\n",
      "  load_time_ms: 0.795\n",
      "  sample_throughput: 6.554\n",
      "  sample_time_ms: 65227.587\n",
      "timestamp: 1638986549\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 25476\n",
      "training_iteration: 68\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:68 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 196 -2.0 -1.0249999999999992\n",
      "blue_1 True True 196 0.005 0.9800000000000008\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 340 0.005 1.7399999999999878\n",
      "blue_1 True True 340 -2.0 -0.24800000000001332\n",
      "agent_timesteps_total: 52024\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-03-40\n",
      "done: false\n",
      "episode_len_mean: 179.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.22052000000001723\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 150\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.159573554992676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013417662121355534\n",
      "        model: {}\n",
      "        policy_loss: -0.07394915074110031\n",
      "        total_loss: 0.17489632964134216\n",
      "        vf_explained_var: -0.14818374812602997\n",
      "        vf_loss: 0.2284674197435379\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.13045597076416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011066663078963757\n",
      "        model: {}\n",
      "        policy_loss: -0.2053832858800888\n",
      "        total_loss: -0.041094835847616196\n",
      "        vf_explained_var: 0.22157993912696838\n",
      "        vf_loss: 0.14748094975948334\n",
      "  num_agent_steps_sampled: 52024\n",
      "  num_agent_steps_trained: 52024\n",
      "  num_steps_sampled: 26012\n",
      "  num_steps_trained: 26012\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 69\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.36699999999999\n",
      "  ram_util_percent: 37.48\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.012690000000002971\n",
      "  blue_1: -0.20783000000000257\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5856055160856153\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 124.08750264280778\n",
      "  mean_inference_ms: 2.9307713282614225\n",
      "  mean_raw_obs_processing_ms: 34.860251352828925\n",
      "time_since_restore: 4044.577712535858\n",
      "time_this_iter_s: 66.21498465538025\n",
      "time_total_s: 4044.577712535858\n",
      "timers:\n",
      "  learn_throughput: 835.491\n",
      "  learn_time_ms: 530.586\n",
      "  load_throughput: 637588.287\n",
      "  load_time_ms: 0.695\n",
      "  sample_throughput: 6.823\n",
      "  sample_time_ms: 64972.71\n",
      "timestamp: 1638986620\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 26012\n",
      "training_iteration: 69\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:69 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 235 -2.0 -0.830000000000003\n",
      "blue_1 True True 235 0.004 1.1519999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 159 0.005 0.7950000000000006\n",
      "blue_1 True True 159 -2.0 -1.2099999999999995\n",
      "agent_timesteps_total: 52812\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-04-50\n",
      "done: false\n",
      "episode_len_mean: 179.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.21771000000001733\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 152\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.462316513061523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010450968518853188\n",
      "        model: {}\n",
      "        policy_loss: -0.04393196478486061\n",
      "        total_loss: 0.2505451738834381\n",
      "        vf_explained_var: -0.025695975869894028\n",
      "        vf_loss: 0.2786047160625458\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.52405834197998\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0074462671764194965\n",
      "        model: {}\n",
      "        policy_loss: -0.11360073834657669\n",
      "        total_loss: 0.2367323786020279\n",
      "        vf_explained_var: -0.39872342348098755\n",
      "        vf_loss: 0.3390240967273712\n",
      "  num_agent_steps_sampled: 52812\n",
      "  num_agent_steps_trained: 52812\n",
      "  num_steps_sampled: 26406\n",
      "  num_steps_trained: 26406\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 70\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.52395833333333\n",
      "  ram_util_percent: 37.626041666666666\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.03129000000000301\n",
      "  blue_1: -0.18642000000000256\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5850272158636919\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 123.88478351469999\n",
      "  mean_inference_ms: 2.926828736098945\n",
      "  mean_raw_obs_processing_ms: 34.810346398225285\n",
      "time_since_restore: 4108.77706694603\n",
      "time_this_iter_s: 64.19935441017151\n",
      "time_total_s: 4108.77706694603\n",
      "timers:\n",
      "  learn_throughput: 818.528\n",
      "  learn_time_ms: 548.668\n",
      "  load_throughput: 645863.853\n",
      "  load_time_ms: 0.695\n",
      "  sample_throughput: 6.76\n",
      "  sample_time_ms: 66431.148\n",
      "timestamp: 1638986690\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 26406\n",
      "training_iteration: 70\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:70 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 106 0.005 0.5300000000000004\n",
      "blue_1 True True 106 -2.0 -1.4749999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 125 0.005 0.6250000000000004\n",
      "blue_1 True True 125 -2.0 -1.3799999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 187 0.005 0.9350000000000007\n",
      "blue_1 True True 187 -2.0 -1.0699999999999994\n",
      "agent_timesteps_total: 53648\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-06-05\n",
      "done: false\n",
      "episode_len_mean: 180.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.20911000000001742\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 155\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.168803215026855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015508921816945076\n",
      "        model: {}\n",
      "        policy_loss: 0.011019829660654068\n",
      "        total_loss: 0.07519666105508804\n",
      "        vf_explained_var: -0.5445041656494141\n",
      "        vf_loss: 0.04062264412641525\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.162248611450195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014014999382197857\n",
      "        model: {}\n",
      "        policy_loss: -0.2079007476568222\n",
      "        total_loss: 0.1027199774980545\n",
      "        vf_explained_var: 0.3830137550830841\n",
      "        vf_loss: 0.2893354296684265\n",
      "  num_agent_steps_sampled: 53648\n",
      "  num_agent_steps_trained: 53648\n",
      "  num_steps_sampled: 26824\n",
      "  num_steps_trained: 26824\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 71\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.66923076923077\n",
      "  ram_util_percent: 37.63365384615385\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.026990000000003046\n",
      "  blue_1: -0.1821200000000026\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.584196268038648\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 123.59214745290812\n",
      "  mean_inference_ms: 2.9212391959486506\n",
      "  mean_raw_obs_processing_ms: 34.721576980936135\n",
      "time_since_restore: 4178.376407146454\n",
      "time_this_iter_s: 69.5993402004242\n",
      "time_total_s: 4178.376407146454\n",
      "timers:\n",
      "  learn_throughput: 819.435\n",
      "  learn_time_ms: 546.474\n",
      "  load_throughput: 643994.285\n",
      "  load_time_ms: 0.695\n",
      "  sample_throughput: 6.624\n",
      "  sample_time_ms: 67603.504\n",
      "timestamp: 1638986765\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 26824\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:70 starting ! -----------------\n",
      "agent_timesteps_total: 53648\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-06-05\n",
      "done: false\n",
      "episode_len_mean: 180.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.20911000000001742\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 155\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.168803215026855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015508921816945076\n",
      "        model: {}\n",
      "        policy_loss: 0.011019829660654068\n",
      "        total_loss: 0.07519666105508804\n",
      "        vf_explained_var: -0.5445041656494141\n",
      "        vf_loss: 0.04062264412641525\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.162248611450195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014014999382197857\n",
      "        model: {}\n",
      "        policy_loss: -0.2079007476568222\n",
      "        total_loss: 0.1027199774980545\n",
      "        vf_explained_var: 0.3830137550830841\n",
      "        vf_loss: 0.2893354296684265\n",
      "  num_agent_steps_sampled: 53648\n",
      "  num_agent_steps_trained: 53648\n",
      "  num_steps_sampled: 26824\n",
      "  num_steps_trained: 26824\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 71\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.66923076923077\n",
      "  ram_util_percent: 37.63365384615385\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.026990000000003046\n",
      "  blue_1: -0.1821200000000026\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.584196268038648\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 123.59214745290812\n",
      "  mean_inference_ms: 2.9212391959486506\n",
      "  mean_raw_obs_processing_ms: 34.721576980936135\n",
      "time_since_restore: 4178.376407146454\n",
      "time_this_iter_s: 69.5993402004242\n",
      "time_total_s: 4178.376407146454\n",
      "timers:\n",
      "  learn_throughput: 819.435\n",
      "  learn_time_ms: 546.474\n",
      "  load_throughput: 643994.285\n",
      "  load_time_ms: 0.695\n",
      "  sample_throughput: 6.624\n",
      "  sample_time_ms: 67603.504\n",
      "timestamp: 1638986765\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 26824\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:71 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 216 0.005 1.079999999999999\n",
      "blue_1 True True 216 -2.0 -0.9250000000000009\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 176 0.005 0.8800000000000007\n",
      "blue_1 True True 176 -2.0 -1.1249999999999993\n",
      "agent_timesteps_total: 54432\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-07-09\n",
      "done: false\n",
      "episode_len_mean: 182.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.19351000000001772\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 157\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.785709381103516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01328942272812128\n",
      "        model: {}\n",
      "        policy_loss: -0.09134615957736969\n",
      "        total_loss: -0.0390811525285244\n",
      "        vf_explained_var: -0.20146313309669495\n",
      "        vf_loss: 0.03208170831203461\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.073162078857422\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00956276897341013\n",
      "        model: {}\n",
      "        policy_loss: -0.1082221120595932\n",
      "        total_loss: 0.07676799595355988\n",
      "        vf_explained_var: 0.6371906995773315\n",
      "        vf_loss: 0.17046666145324707\n",
      "  num_agent_steps_sampled: 54432\n",
      "  num_agent_steps_trained: 54432\n",
      "  num_steps_sampled: 27216\n",
      "  num_steps_trained: 27216\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 72\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.81954022988505\n",
      "  ram_util_percent: 37.656321839080455\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.0008599999999969065\n",
      "  blue_1: -0.19437000000000265\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5836545278367467\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 123.38852992194853\n",
      "  mean_inference_ms: 2.917802220290254\n",
      "  mean_raw_obs_processing_ms: 34.65134398744945\n",
      "time_since_restore: 4236.034529685974\n",
      "time_this_iter_s: 57.658122539520264\n",
      "time_total_s: 4236.034529685974\n",
      "timers:\n",
      "  learn_throughput: 809.203\n",
      "  learn_time_ms: 531.511\n",
      "  load_throughput: 722107.978\n",
      "  load_time_ms: 0.596\n",
      "  sample_throughput: 6.556\n",
      "  sample_time_ms: 65601.028\n",
      "timestamp: 1638986829\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 27216\n",
      "training_iteration: 72\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:72 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 181 -2.0 -1.0999999999999992\n",
      "blue_1 True True 181 0.005 0.9050000000000007\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 281 -2.0 -0.6000000000000079\n",
      "blue_1 True True 281 0.005 1.4319999999999928\n",
      "agent_timesteps_total: 55356\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-08-15\n",
      "done: false\n",
      "episode_len_mean: 183.95\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.17404000000001807\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 159\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.233535766601562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008587241172790527\n",
      "        model: {}\n",
      "        policy_loss: -0.27464041113853455\n",
      "        total_loss: 0.0380682609975338\n",
      "        vf_explained_var: 0.1957637220621109\n",
      "        vf_loss: 0.2996668219566345\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.981606483459473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012301529757678509\n",
      "        model: {}\n",
      "        policy_loss: -0.14159134030342102\n",
      "        total_loss: 0.07917054742574692\n",
      "        vf_explained_var: -0.5562706589698792\n",
      "        vf_loss: 0.2020789533853531\n",
      "  num_agent_steps_sampled: 55356\n",
      "  num_agent_steps_trained: 55356\n",
      "  num_steps_sampled: 27678\n",
      "  num_steps_trained: 27678\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 73\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.29247311827957\n",
      "  ram_util_percent: 37.58494623655914\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: -0.00959000000000315\n",
      "  blue_1: -0.1644500000000027\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5831299863490549\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 123.17824690645371\n",
      "  mean_inference_ms: 2.914369661713545\n",
      "  mean_raw_obs_processing_ms: 34.571560088369594\n",
      "time_since_restore: 4297.405935287476\n",
      "time_this_iter_s: 61.371405601501465\n",
      "time_total_s: 4297.405935287476\n",
      "timers:\n",
      "  learn_throughput: 811.066\n",
      "  learn_time_ms: 501.192\n",
      "  load_throughput: 819782.948\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 6.45\n",
      "  sample_time_ms: 63019.684\n",
      "timestamp: 1638986895\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 27678\n",
      "training_iteration: 73\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:73 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 421 0.005 2.1049999999999773\n",
      "blue_1 True True 421 -1.998 0.2749999999999917\n",
      "agent_timesteps_total: 56198\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-09-12\n",
      "done: false\n",
      "episode_len_mean: 187.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.13499000000001876\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 160\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.14546012878418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011829247698187828\n",
      "        model: {}\n",
      "        policy_loss: -0.2030636966228485\n",
      "        total_loss: -0.11336393654346466\n",
      "        vf_explained_var: -0.7438342571258545\n",
      "        vf_loss: 0.07173410803079605\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.584115028381348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005898236762732267\n",
      "        model: {}\n",
      "        policy_loss: -0.2765975892543793\n",
      "        total_loss: -0.09558352082967758\n",
      "        vf_explained_var: -0.7633318305015564\n",
      "        vf_loss: 0.1720561534166336\n",
      "  num_agent_steps_sampled: 56198\n",
      "  num_agent_steps_trained: 56198\n",
      "  num_steps_sampled: 28099\n",
      "  num_steps_trained: 28099\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 74\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.19220779220778\n",
      "  ram_util_percent: 37.5974025974026\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.029109999999996628\n",
      "  blue_1: -0.16410000000000277\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5828708401208261\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 123.06985820055081\n",
      "  mean_inference_ms: 2.912650441700165\n",
      "  mean_raw_obs_processing_ms: 34.525437304236355\n",
      "time_since_restore: 4348.24356341362\n",
      "time_this_iter_s: 50.83762812614441\n",
      "time_total_s: 4348.24356341362\n",
      "timers:\n",
      "  learn_throughput: 806.377\n",
      "  learn_time_ms: 498.402\n",
      "  load_throughput: 810623.12\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 6.652\n",
      "  sample_time_ms: 60420.384\n",
      "timestamp: 1638986952\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 28099\n",
      "training_iteration: 74\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:74 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 218 0.005 1.0899999999999987\n",
      "blue_1 True True 218 -2.0 -0.9150000000000011\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 114 0.005 0.5700000000000004\n",
      "blue_1 True True 114 -2.0 -1.4349999999999996\n",
      "agent_timesteps_total: 56862\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-10-06\n",
      "done: false\n",
      "episode_len_mean: 184.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.16951000000001798\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 162\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.483261108398438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011441534385085106\n",
      "        model: {}\n",
      "        policy_loss: -0.4762013554573059\n",
      "        total_loss: -0.4407145380973816\n",
      "        vf_explained_var: 0.6083512902259827\n",
      "        vf_loss: 0.018109967932105064\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.154387950897217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012799578718841076\n",
      "        model: {}\n",
      "        policy_loss: -0.3340858221054077\n",
      "        total_loss: 0.01947816275060177\n",
      "        vf_explained_var: 0.6129052042961121\n",
      "        vf_loss: 0.3341246247291565\n",
      "  num_agent_steps_sampled: 56862\n",
      "  num_agent_steps_trained: 56862\n",
      "  num_steps_sampled: 28431\n",
      "  num_steps_trained: 28431\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 75\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.62666666666666\n",
      "  ram_util_percent: 37.574666666666666\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.04205999999999706\n",
      "  blue_1: -0.21157000000000237\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5823806812241762\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 122.86473134273689\n",
      "  mean_inference_ms: 2.9093821499779207\n",
      "  mean_raw_obs_processing_ms: 34.449660145775304\n",
      "time_since_restore: 4396.664166212082\n",
      "time_this_iter_s: 48.420602798461914\n",
      "time_total_s: 4396.664166212082\n",
      "timers:\n",
      "  learn_throughput: 811.093\n",
      "  learn_time_ms: 480.832\n",
      "  load_throughput: 786696.754\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 6.416\n",
      "  sample_time_ms: 60781.915\n",
      "timestamp: 1638987006\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 28431\n",
      "training_iteration: 75\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:75 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 378 0.005 1.8899999999999817\n",
      "blue_1 True True 378 -2.0 -0.04100000000001813\n",
      "agent_timesteps_total: 57618\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-10-56\n",
      "done: false\n",
      "episode_len_mean: 186.97\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.1416700000000185\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 163\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.974848747253418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009504171088337898\n",
      "        model: {}\n",
      "        policy_loss: -0.22564543783664703\n",
      "        total_loss: -0.18846264481544495\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.022748315706849098\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.616418838500977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0058407592587172985\n",
      "        model: {}\n",
      "        policy_loss: -0.6010255217552185\n",
      "        total_loss: -0.3718009293079376\n",
      "        vf_explained_var: -0.49677136540412903\n",
      "        vf_loss: 0.220353901386261\n",
      "  num_agent_steps_sampled: 57618\n",
      "  num_agent_steps_trained: 57618\n",
      "  num_steps_sampled: 28809\n",
      "  num_steps_trained: 28809\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 76\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.042028985507244\n",
      "  ram_util_percent: 37.589855072463756\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.07565999999999691\n",
      "  blue_1: -0.21733000000000252\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5821631444663745\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 122.7623740102989\n",
      "  mean_inference_ms: 2.907893550821537\n",
      "  mean_raw_obs_processing_ms: 34.41173127766961\n",
      "time_since_restore: 4440.880224704742\n",
      "time_this_iter_s: 44.21605849266052\n",
      "time_total_s: 4440.880224704742\n",
      "timers:\n",
      "  learn_throughput: 830.676\n",
      "  learn_time_ms: 481.536\n",
      "  load_throughput: 806713.276\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 6.446\n",
      "  sample_time_ms: 62049.433\n",
      "timestamp: 1638987056\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 28809\n",
      "training_iteration: 76\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:76 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 153 0.005 0.7650000000000006\n",
      "blue_1 True True 153 -2.0 -1.2399999999999993\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 132 -2.0 -1.3449999999999995\n",
      "blue_1 True True 132 0.005 0.6600000000000005\n",
      "agent_timesteps_total: 58188\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-11-49\n",
      "done: false\n",
      "episode_len_mean: 187.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.13257000000001862\n",
      "episode_reward_min: -2.9400000000000004\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 165\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.526232719421387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010638343170285225\n",
      "        model: {}\n",
      "        policy_loss: -0.29020577669143677\n",
      "        total_loss: 0.071741983294487\n",
      "        vf_explained_var: 0.3550049960613251\n",
      "        vf_loss: 0.34579071402549744\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.94411039352417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012685269117355347\n",
      "        model: {}\n",
      "        policy_loss: 0.008861531503498554\n",
      "        total_loss: 0.41766640543937683\n",
      "        vf_explained_var: 0.5076737999916077\n",
      "        vf_loss: 0.3895391821861267\n",
      "  num_agent_steps_sampled: 58188\n",
      "  num_agent_steps_trained: 58188\n",
      "  num_steps_sampled: 29094\n",
      "  num_steps_trained: 29094\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 77\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.5027397260274\n",
      "  ram_util_percent: 37.664383561643845\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08020999999999692\n",
      "  blue_1: -0.21278000000000252\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.8599999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5817352380488492\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 122.5584328546221\n",
      "  mean_inference_ms: 2.9049249548991973\n",
      "  mean_raw_obs_processing_ms: 34.33734990091757\n",
      "time_since_restore: 4488.136524438858\n",
      "time_this_iter_s: 47.2562997341156\n",
      "time_total_s: 4488.136524438858\n",
      "timers:\n",
      "  learn_throughput: 812.739\n",
      "  learn_time_ms: 485.026\n",
      "  load_throughput: 661940.362\n",
      "  load_time_ms: 0.596\n",
      "  sample_throughput: 6.391\n",
      "  sample_time_ms: 61683.675\n",
      "timestamp: 1638987109\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 29094\n",
      "training_iteration: 77\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:77 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 99 -2.0 -1.5099999999999998\n",
      "blue_1 True True 99 0.005 0.49500000000000033\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 94 0.005 0.4700000000000003\n",
      "blue_1 True True 94 -2.0 -1.5349999999999997\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 249 0.005 1.2449999999999954\n",
      "blue_1 True True 249 -2.0 -0.7600000000000044\n",
      "agent_timesteps_total: 59072\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-13-07\n",
      "done: false\n",
      "episode_len_mean: 188.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.10342000000001866\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 168\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.667655944824219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013468273915350437\n",
      "        model: {}\n",
      "        policy_loss: -0.027284936979413033\n",
      "        total_loss: 0.4950982928276062\n",
      "        vf_explained_var: 0.18166330456733704\n",
      "        vf_loss: 0.5019282698631287\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.736403942108154\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01074204035103321\n",
      "        model: {}\n",
      "        policy_loss: -0.271438866853714\n",
      "        total_loss: 0.06198713555932045\n",
      "        vf_explained_var: -0.11901681870222092\n",
      "        vf_loss: 0.3171114921569824\n",
      "  num_agent_steps_sampled: 59072\n",
      "  num_agent_steps_trained: 59072\n",
      "  num_steps_sampled: 29536\n",
      "  num_steps_trained: 29536\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 78\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.1287037037037\n",
      "  ram_util_percent: 37.867592592592594\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10480999999999685\n",
      "  blue_1: -0.2082300000000026\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.785\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5811930983580244\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 122.24757446417021\n",
      "  mean_inference_ms: 2.9007986747556043\n",
      "  mean_raw_obs_processing_ms: 34.21670174111693\n",
      "time_since_restore: 4560.978699922562\n",
      "time_this_iter_s: 72.84217548370361\n",
      "time_total_s: 4560.978699922562\n",
      "timers:\n",
      "  learn_throughput: 804.957\n",
      "  learn_time_ms: 504.375\n",
      "  load_throughput: 814194.322\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.362\n",
      "  sample_time_ms: 63815.818\n",
      "timestamp: 1638987187\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 29536\n",
      "training_iteration: 78\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:78 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 50 -2.0 -1.755\n",
      "blue_1 True True 50 0.005 0.2500000000000001\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 179 0.005 0.8950000000000007\n",
      "blue_1 True True 179 -2.0 -1.1099999999999994\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 187 -2.0 -1.0699999999999994\n",
      "blue_1 True True 187 0.005 0.9350000000000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 59904\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-14-17\n",
      "done: false\n",
      "episode_len_mean: 187.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.12155000000001852\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 171\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.089229583740234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016664696857333183\n",
      "        model: {}\n",
      "        policy_loss: -0.234964981675148\n",
      "        total_loss: 0.1811337172985077\n",
      "        vf_explained_var: 0.02217433787882328\n",
      "        vf_loss: 0.3907892107963562\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.242575645446777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012510290369391441\n",
      "        model: {}\n",
      "        policy_loss: 0.015030418522655964\n",
      "        total_loss: 0.26798802614212036\n",
      "        vf_explained_var: -0.06902070343494415\n",
      "        vf_loss: 0.23395763337612152\n",
      "  num_agent_steps_sampled: 59904\n",
      "  num_agent_steps_trained: 59904\n",
      "  num_steps_sampled: 29952\n",
      "  num_steps_trained: 29952\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 79\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.24020618556701\n",
      "  ram_util_percent: 37.86185567010309\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.07690999999999693\n",
      "  blue_1: -0.19846000000000255\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.785\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5807269101755765\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 121.95418021354793\n",
      "  mean_inference_ms: 2.896960273897827\n",
      "  mean_raw_obs_processing_ms: 34.11022005184786\n",
      "time_since_restore: 4625.419069290161\n",
      "time_this_iter_s: 64.44036936759949\n",
      "time_total_s: 4625.419069290161\n",
      "timers:\n",
      "  learn_throughput: 804.388\n",
      "  learn_time_ms: 489.813\n",
      "  load_throughput: 658677.419\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 6.188\n",
      "  sample_time_ms: 63672.48\n",
      "timestamp: 1638987257\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 29952\n",
      "training_iteration: 79\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:79 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 223 0.005 1.1149999999999982\n",
      "blue_1 True True 223 -2.0 -0.8900000000000017\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 73 0.005 0.3650000000000002\n",
      "blue_1 True True 73 -2.0 -1.6399999999999997\n",
      "agent_timesteps_total: 60496\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-15-07\n",
      "done: false\n",
      "episode_len_mean: 186.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.12815000000001844\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 173\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.285602569580078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009860564954578876\n",
      "        model: {}\n",
      "        policy_loss: -0.3150024712085724\n",
      "        total_loss: -0.23854570090770721\n",
      "        vf_explained_var: -0.07251857221126556\n",
      "        vf_loss: 0.061481017619371414\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.792015552520752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01108888816088438\n",
      "        model: {}\n",
      "        policy_loss: -0.3149928152561188\n",
      "        total_loss: -0.05545466020703316\n",
      "        vf_explained_var: 0.08569546043872833\n",
      "        vf_loss: 0.24269694089889526\n",
      "  num_agent_steps_sampled: 60496\n",
      "  num_agent_steps_trained: 60496\n",
      "  num_steps_sampled: 30248\n",
      "  num_steps_trained: 30248\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 80\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.52608695652174\n",
      "  ram_util_percent: 37.920289855072454\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11370999999999698\n",
      "  blue_1: -0.2418600000000025\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.785\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5804103901427686\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 121.76836251484994\n",
      "  mean_inference_ms: 2.8945872806415593\n",
      "  mean_raw_obs_processing_ms: 34.04510700592096\n",
      "time_since_restore: 4669.527520895004\n",
      "time_this_iter_s: 44.10845160484314\n",
      "time_total_s: 4669.527520895004\n",
      "timers:\n",
      "  learn_throughput: 813.357\n",
      "  learn_time_ms: 472.363\n",
      "  load_throughput: 770881.935\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 6.23\n",
      "  sample_time_ms: 61668.273\n",
      "timestamp: 1638987307\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 30248\n",
      "training_iteration: 80\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:80 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 68 -2.0 -1.6649999999999998\n",
      "blue_1 True True 68 0.005 0.3400000000000002\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 269 0.005 1.3449999999999933\n",
      "blue_1 True True 269 -2.0 -0.6600000000000066\n",
      "agent_timesteps_total: 61170\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-16-00\n",
      "done: false\n",
      "episode_len_mean: 185.43\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.13945000000001828\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 175\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.151233673095703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013658012263476849\n",
      "        model: {}\n",
      "        policy_loss: 0.023480141535401344\n",
      "        total_loss: 0.6211147308349609\n",
      "        vf_explained_var: 0.3030577003955841\n",
      "        vf_loss: 0.5768914818763733\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.2383012771606445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004859550390392542\n",
      "        model: {}\n",
      "        policy_loss: -0.4296502470970154\n",
      "        total_loss: -0.3169098496437073\n",
      "        vf_explained_var: 0.4538739025592804\n",
      "        vf_loss: 0.1053599938750267\n",
      "  num_agent_steps_sampled: 61170\n",
      "  num_agent_steps_trained: 61170\n",
      "  num_steps_sampled: 30585\n",
      "  num_steps_trained: 30585\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 81\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.11891891891892\n",
      "  ram_util_percent: 38.089189189189185\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08800999999999695\n",
      "  blue_1: -0.22746000000000252\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.785\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.580090978727633\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 121.58517209518115\n",
      "  mean_inference_ms: 2.892297356362349\n",
      "  mean_raw_obs_processing_ms: 33.982594064372705\n",
      "time_since_restore: 4717.683013677597\n",
      "time_this_iter_s: 48.15549278259277\n",
      "time_total_s: 4717.683013677597\n",
      "timers:\n",
      "  learn_throughput: 821.171\n",
      "  learn_time_ms: 458.004\n",
      "  load_throughput: 628777.796\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 6.319\n",
      "  sample_time_ms: 59522.185\n",
      "timestamp: 1638987360\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 30585\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:80 starting ! -----------------\n",
      "agent_timesteps_total: 61170\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-16-00\n",
      "done: false\n",
      "episode_len_mean: 185.43\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.145999999999909\n",
      "episode_reward_mean: -0.13945000000001828\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 175\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.151233673095703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013658012263476849\n",
      "        model: {}\n",
      "        policy_loss: 0.023480141535401344\n",
      "        total_loss: 0.6211147308349609\n",
      "        vf_explained_var: 0.3030577003955841\n",
      "        vf_loss: 0.5768914818763733\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.2383012771606445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004859550390392542\n",
      "        model: {}\n",
      "        policy_loss: -0.4296502470970154\n",
      "        total_loss: -0.3169098496437073\n",
      "        vf_explained_var: 0.4538739025592804\n",
      "        vf_loss: 0.1053599938750267\n",
      "  num_agent_steps_sampled: 61170\n",
      "  num_agent_steps_trained: 61170\n",
      "  num_steps_sampled: 30585\n",
      "  num_steps_trained: 30585\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 81\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.11891891891892\n",
      "  ram_util_percent: 38.089189189189185\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08800999999999695\n",
      "  blue_1: -0.22746000000000252\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.785\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.580090978727633\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 121.58517209518115\n",
      "  mean_inference_ms: 2.892297356362349\n",
      "  mean_raw_obs_processing_ms: 33.982594064372705\n",
      "time_since_restore: 4717.683013677597\n",
      "time_this_iter_s: 48.15549278259277\n",
      "time_total_s: 4717.683013677597\n",
      "timers:\n",
      "  learn_throughput: 821.171\n",
      "  learn_time_ms: 458.004\n",
      "  load_throughput: 628777.796\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 6.319\n",
      "  sample_time_ms: 59522.185\n",
      "timestamp: 1638987360\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 30585\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:81 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False False 600 -0.996 2.0039999999999734\n",
      "blue_1 False False 600 -0.995 2.2999999999999603\n",
      "agent_timesteps_total: 62370\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-17-05\n",
      "done: false\n",
      "episode_len_mean: 190.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.08076000000001958\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 176\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.943665981292725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008981694467365742\n",
      "        model: {}\n",
      "        policy_loss: -0.38648149371147156\n",
      "        total_loss: -0.3143952786922455\n",
      "        vf_explained_var: -0.857554018497467\n",
      "        vf_loss: 0.0584452785551548\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.5957865715026855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014197423122823238\n",
      "        model: {}\n",
      "        policy_loss: -0.3253546357154846\n",
      "        total_loss: -0.24503837525844574\n",
      "        vf_explained_var: -0.7644212245941162\n",
      "        vf_loss: 0.06953508406877518\n",
      "  num_agent_steps_sampled: 62370\n",
      "  num_agent_steps_trained: 62370\n",
      "  num_steps_sampled: 31185\n",
      "  num_steps_trained: 31185\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 82\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.145555555555546\n",
      "  ram_util_percent: 38.09666666666667\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10584999999999667\n",
      "  blue_1: -0.18661000000000294\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5799500317194533\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 121.48907779471035\n",
      "  mean_inference_ms: 2.891200702847954\n",
      "  mean_raw_obs_processing_ms: 33.94654920172279\n",
      "time_since_restore: 4776.6500499248505\n",
      "time_this_iter_s: 58.96703624725342\n",
      "time_total_s: 4776.6500499248505\n",
      "timers:\n",
      "  learn_throughput: 838.644\n",
      "  learn_time_ms: 473.264\n",
      "  load_throughput: 568707.043\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.656\n",
      "  sample_time_ms: 59631.03\n",
      "timestamp: 1638987425\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 31185\n",
      "training_iteration: 82\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:82 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 105 0.005 0.5250000000000004\n",
      "blue_1 True True 105 -2.0 -1.4799999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 76 0.005 0.3800000000000002\n",
      "blue_1 True True 76 -2.0 -1.6249999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 101 0.005 0.5050000000000003\n",
      "blue_1 True True 101 -2.0 -1.4999999999999996\n",
      "agent_timesteps_total: 62934\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-18-08\n",
      "done: false\n",
      "episode_len_mean: 188.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.10336000000001919\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 179\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.150737762451172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01307294424623251\n",
      "        model: {}\n",
      "        policy_loss: -0.08532219380140305\n",
      "        total_loss: -0.03390277549624443\n",
      "        vf_explained_var: -0.8252537250518799\n",
      "        vf_loss: 0.031564876437187195\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.222251892089844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01834714785218239\n",
      "        model: {}\n",
      "        policy_loss: -0.2054697722196579\n",
      "        total_loss: 0.6295145750045776\n",
      "        vf_explained_var: 0.36183062195777893\n",
      "        vf_loss: 0.8210518956184387\n",
      "  num_agent_steps_sampled: 62934\n",
      "  num_agent_steps_trained: 62934\n",
      "  num_steps_sampled: 31467\n",
      "  num_steps_trained: 31467\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 83\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.00689655172414\n",
      "  ram_util_percent: 38.07011494252873\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1145999999999967\n",
      "  blue_1: -0.21796000000000293\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5795363500508985\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 121.21101312672283\n",
      "  mean_inference_ms: 2.8879534897628703\n",
      "  mean_raw_obs_processing_ms: 33.84483165542317\n",
      "time_since_restore: 4833.872003316879\n",
      "time_this_iter_s: 57.22195339202881\n",
      "time_total_s: 4833.872003316879\n",
      "timers:\n",
      "  learn_throughput: 829.276\n",
      "  learn_time_ms: 456.904\n",
      "  load_throughput: 473236.194\n",
      "  load_time_ms: 0.801\n",
      "  sample_throughput: 6.395\n",
      "  sample_time_ms: 59245.589\n",
      "timestamp: 1638987488\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 31467\n",
      "training_iteration: 83\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:83 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 153 0.005 0.7650000000000006\n",
      "blue_1 True True 153 -2.0 -1.2399999999999993\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 162 0.005 0.8100000000000006\n",
      "blue_1 True True 162 -2.0 -1.1949999999999994\n",
      "agent_timesteps_total: 63564\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-19-04\n",
      "done: false\n",
      "episode_len_mean: 188.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.10076000000001921\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 181\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.953692436218262\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01196999754756689\n",
      "        model: {}\n",
      "        policy_loss: -0.19460830092430115\n",
      "        total_loss: -0.15214058756828308\n",
      "        vf_explained_var: -0.22127395868301392\n",
      "        vf_loss: 0.024288278073072433\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.366377830505371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01831500045955181\n",
      "        model: {}\n",
      "        policy_loss: -0.33826300501823425\n",
      "        total_loss: -0.12948940694332123\n",
      "        vf_explained_var: 0.36312511563301086\n",
      "        vf_loss: 0.19486567378044128\n",
      "  num_agent_steps_sampled: 63564\n",
      "  num_agent_steps_trained: 63564\n",
      "  num_steps_sampled: 31782\n",
      "  num_steps_trained: 31782\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 84\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.57435897435898\n",
      "  ram_util_percent: 38.08461538461538\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1158999999999967\n",
      "  blue_1: -0.2166600000000029\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5792759129797039\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 121.0322115625458\n",
      "  mean_inference_ms: 2.885842395858296\n",
      "  mean_raw_obs_processing_ms: 33.77648603467096\n",
      "time_since_restore: 4884.902794599533\n",
      "time_this_iter_s: 51.03079128265381\n",
      "time_total_s: 4884.902794599533\n",
      "timers:\n",
      "  learn_throughput: 835.69\n",
      "  learn_time_ms: 440.714\n",
      "  load_throughput: 525447.18\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.214\n",
      "  sample_time_ms: 59264.707\n",
      "timestamp: 1638987544\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 31782\n",
      "training_iteration: 84\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:84 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 61 -2.0 -1.6999999999999997\n",
      "blue_1 True True 61 0.005 0.30500000000000016\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 427 -1.998 0.397999999999991\n",
      "blue_1 True True 427 0.005 2.1349999999999767\n",
      "agent_timesteps_total: 64540\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-20-15\n",
      "done: false\n",
      "episode_len_mean: 192.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.06308000000001981\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 183\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.272305488586426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01026364229619503\n",
      "        model: {}\n",
      "        policy_loss: -0.3506792187690735\n",
      "        total_loss: 0.07731081545352936\n",
      "        vf_explained_var: 0.1101987287402153\n",
      "        vf_loss: 0.41240212321281433\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.870674133300781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02493138238787651\n",
      "        model: {}\n",
      "        policy_loss: -0.08228445053100586\n",
      "        total_loss: 0.16375404596328735\n",
      "        vf_explained_var: -0.4876662492752075\n",
      "        vf_loss: 0.2271062433719635\n",
      "  num_agent_steps_sampled: 64540\n",
      "  num_agent_steps_trained: 64540\n",
      "  num_steps_sampled: 32270\n",
      "  num_steps_trained: 32270\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 85\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.64329896907217\n",
      "  ram_util_percent: 38.07835051546392\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.13607999999999662\n",
      "  blue_1: -0.19916000000000314\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5790767536250252\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 120.85386059552354\n",
      "  mean_inference_ms: 2.883835348971935\n",
      "  mean_raw_obs_processing_ms: 33.70778968135431\n",
      "time_since_restore: 4950.108796596527\n",
      "time_this_iter_s: 65.20600199699402\n",
      "time_total_s: 4950.108796596527\n",
      "timers:\n",
      "  learn_throughput: 838.426\n",
      "  learn_time_ms: 457.882\n",
      "  load_throughput: 547833.868\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.303\n",
      "  sample_time_ms: 60908.89\n",
      "timestamp: 1638987615\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 32270\n",
      "training_iteration: 85\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:85 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 141 0.005 0.7050000000000005\n",
      "blue_1 True True 141 -2.0 -1.2999999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 135 0.005 0.6750000000000005\n",
      "blue_1 True True 135 -2.0 -1.3299999999999996\n",
      "agent_timesteps_total: 65092\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-21-04\n",
      "done: false\n",
      "episode_len_mean: 191.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.07826000000001943\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 185\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.969242572784424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014594229869544506\n",
      "        model: {}\n",
      "        policy_loss: -0.23219749331474304\n",
      "        total_loss: -0.19069026410579681\n",
      "        vf_explained_var: 0.6924189925193787\n",
      "        vf_loss: 0.019342225044965744\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.276755332946777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014082258567214012\n",
      "        model: {}\n",
      "        policy_loss: -0.22391490638256073\n",
      "        total_loss: 0.14980389177799225\n",
      "        vf_explained_var: 0.7021213173866272\n",
      "        vf_loss: 0.35767823457717896\n",
      "  num_agent_steps_sampled: 65092\n",
      "  num_agent_steps_trained: 65092\n",
      "  num_steps_sampled: 32546\n",
      "  num_steps_trained: 32546\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 86\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.400000000000006\n",
      "  ram_util_percent: 38.08382352941176\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.14867999999999681\n",
      "  blue_1: -0.22694000000000297\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5788682565754464\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 120.6713713225735\n",
      "  mean_inference_ms: 2.881881489967601\n",
      "  mean_raw_obs_processing_ms: 33.635965639816995\n",
      "time_since_restore: 4993.307520866394\n",
      "time_this_iter_s: 43.19872426986694\n",
      "time_total_s: 4993.307520866394\n",
      "timers:\n",
      "  learn_throughput: 814.124\n",
      "  learn_time_ms: 459.021\n",
      "  load_throughput: 533441.584\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.144\n",
      "  sample_time_ms: 60824.149\n",
      "timestamp: 1638987664\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 32546\n",
      "training_iteration: 86\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:86 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 189 -2.0 -1.0599999999999992\n",
      "blue_1 True True 189 0.005 0.9450000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 129 0.005 0.6450000000000005\n",
      "blue_1 True True 129 -2.0 -1.3599999999999994\n",
      "agent_timesteps_total: 65728\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-21-56\n",
      "done: false\n",
      "episode_len_mean: 191.71\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.07116000000001957\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 187\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.724408149719238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01591305434703827\n",
      "        model: {}\n",
      "        policy_loss: 0.11798235028982162\n",
      "        total_loss: 0.69468092918396\n",
      "        vf_explained_var: 0.032295722514390945\n",
      "        vf_loss: 0.5525306463241577\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.314929008483887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010549549013376236\n",
      "        model: {}\n",
      "        policy_loss: -0.4428052306175232\n",
      "        total_loss: -0.19255080819129944\n",
      "        vf_explained_var: -0.8362733125686646\n",
      "        vf_loss: 0.23823784291744232\n",
      "  num_agent_steps_sampled: 65728\n",
      "  num_agent_steps_trained: 65728\n",
      "  num_steps_sampled: 32864\n",
      "  num_steps_trained: 32864\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 87\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.62222222222223\n",
      "  ram_util_percent: 38.00972222222222\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.13217999999999683\n",
      "  blue_1: -0.20334000000000294\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5786399542420618\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 120.4837016754889\n",
      "  mean_inference_ms: 2.8799723705679696\n",
      "  mean_raw_obs_processing_ms: 33.56018901159457\n",
      "time_since_restore: 5039.820024490356\n",
      "time_this_iter_s: 46.5125036239624\n",
      "time_total_s: 5039.820024490356\n",
      "timers:\n",
      "  learn_throughput: 822.921\n",
      "  learn_time_ms: 458.124\n",
      "  load_throughput: 538353.741\n",
      "  load_time_ms: 0.7\n",
      "  sample_throughput: 6.205\n",
      "  sample_time_ms: 60753.169\n",
      "timestamp: 1638987716\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 32864\n",
      "training_iteration: 87\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:87 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 149 0.005 0.7450000000000006\n",
      "blue_1 True True 149 -2.0 -1.2599999999999993\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 184 -2.0 -1.0849999999999993\n",
      "blue_1 True True 184 0.005 0.9200000000000007\n",
      "agent_timesteps_total: 66394\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-22-51\n",
      "done: false\n",
      "episode_len_mean: 190.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.08551000000001929\n",
      "episode_reward_min: -1.745\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 189\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.195440292358398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007449055556207895\n",
      "        model: {}\n",
      "        policy_loss: -0.523794412612915\n",
      "        total_loss: -0.3838126063346863\n",
      "        vf_explained_var: 0.42233842611312866\n",
      "        vf_loss: 0.12866856157779694\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.822856426239014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016425320878624916\n",
      "        model: {}\n",
      "        policy_loss: 0.19444765150547028\n",
      "        total_loss: 0.5336235165596008\n",
      "        vf_explained_var: 0.5726462602615356\n",
      "        vf_loss: 0.32046642899513245\n",
      "  num_agent_steps_sampled: 66394\n",
      "  num_agent_steps_trained: 66394\n",
      "  num_steps_sampled: 33197\n",
      "  num_steps_trained: 33197\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 88\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.18815789473684\n",
      "  ram_util_percent: 37.998684210526314\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10477999999999695\n",
      "  blue_1: -0.19029000000000287\n",
      "policy_reward_min:\n",
      "  blue_0: -1.875\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5784326729677232\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 120.30779170650345\n",
      "  mean_inference_ms: 2.8781863671549526\n",
      "  mean_raw_obs_processing_ms: 33.49305964276116\n",
      "time_since_restore: 5089.354745864868\n",
      "time_this_iter_s: 49.53472137451172\n",
      "time_total_s: 5089.354745864868\n",
      "timers:\n",
      "  learn_throughput: 830.779\n",
      "  learn_time_ms: 440.671\n",
      "  load_throughput: 457603.616\n",
      "  load_time_ms: 0.8\n",
      "  sample_throughput: 6.265\n",
      "  sample_time_ms: 58439.115\n",
      "timestamp: 1638987771\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 33197\n",
      "training_iteration: 88\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:88 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 133 -2.0 -1.3399999999999994\n",
      "blue_1 True True 133 0.005 0.6650000000000005\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 210 0.005 1.0499999999999996\n",
      "blue_1 True True 210 -2.0 -0.9669999999999999\n",
      "agent_timesteps_total: 67080\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-23-55\n",
      "done: false\n",
      "episode_len_mean: 190.01\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.08793000000001908\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 191\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.342712879180908\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015361475758254528\n",
      "        model: {}\n",
      "        policy_loss: 0.0895974189043045\n",
      "        total_loss: 0.6443539261817932\n",
      "        vf_explained_var: 0.19337965548038483\n",
      "        vf_loss: 0.5314263105392456\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.344411849975586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007358533795922995\n",
      "        model: {}\n",
      "        policy_loss: -0.4618786871433258\n",
      "        total_loss: -0.41509902477264404\n",
      "        vf_explained_var: -0.27545684576034546\n",
      "        vf_loss: 0.03839785233139992\n",
      "  num_agent_steps_sampled: 67080\n",
      "  num_agent_steps_trained: 67080\n",
      "  num_steps_sampled: 33540\n",
      "  num_steps_trained: 33540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 89\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 70.74772727272727\n",
      "  ram_util_percent: 38.36590909090909\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.437999999999971\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.12367999999999704\n",
      "  blue_1: -0.2116100000000027\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5782690398885584\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 120.14652273867655\n",
      "  mean_inference_ms: 2.876697307466744\n",
      "  mean_raw_obs_processing_ms: 33.4345878687813\n",
      "time_since_restore: 5147.003279685974\n",
      "time_this_iter_s: 57.64853382110596\n",
      "time_total_s: 5147.003279685974\n",
      "timers:\n",
      "  learn_throughput: 848.586\n",
      "  learn_time_ms: 422.821\n",
      "  load_throughput: 512189.87\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.211\n",
      "  sample_time_ms: 57772.471\n",
      "timestamp: 1638987835\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 33540\n",
      "training_iteration: 89\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:89 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 116 0.005 0.5800000000000004\n",
      "blue_1 True True 116 -2.0 -1.4249999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 117 0.005 0.5850000000000004\n",
      "blue_1 True True 117 -2.0 -1.4199999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 360 -2.0 -0.20500000000001628\n",
      "blue_1 True True 360 0.024 2.5909999999999944\n",
      "agent_timesteps_total: 68266\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-25-28\n",
      "done: false\n",
      "episode_len_mean: 188.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.09126000000001859\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 194\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.091401100158691\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006840743590146303\n",
      "        model: {}\n",
      "        policy_loss: -0.38585183024406433\n",
      "        total_loss: -0.30296897888183594\n",
      "        vf_explained_var: 0.4263302981853485\n",
      "        vf_loss: 0.07249345630407333\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.317766189575195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02277851104736328\n",
      "        model: {}\n",
      "        policy_loss: 0.01232430525124073\n",
      "        total_loss: 0.885326087474823\n",
      "        vf_explained_var: -0.2856614291667938\n",
      "        vf_loss: 0.8470556139945984\n",
      "  num_agent_steps_sampled: 68266\n",
      "  num_agent_steps_trained: 68266\n",
      "  num_steps_sampled: 34133\n",
      "  num_steps_trained: 34133\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 90\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.102307692307704\n",
      "  ram_util_percent: 38.07846153846154\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11812999999999697\n",
      "  blue_1: -0.2093900000000027\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5780578156190316\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.90939068573148\n",
      "  mean_inference_ms: 2.8744989359437114\n",
      "  mean_raw_obs_processing_ms: 33.346972572282574\n",
      "time_since_restore: 5235.315556764603\n",
      "time_this_iter_s: 88.31227707862854\n",
      "time_total_s: 5235.315556764603\n",
      "timers:\n",
      "  learn_throughput: 853.964\n",
      "  learn_time_ms: 454.937\n",
      "  load_throughput: 483800.096\n",
      "  load_time_ms: 0.803\n",
      "  sample_throughput: 6.251\n",
      "  sample_time_ms: 62147.312\n",
      "timestamp: 1638987928\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 34133\n",
      "training_iteration: 90\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:90 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 58 -2.0 -1.7149999999999999\n",
      "blue_1 True True 58 0.005 0.29000000000000015\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 126 0.005 0.6300000000000004\n",
      "blue_1 True True 126 -2.0 -1.3749999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 213 0.005 1.0649999999999993\n",
      "blue_1 True True 213 -2.0 -0.9400000000000006\n",
      "agent_timesteps_total: 69060\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-26-47\n",
      "done: false\n",
      "episode_len_mean: 189.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.08266000000001877\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 197\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.279415130615234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012288141064345837\n",
      "        model: {}\n",
      "        policy_loss: -0.09144143760204315\n",
      "        total_loss: 0.2931383550167084\n",
      "        vf_explained_var: -0.10043829679489136\n",
      "        vf_loss: 0.3659171760082245\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.606553077697754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014115910977125168\n",
      "        model: {}\n",
      "        policy_loss: -0.1493615061044693\n",
      "        total_loss: 0.20867955684661865\n",
      "        vf_explained_var: 0.2206924706697464\n",
      "        vf_loss: 0.33392274379730225\n",
      "  num_agent_steps_sampled: 69060\n",
      "  num_agent_steps_trained: 69060\n",
      "  num_steps_sampled: 34530\n",
      "  num_steps_trained: 34530\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 91\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.062385321100926\n",
      "  ram_util_percent: 38.1091743119266\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10237999999999696\n",
      "  blue_1: -0.18504000000000276\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5778777404467909\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.67775775664235\n",
      "  mean_inference_ms: 2.8724469793233203\n",
      "  mean_raw_obs_processing_ms: 33.256456468382936\n",
      "time_since_restore: 5308.3984375\n",
      "time_this_iter_s: 73.08288073539734\n",
      "time_total_s: 5308.3984375\n",
      "timers:\n",
      "  learn_throughput: 835.997\n",
      "  learn_time_ms: 471.892\n",
      "  load_throughput: 491286.499\n",
      "  load_time_ms: 0.803\n",
      "  sample_throughput: 6.101\n",
      "  sample_time_ms: 64657.389\n",
      "timestamp: 1638988007\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 34530\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:90 starting ! -----------------\n",
      "agent_timesteps_total: 69060\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-26-47\n",
      "done: false\n",
      "episode_len_mean: 189.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.08266000000001877\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 197\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.279415130615234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012288141064345837\n",
      "        model: {}\n",
      "        policy_loss: -0.09144143760204315\n",
      "        total_loss: 0.2931383550167084\n",
      "        vf_explained_var: -0.10043829679489136\n",
      "        vf_loss: 0.3659171760082245\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.606553077697754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014115910977125168\n",
      "        model: {}\n",
      "        policy_loss: -0.1493615061044693\n",
      "        total_loss: 0.20867955684661865\n",
      "        vf_explained_var: 0.2206924706697464\n",
      "        vf_loss: 0.33392274379730225\n",
      "  num_agent_steps_sampled: 69060\n",
      "  num_agent_steps_trained: 69060\n",
      "  num_steps_sampled: 34530\n",
      "  num_steps_trained: 34530\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 91\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.062385321100926\n",
      "  ram_util_percent: 38.1091743119266\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10237999999999696\n",
      "  blue_1: -0.18504000000000276\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5778777404467909\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.67775775664235\n",
      "  mean_inference_ms: 2.8724469793233203\n",
      "  mean_raw_obs_processing_ms: 33.256456468382936\n",
      "time_since_restore: 5308.3984375\n",
      "time_this_iter_s: 73.08288073539734\n",
      "time_total_s: 5308.3984375\n",
      "timers:\n",
      "  learn_throughput: 835.997\n",
      "  learn_time_ms: 471.892\n",
      "  load_throughput: 491286.499\n",
      "  load_time_ms: 0.803\n",
      "  sample_throughput: 6.101\n",
      "  sample_time_ms: 64657.389\n",
      "timestamp: 1638988007\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 34530\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:91 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 286 0.005 1.4299999999999915\n",
      "blue_1 True True 286 -2.0 -0.5470000000000084\n",
      "agent_timesteps_total: 69632\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-27-29\n",
      "done: false\n",
      "episode_len_mean: 191.14\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.06858000000001903\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 198\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.445343971252441\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014329740777611732\n",
      "        model: {}\n",
      "        policy_loss: -0.2666586637496948\n",
      "        total_loss: -0.22050760686397552\n",
      "        vf_explained_var: -0.7066609263420105\n",
      "        vf_loss: 0.024387745186686516\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.492609977722168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006680018734186888\n",
      "        model: {}\n",
      "        policy_loss: -0.295901358127594\n",
      "        total_loss: -0.1040797010064125\n",
      "        vf_explained_var: 0.40809106826782227\n",
      "        vf_loss: 0.18040825426578522\n",
      "  num_agent_steps_sampled: 69632\n",
      "  num_agent_steps_trained: 69632\n",
      "  num_steps_sampled: 34816\n",
      "  num_steps_trained: 34816\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 92\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.71551724137932\n",
      "  ram_util_percent: 38.179310344827584\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10927999999999687\n",
      "  blue_1: -0.17786000000000282\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.57781729880526\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.60122391333309\n",
      "  mean_inference_ms: 2.8717895525044987\n",
      "  mean_raw_obs_processing_ms: 33.22518812769391\n",
      "time_since_restore: 5344.453992605209\n",
      "time_this_iter_s: 36.05555510520935\n",
      "time_total_s: 5344.453992605209\n",
      "timers:\n",
      "  learn_throughput: 825.354\n",
      "  learn_time_ms: 439.932\n",
      "  load_throughput: 516324.852\n",
      "  load_time_ms: 0.703\n",
      "  sample_throughput: 5.818\n",
      "  sample_time_ms: 62411.895\n",
      "timestamp: 1638988049\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 34816\n",
      "training_iteration: 92\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:92 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 239 -2.0 -0.8100000000000034\n",
      "blue_1 True True 239 0.005 1.1949999999999965\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 200 0.005 1.0000000000000007\n",
      "blue_1 True True 200 -2.0 -1.0049999999999992\n",
      "agent_timesteps_total: 70510\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-28-36\n",
      "done: false\n",
      "episode_len_mean: 192.85\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.05148000000001937\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 200\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.625246047973633\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016484232619404793\n",
      "        model: {}\n",
      "        policy_loss: -0.032557412981987\n",
      "        total_loss: 0.3729827105998993\n",
      "        vf_explained_var: 0.21470732986927032\n",
      "        vf_loss: 0.38050469756126404\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.50256872177124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007827111519873142\n",
      "        model: {}\n",
      "        policy_loss: -0.33754366636276245\n",
      "        total_loss: -0.1853904128074646\n",
      "        vf_explained_var: -0.4408121705055237\n",
      "        vf_loss: 0.138779878616333\n",
      "  num_agent_steps_sampled: 70510\n",
      "  num_agent_steps_trained: 70510\n",
      "  num_steps_sampled: 35255\n",
      "  num_steps_trained: 35255\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 93\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.32065217391305\n",
      "  ram_util_percent: 38.11304347826088\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11782999999999678\n",
      "  blue_1: -0.16931000000000287\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5777107974221004\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.44729874244709\n",
      "  mean_inference_ms: 2.870453584415427\n",
      "  mean_raw_obs_processing_ms: 33.16043340023784\n",
      "time_since_restore: 5406.148499250412\n",
      "time_this_iter_s: 61.69450664520264\n",
      "time_total_s: 5406.148499250412\n",
      "timers:\n",
      "  learn_throughput: 830.538\n",
      "  learn_time_ms: 456.09\n",
      "  load_throughput: 630827.585\n",
      "  load_time_ms: 0.6\n",
      "  sample_throughput: 6.031\n",
      "  sample_time_ms: 62812.252\n",
      "timestamp: 1638988116\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 35255\n",
      "training_iteration: 93\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:93 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 301 -2.0 -0.50000000000001\n",
      "blue_1 True True 301 0.005 1.50499999999999\n",
      "agent_timesteps_total: 71112\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-29-17\n",
      "done: false\n",
      "episode_len_mean: 193.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.04008000000001965\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 201\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.23192024230957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009346378035843372\n",
      "        model: {}\n",
      "        policy_loss: -0.39614057540893555\n",
      "        total_loss: -0.31304678320884705\n",
      "        vf_explained_var: 0.468841552734375\n",
      "        vf_loss: 0.06889894604682922\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.739221572875977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0132970055565238\n",
      "        model: {}\n",
      "        policy_loss: -0.20069748163223267\n",
      "        total_loss: -0.14582501351833344\n",
      "        vf_explained_var: -0.654714822769165\n",
      "        vf_loss: 0.03215330094099045\n",
      "  num_agent_steps_sampled: 71112\n",
      "  num_agent_steps_trained: 71112\n",
      "  num_steps_sampled: 35556\n",
      "  num_steps_trained: 35556\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 94\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.67192982456141\n",
      "  ram_util_percent: 38.07719298245614\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10347999999999667\n",
      "  blue_1: -0.143560000000003\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.577665553677723\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.36882558248747\n",
      "  mean_inference_ms: 2.869790421527371\n",
      "  mean_raw_obs_processing_ms: 33.12663242898118\n",
      "time_since_restore: 5441.66436624527\n",
      "time_this_iter_s: 35.51586699485779\n",
      "time_total_s: 5441.66436624527\n",
      "timers:\n",
      "  learn_throughput: 828.312\n",
      "  learn_time_ms: 455.625\n",
      "  load_throughput: 628496.121\n",
      "  load_time_ms: 0.6\n",
      "  sample_throughput: 6.159\n",
      "  sample_time_ms: 61277.964\n",
      "timestamp: 1638988157\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 35556\n",
      "training_iteration: 94\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:94 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 218 0.005 1.0899999999999987\n",
      "blue_1 True True 218 -2.0 -0.9150000000000011\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 179 0.005 0.8950000000000007\n",
      "blue_1 True True 179 -2.0 -1.1099999999999994\n",
      "agent_timesteps_total: 71906\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-30-17\n",
      "done: false\n",
      "episode_len_mean: 195.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.026980000000019936\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 203\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.217826843261719\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014542633667588234\n",
      "        model: {}\n",
      "        policy_loss: -0.11479631066322327\n",
      "        total_loss: -0.04903050512075424\n",
      "        vf_explained_var: 0.06013329699635506\n",
      "        vf_loss: 0.043679170310497284\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.066524505615234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011792386882007122\n",
      "        model: {}\n",
      "        policy_loss: -0.1371299922466278\n",
      "        total_loss: 0.22662369906902313\n",
      "        vf_explained_var: 0.6167809963226318\n",
      "        vf_loss: 0.34360530972480774\n",
      "  num_agent_steps_sampled: 71906\n",
      "  num_agent_steps_trained: 71906\n",
      "  num_steps_sampled: 35953\n",
      "  num_steps_trained: 35953\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 95\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.70238095238095\n",
      "  ram_util_percent: 38.06666666666667\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11002999999999669\n",
      "  blue_1: -0.137010000000003\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5775929537227404\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.20722924441178\n",
      "  mean_inference_ms: 2.8685826724751946\n",
      "  mean_raw_obs_processing_ms: 33.05614376842293\n",
      "time_since_restore: 5496.428840398788\n",
      "time_this_iter_s: 54.76447415351868\n",
      "time_total_s: 5496.428840398788\n",
      "timers:\n",
      "  learn_throughput: 809.58\n",
      "  learn_time_ms: 454.927\n",
      "  load_throughput: 613219.866\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 6.114\n",
      "  sample_time_ms: 60236.367\n",
      "timestamp: 1638988217\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 35953\n",
      "training_iteration: 95\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:95 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 268 -2.0 -0.6650000000000065\n",
      "blue_1 True True 268 0.005 1.3659999999999934\n",
      "agent_timesteps_total: 72442\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-30-57\n",
      "done: false\n",
      "episode_len_mean: 195.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.02632000000001988\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 204\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.84735107421875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012420710176229477\n",
      "        model: {}\n",
      "        policy_loss: -0.19768603146076202\n",
      "        total_loss: 0.10870219767093658\n",
      "        vf_explained_var: 0.028598492965102196\n",
      "        vf_loss: 0.2875242829322815\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.770040512084961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013706672005355358\n",
      "        model: {}\n",
      "        policy_loss: -0.127163365483284\n",
      "        total_loss: 0.22674015164375305\n",
      "        vf_explained_var: -0.875876247882843\n",
      "        vf_loss: 0.33048442006111145\n",
      "  num_agent_steps_sampled: 72442\n",
      "  num_agent_steps_trained: 72442\n",
      "  num_steps_sampled: 36221\n",
      "  num_steps_trained: 36221\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 96\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.99629629629629\n",
      "  ram_util_percent: 38.072222222222216\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.09017999999999673\n",
      "  blue_1: -0.11650000000000305\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5775488523535839\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.13135349114972\n",
      "  mean_inference_ms: 2.8679870973655657\n",
      "  mean_raw_obs_processing_ms: 33.02232142751349\n",
      "time_since_restore: 5530.357109308243\n",
      "time_this_iter_s: 33.928268909454346\n",
      "time_total_s: 5530.357109308243\n",
      "timers:\n",
      "  learn_throughput: 807.994\n",
      "  learn_time_ms: 454.83\n",
      "  load_throughput: 733409.488\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 6.197\n",
      "  sample_time_ms: 59306.569\n",
      "timestamp: 1638988257\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 36221\n",
      "training_iteration: 96\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:96 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 260 -2.0 -0.7050000000000056\n",
      "blue_1 True True 260 0.005 1.2999999999999943\n",
      "agent_timesteps_total: 72962\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-31-38\n",
      "done: false\n",
      "episode_len_mean: 196.31\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.016620000000020122\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 205\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.697426795959473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013994663953781128\n",
      "        model: {}\n",
      "        policy_loss: -0.13098792731761932\n",
      "        total_loss: 0.03429168835282326\n",
      "        vf_explained_var: 0.15751788020133972\n",
      "        vf_loss: 0.14402523636817932\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.969616889953613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016144733875989914\n",
      "        model: {}\n",
      "        policy_loss: -0.11137361824512482\n",
      "        total_loss: -0.07346649467945099\n",
      "        vf_explained_var: -0.49618643522262573\n",
      "        vf_loss: 0.010322334244847298\n",
      "  num_agent_steps_sampled: 72962\n",
      "  num_agent_steps_trained: 72962\n",
      "  num_steps_sampled: 36481\n",
      "  num_steps_trained: 36481\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 97\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.005263157894746\n",
      "  ram_util_percent: 38.07017543859649\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.07497999999999666\n",
      "  blue_1: -0.0916000000000031\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5775080118158699\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 119.0576662724676\n",
      "  mean_inference_ms: 2.8673947033802314\n",
      "  mean_raw_obs_processing_ms: 32.987373999266985\n",
      "time_since_restore: 5565.846607685089\n",
      "time_this_iter_s: 35.48949837684631\n",
      "time_total_s: 5565.846607685089\n",
      "timers:\n",
      "  learn_throughput: 799.45\n",
      "  learn_time_ms: 452.436\n",
      "  load_throughput: 900504.397\n",
      "  load_time_ms: 0.402\n",
      "  sample_throughput: 6.214\n",
      "  sample_time_ms: 58204.464\n",
      "timestamp: 1638988298\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 36481\n",
      "training_iteration: 97\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:97 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 151 0.005 0.7550000000000006\n",
      "blue_1 True True 151 -2.001 -1.2589999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 239 0.005 1.1949999999999965\n",
      "blue_1 True True 239 -2.0 -0.8100000000000034\n",
      "agent_timesteps_total: 73742\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-32-33\n",
      "done: false\n",
      "episode_len_mean: 197.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: -0.0004100000000204318\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 207\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.140642166137695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011683751828968525\n",
      "        model: {}\n",
      "        policy_loss: -0.08858008682727814\n",
      "        total_loss: 0.014425625093281269\n",
      "        vf_explained_var: -0.2879975736141205\n",
      "        vf_loss: 0.08526100218296051\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.788501739501953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01407159585505724\n",
      "        model: {}\n",
      "        policy_loss: -0.1166442334651947\n",
      "        total_loss: 0.3586218059062958\n",
      "        vf_explained_var: 0.3999638557434082\n",
      "        vf_loss: 0.4512234032154083\n",
      "  num_agent_steps_sampled: 73742\n",
      "  num_agent_steps_trained: 73742\n",
      "  num_steps_sampled: 36871\n",
      "  num_steps_trained: 36871\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 98\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.60263157894737\n",
      "  ram_util_percent: 38.04342105263158\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10317999999999662\n",
      "  blue_1: -0.10359000000000312\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5774229807552469\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.90773737500929\n",
      "  mean_inference_ms: 2.8662431383794367\n",
      "  mean_raw_obs_processing_ms: 32.91573464195952\n",
      "time_since_restore: 5615.3086659908295\n",
      "time_this_iter_s: 49.462058305740356\n",
      "time_total_s: 5615.3086659908295\n",
      "timers:\n",
      "  learn_throughput: 784.885\n",
      "  learn_time_ms: 468.094\n",
      "  load_throughput: 914749.667\n",
      "  load_time_ms: 0.402\n",
      "  sample_throughput: 6.316\n",
      "  sample_time_ms: 58172.154\n",
      "timestamp: 1638988353\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 36871\n",
      "training_iteration: 98\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:98 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 513 -0.995 1.5649999999999675\n",
      "blue_1 False True 513 -0.973 1.5619999999999878\n",
      "agent_timesteps_total: 74768\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-33-40\n",
      "done: false\n",
      "episode_len_mean: 201.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.035409999999978604\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 208\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.380727767944336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009608662687242031\n",
      "        model: {}\n",
      "        policy_loss: -0.06567011773586273\n",
      "        total_loss: 0.09485365450382233\n",
      "        vf_explained_var: -0.7648394703865051\n",
      "        vf_loss: 0.14593060314655304\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.56593132019043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008332755416631699\n",
      "        model: {}\n",
      "        policy_loss: -0.05704249441623688\n",
      "        total_loss: 0.23300950229167938\n",
      "        vf_explained_var: -0.44940808415412903\n",
      "        vf_loss: 0.2758147120475769\n",
      "  num_agent_steps_sampled: 74768\n",
      "  num_agent_steps_trained: 74768\n",
      "  num_steps_sampled: 37384\n",
      "  num_steps_trained: 37384\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 99\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.91505376344087\n",
      "  ram_util_percent: 38.02150537634409\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1311299999999963\n",
      "  blue_1: -0.09572000000000326\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.67\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.577376531219392\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.83227874191351\n",
      "  mean_inference_ms: 2.8656593237494428\n",
      "  mean_raw_obs_processing_ms: 32.876559041731625\n",
      "time_since_restore: 5676.790407180786\n",
      "time_this_iter_s: 61.481741189956665\n",
      "time_total_s: 5676.790407180786\n",
      "timers:\n",
      "  learn_throughput: 766.951\n",
      "  learn_time_ms: 501.206\n",
      "  load_throughput: 766735.047\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 6.568\n",
      "  sample_time_ms: 58522.649\n",
      "timestamp: 1638988420\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 37384\n",
      "training_iteration: 99\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:99 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 38 0.005 0.19000000000000009\n",
      "blue_1 True True 38 -2.0 -1.815\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 156 -2.0 -1.2249999999999994\n",
      "blue_1 True True 156 0.005 0.7800000000000006\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 131 -2.0 -1.3499999999999996\n",
      "blue_1 True True 131 0.005 0.6550000000000005\n",
      "agent_timesteps_total: 75418\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-34-42\n",
      "done: false\n",
      "episode_len_mean: 200.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.02460999999997874\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 211\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.284487247467041\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014120926149189472\n",
      "        model: {}\n",
      "        policy_loss: -0.3730287253856659\n",
      "        total_loss: 0.08402205258607864\n",
      "        vf_explained_var: 0.20037029683589935\n",
      "        vf_loss: 0.4356045424938202\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.51062536239624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009670373983681202\n",
      "        model: {}\n",
      "        policy_loss: 0.019168352708220482\n",
      "        total_loss: 0.5227940678596497\n",
      "        vf_explained_var: 0.2517549693584442\n",
      "        vf_loss: 0.4871029555797577\n",
      "  num_agent_steps_sampled: 75418\n",
      "  num_agent_steps_trained: 75418\n",
      "  num_steps_sampled: 37709\n",
      "  num_steps_trained: 37709\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 100\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.597674418604655\n",
      "  ram_util_percent: 38.04418604651163\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10567999999999628\n",
      "  blue_1: -0.08107000000000322\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5772302654728156\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.61024329291486\n",
      "  mean_inference_ms: 2.863940280782482\n",
      "  mean_raw_obs_processing_ms: 32.76188255656985\n",
      "time_since_restore: 5733.331062078476\n",
      "time_this_iter_s: 56.54065489768982\n",
      "time_total_s: 5733.331062078476\n",
      "timers:\n",
      "  learn_throughput: 763.438\n",
      "  learn_time_ms: 468.407\n",
      "  load_throughput: 896576.669\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.454\n",
      "  sample_time_ms: 55405.66\n",
      "timestamp: 1638988482\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 37709\n",
      "training_iteration: 100\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:100 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 95 0.005 0.4750000000000003\n",
      "blue_1 True True 95 -2.0 -1.5299999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 189 -2.0 -1.0599999999999992\n",
      "blue_1 True True 189 0.005 0.9450000000000007\n",
      "agent_timesteps_total: 75986\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-35-32\n",
      "done: false\n",
      "episode_len_mean: 199.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.014609999999978958\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 213\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.357046127319336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010001135990023613\n",
      "        model: {}\n",
      "        policy_loss: -0.2833748161792755\n",
      "        total_loss: -0.0508008636534214\n",
      "        vf_explained_var: -0.3788657486438751\n",
      "        vf_loss: 0.21738477051258087\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.666420936584473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015130040235817432\n",
      "        model: {}\n",
      "        policy_loss: -0.04498545452952385\n",
      "        total_loss: 0.5353516936302185\n",
      "        vf_explained_var: 0.21027155220508575\n",
      "        vf_loss: 0.5544860363006592\n",
      "  num_agent_steps_sampled: 75986\n",
      "  num_agent_steps_trained: 75986\n",
      "  num_steps_sampled: 37993\n",
      "  num_steps_trained: 37993\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 101\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.65217391304347\n",
      "  ram_util_percent: 38.04057971014493\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.12072999999999627\n",
      "  blue_1: -0.1061200000000032\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5771454089589103\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.46743544635352\n",
      "  mean_inference_ms: 2.86284557667006\n",
      "  mean_raw_obs_processing_ms: 32.68844871551374\n",
      "time_since_restore: 5777.35977101326\n",
      "time_this_iter_s: 44.028708934783936\n",
      "time_total_s: 5777.35977101326\n",
      "timers:\n",
      "  learn_throughput: 772.717\n",
      "  learn_time_ms: 448.159\n",
      "  load_throughput: 868608.704\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.598\n",
      "  sample_time_ms: 52484.977\n",
      "timestamp: 1638988532\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 37993\n",
      "training_iteration: 101\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:100 starting ! -----------------\n",
      "agent_timesteps_total: 75986\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-35-32\n",
      "done: false\n",
      "episode_len_mean: 199.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.014609999999978958\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 213\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.357046127319336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010001135990023613\n",
      "        model: {}\n",
      "        policy_loss: -0.2833748161792755\n",
      "        total_loss: -0.0508008636534214\n",
      "        vf_explained_var: -0.3788657486438751\n",
      "        vf_loss: 0.21738477051258087\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.666420936584473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015130040235817432\n",
      "        model: {}\n",
      "        policy_loss: -0.04498545452952385\n",
      "        total_loss: 0.5353516936302185\n",
      "        vf_explained_var: 0.21027155220508575\n",
      "        vf_loss: 0.5544860363006592\n",
      "  num_agent_steps_sampled: 75986\n",
      "  num_agent_steps_trained: 75986\n",
      "  num_steps_sampled: 37993\n",
      "  num_steps_trained: 37993\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 101\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.65217391304347\n",
      "  ram_util_percent: 38.04057971014493\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.12072999999999627\n",
      "  blue_1: -0.1061200000000032\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5771454089589103\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.46743544635352\n",
      "  mean_inference_ms: 2.86284557667006\n",
      "  mean_raw_obs_processing_ms: 32.68844871551374\n",
      "time_since_restore: 5777.35977101326\n",
      "time_this_iter_s: 44.028708934783936\n",
      "time_total_s: 5777.35977101326\n",
      "timers:\n",
      "  learn_throughput: 772.717\n",
      "  learn_time_ms: 448.159\n",
      "  load_throughput: 868608.704\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.598\n",
      "  sample_time_ms: 52484.977\n",
      "timestamp: 1638988532\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 37993\n",
      "training_iteration: 101\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:101 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.0159999999999596\n",
      "agent_timesteps_total: 77186\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-36-40\n",
      "done: false\n",
      "episode_len_mean: 204.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.06281999999997791\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 214\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.026267051696777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007029486820101738\n",
      "        model: {}\n",
      "        policy_loss: -0.36548006534576416\n",
      "        total_loss: -0.2664636969566345\n",
      "        vf_explained_var: -0.9102010726928711\n",
      "        vf_loss: 0.0883403792977333\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.340615749359131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0049684722907841206\n",
      "        model: {}\n",
      "        policy_loss: -0.3839579224586487\n",
      "        total_loss: -0.28094184398651123\n",
      "        vf_explained_var: -0.7951334714889526\n",
      "        vf_loss: 0.09452700614929199\n",
      "  num_agent_steps_sampled: 77186\n",
      "  num_agent_steps_trained: 77186\n",
      "  num_steps_sampled: 38593\n",
      "  num_steps_trained: 38593\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 102\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.007446808510636\n",
      "  ram_util_percent: 37.998936170212765\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.254999999999974\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.13472999999999588\n",
      "  blue_1: -0.07191000000000364\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.577121621132299\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.38891012815799\n",
      "  mean_inference_ms: 2.862305338184466\n",
      "  mean_raw_obs_processing_ms: 32.64523003008327\n",
      "time_since_restore: 5840.062585353851\n",
      "time_this_iter_s: 62.70281434059143\n",
      "time_total_s: 5840.062585353851\n",
      "timers:\n",
      "  learn_throughput: 787.987\n",
      "  learn_time_ms: 479.323\n",
      "  load_throughput: 757767.445\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 6.855\n",
      "  sample_time_ms: 55100.058\n",
      "timestamp: 1638988600\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 38593\n",
      "training_iteration: 102\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:102 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 204 -2.0 -0.9849999999999997\n",
      "blue_1 True True 204 0.006 1.0460000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 588 0.005 2.9399999999999595\n",
      "blue_1 True True 588 -2.0 1.2749999999999608\n",
      "agent_timesteps_total: 78770\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-38-23\n",
      "done: false\n",
      "episode_len_mean: 210.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.1266799999999767\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 216\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.11861801147461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009679224342107773\n",
      "        model: {}\n",
      "        policy_loss: -0.05011831969022751\n",
      "        total_loss: 0.255251407623291\n",
      "        vf_explained_var: -0.33486267924308777\n",
      "        vf_loss: 0.29066941142082214\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.318134307861328\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010310819372534752\n",
      "        model: {}\n",
      "        policy_loss: -0.15167751908302307\n",
      "        total_loss: -0.03800733759999275\n",
      "        vf_explained_var: -0.27019572257995605\n",
      "        vf_loss: 0.10486168414354324\n",
      "  num_agent_steps_sampled: 78770\n",
      "  num_agent_steps_trained: 78770\n",
      "  num_steps_sampled: 39385\n",
      "  num_steps_trained: 39385\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 103\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.96267605633802\n",
      "  ram_util_percent: 37.99366197183099\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.16482999999999545\n",
      "  blue_1: -0.038150000000004035\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770785730371272\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.22726157556261\n",
      "  mean_inference_ms: 2.861200072813131\n",
      "  mean_raw_obs_processing_ms: 32.55213387704529\n",
      "time_since_restore: 5937.660273313522\n",
      "time_this_iter_s: 97.59768795967102\n",
      "time_total_s: 5937.660273313522\n",
      "timers:\n",
      "  learn_throughput: 786.367\n",
      "  learn_time_ms: 525.2\n",
      "  load_throughput: 690468.571\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.039\n",
      "  sample_time_ms: 58674.23\n",
      "timestamp: 1638988703\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 39385\n",
      "training_iteration: 103\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:103 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 61 0.005 0.30500000000000016\n",
      "blue_1 True True 61 -2.0 -1.6999999999999997\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 134 0.005 0.6700000000000005\n",
      "blue_1 True True 134 -2.0 -1.3349999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 204 -2.0 -0.9849999999999997\n",
      "blue_1 True True 204 0.004 1.0040000000000007\n",
      "agent_timesteps_total: 79568\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-39-41\n",
      "done: false\n",
      "episode_len_mean: 205.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.07652999999997782\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 219\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.500276565551758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010232334025204182\n",
      "        model: {}\n",
      "        policy_loss: -0.16778361797332764\n",
      "        total_loss: 0.0362204909324646\n",
      "        vf_explained_var: -0.2614923119544983\n",
      "        vf_loss: 0.18846377730369568\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.815847396850586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01731807179749012\n",
      "        model: {}\n",
      "        policy_loss: -0.0579010434448719\n",
      "        total_loss: 0.6564162969589233\n",
      "        vf_explained_var: 0.18866035342216492\n",
      "        vf_loss: 0.6995225548744202\n",
      "  num_agent_steps_sampled: 79568\n",
      "  num_agent_steps_trained: 79568\n",
      "  num_steps_sampled: 39784\n",
      "  num_steps_trained: 39784\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 104\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.736448598130835\n",
      "  ram_util_percent: 37.8803738317757\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.14037999999999579\n",
      "  blue_1: -0.06385000000000382\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770243794854469\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 118.00971389706015\n",
      "  mean_inference_ms: 2.859598515552097\n",
      "  mean_raw_obs_processing_ms: 32.43433112363239\n",
      "time_since_restore: 6009.581490516663\n",
      "time_this_iter_s: 71.92121720314026\n",
      "time_total_s: 6009.581490516663\n",
      "timers:\n",
      "  learn_throughput: 780.142\n",
      "  learn_time_ms: 541.952\n",
      "  load_throughput: 605818.438\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.782\n",
      "  sample_time_ms: 62344.977\n",
      "timestamp: 1638988781\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 39784\n",
      "training_iteration: 104\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:104 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 236 -2.0 -0.8250000000000031\n",
      "blue_1 True True 236 0.005 1.1799999999999968\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 318 -2.0 -0.4150000000000118\n",
      "blue_1 True True 318 0.005 1.6329999999999885\n",
      "agent_timesteps_total: 80676\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-40-58\n",
      "done: false\n",
      "episode_len_mean: 209.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.12005999999997709\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 221\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.31899356842041\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011778314597904682\n",
      "        model: {}\n",
      "        policy_loss: -0.20463059842586517\n",
      "        total_loss: 0.04655207321047783\n",
      "        vf_explained_var: 0.02031019888818264\n",
      "        vf_loss: 0.23329433798789978\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.01546573638916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019281882792711258\n",
      "        model: {}\n",
      "        policy_loss: -0.18393217027187347\n",
      "        total_loss: -0.07632508128881454\n",
      "        vf_explained_var: -0.15402308106422424\n",
      "        vf_loss: 0.09113464504480362\n",
      "  num_agent_steps_sampled: 80676\n",
      "  num_agent_steps_trained: 80676\n",
      "  num_steps_sampled: 40338\n",
      "  num_steps_trained: 40338\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 105\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.26355140186916\n",
      "  ram_util_percent: 37.886915887850456\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1418799999999956\n",
      "  blue_1: -0.021820000000003982\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770209790908747\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.86031306571807\n",
      "  mean_inference_ms: 2.858590169487006\n",
      "  mean_raw_obs_processing_ms: 32.34856239335452\n",
      "time_since_restore: 6081.5947852134705\n",
      "time_this_iter_s: 72.01329469680786\n",
      "time_total_s: 6081.5947852134705\n",
      "timers:\n",
      "  learn_throughput: 786.954\n",
      "  learn_time_ms: 557.211\n",
      "  load_throughput: 628250.147\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.844\n",
      "  sample_time_ms: 64067.537\n",
      "timestamp: 1638988858\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 40338\n",
      "training_iteration: 105\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:105 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 180 0.005 0.9000000000000007\n",
      "blue_1 True True 180 -2.0 -1.1049999999999993\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 268 -2.0 -0.6650000000000065\n",
      "blue_1 True True 268 0.006 1.349999999999995\n",
      "agent_timesteps_total: 81572\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-42-02\n",
      "done: false\n",
      "episode_len_mean: 210.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.12405999999997702\n",
      "episode_reward_min: -1.665\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 223\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.057602882385254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005946049466729164\n",
      "        model: {}\n",
      "        policy_loss: -0.38923677802085876\n",
      "        total_loss: -0.35728156566619873\n",
      "        vf_explained_var: 0.04762556403875351\n",
      "        vf_loss: 0.022924624383449554\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.237624168395996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020649120211601257\n",
      "        model: {}\n",
      "        policy_loss: -0.0015663531376048923\n",
      "        total_loss: 0.5173285007476807\n",
      "        vf_explained_var: -0.30500364303588867\n",
      "        vf_loss: 0.5012544393539429\n",
      "  num_agent_steps_sampled: 81572\n",
      "  num_agent_steps_trained: 81572\n",
      "  num_steps_sampled: 40786\n",
      "  num_steps_trained: 40786\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 106\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.39886363636364\n",
      "  ram_util_percent: 37.82386363636364\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.14382999999999557\n",
      "  blue_1: -0.019770000000003975\n",
      "policy_reward_min:\n",
      "  blue_0: -1.835\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770202995457026\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.70865609826011\n",
      "  mean_inference_ms: 2.85759333437128\n",
      "  mean_raw_obs_processing_ms: 32.26102235516448\n",
      "time_since_restore: 6139.693826913834\n",
      "time_this_iter_s: 58.09904170036316\n",
      "time_total_s: 6139.693826913834\n",
      "timers:\n",
      "  learn_throughput: 796.731\n",
      "  learn_time_ms: 572.967\n",
      "  load_throughput: 654039.206\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.866\n",
      "  sample_time_ms: 66484.339\n",
      "timestamp: 1638988922\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 40786\n",
      "training_iteration: 106\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:106 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 399 0.005 1.9949999999999795\n",
      "blue_1 True True 399 -1.998 -0.0680000000000105\n",
      "agent_timesteps_total: 82370\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-42-53\n",
      "done: false\n",
      "episode_len_mean: 213.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.15997999999997653\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 224\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.646687507629395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013761190697550774\n",
      "        model: {}\n",
      "        policy_loss: -0.13828615844249725\n",
      "        total_loss: -0.07322180271148682\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.04416455700993538\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.241066932678223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01160828024148941\n",
      "        model: {}\n",
      "        policy_loss: -0.16744421422481537\n",
      "        total_loss: 0.13398462533950806\n",
      "        vf_explained_var: 0.056428633630275726\n",
      "        vf_loss: 0.2865534722805023\n",
      "  num_agent_steps_sampled: 82370\n",
      "  num_agent_steps_trained: 82370\n",
      "  num_steps_sampled: 41185\n",
      "  num_steps_trained: 41185\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 107\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.95857142857143\n",
      "  ram_util_percent: 37.80000000000001\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.18212999999999538\n",
      "  blue_1: -0.022150000000004083\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770219216453512\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.63137051182794\n",
      "  mean_inference_ms: 2.8570996807924605\n",
      "  mean_raw_obs_processing_ms: 32.21540552738461\n",
      "time_since_restore: 6185.000972986221\n",
      "time_this_iter_s: 45.307146072387695\n",
      "time_total_s: 6185.000972986221\n",
      "timers:\n",
      "  learn_throughput: 792.967\n",
      "  learn_time_ms: 593.215\n",
      "  load_throughput: 589659.474\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 6.973\n",
      "  sample_time_ms: 67457.186\n",
      "timestamp: 1638988973\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 41185\n",
      "training_iteration: 107\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:107 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 596 0.005 2.9799999999999587\n",
      "blue_1 True True 596 -1.998 1.0089999999999584\n",
      "agent_timesteps_total: 83562\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-44-07\n",
      "done: false\n",
      "episode_len_mean: 217.85\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.20161999999997562\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 225\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.808180809020996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011492257006466389\n",
      "        model: {}\n",
      "        policy_loss: -0.3537748157978058\n",
      "        total_loss: -0.3120836317539215\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.024237295612692833\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.686037063598633\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0060051982291042805\n",
      "        model: {}\n",
      "        policy_loss: -0.39332351088523865\n",
      "        total_loss: -0.3247744143009186\n",
      "        vf_explained_var: -0.280398428440094\n",
      "        vf_loss: 0.06085379421710968\n",
      "  num_agent_steps_sampled: 83562\n",
      "  num_agent_steps_trained: 83562\n",
      "  num_steps_sampled: 41781\n",
      "  num_steps_trained: 41781\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 108\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.149019607843144\n",
      "  ram_util_percent: 37.85490196078432\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.22282999999999492\n",
      "  blue_1: -0.0212100000000045\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770264755797332\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.561229891706\n",
      "  mean_inference_ms: 2.856639125183156\n",
      "  mean_raw_obs_processing_ms: 32.174616522711894\n",
      "time_since_restore: 6253.778558731079\n",
      "time_this_iter_s: 68.77758574485779\n",
      "time_total_s: 6253.778558731079\n",
      "timers:\n",
      "  learn_throughput: 798.56\n",
      "  learn_time_ms: 614.857\n",
      "  load_throughput: 615445.36\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 7.076\n",
      "  sample_time_ms: 69393.27\n",
      "timestamp: 1638989047\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 41781\n",
      "training_iteration: 108\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:108 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 141 -2.0 -1.2999999999999994\n",
      "blue_1 True True 141 0.005 0.7050000000000005\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 485 -2.0 0.4199999999999706\n",
      "blue_1 True True 485 0.004 2.353999999999982\n",
      "agent_timesteps_total: 84814\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-45-33\n",
      "done: false\n",
      "episode_len_mean: 215.43\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.17519999999997576\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 227\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.820398330688477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011590019799768925\n",
      "        model: {}\n",
      "        policy_loss: -0.28907522559165955\n",
      "        total_loss: 0.13389995694160461\n",
      "        vf_explained_var: -0.13222314417362213\n",
      "        vf_loss: 0.4053727984428406\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.871752738952637\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012717287056148052\n",
      "        model: {}\n",
      "        policy_loss: -0.09959494322538376\n",
      "        total_loss: -0.07019419968128204\n",
      "        vf_explained_var: -0.5049363374710083\n",
      "        vf_loss: 0.013104238547384739\n",
      "  num_agent_steps_sampled: 84814\n",
      "  num_agent_steps_trained: 84814\n",
      "  num_steps_sampled: 42407\n",
      "  num_steps_trained: 42407\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 109\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.285714285714285\n",
      "  ram_util_percent: 37.86638655462182\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1992199999999951\n",
      "  blue_1: -0.024020000000004198\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770347493161119\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.42238027506077\n",
      "  mean_inference_ms: 2.855708640950311\n",
      "  mean_raw_obs_processing_ms: 32.09115748463507\n",
      "time_since_restore: 6334.1271159648895\n",
      "time_this_iter_s: 80.34855723381042\n",
      "time_total_s: 6334.1271159648895\n",
      "timers:\n",
      "  learn_throughput: 817.87\n",
      "  learn_time_ms: 614.156\n",
      "  load_throughput: 719535.143\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 7.044\n",
      "  sample_time_ms: 71308.481\n",
      "timestamp: 1638989133\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 42407\n",
      "training_iteration: 109\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:109 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 425 -2.0 0.11999999999997701\n",
      "blue_1 True True 425 0.005 2.156999999999977\n",
      "agent_timesteps_total: 85664\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-46-31\n",
      "done: false\n",
      "episode_len_mean: 216.31\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.18402999999997546\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 228\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.178214073181152\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007635403424501419\n",
      "        model: {}\n",
      "        policy_loss: -0.289538711309433\n",
      "        total_loss: -0.11257899552583694\n",
      "        vf_explained_var: -0.3750910460948944\n",
      "        vf_loss: 0.16536344587802887\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.402503967285156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013021980412304401\n",
      "        model: {}\n",
      "        policy_loss: -0.21765021979808807\n",
      "        total_loss: -0.18168902397155762\n",
      "        vf_explained_var: -0.8282753229141235\n",
      "        vf_loss: 0.019274216145277023\n",
      "  num_agent_steps_sampled: 85664\n",
      "  num_agent_steps_trained: 85664\n",
      "  num_steps_sampled: 42832\n",
      "  num_steps_trained: 42832\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 110\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.89113924050631\n",
      "  ram_util_percent: 37.89620253164558\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.18327999999999495\n",
      "  blue_1: 0.0007499999999957163\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770314636291357\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.35920745203418\n",
      "  mean_inference_ms: 2.8552564634598503\n",
      "  mean_raw_obs_processing_ms: 32.05138007558332\n",
      "time_since_restore: 6385.981628894806\n",
      "time_this_iter_s: 51.85451292991638\n",
      "time_total_s: 6385.981628894806\n",
      "timers:\n",
      "  learn_throughput: 810.615\n",
      "  learn_time_ms: 631.99\n",
      "  load_throughput: 642163.097\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 7.234\n",
      "  sample_time_ms: 70819.928\n",
      "timestamp: 1638989191\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 42832\n",
      "training_iteration: 110\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:110 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 142 0.005 0.7100000000000005\n",
      "blue_1 True True 142 -2.0 -1.2949999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 238 0.005 1.1899999999999966\n",
      "blue_1 True True 238 -1.999 -0.7580000000000029\n",
      "agent_timesteps_total: 86424\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-47-28\n",
      "done: false\n",
      "episode_len_mean: 217.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.19709999999997532\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 230\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.34562873840332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017078246921300888\n",
      "        model: {}\n",
      "        policy_loss: -0.21720115840435028\n",
      "        total_loss: -0.17250189185142517\n",
      "        vf_explained_var: 0.3915834128856659\n",
      "        vf_loss: 0.018761685118079185\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.9598517417907715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01905873976647854\n",
      "        model: {}\n",
      "        policy_loss: -0.3580922782421112\n",
      "        total_loss: 0.25669431686401367\n",
      "        vf_explained_var: 0.1035265401005745\n",
      "        vf_loss: 0.5903639197349548\n",
      "  num_agent_steps_sampled: 86424\n",
      "  num_agent_steps_trained: 86424\n",
      "  num_steps_sampled: 43212\n",
      "  num_steps_trained: 43212\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 111\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.72875\n",
      "  ram_util_percent: 37.89125\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.20957999999999496\n",
      "  blue_1: -0.01248000000000433\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770365101194994\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.22679667234392\n",
      "  mean_inference_ms: 2.8543879855631995\n",
      "  mean_raw_obs_processing_ms: 31.96822895112447\n",
      "time_since_restore: 6438.377375364304\n",
      "time_this_iter_s: 52.39574646949768\n",
      "time_total_s: 6438.377375364304\n",
      "timers:\n",
      "  learn_throughput: 823.466\n",
      "  learn_time_ms: 633.785\n",
      "  load_throughput: 653844.875\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 7.282\n",
      "  sample_time_ms: 71672.094\n",
      "timestamp: 1638989248\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 43212\n",
      "training_iteration: 111\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:110 starting ! -----------------\n",
      "agent_timesteps_total: 86424\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-47-28\n",
      "done: false\n",
      "episode_len_mean: 217.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.19709999999997532\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 230\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.34562873840332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017078246921300888\n",
      "        model: {}\n",
      "        policy_loss: -0.21720115840435028\n",
      "        total_loss: -0.17250189185142517\n",
      "        vf_explained_var: 0.3915834128856659\n",
      "        vf_loss: 0.018761685118079185\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.9598517417907715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01905873976647854\n",
      "        model: {}\n",
      "        policy_loss: -0.3580922782421112\n",
      "        total_loss: 0.25669431686401367\n",
      "        vf_explained_var: 0.1035265401005745\n",
      "        vf_loss: 0.5903639197349548\n",
      "  num_agent_steps_sampled: 86424\n",
      "  num_agent_steps_trained: 86424\n",
      "  num_steps_sampled: 43212\n",
      "  num_steps_trained: 43212\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 111\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 46.72875\n",
      "  ram_util_percent: 37.89125\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.20957999999999496\n",
      "  blue_1: -0.01248000000000433\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770365101194994\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.22679667234392\n",
      "  mean_inference_ms: 2.8543879855631995\n",
      "  mean_raw_obs_processing_ms: 31.96822895112447\n",
      "time_since_restore: 6438.377375364304\n",
      "time_this_iter_s: 52.39574646949768\n",
      "time_total_s: 6438.377375364304\n",
      "timers:\n",
      "  learn_throughput: 823.466\n",
      "  learn_time_ms: 633.785\n",
      "  load_throughput: 653844.875\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 7.282\n",
      "  sample_time_ms: 71672.094\n",
      "timestamp: 1638989248\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 43212\n",
      "training_iteration: 111\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:111 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False True 417 -0.995 1.0849999999999778\n",
      "blue_1 False True 417 -0.976 1.7139999999999924\n",
      "agent_timesteps_total: 87258\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-48-24\n",
      "done: false\n",
      "episode_len_mean: 220.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.23283999999997484\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 231\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.103047370910645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0111727649345994\n",
      "        model: {}\n",
      "        policy_loss: -0.2657844126224518\n",
      "        total_loss: -0.1771230548620224\n",
      "        vf_explained_var: -0.4140321612358093\n",
      "        vf_loss: 0.07169270515441895\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.903227806091309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008647088892757893\n",
      "        model: {}\n",
      "        policy_loss: -0.28474223613739014\n",
      "        total_loss: -0.14045217633247375\n",
      "        vf_explained_var: -0.4961656332015991\n",
      "        vf_loss: 0.13320934772491455\n",
      "  num_agent_steps_sampled: 87258\n",
      "  num_agent_steps_trained: 87258\n",
      "  num_steps_sampled: 43629\n",
      "  num_steps_trained: 43629\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 112\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.6987012987013\n",
      "  ram_util_percent: 37.911688311688316\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 2.5909999999999944\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.23432999999999474\n",
      "  blue_1: -0.0014900000000044034\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770379508374379\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.15929509687781\n",
      "  mean_inference_ms: 2.853945842158337\n",
      "  mean_raw_obs_processing_ms: 31.925103485471126\n",
      "time_since_restore: 6488.045031547546\n",
      "time_this_iter_s: 49.6676561832428\n",
      "time_total_s: 6488.045031547546\n",
      "timers:\n",
      "  learn_throughput: 813.942\n",
      "  learn_time_ms: 618.717\n",
      "  load_throughput: 628421.842\n",
      "  load_time_ms: 0.801\n",
      "  sample_throughput: 7.155\n",
      "  sample_time_ms: 70381.432\n",
      "timestamp: 1638989304\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 43629\n",
      "training_iteration: 112\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:112 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 573 -2.0 0.8599999999999612\n",
      "blue_1 True True 573 0.004 3.1949999999999963\n",
      "agent_timesteps_total: 88404\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-49-37\n",
      "done: false\n",
      "episode_len_mean: 224.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.27413999999997357\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 232\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.748944282531738\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009729232639074326\n",
      "        model: {}\n",
      "        policy_loss: -0.32386481761932373\n",
      "        total_loss: -0.20460724830627441\n",
      "        vf_explained_var: -0.6291359663009644\n",
      "        vf_loss: 0.10448126494884491\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.532697677612305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00971171259880066\n",
      "        model: {}\n",
      "        policy_loss: -0.06676150858402252\n",
      "        total_loss: -0.04725959151983261\n",
      "        vf_explained_var: -0.6930234432220459\n",
      "        vf_loss: 0.007056890055537224\n",
      "  num_agent_steps_sampled: 88404\n",
      "  num_agent_steps_trained: 88404\n",
      "  num_steps_sampled: 44202\n",
      "  num_steps_trained: 44202\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 113\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.05\n",
      "  ram_util_percent: 37.904999999999994\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.23327999999999438\n",
      "  blue_1: 0.040859999999995573\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770383525521818\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 117.09190500167396\n",
      "  mean_inference_ms: 2.853510285302788\n",
      "  mean_raw_obs_processing_ms: 31.879578193713737\n",
      "time_since_restore: 6555.349494457245\n",
      "time_this_iter_s: 67.30446290969849\n",
      "time_total_s: 6555.349494457245\n",
      "timers:\n",
      "  learn_throughput: 811.507\n",
      "  learn_time_ms: 593.587\n",
      "  load_throughput: 536198.577\n",
      "  load_time_ms: 0.898\n",
      "  sample_throughput: 7.151\n",
      "  sample_time_ms: 67360.443\n",
      "timestamp: 1638989377\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 44202\n",
      "training_iteration: 113\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:113 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 225 -2.0 -0.8800000000000019\n",
      "blue_1 True True 225 0.005 1.124999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 195 -2.0 -1.0299999999999994\n",
      "blue_1 True True 195 0.007 1.0070000000000006\n",
      "agent_timesteps_total: 89244\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-50-44\n",
      "done: false\n",
      "episode_len_mean: 225.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.2866599999999734\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 234\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.347596168518066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015476509928703308\n",
      "        model: {}\n",
      "        policy_loss: -0.24006612598896027\n",
      "        total_loss: 0.07142341881990433\n",
      "        vf_explained_var: 0.4215855002403259\n",
      "        vf_loss: 0.28798457980155945\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.024826049804688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015182469971477985\n",
      "        model: {}\n",
      "        policy_loss: -0.2033909261226654\n",
      "        total_loss: -0.155881866812706\n",
      "        vf_explained_var: -0.6446556448936462\n",
      "        vf_loss: 0.028053538873791695\n",
      "  num_agent_steps_sampled: 89244\n",
      "  num_agent_steps_trained: 89244\n",
      "  num_steps_sampled: 44622\n",
      "  num_steps_trained: 44622\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 114\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.24285714285714\n",
      "  ram_util_percent: 37.90769230769231\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19927999999999435\n",
      "  blue_1: 0.08737999999999556\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770312786833626\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.96056656999562\n",
      "  mean_inference_ms: 2.852651794744143\n",
      "  mean_raw_obs_processing_ms: 31.78952279545047\n",
      "time_since_restore: 6616.790761709213\n",
      "time_this_iter_s: 61.441267251968384\n",
      "time_total_s: 6616.790761709213\n",
      "timers:\n",
      "  learn_throughput: 815.315\n",
      "  learn_time_ms: 593.39\n",
      "  load_throughput: 540170.44\n",
      "  load_time_ms: 0.896\n",
      "  sample_throughput: 7.299\n",
      "  sample_time_ms: 66282.115\n",
      "timestamp: 1638989444\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 44622\n",
      "training_iteration: 114\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:114 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 272 0.005 1.359999999999993\n",
      "blue_1 True True 272 -2.0 -0.6450000000000069\n",
      "agent_timesteps_total: 89788\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-51-23\n",
      "done: false\n",
      "episode_len_mean: 225.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.2862599999999733\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 235\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.678864479064941\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014321759343147278\n",
      "        model: {}\n",
      "        policy_loss: -0.13074788451194763\n",
      "        total_loss: 0.07772152125835419\n",
      "        vf_explained_var: -0.5799590945243835\n",
      "        vf_loss: 0.18671821057796478\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.075156211853027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011138291098177433\n",
      "        model: {}\n",
      "        policy_loss: -0.19401544332504272\n",
      "        total_loss: 0.15512295067310333\n",
      "        vf_explained_var: 0.36163797974586487\n",
      "        vf_loss: 0.3348652422428131\n",
      "  num_agent_steps_sampled: 89788\n",
      "  num_agent_steps_trained: 89788\n",
      "  num_steps_sampled: 44894\n",
      "  num_steps_trained: 44894\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 115\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.88727272727273\n",
      "  ram_util_percent: 37.900000000000006\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19907999999999432\n",
      "  blue_1: 0.08717999999999555\n",
      "policy_reward_min:\n",
      "  blue_0: -1.755\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770242091942858\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.89560754692393\n",
      "  mean_inference_ms: 2.8522359650695948\n",
      "  mean_raw_obs_processing_ms: 31.744731832454672\n",
      "time_since_restore: 6650.638813734055\n",
      "time_this_iter_s: 33.84805202484131\n",
      "time_total_s: 6650.638813734055\n",
      "timers:\n",
      "  learn_throughput: 810.71\n",
      "  learn_time_ms: 561.977\n",
      "  load_throughput: 572441.706\n",
      "  load_time_ms: 0.796\n",
      "  sample_throughput: 7.29\n",
      "  sample_time_ms: 62496.85\n",
      "timestamp: 1638989483\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 44894\n",
      "training_iteration: 115\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:115 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 47 -2.0 -1.7699999999999998\n",
      "blue_1 True True 47 0.005 0.23500000000000013\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 244 -2.0 -0.7850000000000039\n",
      "blue_1 True True 244 0.005 1.219999999999996\n",
      "agent_timesteps_total: 90370\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-52-16\n",
      "done: false\n",
      "episode_len_mean: 222.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.2548599999999738\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 237\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.944298267364502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012993530370295048\n",
      "        model: {}\n",
      "        policy_loss: -0.2632477283477783\n",
      "        total_loss: 0.3235565423965454\n",
      "        vf_explained_var: 0.14146679639816284\n",
      "        vf_loss: 0.5670703053474426\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.077855110168457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01439401414245367\n",
      "        model: {}\n",
      "        policy_loss: -0.19640308618545532\n",
      "        total_loss: -0.08792836219072342\n",
      "        vf_explained_var: 0.10791361331939697\n",
      "        vf_loss: 0.09002958983182907\n",
      "  num_agent_steps_sampled: 90370\n",
      "  num_agent_steps_trained: 90370\n",
      "  num_steps_sampled: 45185\n",
      "  num_steps_trained: 45185\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 116\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.33783783783783\n",
      "  ram_util_percent: 37.92297297297297\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1749299999999946\n",
      "  blue_1: 0.07992999999999557\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770074996517013\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.77053628167786\n",
      "  mean_inference_ms: 2.8514847303562574\n",
      "  mean_raw_obs_processing_ms: 31.663247787192258\n",
      "time_since_restore: 6698.123872518539\n",
      "time_this_iter_s: 47.48505878448486\n",
      "time_total_s: 6698.123872518539\n",
      "timers:\n",
      "  learn_throughput: 803.59\n",
      "  learn_time_ms: 547.418\n",
      "  load_throughput: 552715.334\n",
      "  load_time_ms: 0.796\n",
      "  sample_throughput: 7.162\n",
      "  sample_time_ms: 61419.62\n",
      "timestamp: 1638989536\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 45185\n",
      "training_iteration: 116\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:116 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 72 0.005 0.3600000000000002\n",
      "blue_1 True True 72 -2.0 -1.6449999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 208 0.005 1.0399999999999998\n",
      "blue_1 True True 208 -2.0 -0.9650000000000001\n",
      "agent_timesteps_total: 90930\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-53-08\n",
      "done: false\n",
      "episode_len_mean: 218.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.21277999999997457\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 239\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.9544806480407715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01807299070060253\n",
      "        model: {}\n",
      "        policy_loss: -0.08949089050292969\n",
      "        total_loss: 0.09827616065740585\n",
      "        vf_explained_var: 0.256041020154953\n",
      "        vf_loss: 0.16031868755817413\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.02846097946167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01806926354765892\n",
      "        model: {}\n",
      "        policy_loss: -0.21747617423534393\n",
      "        total_loss: 0.430301308631897\n",
      "        vf_explained_var: 0.15054169297218323\n",
      "        vf_loss: 0.624622642993927\n",
      "  num_agent_steps_sampled: 90930\n",
      "  num_agent_steps_trained: 90930\n",
      "  num_steps_sampled: 45465\n",
      "  num_steps_trained: 45465\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 117\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.49577464788734\n",
      "  ram_util_percent: 37.91126760563381\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1941299999999949\n",
      "  blue_1: 0.018649999999995878\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770255512046039\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.6556738858174\n",
      "  mean_inference_ms: 2.8507753603553625\n",
      "  mean_raw_obs_processing_ms: 31.593202315714816\n",
      "time_since_restore: 6744.251680135727\n",
      "time_this_iter_s: 46.1278076171875\n",
      "time_total_s: 6744.251680135727\n",
      "timers:\n",
      "  learn_throughput: 812.972\n",
      "  learn_time_ms: 526.463\n",
      "  load_throughput: 537860.173\n",
      "  load_time_ms: 0.796\n",
      "  sample_throughput: 6.958\n",
      "  sample_time_ms: 61513.372\n",
      "timestamp: 1638989588\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 45465\n",
      "training_iteration: 117\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:117 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 112 0.005 0.5600000000000004\n",
      "blue_1 True True 112 -2.0 -1.4449999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 300 0.005 1.49999999999999\n",
      "blue_1 True True 300 -2.0 -0.43900000000001116\n",
      "agent_timesteps_total: 91754\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-54-14\n",
      "done: false\n",
      "episode_len_mean: 220.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.23663999999997415\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 241\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.698579788208008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013733196072280407\n",
      "        model: {}\n",
      "        policy_loss: -0.2038341462612152\n",
      "        total_loss: -0.1449458748102188\n",
      "        vf_explained_var: -0.46265745162963867\n",
      "        vf_loss: 0.038030970841646194\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.771615028381348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014751741662621498\n",
      "        model: {}\n",
      "        policy_loss: -0.2162744700908661\n",
      "        total_loss: 0.18381868302822113\n",
      "        vf_explained_var: 0.0805339515209198\n",
      "        vf_loss: 0.3811896741390228\n",
      "  num_agent_steps_sampled: 91754\n",
      "  num_agent_steps_trained: 91754\n",
      "  num_steps_sampled: 45877\n",
      "  num_steps_trained: 45877\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 118\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.510989010989015\n",
      "  ram_util_percent: 37.97472527472527\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.24582999999999472\n",
      "  blue_1: -0.00919000000000426\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770528123446003\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.5393934601185\n",
      "  mean_inference_ms: 2.850096415705903\n",
      "  mean_raw_obs_processing_ms: 31.521704151010354\n",
      "time_since_restore: 6804.6321976184845\n",
      "time_this_iter_s: 60.38051748275757\n",
      "time_total_s: 6804.6321976184845\n",
      "timers:\n",
      "  learn_throughput: 812.987\n",
      "  learn_time_ms: 503.821\n",
      "  load_throughput: 514860.62\n",
      "  load_time_ms: 0.796\n",
      "  sample_throughput: 6.751\n",
      "  sample_time_ms: 60668.499\n",
      "timestamp: 1638989654\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 45877\n",
      "training_iteration: 118\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:118 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.039999999999958\n",
      "agent_timesteps_total: 92954\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-55-27\n",
      "done: false\n",
      "episode_len_mean: 223.97\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.303999999999868\n",
      "episode_reward_mean: 0.2675899999999734\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 242\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.508907794952393\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00882138591259718\n",
      "        model: {}\n",
      "        policy_loss: -0.3876235783100128\n",
      "        total_loss: -0.34154465794563293\n",
      "        vf_explained_var: -0.6531693935394287\n",
      "        vf_loss: 0.03268140181899071\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.257528305053711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00918089784681797\n",
      "        model: {}\n",
      "        policy_loss: -0.4122870862483978\n",
      "        total_loss: -0.26958897709846497\n",
      "        vf_explained_var: -0.8172223567962646\n",
      "        vf_loss: 0.13093331456184387\n",
      "  num_agent_steps_sampled: 92954\n",
      "  num_agent_steps_trained: 92954\n",
      "  num_steps_sampled: 46477\n",
      "  num_steps_trained: 46477\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 119\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.5186274509804\n",
      "  ram_util_percent: 37.98725490196079\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2715299999999944\n",
      "  blue_1: -0.003940000000004624\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.577062081761625\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.47934301357796\n",
      "  mean_inference_ms: 2.849746321887453\n",
      "  mean_raw_obs_processing_ms: 31.48341104529918\n",
      "time_since_restore: 6872.770146369934\n",
      "time_this_iter_s: 68.13794875144958\n",
      "time_total_s: 6872.770146369934\n",
      "timers:\n",
      "  learn_throughput: 806.704\n",
      "  learn_time_ms: 504.522\n",
      "  load_throughput: 454615.64\n",
      "  load_time_ms: 0.895\n",
      "  sample_throughput: 6.85\n",
      "  sample_time_ms: 59420.138\n",
      "timestamp: 1638989727\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 46477\n",
      "training_iteration: 119\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:119 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.50099999999996\n",
      "agent_timesteps_total: 94154\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-56-44\n",
      "done: false\n",
      "episode_len_mean: 225.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.2879999999999733\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 243\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.788362503051758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004508101847022772\n",
      "        model: {}\n",
      "        policy_loss: -0.3826887011528015\n",
      "        total_loss: -0.369390070438385\n",
      "        vf_explained_var: -0.5869614481925964\n",
      "        vf_loss: 0.006451974622905254\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.934805870056152\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0070753442123532295\n",
      "        model: {}\n",
      "        policy_loss: -0.3750555217266083\n",
      "        total_loss: -0.34449321031570435\n",
      "        vf_explained_var: -0.24475756287574768\n",
      "        vf_loss: 0.021495657041668892\n",
      "  num_agent_steps_sampled: 94154\n",
      "  num_agent_steps_trained: 94154\n",
      "  num_steps_sampled: 47077\n",
      "  num_steps_trained: 47077\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 120\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.873333333333335\n",
      "  ram_util_percent: 37.99428571428571\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2689799999999943\n",
      "  blue_1: 0.01901999999999521\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770716998237764\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.42489836304021\n",
      "  mean_inference_ms: 2.849395283324808\n",
      "  mean_raw_obs_processing_ms: 31.44634158109888\n",
      "time_since_restore: 6943.4684290885925\n",
      "time_this_iter_s: 70.69828271865845\n",
      "time_total_s: 6943.4684290885925\n",
      "timers:\n",
      "  learn_throughput: 817.003\n",
      "  learn_time_ms: 519.582\n",
      "  load_throughput: 474049.375\n",
      "  load_time_ms: 0.895\n",
      "  sample_throughput: 6.926\n",
      "  sample_time_ms: 61295.12\n",
      "timestamp: 1638989804\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 47077\n",
      "training_iteration: 120\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:120 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 163 0.005 0.8150000000000006\n",
      "blue_1 True True 163 -2.0 -1.1899999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 225 -2.0 -0.8800000000000019\n",
      "blue_1 True True 225 0.006 1.1289999999999987\n",
      "agent_timesteps_total: 94930\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-57-49\n",
      "done: false\n",
      "episode_len_mean: 224.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.27897999999997336\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 245\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.353818893432617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02169610746204853\n",
      "        model: {}\n",
      "        policy_loss: -0.11274133622646332\n",
      "        total_loss: 0.2447873055934906\n",
      "        vf_explained_var: 0.5053895115852356\n",
      "        vf_loss: 0.34105318784713745\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.374075889587402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012330371886491776\n",
      "        model: {}\n",
      "        policy_loss: -0.05861538648605347\n",
      "        total_loss: 0.42431968450546265\n",
      "        vf_explained_var: 0.08887941390275955\n",
      "        vf_loss: 0.46713438630104065\n",
      "  num_agent_steps_sampled: 94930\n",
      "  num_agent_steps_trained: 94930\n",
      "  num_steps_sampled: 47465\n",
      "  num_steps_trained: 47465\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 121\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.334444444444436\n",
      "  ram_util_percent: 38.03555555555556\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.28421999999999426\n",
      "  blue_1: -0.005240000000004717\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770794906873323\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.3251127436151\n",
      "  mean_inference_ms: 2.8487027075677145\n",
      "  mean_raw_obs_processing_ms: 31.37496164819911\n",
      "time_since_restore: 7003.137805938721\n",
      "time_this_iter_s: 59.669376850128174\n",
      "time_total_s: 7003.137805938721\n",
      "timers:\n",
      "  learn_throughput: 795.188\n",
      "  learn_time_ms: 534.842\n",
      "  load_throughput: 534643.336\n",
      "  load_time_ms: 0.795\n",
      "  sample_throughput: 6.858\n",
      "  sample_time_ms: 62019.665\n",
      "timestamp: 1638989869\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 47465\n",
      "training_iteration: 121\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:120 starting ! -----------------\n",
      "agent_timesteps_total: 94930\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-57-49\n",
      "done: false\n",
      "episode_len_mean: 224.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.27897999999997336\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 245\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.353818893432617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02169610746204853\n",
      "        model: {}\n",
      "        policy_loss: -0.11274133622646332\n",
      "        total_loss: 0.2447873055934906\n",
      "        vf_explained_var: 0.5053895115852356\n",
      "        vf_loss: 0.34105318784713745\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.374075889587402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012330371886491776\n",
      "        model: {}\n",
      "        policy_loss: -0.05861538648605347\n",
      "        total_loss: 0.42431968450546265\n",
      "        vf_explained_var: 0.08887941390275955\n",
      "        vf_loss: 0.46713438630104065\n",
      "  num_agent_steps_sampled: 94930\n",
      "  num_agent_steps_trained: 94930\n",
      "  num_steps_sampled: 47465\n",
      "  num_steps_trained: 47465\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 121\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.334444444444436\n",
      "  ram_util_percent: 38.03555555555556\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.28421999999999426\n",
      "  blue_1: -0.005240000000004717\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770794906873323\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.3251127436151\n",
      "  mean_inference_ms: 2.8487027075677145\n",
      "  mean_raw_obs_processing_ms: 31.37496164819911\n",
      "time_since_restore: 7003.137805938721\n",
      "time_this_iter_s: 59.669376850128174\n",
      "time_total_s: 7003.137805938721\n",
      "timers:\n",
      "  learn_throughput: 795.188\n",
      "  learn_time_ms: 534.842\n",
      "  load_throughput: 534643.336\n",
      "  load_time_ms: 0.795\n",
      "  sample_throughput: 6.858\n",
      "  sample_time_ms: 62019.665\n",
      "timestamp: 1638989869\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 47465\n",
      "training_iteration: 121\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:121 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 166 0.005 0.8300000000000006\n",
      "blue_1 True True 166 -2.0 -1.1749999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 307 0.005 1.5349999999999893\n",
      "blue_1 True True 307 -2.0 -0.37400000000001055\n",
      "agent_timesteps_total: 95876\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_03-59-00\n",
      "done: false\n",
      "episode_len_mean: 227.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.30403999999997305\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 247\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.017255783081055\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01780882105231285\n",
      "        model: {}\n",
      "        policy_loss: -0.08854034543037415\n",
      "        total_loss: -0.006368895526975393\n",
      "        vf_explained_var: -0.06597904115915298\n",
      "        vf_loss: 0.061886098235845566\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.270896911621094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01759210228919983\n",
      "        model: {}\n",
      "        policy_loss: -0.2905876040458679\n",
      "        total_loss: 0.04559476673603058\n",
      "        vf_explained_var: 0.09323250502347946\n",
      "        vf_loss: 0.3136391043663025\n",
      "  num_agent_steps_sampled: 95876\n",
      "  num_agent_steps_trained: 95876\n",
      "  num_steps_sampled: 47938\n",
      "  num_steps_trained: 47938\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 122\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.82551020408163\n",
      "  ram_util_percent: 38.06938775510204\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.31631999999999416\n",
      "  blue_1: -0.012280000000004813\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770745158952875\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.2256031288988\n",
      "  mean_inference_ms: 2.848028945829709\n",
      "  mean_raw_obs_processing_ms: 31.302322172855575\n",
      "time_since_restore: 7068.923959970474\n",
      "time_this_iter_s: 65.78615403175354\n",
      "time_total_s: 7068.923959970474\n",
      "timers:\n",
      "  learn_throughput: 805.522\n",
      "  learn_time_ms: 534.932\n",
      "  load_throughput: 543900.085\n",
      "  load_time_ms: 0.792\n",
      "  sample_throughput: 6.77\n",
      "  sample_time_ms: 63650.076\n",
      "timestamp: 1638989940\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 47938\n",
      "training_iteration: 122\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:122 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 98 0.005 0.4900000000000003\n",
      "blue_1 True True 98 -2.0 -1.5149999999999997\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.029999999999958\n",
      "agent_timesteps_total: 97272\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-00-37\n",
      "done: false\n",
      "episode_len_mean: 229.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.3305899999999726\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 249\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.983354568481445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012005869299173355\n",
      "        model: {}\n",
      "        policy_loss: -0.2831079959869385\n",
      "        total_loss: -0.2261706292629242\n",
      "        vf_explained_var: -0.5839776396751404\n",
      "        vf_loss: 0.04326190426945686\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.66966438293457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01518417987972498\n",
      "        model: {}\n",
      "        policy_loss: -0.18263177573680878\n",
      "        total_loss: 0.15644434094429016\n",
      "        vf_explained_var: -0.6849608421325684\n",
      "        vf_loss: 0.31961843371391296\n",
      "  num_agent_steps_sampled: 97272\n",
      "  num_agent_steps_trained: 97272\n",
      "  num_steps_sampled: 48636\n",
      "  num_steps_trained: 48636\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 123\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.46791044776119\n",
      "  ram_util_percent: 38.08432835820895\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3595199999999938\n",
      "  blue_1: -0.028930000000005188\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770719638181305\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.12917545222618\n",
      "  mean_inference_ms: 2.847379721922506\n",
      "  mean_raw_obs_processing_ms: 31.228187447243716\n",
      "time_since_restore: 7160.271065711975\n",
      "time_this_iter_s: 91.34710574150085\n",
      "time_total_s: 7160.271065711975\n",
      "timers:\n",
      "  learn_throughput: 813.723\n",
      "  learn_time_ms: 544.903\n",
      "  load_throughput: 637666.516\n",
      "  load_time_ms: 0.695\n",
      "  sample_throughput: 6.713\n",
      "  sample_time_ms: 66047.123\n",
      "timestamp: 1638990037\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 48636\n",
      "training_iteration: 123\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:123 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 185 0.005 0.9250000000000007\n",
      "blue_1 True True 185 -2.0 -1.0799999999999992\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 177 -2.0 -1.1199999999999992\n",
      "blue_1 True True 177 0.005 0.8850000000000007\n",
      "agent_timesteps_total: 97996\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-01-38\n",
      "done: false\n",
      "episode_len_mean: 227.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.30854999999997285\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 251\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.078264236450195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008941528387367725\n",
      "        model: {}\n",
      "        policy_loss: -0.6081229448318481\n",
      "        total_loss: -0.5428589582443237\n",
      "        vf_explained_var: 0.25359708070755005\n",
      "        vf_loss: 0.05507897958159447\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.885871410369873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01405148021876812\n",
      "        model: {}\n",
      "        policy_loss: 0.24246731400489807\n",
      "        total_loss: 0.6151891350746155\n",
      "        vf_explained_var: 0.48262205719947815\n",
      "        vf_loss: 0.3547155559062958\n",
      "  num_agent_steps_sampled: 97996\n",
      "  num_agent_steps_trained: 97996\n",
      "  num_steps_sampled: 48998\n",
      "  num_steps_trained: 48998\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 124\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.33333333333333\n",
      "  ram_util_percent: 38.0297619047619\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.34846999999999406\n",
      "  blue_1: -0.03992000000000504\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770649443585613\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 116.03595939632773\n",
      "  mean_inference_ms: 2.84672323117205\n",
      "  mean_raw_obs_processing_ms: 31.157329110072343\n",
      "time_since_restore: 7215.6255939006805\n",
      "time_this_iter_s: 55.354528188705444\n",
      "time_total_s: 7215.6255939006805\n",
      "timers:\n",
      "  learn_throughput: 822.811\n",
      "  learn_time_ms: 531.835\n",
      "  load_throughput: 731391.684\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 6.685\n",
      "  sample_time_ms: 65462.24\n",
      "timestamp: 1638990098\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 48998\n",
      "training_iteration: 124\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:124 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 51 0.005 0.2550000000000001\n",
      "blue_1 True True 51 -2.0 -1.75\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 432 -2.0 0.15499999999997627\n",
      "blue_1 True True 432 0.005 2.1219999999999803\n",
      "agent_timesteps_total: 98962\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-02-52\n",
      "done: false\n",
      "episode_len_mean: 229.69\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.32996999999997234\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 253\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.622870445251465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01041899248957634\n",
      "        model: {}\n",
      "        policy_loss: -0.48335957527160645\n",
      "        total_loss: -0.42472606897354126\n",
      "        vf_explained_var: 0.17347076535224915\n",
      "        vf_loss: 0.0467655248939991\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.66397476196289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016282349824905396\n",
      "        model: {}\n",
      "        policy_loss: -0.01468014344573021\n",
      "        total_loss: 0.43341338634490967\n",
      "        vf_explained_var: -0.5898541808128357\n",
      "        vf_loss: 0.4272285997867584\n",
      "  num_agent_steps_sampled: 98962\n",
      "  num_agent_steps_trained: 98962\n",
      "  num_steps_sampled: 49481\n",
      "  num_steps_trained: 49481\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 125\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.86960784313725\n",
      "  ram_util_percent: 37.99019607843137\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3393199999999938\n",
      "  blue_1: -0.009350000000005253\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770429642046392\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.9405427998723\n",
      "  mean_inference_ms: 2.846063570476501\n",
      "  mean_raw_obs_processing_ms: 31.08477989670376\n",
      "time_since_restore: 7283.998432636261\n",
      "time_this_iter_s: 68.37283873558044\n",
      "time_total_s: 7283.998432636261\n",
      "timers:\n",
      "  learn_throughput: 829.807\n",
      "  learn_time_ms: 552.779\n",
      "  load_throughput: 657146.308\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.659\n",
      "  sample_time_ms: 68884.558\n",
      "timestamp: 1638990172\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 49481\n",
      "training_iteration: 125\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:125 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 111 0.005 0.5550000000000004\n",
      "blue_1 True True 111 -2.0 -1.4499999999999997\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 326 -2.0 -0.37500000000001266\n",
      "blue_1 True True 326 0.004 1.6469999999999878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 99836\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-04-04\n",
      "done: false\n",
      "episode_len_mean: 230.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.3426399999999721\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 255\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.088632583618164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00770656717941165\n",
      "        model: {}\n",
      "        policy_loss: -0.348425030708313\n",
      "        total_loss: -0.2440631240606308\n",
      "        vf_explained_var: 0.39506733417510986\n",
      "        vf_loss: 0.09558363258838654\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.73184585571289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013582315295934677\n",
      "        model: {}\n",
      "        policy_loss: -0.10360165685415268\n",
      "        total_loss: 0.11128388345241547\n",
      "        vf_explained_var: -0.15377375483512878\n",
      "        vf_loss: 0.19748054444789886\n",
      "  num_agent_steps_sampled: 99836\n",
      "  num_agent_steps_trained: 99836\n",
      "  num_steps_sampled: 49918\n",
      "  num_steps_trained: 49918\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 126\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.56262626262625\n",
      "  ram_util_percent: 38.0070707070707\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3255199999999936\n",
      "  blue_1: 0.017119999999994612\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770227932941991\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.84585085446547\n",
      "  mean_inference_ms: 2.845409303573433\n",
      "  mean_raw_obs_processing_ms: 31.01050689446569\n",
      "time_since_restore: 7350.087976932526\n",
      "time_this_iter_s: 66.08954429626465\n",
      "time_total_s: 7350.087976932526\n",
      "timers:\n",
      "  learn_throughput: 834.684\n",
      "  learn_time_ms: 567.041\n",
      "  load_throughput: 678062.671\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.689\n",
      "  sample_time_ms: 70753.476\n",
      "timestamp: 1638990244\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 49918\n",
      "training_iteration: 126\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:126 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 329 0.005 1.644999999999987\n",
      "blue_1 True True 329 -1.998 -0.25800000000000667\n",
      "agent_timesteps_total: 100494\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-04-52\n",
      "done: false\n",
      "episode_len_mean: 232.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.354959999999972\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 256\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.4904375076293945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01292974129319191\n",
      "        model: {}\n",
      "        policy_loss: -0.46535369753837585\n",
      "        total_loss: -0.4217621088027954\n",
      "        vf_explained_var: -0.5468263030052185\n",
      "        vf_loss: 0.028863804414868355\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.6209077835083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006239226553589106\n",
      "        model: {}\n",
      "        policy_loss: -0.45267540216445923\n",
      "        total_loss: -0.3980309069156647\n",
      "        vf_explained_var: 0.041915517300367355\n",
      "        vf_loss: 0.04664929583668709\n",
      "  num_agent_steps_sampled: 100494\n",
      "  num_agent_steps_trained: 100494\n",
      "  num_steps_sampled: 50247\n",
      "  num_steps_trained: 50247\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 127\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.02089552238805\n",
      "  ram_util_percent: 38.05970149253731\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3311699999999934\n",
      "  blue_1: 0.02378999999999454\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.577018096922808\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.79884392960763\n",
      "  mean_inference_ms: 2.8450820699269475\n",
      "  mean_raw_obs_processing_ms: 30.973162045131662\n",
      "time_since_restore: 7392.871401548386\n",
      "time_this_iter_s: 42.783424615859985\n",
      "time_total_s: 7392.871401548386\n",
      "timers:\n",
      "  learn_throughput: 838.882\n",
      "  learn_time_ms: 570.044\n",
      "  load_throughput: 685035.75\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.79\n",
      "  sample_time_ms: 70424.698\n",
      "timestamp: 1638990292\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 50247\n",
      "training_iteration: 127\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:127 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 93 0.005 0.4650000000000003\n",
      "blue_1 True True 93 -2.0 -1.5399999999999996\n",
      "LOSE\n",
      "blue_0 False True 518 -0.995 1.589999999999967\n",
      "blue_1 False True 518 -0.993 1.8989999999999974\n",
      "agent_timesteps_total: 101716\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-06-16\n",
      "done: false\n",
      "episode_len_mean: 234.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.3834999999999713\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 258\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.453248023986816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009868297725915909\n",
      "        model: {}\n",
      "        policy_loss: -0.412049263715744\n",
      "        total_loss: -0.3818645477294922\n",
      "        vf_explained_var: -0.27419790625572205\n",
      "        vf_loss: 0.018944092094898224\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.097987174987793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012071696110069752\n",
      "        model: {}\n",
      "        policy_loss: -0.2511758506298065\n",
      "        total_loss: 0.015258731320500374\n",
      "        vf_explained_var: -0.6021530032157898\n",
      "        vf_loss: 0.2509653866291046\n",
      "  num_agent_steps_sampled: 101716\n",
      "  num_agent_steps_trained: 101716\n",
      "  num_steps_sampled: 50858\n",
      "  num_steps_trained: 50858\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 128\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.849999999999994\n",
      "  ram_util_percent: 38.0887931034483\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.35391999999999313\n",
      "  blue_1: 0.029579999999994513\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5770069690314468\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.70523217774918\n",
      "  mean_inference_ms: 2.8444222869906053\n",
      "  mean_raw_obs_processing_ms: 30.897383340642627\n",
      "time_since_restore: 7471.2957100868225\n",
      "time_this_iter_s: 78.42430853843689\n",
      "time_total_s: 7471.2957100868225\n",
      "timers:\n",
      "  learn_throughput: 850.352\n",
      "  learn_time_ms: 585.758\n",
      "  load_throughput: 713421.262\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 6.897\n",
      "  sample_time_ms: 72219.506\n",
      "timestamp: 1638990376\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 50858\n",
      "training_iteration: 128\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:128 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 138 -2.0 -1.3149999999999995\n",
      "blue_1 True True 138 0.005 0.6900000000000005\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 406 0.005 2.029999999999979\n",
      "blue_1 True True 406 -1.998 0.10299999999998355\n",
      "agent_timesteps_total: 102804\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-07-25\n",
      "done: false\n",
      "episode_len_mean: 233.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.3664599999999716\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 260\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.763547897338867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01726066693663597\n",
      "        model: {}\n",
      "        policy_loss: -0.07756345719099045\n",
      "        total_loss: 0.34252870082855225\n",
      "        vf_explained_var: -0.25775718688964844\n",
      "        vf_loss: 0.40043118596076965\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.767655372619629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016734343022108078\n",
      "        model: {}\n",
      "        policy_loss: -0.18982385098934174\n",
      "        total_loss: 0.08271132409572601\n",
      "        vf_explained_var: -0.3433762192726135\n",
      "        vf_loss: 0.25109103322029114\n",
      "  num_agent_steps_sampled: 102804\n",
      "  num_agent_steps_trained: 102804\n",
      "  num_steps_sampled: 51402\n",
      "  num_steps_trained: 51402\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 129\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.33052631578947\n",
      "  ram_util_percent: 38.084210526315786\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3460199999999933\n",
      "  blue_1: 0.020439999999994508\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.577002986726248\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.61153541816974\n",
      "  mean_inference_ms: 2.8437957345413083\n",
      "  mean_raw_obs_processing_ms: 30.824090910895094\n",
      "time_since_restore: 7534.519617319107\n",
      "time_this_iter_s: 63.223907232284546\n",
      "time_total_s: 7534.519617319107\n",
      "timers:\n",
      "  learn_throughput: 840.219\n",
      "  learn_time_ms: 586.157\n",
      "  load_throughput: 822920.373\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 6.865\n",
      "  sample_time_ms: 71745.236\n",
      "timestamp: 1638990445\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 51402\n",
      "training_iteration: 129\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:129 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 178 0.005 0.8900000000000007\n",
      "blue_1 True True 178 -2.0 -1.1149999999999993\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 282 -2.0 -0.595000000000008\n",
      "blue_1 True True 282 0.007 1.423999999999992\n",
      "agent_timesteps_total: 103724\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-08-30\n",
      "done: false\n",
      "episode_len_mean: 234.31\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.3793999999999713\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 262\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.52883529663086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008502266369760036\n",
      "        model: {}\n",
      "        policy_loss: -0.42224371433258057\n",
      "        total_loss: -0.3513264060020447\n",
      "        vf_explained_var: 0.1879502385854721\n",
      "        vf_loss: 0.06123265251517296\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.48537540435791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01637464016675949\n",
      "        model: {}\n",
      "        policy_loss: 0.04059915244579315\n",
      "        total_loss: 0.4326413571834564\n",
      "        vf_explained_var: 0.0018281990196555853\n",
      "        vf_loss: 0.37105894088745117\n",
      "  num_agent_steps_sampled: 103724\n",
      "  num_agent_steps_trained: 103724\n",
      "  num_steps_sampled: 51862\n",
      "  num_steps_trained: 51862\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 130\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.72747252747253\n",
      "  ram_util_percent: 38.089010989010994\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.33236999999999317\n",
      "  blue_1: 0.04702999999999444\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5769920697497564\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.52041146819873\n",
      "  mean_inference_ms: 2.8431741927104066\n",
      "  mean_raw_obs_processing_ms: 30.752531045578998\n",
      "time_since_restore: 7594.404196023941\n",
      "time_this_iter_s: 59.884578704833984\n",
      "time_total_s: 7594.404196023941\n",
      "timers:\n",
      "  learn_throughput: 838.301\n",
      "  learn_time_ms: 570.797\n",
      "  load_throughput: 959723.826\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.771\n",
      "  sample_time_ms: 70674.098\n",
      "timestamp: 1638990510\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 51862\n",
      "training_iteration: 130\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:130 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 115 -2.0 -1.4299999999999997\n",
      "blue_1 True True 115 0.005 0.5750000000000004\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 93 0.005 0.4650000000000003\n",
      "blue_1 True True 93 -2.0 -1.5399999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 222 0.005 1.1099999999999983\n",
      "blue_1 True True 222 -2.0 -0.8950000000000016\n",
      "agent_timesteps_total: 104584\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-09-43\n",
      "done: false\n",
      "episode_len_mean: 231.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.35535999999997175\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 265\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.923445701599121\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02366683818399906\n",
      "        model: {}\n",
      "        policy_loss: -0.03844570741057396\n",
      "        total_loss: 0.3740655779838562\n",
      "        vf_explained_var: 0.6152238845825195\n",
      "        vf_loss: 0.3855532705783844\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.110767364501953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015009608119726181\n",
      "        model: {}\n",
      "        policy_loss: -0.17938047647476196\n",
      "        total_loss: 0.18568480014801025\n",
      "        vf_explained_var: 0.334984689950943\n",
      "        vf_loss: 0.3458313047885895\n",
      "  num_agent_steps_sampled: 104584\n",
      "  num_agent_steps_trained: 104584\n",
      "  num_steps_sampled: 52292\n",
      "  num_steps_trained: 52292\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 131\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.098\n",
      "  ram_util_percent: 38.083\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.32071999999999334\n",
      "  blue_1: 0.03463999999999462\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5769744264307413\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.3909037828259\n",
      "  mean_inference_ms: 2.8422505710616544\n",
      "  mean_raw_obs_processing_ms: 30.652927648057666\n",
      "time_since_restore: 7661.700062513351\n",
      "time_this_iter_s: 67.2958664894104\n",
      "time_total_s: 7661.700062513351\n",
      "timers:\n",
      "  learn_throughput: 844.623\n",
      "  learn_time_ms: 571.497\n",
      "  load_throughput: 968147.734\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.758\n",
      "  sample_time_ms: 71422.204\n",
      "timestamp: 1638990583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 52292\n",
      "training_iteration: 131\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:130 starting ! -----------------\n",
      "agent_timesteps_total: 104584\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-09-43\n",
      "done: false\n",
      "episode_len_mean: 231.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.35535999999997175\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 265\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.923445701599121\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02366683818399906\n",
      "        model: {}\n",
      "        policy_loss: -0.03844570741057396\n",
      "        total_loss: 0.3740655779838562\n",
      "        vf_explained_var: 0.6152238845825195\n",
      "        vf_loss: 0.3855532705783844\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.110767364501953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015009608119726181\n",
      "        model: {}\n",
      "        policy_loss: -0.17938047647476196\n",
      "        total_loss: 0.18568480014801025\n",
      "        vf_explained_var: 0.334984689950943\n",
      "        vf_loss: 0.3458313047885895\n",
      "  num_agent_steps_sampled: 104584\n",
      "  num_agent_steps_trained: 104584\n",
      "  num_steps_sampled: 52292\n",
      "  num_steps_trained: 52292\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 131\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.098\n",
      "  ram_util_percent: 38.083\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.32071999999999334\n",
      "  blue_1: 0.03463999999999462\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5769744264307413\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.3909037828259\n",
      "  mean_inference_ms: 2.8422505710616544\n",
      "  mean_raw_obs_processing_ms: 30.652927648057666\n",
      "time_since_restore: 7661.700062513351\n",
      "time_this_iter_s: 67.2958664894104\n",
      "time_total_s: 7661.700062513351\n",
      "timers:\n",
      "  learn_throughput: 844.623\n",
      "  learn_time_ms: 571.497\n",
      "  load_throughput: 968147.734\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.758\n",
      "  sample_time_ms: 71422.204\n",
      "timestamp: 1638990583\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 52292\n",
      "training_iteration: 131\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:131 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 150 0.005 0.7500000000000006\n",
      "blue_1 True True 150 -2.0 -1.2549999999999994\n",
      "LOSE\n",
      "blue_0 False True 438 -0.995 1.1899999999999755\n",
      "blue_1 False True 438 -0.996 1.553999999999994\n",
      "agent_timesteps_total: 105760\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-11-06\n",
      "done: false\n",
      "episode_len_mean: 235.93\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.3985499999999711\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 267\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.711165428161621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006275803782045841\n",
      "        model: {}\n",
      "        policy_loss: -0.3424277603626251\n",
      "        total_loss: -0.273715615272522\n",
      "        vf_explained_var: 0.06141302362084389\n",
      "        vf_loss: 0.057989344000816345\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.403119087219238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012631854973733425\n",
      "        model: {}\n",
      "        policy_loss: -0.21088644862174988\n",
      "        total_loss: 0.2342846542596817\n",
      "        vf_explained_var: -0.26044946908950806\n",
      "        vf_loss: 0.4289840757846832\n",
      "  num_agent_steps_sampled: 105760\n",
      "  num_agent_steps_trained: 105760\n",
      "  num_steps_sampled: 52880\n",
      "  num_steps_trained: 52880\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 132\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.6929203539823\n",
      "  ram_util_percent: 38.31327433628319\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.35051999999999317\n",
      "  blue_1: 0.048029999999994556\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5769439952322116\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.3016303738037\n",
      "  mean_inference_ms: 2.8416220572623545\n",
      "  mean_raw_obs_processing_ms: 30.582189392546944\n",
      "time_since_restore: 7738.859509468079\n",
      "time_this_iter_s: 77.15944695472717\n",
      "time_total_s: 7738.859509468079\n",
      "timers:\n",
      "  learn_throughput: 842.949\n",
      "  learn_time_ms: 586.275\n",
      "  load_throughput: 1238912.819\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.813\n",
      "  sample_time_ms: 72542.123\n",
      "timestamp: 1638990666\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 52880\n",
      "training_iteration: 132\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:132 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 101 -2.0 -1.4999999999999996\n",
      "blue_1 True True 101 0.005 0.5050000000000003\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 100 -2.0 -1.5049999999999997\n",
      "blue_1 True True 100 0.005 0.5000000000000003\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.069999999999958\n",
      "agent_timesteps_total: 107362\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-12-59\n",
      "done: false\n",
      "episode_len_mean: 239.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.43159999999997056\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 270\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.097107887268066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016832930967211723\n",
      "        model: {}\n",
      "        policy_loss: -0.11218605935573578\n",
      "        total_loss: 0.34080854058265686\n",
      "        vf_explained_var: -0.33336588740348816\n",
      "        vf_loss: 0.424234002828598\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.016068458557129\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01062789186835289\n",
      "        model: {}\n",
      "        policy_loss: -0.17690330743789673\n",
      "        total_loss: -0.11074497550725937\n",
      "        vf_explained_var: -0.582423210144043\n",
      "        vf_loss: 0.05253928527235985\n",
      "  num_agent_steps_sampled: 107362\n",
      "  num_agent_steps_trained: 107362\n",
      "  num_steps_sampled: 53681\n",
      "  num_steps_trained: 53681\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 133\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.70636942675159\n",
      "  ram_util_percent: 38.15477707006369\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.33661999999999276\n",
      "  blue_1: 0.09497999999999418\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5768966395745813\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.16795581111138\n",
      "  mean_inference_ms: 2.8406453053306633\n",
      "  mean_raw_obs_processing_ms: 30.47052019693539\n",
      "time_since_restore: 7846.69962978363\n",
      "time_this_iter_s: 107.84012031555176\n",
      "time_total_s: 7846.69962978363\n",
      "timers:\n",
      "  learn_throughput: 839.938\n",
      "  learn_time_ms: 600.64\n",
      "  load_throughput: 1686748.799\n",
      "  load_time_ms: 0.299\n",
      "  sample_throughput: 6.8\n",
      "  sample_time_ms: 74192.723\n",
      "timestamp: 1638990779\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 53681\n",
      "training_iteration: 133\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:133 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 171 0.005 0.8550000000000006\n",
      "blue_1 True True 171 -2.0 -1.1499999999999995\n",
      "LOSE\n",
      "blue_0 False True 413 -0.995 1.0649999999999782\n",
      "blue_1 False True 413 -0.993 1.3399999999999896\n",
      "agent_timesteps_total: 108530\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-14-19\n",
      "done: false\n",
      "episode_len_mean: 240.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.4517999999999703\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 272\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.30768871307373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009749491699039936\n",
      "        model: {}\n",
      "        policy_loss: -0.3370131850242615\n",
      "        total_loss: -0.2575491964817047\n",
      "        vf_explained_var: -0.10773368179798126\n",
      "        vf_loss: 0.06280605494976044\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.938950061798096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014169454574584961\n",
      "        model: {}\n",
      "        policy_loss: -0.12472879141569138\n",
      "        total_loss: 0.24243757128715515\n",
      "        vf_explained_var: 0.1570514291524887\n",
      "        vf_loss: 0.3490089774131775\n",
      "  num_agent_steps_sampled: 108530\n",
      "  num_agent_steps_trained: 108530\n",
      "  num_steps_sampled: 54265\n",
      "  num_steps_trained: 54265\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 134\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.82201834862385\n",
      "  ram_util_percent: 38.20091743119265\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3553699999999926\n",
      "  blue_1: 0.09642999999999408\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5768677692817537\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 115.07828773863858\n",
      "  mean_inference_ms: 2.839989476920231\n",
      "  mean_raw_obs_processing_ms: 30.392832837969298\n",
      "time_since_restore: 7920.435062885284\n",
      "time_this_iter_s: 73.73543310165405\n",
      "time_total_s: 7920.435062885284\n",
      "timers:\n",
      "  learn_throughput: 835.418\n",
      "  learn_time_ms: 630.463\n",
      "  load_throughput: 1330005.97\n",
      "  load_time_ms: 0.396\n",
      "  sample_throughput: 6.929\n",
      "  sample_time_ms: 76015.923\n",
      "timestamp: 1638990859\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 54265\n",
      "training_iteration: 134\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:134 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 170 -2.0 -1.1549999999999994\n",
      "blue_1 True True 170 0.005 0.8500000000000006\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 321 0.005 1.6049999999999878\n",
      "blue_1 True True 321 -1.998 -0.17300000000001048\n",
      "agent_timesteps_total: 109512\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-15-33\n",
      "done: false\n",
      "episode_len_mean: 244.4\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.48906999999997\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 274\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.412962436676025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012494092807173729\n",
      "        model: {}\n",
      "        policy_loss: 0.01551692746579647\n",
      "        total_loss: 0.3429415822029114\n",
      "        vf_explained_var: 0.07422732561826706\n",
      "        vf_loss: 0.3060773015022278\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.951168060302734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007695318665355444\n",
      "        model: {}\n",
      "        policy_loss: -0.4789697825908661\n",
      "        total_loss: -0.4436291456222534\n",
      "        vf_explained_var: 0.031220562756061554\n",
      "        vf_loss: 0.02547948621213436\n",
      "  num_agent_steps_sampled: 109512\n",
      "  num_agent_steps_trained: 109512\n",
      "  num_steps_sampled: 54756\n",
      "  num_steps_trained: 54756\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 135\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.062135922330086\n",
      "  ram_util_percent: 38.157281553398064\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3728699999999924\n",
      "  blue_1: 0.11619999999999399\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5768464892649424\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.99109490494862\n",
      "  mean_inference_ms: 2.839339762858009\n",
      "  mean_raw_obs_processing_ms: 30.313541655542718\n",
      "time_since_restore: 7989.591271400452\n",
      "time_this_iter_s: 69.15620851516724\n",
      "time_total_s: 7989.591271400452\n",
      "timers:\n",
      "  learn_throughput: 842.287\n",
      "  learn_time_ms: 626.271\n",
      "  load_throughput: 1331785.566\n",
      "  load_time_ms: 0.396\n",
      "  sample_throughput: 6.929\n",
      "  sample_time_ms: 76125.861\n",
      "timestamp: 1638990933\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 54756\n",
      "training_iteration: 135\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:135 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 129 -2.0 -1.3599999999999994\n",
      "blue_1 True True 129 0.005 0.6450000000000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 566 0.005 2.829999999999962\n",
      "blue_1 True True 566 -2.0 0.884999999999962\n",
      "agent_timesteps_total: 110902\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-17-07\n",
      "done: false\n",
      "episode_len_mean: 242.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.4691799999999706\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 276\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.466178894042969\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012111833319067955\n",
      "        model: {}\n",
      "        policy_loss: -0.08757425844669342\n",
      "        total_loss: 0.17406272888183594\n",
      "        vf_explained_var: -0.39144906401634216\n",
      "        vf_loss: 0.2409427911043167\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.91918659210205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009605852887034416\n",
      "        model: {}\n",
      "        policy_loss: -0.262836217880249\n",
      "        total_loss: -0.15605074167251587\n",
      "        vf_explained_var: -0.4551595151424408\n",
      "        vf_loss: 0.09447608888149261\n",
      "  num_agent_steps_sampled: 110902\n",
      "  num_agent_steps_trained: 110902\n",
      "  num_steps_sampled: 55451\n",
      "  num_steps_trained: 55451\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 136\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.71395348837209\n",
      "  ram_util_percent: 37.87596899224806\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.35407999999999235\n",
      "  blue_1: 0.11509999999999408\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5768233598153865\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.90989733868646\n",
      "  mean_inference_ms: 2.8386835723440105\n",
      "  mean_raw_obs_processing_ms: 30.23548746532505\n",
      "time_since_restore: 8077.752474784851\n",
      "time_this_iter_s: 88.16120338439941\n",
      "time_total_s: 8077.752474784851\n",
      "timers:\n",
      "  learn_throughput: 843.332\n",
      "  learn_time_ms: 656.088\n",
      "  load_throughput: 1109590.439\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.067\n",
      "  sample_time_ms: 78297.245\n",
      "timestamp: 1638991027\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 55451\n",
      "training_iteration: 136\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:136 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 328 0.005 1.639999999999987\n",
      "blue_1 True True 328 -1.998 -0.25300000000001144\n",
      "agent_timesteps_total: 111558\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-17-51\n",
      "done: false\n",
      "episode_len_mean: 244.89\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.49259999999997034\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 277\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.661981582641602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015676135197281837\n",
      "        model: {}\n",
      "        policy_loss: -0.4245334267616272\n",
      "        total_loss: -0.3825525939464569\n",
      "        vf_explained_var: -0.9315491318702698\n",
      "        vf_loss: 0.015196716412901878\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.09349250793457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009546499699354172\n",
      "        model: {}\n",
      "        policy_loss: -0.4975958466529846\n",
      "        total_loss: -0.43662741780281067\n",
      "        vf_explained_var: 0.007344738580286503\n",
      "        vf_loss: 0.048735082149505615\n",
      "  num_agent_steps_sampled: 111558\n",
      "  num_agent_steps_trained: 111558\n",
      "  num_steps_sampled: 55779\n",
      "  num_steps_trained: 55779\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 137\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.35409836065573\n",
      "  ram_util_percent: 37.77540983606556\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3652299999999922\n",
      "  blue_1: 0.12736999999999393\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5768107986120299\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.86951729110662\n",
      "  mean_inference_ms: 2.8383599007227525\n",
      "  mean_raw_obs_processing_ms: 30.195540632629317\n",
      "time_since_restore: 8116.555566310883\n",
      "time_this_iter_s: 38.803091526031494\n",
      "time_total_s: 8116.555566310883\n",
      "timers:\n",
      "  learn_throughput: 840.627\n",
      "  learn_time_ms: 658.081\n",
      "  load_throughput: 1386571.634\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.099\n",
      "  sample_time_ms: 77927.891\n",
      "timestamp: 1638991071\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 55779\n",
      "training_iteration: 137\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:137 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 64 -2.0 -1.6849999999999998\n",
      "blue_1 True True 64 0.005 0.3200000000000002\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 225 0.005 1.124999999999998\n",
      "blue_1 True True 225 -2.0 -0.8930000000000016\n",
      "agent_timesteps_total: 112136\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-18-47\n",
      "done: false\n",
      "episode_len_mean: 246.01\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5036699999999701\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 279\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.105207443237305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015330255962908268\n",
      "        model: {}\n",
      "        policy_loss: -0.0029511149041354656\n",
      "        total_loss: 0.47901150584220886\n",
      "        vf_explained_var: -0.35662031173706055\n",
      "        vf_loss: 0.45576944947242737\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.956338882446289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011309653520584106\n",
      "        model: {}\n",
      "        policy_loss: -0.34593623876571655\n",
      "        total_loss: -0.20987637341022491\n",
      "        vf_explained_var: 0.3984256386756897\n",
      "        vf_loss: 0.12156716734170914\n",
      "  num_agent_steps_sampled: 112136\n",
      "  num_agent_steps_trained: 112136\n",
      "  num_steps_sampled: 56068\n",
      "  num_steps_trained: 56068\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 138\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.819480519480514\n",
      "  ram_util_percent: 37.67662337662336\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3507799999999922\n",
      "  blue_1: 0.15288999999999395\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5767840336676813\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.79044201993027\n",
      "  mean_inference_ms: 2.837699748329133\n",
      "  mean_raw_obs_processing_ms: 30.116746039850646\n",
      "time_since_restore: 8166.367273569107\n",
      "time_this_iter_s: 49.81170725822449\n",
      "time_total_s: 8166.367273569107\n",
      "timers:\n",
      "  learn_throughput: 832.503\n",
      "  learn_time_ms: 625.824\n",
      "  load_throughput: 1305785.709\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.937\n",
      "  sample_time_ms: 75103.529\n",
      "timestamp: 1638991127\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 56068\n",
      "training_iteration: 138\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:138 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 136 0.005 0.6800000000000005\n",
      "blue_1 True True 136 -2.0 -1.3249999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 79 -2.0 -1.6099999999999999\n",
      "blue_1 True True 79 0.005 0.39500000000000024\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 281 -2.0 -0.6000000000000079\n",
      "blue_1 True True 281 0.005 1.4189999999999918\n",
      "agent_timesteps_total: 113128\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-20-08\n",
      "done: false\n",
      "episode_len_mean: 247.21\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.51580999999997\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 282\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.022858142852783\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01579304039478302\n",
      "        model: {}\n",
      "        policy_loss: -0.37615764141082764\n",
      "        total_loss: 0.1266251504421234\n",
      "        vf_explained_var: -0.3437144458293915\n",
      "        vf_loss: 0.4757988750934601\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.898818492889404\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020785728469491005\n",
      "        model: {}\n",
      "        policy_loss: 0.05342530831694603\n",
      "        total_loss: 0.5176262855529785\n",
      "        vf_explained_var: -0.0940534770488739\n",
      "        vf_loss: 0.4375651478767395\n",
      "  num_agent_steps_sampled: 113128\n",
      "  num_agent_steps_trained: 113128\n",
      "  num_steps_sampled: 56564\n",
      "  num_steps_trained: 56564\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 139\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.79732142857142\n",
      "  ram_util_percent: 37.16696428571429\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.33672999999999215\n",
      "  blue_1: 0.17907999999999386\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5767440932032099\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.67212391673678\n",
      "  mean_inference_ms: 2.836757494472301\n",
      "  mean_raw_obs_processing_ms: 30.00037561143183\n",
      "time_since_restore: 8241.713588237762\n",
      "time_this_iter_s: 75.3463146686554\n",
      "time_total_s: 8241.713588237762\n",
      "timers:\n",
      "  learn_throughput: 846.281\n",
      "  learn_time_ms: 609.963\n",
      "  load_throughput: 1034151.569\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.765\n",
      "  sample_time_ms: 76299.927\n",
      "timestamp: 1638991208\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 56564\n",
      "training_iteration: 139\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:139 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 196 0.005 0.9800000000000008\n",
      "blue_1 True True 196 -2.0 -1.0249999999999992\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 56 0.005 0.28000000000000014\n",
      "blue_1 True True 56 -2.0 -1.7249999999999999\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 127 -2.0 -1.3699999999999997\n",
      "blue_1 True True 127 0.005 0.6350000000000005\n",
      "agent_timesteps_total: 113886\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-21-19\n",
      "done: false\n",
      "episode_len_mean: 243.97\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.48072999999997035\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 285\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.124265670776367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009304244071245193\n",
      "        model: {}\n",
      "        policy_loss: -0.671421468257904\n",
      "        total_loss: -0.6031703352928162\n",
      "        vf_explained_var: -0.09001973271369934\n",
      "        vf_loss: 0.052354030311107635\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.368268013000488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011532949283719063\n",
      "        model: {}\n",
      "        policy_loss: 0.4137931764125824\n",
      "        total_loss: 0.8063009977340698\n",
      "        vf_explained_var: 0.5215973258018494\n",
      "        vf_loss: 0.37033960223197937\n",
      "  num_agent_steps_sampled: 113886\n",
      "  num_agent_steps_trained: 113886\n",
      "  num_steps_sampled: 56943\n",
      "  num_steps_trained: 56943\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 140\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.43163265306123\n",
      "  ram_util_percent: 37.00918367346939\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3178499999999922\n",
      "  blue_1: 0.16287999999999406\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5766966428870551\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.56031733751705\n",
      "  mean_inference_ms: 2.8358444569488044\n",
      "  mean_raw_obs_processing_ms: 29.88841914027624\n",
      "time_since_restore: 8307.154438018799\n",
      "time_this_iter_s: 65.44084978103638\n",
      "time_total_s: 8307.154438018799\n",
      "timers:\n",
      "  learn_throughput: 857.972\n",
      "  learn_time_ms: 592.211\n",
      "  load_throughput: 1017924.084\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.611\n",
      "  sample_time_ms: 76857.856\n",
      "timestamp: 1638991279\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 56943\n",
      "training_iteration: 140\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:140 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 72 -2.0 -1.6449999999999998\n",
      "blue_1 True True 72 0.005 0.3600000000000002\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 213 0.005 1.0649999999999993\n",
      "blue_1 True True 213 -2.0 -0.9400000000000006\n",
      "agent_timesteps_total: 114456\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-22-06\n",
      "done: false\n",
      "episode_len_mean: 243.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.4774299999999705\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 287\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.956353187561035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017744533717632294\n",
      "        model: {}\n",
      "        policy_loss: -0.05793922394514084\n",
      "        total_loss: 0.37085309624671936\n",
      "        vf_explained_var: 0.07925399392843246\n",
      "        vf_loss: 0.3984741270542145\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.699061870574951\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009732301346957684\n",
      "        model: {}\n",
      "        policy_loss: -0.27667829394340515\n",
      "        total_loss: -0.005204604938626289\n",
      "        vf_explained_var: -0.3437120020389557\n",
      "        vf_loss: 0.25276654958724976\n",
      "  num_agent_steps_sampled: 114456\n",
      "  num_agent_steps_trained: 114456\n",
      "  num_steps_sampled: 57228\n",
      "  num_steps_trained: 57228\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 141\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.122727272727275\n",
      "  ram_util_percent: 37.01969696969697\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3161999999999922\n",
      "  blue_1: 0.16122999999999402\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5766819815264552\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.48704503938217\n",
      "  mean_inference_ms: 2.8352595242217307\n",
      "  mean_raw_obs_processing_ms: 29.81377254289597\n",
      "time_since_restore: 8349.01448917389\n",
      "time_this_iter_s: 41.86005115509033\n",
      "time_total_s: 8349.01448917389\n",
      "timers:\n",
      "  learn_throughput: 858.801\n",
      "  learn_time_ms: 574.755\n",
      "  load_throughput: 824167.378\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 6.642\n",
      "  sample_time_ms: 74318.494\n",
      "timestamp: 1638991326\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 57228\n",
      "training_iteration: 141\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:140 starting ! -----------------\n",
      "agent_timesteps_total: 114456\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-22-06\n",
      "done: false\n",
      "episode_len_mean: 243.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.4774299999999705\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 287\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.956353187561035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017744533717632294\n",
      "        model: {}\n",
      "        policy_loss: -0.05793922394514084\n",
      "        total_loss: 0.37085309624671936\n",
      "        vf_explained_var: 0.07925399392843246\n",
      "        vf_loss: 0.3984741270542145\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.699061870574951\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009732301346957684\n",
      "        model: {}\n",
      "        policy_loss: -0.27667829394340515\n",
      "        total_loss: -0.005204604938626289\n",
      "        vf_explained_var: -0.3437120020389557\n",
      "        vf_loss: 0.25276654958724976\n",
      "  num_agent_steps_sampled: 114456\n",
      "  num_agent_steps_trained: 114456\n",
      "  num_steps_sampled: 57228\n",
      "  num_steps_trained: 57228\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 141\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.122727272727275\n",
      "  ram_util_percent: 37.01969696969697\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3161999999999922\n",
      "  blue_1: 0.16122999999999402\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5766819815264552\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.48704503938217\n",
      "  mean_inference_ms: 2.8352595242217307\n",
      "  mean_raw_obs_processing_ms: 29.81377254289597\n",
      "time_since_restore: 8349.01448917389\n",
      "time_this_iter_s: 41.86005115509033\n",
      "time_total_s: 8349.01448917389\n",
      "timers:\n",
      "  learn_throughput: 858.801\n",
      "  learn_time_ms: 574.755\n",
      "  load_throughput: 824167.378\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 6.642\n",
      "  sample_time_ms: 74318.494\n",
      "timestamp: 1638991326\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 57228\n",
      "training_iteration: 141\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:141 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 267 -2.0 -0.6700000000000064\n",
      "blue_1 True True 267 0.005 1.3349999999999935\n",
      "agent_timesteps_total: 114990\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-22-45\n",
      "done: false\n",
      "episode_len_mean: 244.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.4892299999999702\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 288\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.062271118164062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00988882314413786\n",
      "        model: {}\n",
      "        policy_loss: -0.1716659516096115\n",
      "        total_loss: -0.04405174404382706\n",
      "        vf_explained_var: 0.48781096935272217\n",
      "        vf_loss: 0.11071821302175522\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.631226539611816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012559971772134304\n",
      "        model: {}\n",
      "        policy_loss: -0.14182160794734955\n",
      "        total_loss: 0.13274754583835602\n",
      "        vf_explained_var: -0.6857044100761414\n",
      "        vf_loss: 0.2504267990589142\n",
      "  num_agent_steps_sampled: 114990\n",
      "  num_agent_steps_trained: 114990\n",
      "  num_steps_sampled: 57495\n",
      "  num_steps_trained: 57495\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 142\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.39814814814814\n",
      "  ram_util_percent: 36.99074074074075\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.30204999999999216\n",
      "  blue_1: 0.18717999999999396\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5766709290622738\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.45036144810828\n",
      "  mean_inference_ms: 2.8349713381662447\n",
      "  mean_raw_obs_processing_ms: 29.775946133597422\n",
      "time_since_restore: 8382.275111913681\n",
      "time_this_iter_s: 33.26062273979187\n",
      "time_total_s: 8382.275111913681\n",
      "timers:\n",
      "  learn_throughput: 849.378\n",
      "  learn_time_ms: 543.338\n",
      "  load_throughput: 770569.783\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 6.598\n",
      "  sample_time_ms: 69944.207\n",
      "timestamp: 1638991365\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 57495\n",
      "training_iteration: 142\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:142 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 243 0.005 1.214999999999996\n",
      "blue_1 True True 243 -2.0 -0.7900000000000038\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 321 0.005 1.6049999999999878\n",
      "blue_1 True True 321 -2.0 -0.4300000000000088\n",
      "agent_timesteps_total: 116118\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-24-00\n",
      "done: false\n",
      "episode_len_mean: 247.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5136299999999696\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 290\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.064812660217285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018664229661226273\n",
      "        model: {}\n",
      "        policy_loss: -0.062953881919384\n",
      "        total_loss: 0.006846689153462648\n",
      "        vf_explained_var: -0.5202711820602417\n",
      "        vf_loss: 0.03791099414229393\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.434231758117676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01058482937514782\n",
      "        model: {}\n",
      "        policy_loss: -0.24298395216464996\n",
      "        total_loss: -0.033605560660362244\n",
      "        vf_explained_var: 0.41621601581573486\n",
      "        vf_loss: 0.18903255462646484\n",
      "  num_agent_steps_sampled: 116118\n",
      "  num_agent_steps_trained: 116118\n",
      "  num_steps_sampled: 58059\n",
      "  num_steps_trained: 58059\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 143\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.611538461538466\n",
      "  ram_util_percent: 37.01346153846154\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.35449999999999193\n",
      "  blue_1: 0.15912999999999383\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5766430417733498\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.37293216558402\n",
      "  mean_inference_ms: 2.834307128240477\n",
      "  mean_raw_obs_processing_ms: 29.698573484347694\n",
      "time_since_restore: 8451.915451526642\n",
      "time_this_iter_s: 69.64033961296082\n",
      "time_total_s: 8451.915451526642\n",
      "timers:\n",
      "  learn_throughput: 845.163\n",
      "  learn_time_ms: 518.006\n",
      "  load_throughput: 730997.727\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 6.622\n",
      "  sample_time_ms: 66114.338\n",
      "timestamp: 1638991440\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 58059\n",
      "training_iteration: 143\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:143 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 442 -0.995 1.209999999999975\n",
      "blue_1 False True 442 -0.993 1.511999999999993\n",
      "agent_timesteps_total: 117002\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-24-59\n",
      "done: false\n",
      "episode_len_mean: 249.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5400199999999693\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 291\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.139104843139648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008508915081620216\n",
      "        model: {}\n",
      "        policy_loss: -0.33430251479148865\n",
      "        total_loss: -0.28053298592567444\n",
      "        vf_explained_var: -0.6495973467826843\n",
      "        vf_loss: 0.03923126682639122\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.395119667053223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009845821186900139\n",
      "        model: {}\n",
      "        policy_loss: -0.30056944489479065\n",
      "        total_loss: -0.12170184403657913\n",
      "        vf_explained_var: -0.7341296672821045\n",
      "        vf_loss: 0.15994228422641754\n",
      "  num_agent_steps_sampled: 117002\n",
      "  num_agent_steps_trained: 117002\n",
      "  num_steps_sampled: 58501\n",
      "  num_steps_trained: 58501\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 144\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.2962962962963\n",
      "  ram_util_percent: 37.04197530864198\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3560999999999917\n",
      "  blue_1: 0.18391999999999378\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5766253676076082\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.3329002485862\n",
      "  mean_inference_ms: 2.8339312671527823\n",
      "  mean_raw_obs_processing_ms: 29.658687457464506\n",
      "time_since_restore: 8505.718952417374\n",
      "time_this_iter_s: 53.80350089073181\n",
      "time_total_s: 8505.718952417374\n",
      "timers:\n",
      "  learn_throughput: 844.245\n",
      "  learn_time_ms: 501.75\n",
      "  load_throughput: 843840.976\n",
      "  load_time_ms: 0.502\n",
      "  sample_throughput: 6.607\n",
      "  sample_time_ms: 64112.73\n",
      "timestamp: 1638991499\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 58501\n",
      "training_iteration: 144\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:144 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 130 0.005 0.6500000000000005\n",
      "blue_1 True True 130 -2.0 -1.3549999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 128 0.005 0.6400000000000005\n",
      "blue_1 True True 128 -2.0 -1.3649999999999995\n",
      "agent_timesteps_total: 117518\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-25-48\n",
      "done: false\n",
      "episode_len_mean: 249.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5425199999999691\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 293\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.651008129119873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014011679217219353\n",
      "        model: {}\n",
      "        policy_loss: -0.07763142883777618\n",
      "        total_loss: -0.03359578549861908\n",
      "        vf_explained_var: 0.22108861804008484\n",
      "        vf_loss: 0.020095378160476685\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.839724540710449\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012291808612644672\n",
      "        model: {}\n",
      "        policy_loss: -0.08440590649843216\n",
      "        total_loss: 0.8360010385513306\n",
      "        vf_explained_var: 0.2954517900943756\n",
      "        vf_loss: 0.8967799544334412\n",
      "  num_agent_steps_sampled: 117518\n",
      "  num_agent_steps_trained: 117518\n",
      "  num_steps_sampled: 58759\n",
      "  num_steps_trained: 58759\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 145\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.32238805970149\n",
      "  ram_util_percent: 36.974626865671645\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3573499999999916\n",
      "  blue_1: 0.18516999999999373\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5765838582712276\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.2536406841643\n",
      "  mean_inference_ms: 2.8332218025022327\n",
      "  mean_raw_obs_processing_ms: 29.581459463406595\n",
      "time_since_restore: 8548.729745864868\n",
      "time_this_iter_s: 43.01079344749451\n",
      "time_total_s: 8548.729745864868\n",
      "timers:\n",
      "  learn_throughput: 827.07\n",
      "  learn_time_ms: 483.998\n",
      "  load_throughput: 995245.934\n",
      "  load_time_ms: 0.402\n",
      "  sample_throughput: 6.509\n",
      "  sample_time_ms: 61496.903\n",
      "timestamp: 1638991548\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 58759\n",
      "training_iteration: 145\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:145 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 422 -2.0 0.10499999999997733\n",
      "blue_1 True True 422 0.007 2.4339999999999926\n",
      "agent_timesteps_total: 118362\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-26-45\n",
      "done: false\n",
      "episode_len_mean: 250.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5440499999999691\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 294\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.914447784423828\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007890220731496811\n",
      "        model: {}\n",
      "        policy_loss: -0.27235081791877747\n",
      "        total_loss: -0.09347325563430786\n",
      "        vf_explained_var: -0.4254544973373413\n",
      "        vf_loss: 0.16539637744426727\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.485901832580566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005572283174842596\n",
      "        model: {}\n",
      "        policy_loss: -0.25360000133514404\n",
      "        total_loss: 0.10043410956859589\n",
      "        vf_explained_var: -0.7745009660720825\n",
      "        vf_loss: 0.3433232605457306\n",
      "  num_agent_steps_sampled: 118362\n",
      "  num_agent_steps_trained: 118362\n",
      "  num_steps_sampled: 59181\n",
      "  num_steps_trained: 59181\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 146\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.795\n",
      "  ram_util_percent: 37.12\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.36044999999999155\n",
      "  blue_1: 0.18359999999999374\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5765650329647656\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.2134007379109\n",
      "  mean_inference_ms: 2.83286681861228\n",
      "  mean_raw_obs_processing_ms: 29.541835680023752\n",
      "time_since_restore: 8600.575051784515\n",
      "time_this_iter_s: 51.84530591964722\n",
      "time_total_s: 8600.575051784515\n",
      "timers:\n",
      "  learn_throughput: 816.959\n",
      "  learn_time_ms: 456.571\n",
      "  load_throughput: 934573.114\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.445\n",
      "  sample_time_ms: 57873.506\n",
      "timestamp: 1638991605\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 59181\n",
      "training_iteration: 146\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:146 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 293 -2.0 -0.5400000000000091\n",
      "blue_1 True True 293 0.005 1.4489999999999925\n",
      "agent_timesteps_total: 118948\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-27-29\n",
      "done: false\n",
      "episode_len_mean: 252.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5673899999999688\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 295\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.268702507019043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007259249687194824\n",
      "        model: {}\n",
      "        policy_loss: -0.3361699879169464\n",
      "        total_loss: -0.1546652764081955\n",
      "        vf_explained_var: -0.5109124779701233\n",
      "        vf_loss: 0.16910164058208466\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.496622085571289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0118093965575099\n",
      "        model: {}\n",
      "        policy_loss: -0.13793452084064484\n",
      "        total_loss: -0.08173477649688721\n",
      "        vf_explained_var: -0.4281461834907532\n",
      "        vf_loss: 0.03350009024143219\n",
      "  num_agent_steps_sampled: 118948\n",
      "  num_agent_steps_trained: 118948\n",
      "  num_steps_sampled: 59474\n",
      "  num_steps_trained: 59474\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 147\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.63166666666667\n",
      "  ram_util_percent: 37.19833333333333\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.37219999999999154\n",
      "  blue_1: 0.19518999999999362\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5765477801437938\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.17051770866463\n",
      "  mean_inference_ms: 2.832513123744404\n",
      "  mean_raw_obs_processing_ms: 29.500585289249237\n",
      "time_since_restore: 8638.689659357071\n",
      "time_this_iter_s: 38.11460757255554\n",
      "time_total_s: 8638.689659357071\n",
      "timers:\n",
      "  learn_throughput: 817.04\n",
      "  learn_time_ms: 452.242\n",
      "  load_throughput: 925803.661\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.395\n",
      "  sample_time_ms: 57780.661\n",
      "timestamp: 1638991649\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 59474\n",
      "training_iteration: 147\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:147 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 437 -0.995 1.1849999999999756\n",
      "blue_1 False True 437 -0.976 1.487999999999992\n",
      "agent_timesteps_total: 119822\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-28-24\n",
      "done: false\n",
      "episode_len_mean: 255.94\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6015699999999682\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 296\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.630159378051758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009842021390795708\n",
      "        model: {}\n",
      "        policy_loss: -0.3033324182033539\n",
      "        total_loss: -0.18127979338169098\n",
      "        vf_explained_var: -0.7175369262695312\n",
      "        vf_loss: 0.10523660480976105\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.201803207397461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008312609978020191\n",
      "        model: {}\n",
      "        policy_loss: -0.3147439658641815\n",
      "        total_loss: -0.2424619495868683\n",
      "        vf_explained_var: -0.2843326926231384\n",
      "        vf_loss: 0.05630377307534218\n",
      "  num_agent_steps_sampled: 119822\n",
      "  num_agent_steps_trained: 119822\n",
      "  num_steps_sampled: 59911\n",
      "  num_steps_trained: 59911\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 148\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.20131578947369\n",
      "  ram_util_percent: 37.14473684210527\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.37774999999999126\n",
      "  blue_1: 0.22381999999999358\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.576528649796019\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.12639426659908\n",
      "  mean_inference_ms: 2.83215979270867\n",
      "  mean_raw_obs_processing_ms: 29.458259671540176\n",
      "time_since_restore: 8688.259433031082\n",
      "time_this_iter_s: 49.56977367401123\n",
      "time_total_s: 8688.259433031082\n",
      "timers:\n",
      "  learn_throughput: 818.731\n",
      "  learn_time_ms: 469.385\n",
      "  load_throughput: 958192.264\n",
      "  load_time_ms: 0.401\n",
      "  sample_throughput: 6.657\n",
      "  sample_time_ms: 57730.359\n",
      "timestamp: 1638991704\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 59911\n",
      "training_iteration: 148\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:148 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 88 -2.0 -1.5649999999999997\n",
      "blue_1 True True 88 0.005 0.4400000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 66 0.005 0.3300000000000002\n",
      "blue_1 True True 66 -2.0 -1.6749999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 152 -2.0 -1.2449999999999994\n",
      "blue_1 True True 152 0.005 0.7600000000000006\n",
      "agent_timesteps_total: 120434\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-29-32\n",
      "done: false\n",
      "episode_len_mean: 251.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5580899999999689\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 299\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.8783674240112305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015255909413099289\n",
      "        model: {}\n",
      "        policy_loss: -0.28056827187538147\n",
      "        total_loss: 0.30747106671333313\n",
      "        vf_explained_var: 0.14487603306770325\n",
      "        vf_loss: 0.5619732737541199\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.714852333068848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012967048212885857\n",
      "        model: {}\n",
      "        policy_loss: -0.028498150408267975\n",
      "        total_loss: 0.5725472569465637\n",
      "        vf_explained_var: 0.0652298629283905\n",
      "        vf_loss: 0.5761206746101379\n",
      "  num_agent_steps_sampled: 120434\n",
      "  num_agent_steps_trained: 120434\n",
      "  num_steps_sampled: 60217\n",
      "  num_steps_trained: 60217\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 149\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.16989247311828\n",
      "  ram_util_percent: 37.082795698924734\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3360999999999914\n",
      "  blue_1: 0.22198999999999372\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5764744593158607\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 114.00199679495387\n",
      "  mean_inference_ms: 2.8311297606742225\n",
      "  mean_raw_obs_processing_ms: 29.3381132089873\n",
      "time_since_restore: 8750.038056612015\n",
      "time_this_iter_s: 61.77862358093262\n",
      "time_total_s: 8750.038056612015\n",
      "timers:\n",
      "  learn_throughput: 810.295\n",
      "  learn_time_ms: 450.823\n",
      "  load_throughput: 1213991.959\n",
      "  load_time_ms: 0.301\n",
      "  sample_throughput: 6.476\n",
      "  sample_time_ms: 56406.746\n",
      "timestamp: 1638991772\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 60217\n",
      "training_iteration: 149\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:149 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 308 0.005 1.5399999999999892\n",
      "blue_1 True True 308 -2.0 -0.46500000000001074\n",
      "agent_timesteps_total: 121050\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-30-13\n",
      "done: false\n",
      "episode_len_mean: 252.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5688899999999686\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 300\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.328062057495117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005667053163051605\n",
      "        model: {}\n",
      "        policy_loss: -0.3950314223766327\n",
      "        total_loss: -0.1666717529296875\n",
      "        vf_explained_var: -0.9590904116630554\n",
      "        vf_loss: 0.21867699921131134\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.9974236488342285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0079672085121274\n",
      "        model: {}\n",
      "        policy_loss: -0.41792088747024536\n",
      "        total_loss: -0.28603604435920715\n",
      "        vf_explained_var: -0.05856623873114586\n",
      "        vf_loss: 0.11657048761844635\n",
      "  num_agent_steps_sampled: 121050\n",
      "  num_agent_steps_trained: 121050\n",
      "  num_steps_sampled: 60525\n",
      "  num_steps_trained: 60525\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 150\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.843859649122805\n",
      "  ram_util_percent: 37.09122807017545\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3414999999999913\n",
      "  blue_1: 0.22738999999999363\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5764592667161187\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.96044955730403\n",
      "  mean_inference_ms: 2.8308035462632404\n",
      "  mean_raw_obs_processing_ms: 29.298435572666925\n",
      "time_since_restore: 8785.589851856232\n",
      "time_this_iter_s: 35.55179524421692\n",
      "time_total_s: 8785.589851856232\n",
      "timers:\n",
      "  learn_throughput: 792.968\n",
      "  learn_time_ms: 451.721\n",
      "  load_throughput: 893806.706\n",
      "  load_time_ms: 0.401\n",
      "  sample_throughput: 6.708\n",
      "  sample_time_ms: 53400.529\n",
      "timestamp: 1638991813\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 60525\n",
      "training_iteration: 150\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:150 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 158 0.005 0.7900000000000006\n",
      "blue_1 True True 158 -2.0 -1.2149999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 596 0.005 2.9799999999999587\n",
      "blue_1 True True 596 -2.0 1.0069999999999588\n",
      "agent_timesteps_total: 122558\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-31-50\n",
      "done: false\n",
      "episode_len_mean: 255.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5927099999999682\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 302\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.400657653808594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019308585673570633\n",
      "        model: {}\n",
      "        policy_loss: 0.008249957114458084\n",
      "        total_loss: 0.08649646490812302\n",
      "        vf_explained_var: -0.5204455852508545\n",
      "        vf_loss: 0.045255981385707855\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.753559589385986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012039451859891415\n",
      "        model: {}\n",
      "        policy_loss: -0.267452597618103\n",
      "        total_loss: 0.0910070538520813\n",
      "        vf_explained_var: -0.22715751826763153\n",
      "        vf_loss: 0.3353177607059479\n",
      "  num_agent_steps_sampled: 122558\n",
      "  num_agent_steps_trained: 122558\n",
      "  num_steps_sampled: 61279\n",
      "  num_steps_trained: 61279\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 151\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.897014925373135\n",
      "  ram_util_percent: 37.102238805970146\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.37329999999999103\n",
      "  blue_1: 0.2194099999999933\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5764207239219076\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.8798503599971\n",
      "  mean_inference_ms: 2.8301089887476167\n",
      "  mean_raw_obs_processing_ms: 29.218543892453134\n",
      "time_since_restore: 8877.347472190857\n",
      "time_this_iter_s: 91.75762033462524\n",
      "time_total_s: 8877.347472190857\n",
      "timers:\n",
      "  learn_throughput: 812.32\n",
      "  learn_time_ms: 498.695\n",
      "  load_throughput: 1010233.992\n",
      "  load_time_ms: 0.401\n",
      "  sample_throughput: 6.944\n",
      "  sample_time_ms: 58342.288\n",
      "timestamp: 1638991910\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 61279\n",
      "training_iteration: 151\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:150 starting ! -----------------\n",
      "agent_timesteps_total: 122558\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-31-50\n",
      "done: false\n",
      "episode_len_mean: 255.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5927099999999682\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 302\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.400657653808594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019308585673570633\n",
      "        model: {}\n",
      "        policy_loss: 0.008249957114458084\n",
      "        total_loss: 0.08649646490812302\n",
      "        vf_explained_var: -0.5204455852508545\n",
      "        vf_loss: 0.045255981385707855\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.753559589385986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012039451859891415\n",
      "        model: {}\n",
      "        policy_loss: -0.267452597618103\n",
      "        total_loss: 0.0910070538520813\n",
      "        vf_explained_var: -0.22715751826763153\n",
      "        vf_loss: 0.3353177607059479\n",
      "  num_agent_steps_sampled: 122558\n",
      "  num_agent_steps_trained: 122558\n",
      "  num_steps_sampled: 61279\n",
      "  num_steps_trained: 61279\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 151\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.897014925373135\n",
      "  ram_util_percent: 37.102238805970146\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.37329999999999103\n",
      "  blue_1: 0.2194099999999933\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5764207239219076\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.8798503599971\n",
      "  mean_inference_ms: 2.8301089887476167\n",
      "  mean_raw_obs_processing_ms: 29.218543892453134\n",
      "time_since_restore: 8877.347472190857\n",
      "time_this_iter_s: 91.75762033462524\n",
      "time_total_s: 8877.347472190857\n",
      "timers:\n",
      "  learn_throughput: 812.32\n",
      "  learn_time_ms: 498.695\n",
      "  load_throughput: 1010233.992\n",
      "  load_time_ms: 0.401\n",
      "  sample_throughput: 6.944\n",
      "  sample_time_ms: 58342.288\n",
      "timestamp: 1638991910\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 61279\n",
      "training_iteration: 151\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:151 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.0419999999999585\n",
      "agent_timesteps_total: 123758\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-33-06\n",
      "done: false\n",
      "episode_len_mean: 259.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6352799999999672\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 303\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.55877685546875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005980545189231634\n",
      "        model: {}\n",
      "        policy_loss: -0.3507902920246124\n",
      "        total_loss: -0.3268652558326721\n",
      "        vf_explained_var: -0.6654624938964844\n",
      "        vf_loss: 0.013706672005355358\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.388470649719238\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004696263931691647\n",
      "        model: {}\n",
      "        policy_loss: -0.37468650937080383\n",
      "        total_loss: -0.320475697517395\n",
      "        vf_explained_var: -0.5976698398590088\n",
      "        vf_loss: 0.045183777809143066\n",
      "  num_agent_steps_sampled: 123758\n",
      "  num_agent_steps_trained: 123758\n",
      "  num_steps_sampled: 61879\n",
      "  num_steps_trained: 61879\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 152\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.52692307692307\n",
      "  ram_util_percent: 37.18076923076923\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3843499999999906\n",
      "  blue_1: 0.2509299999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5763980385613618\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.83935317936252\n",
      "  mean_inference_ms: 2.8297416244197704\n",
      "  mean_raw_obs_processing_ms: 29.1770467163012\n",
      "time_since_restore: 8948.058383703232\n",
      "time_this_iter_s: 70.71091151237488\n",
      "time_total_s: 8948.058383703232\n",
      "timers:\n",
      "  learn_throughput: 824.364\n",
      "  learn_time_ms: 531.804\n",
      "  load_throughput: 875444.141\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.059\n",
      "  sample_time_ms: 62100.917\n",
      "timestamp: 1638991986\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 61879\n",
      "training_iteration: 152\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:152 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 191 -2.0 -1.0499999999999994\n",
      "blue_1 True True 191 0.005 0.9550000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 189 0.005 0.9450000000000007\n",
      "blue_1 True True 189 -2.001 -1.068999999999999\n",
      "agent_timesteps_total: 124518\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-34-07\n",
      "done: false\n",
      "episode_len_mean: 257.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6201299999999677\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 305\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.047163963317871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013533727265894413\n",
      "        model: {}\n",
      "        policy_loss: 0.24905388057231903\n",
      "        total_loss: 0.8419859409332275\n",
      "        vf_explained_var: 0.33076319098472595\n",
      "        vf_loss: 0.5698084235191345\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9610840082168579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.740567207336426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011716536246240139\n",
      "        model: {}\n",
      "        policy_loss: -0.6395800113677979\n",
      "        total_loss: -0.5778610110282898\n",
      "        vf_explained_var: -0.15082387626171112\n",
      "        vf_loss: 0.050458524376153946\n",
      "  num_agent_steps_sampled: 124518\n",
      "  num_agent_steps_trained: 124518\n",
      "  num_steps_sampled: 62259\n",
      "  num_steps_trained: 62259\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 153\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.77023809523809\n",
      "  ram_util_percent: 37.18095238095238\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3969999999999907\n",
      "  blue_1: 0.22312999999999303\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.576349508170164\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.76088905227964\n",
      "  mean_inference_ms: 2.8290395117311316\n",
      "  mean_raw_obs_processing_ms: 29.096539103507208\n",
      "time_since_restore: 9002.877694129944\n",
      "time_this_iter_s: 54.819310426712036\n",
      "time_total_s: 9002.877694129944\n",
      "timers:\n",
      "  learn_throughput: 837.846\n",
      "  learn_time_ms: 501.285\n",
      "  load_throughput: 699383.707\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 6.921\n",
      "  sample_time_ms: 60682.488\n",
      "timestamp: 1638992047\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 62259\n",
      "training_iteration: 153\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:153 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 158 0.005 0.7900000000000006\n",
      "blue_1 True True 158 -2.0 -1.2149999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 402 0.005 2.0099999999999794\n",
      "blue_1 True True 402 -1.998 0.07399999999998319\n",
      "agent_timesteps_total: 125638\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-35-25\n",
      "done: false\n",
      "episode_len_mean: 259.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6379099999999672\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 307\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.046490669250488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012560492381453514\n",
      "        model: {}\n",
      "        policy_loss: -0.0889715701341629\n",
      "        total_loss: 0.022559847682714462\n",
      "        vf_explained_var: -0.28001177310943604\n",
      "        vf_loss: 0.09007064253091812\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9610840082168579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.8561320304870605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02120182104408741\n",
      "        model: {}\n",
      "        policy_loss: -0.24902327358722687\n",
      "        total_loss: 0.03707953914999962\n",
      "        vf_explained_var: 0.2707900106906891\n",
      "        vf_loss: 0.26572608947753906\n",
      "  num_agent_steps_sampled: 125638\n",
      "  num_agent_steps_trained: 125638\n",
      "  num_steps_sampled: 62819\n",
      "  num_steps_trained: 62819\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 154\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.66448598130842\n",
      "  ram_util_percent: 37.18878504672897\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.40549999999999053\n",
      "  blue_1: 0.2324099999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5762944498209827\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.68648020532105\n",
      "  mean_inference_ms: 2.8283469742081637\n",
      "  mean_raw_obs_processing_ms: 29.01600865544111\n",
      "time_since_restore: 9075.700192689896\n",
      "time_this_iter_s: 72.82249855995178\n",
      "time_total_s: 9075.700192689896\n",
      "timers:\n",
      "  learn_throughput: 835.944\n",
      "  learn_time_ms: 516.542\n",
      "  load_throughput: 616628.806\n",
      "  load_time_ms: 0.7\n",
      "  sample_throughput: 6.904\n",
      "  sample_time_ms: 62540.845\n",
      "timestamp: 1638992125\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 62819\n",
      "training_iteration: 154\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:154 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 271 0.005 1.354999999999993\n",
      "blue_1 True True 271 -2.0 -0.6160000000000068\n",
      "agent_timesteps_total: 126180\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-36-05\n",
      "done: false\n",
      "episode_len_mean: 257.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.614029999999968\n",
      "episode_reward_min: -1.6249999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 308\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.078611373901367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013758682645857334\n",
      "        model: {}\n",
      "        policy_loss: -0.15919996798038483\n",
      "        total_loss: -0.1281542181968689\n",
      "        vf_explained_var: -0.44966983795166016\n",
      "        vf_loss: 0.00753773981705308\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.441625952720642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.32029914855957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013673127628862858\n",
      "        model: {}\n",
      "        policy_loss: -0.21798580884933472\n",
      "        total_loss: 0.044688574969768524\n",
      "        vf_explained_var: 0.40039756894111633\n",
      "        vf_loss: 0.24296283721923828\n",
      "  num_agent_steps_sampled: 126180\n",
      "  num_agent_steps_trained: 126180\n",
      "  num_steps_sampled: 63090\n",
      "  num_steps_trained: 63090\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 155\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.04642857142857\n",
      "  ram_util_percent: 37.18571428571429\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.4033999999999907\n",
      "  blue_1: 0.2106299999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.815\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5762660047072666\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.65047370968017\n",
      "  mean_inference_ms: 2.8280311527890856\n",
      "  mean_raw_obs_processing_ms: 28.978233539062803\n",
      "time_since_restore: 9110.20667552948\n",
      "time_this_iter_s: 34.50648283958435\n",
      "time_total_s: 9110.20667552948\n",
      "timers:\n",
      "  learn_throughput: 837.273\n",
      "  learn_time_ms: 517.275\n",
      "  load_throughput: 618485.262\n",
      "  load_time_ms: 0.7\n",
      "  sample_throughput: 7.019\n",
      "  sample_time_ms: 61706.376\n",
      "timestamp: 1638992165\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 63090\n",
      "training_iteration: 155\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:155 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 60 -2.0 -1.7049999999999998\n",
      "blue_1 True True 60 0.005 0.30000000000000016\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 141 -2.0 -1.2999999999999994\n",
      "blue_1 True True 141 0.005 0.7050000000000005\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 204 -2.0 -0.9849999999999997\n",
      "blue_1 True True 204 0.005 1.0200000000000002\n",
      "agent_timesteps_total: 126990\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-37-16\n",
      "done: false\n",
      "episode_len_mean: 257.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6220299999999679\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 311\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.173425674438477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013470185920596123\n",
      "        model: {}\n",
      "        policy_loss: -0.15325339138507843\n",
      "        total_loss: 0.4041772484779358\n",
      "        vf_explained_var: 0.19057820737361908\n",
      "        vf_loss: 0.5344155430793762\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.441625952720642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.147035598754883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016950443387031555\n",
      "        model: {}\n",
      "        policy_loss: -0.11906023323535919\n",
      "        total_loss: 0.012510836124420166\n",
      "        vf_explained_var: -0.6131784915924072\n",
      "        vf_loss: 0.1071348711848259\n",
      "  num_agent_steps_sampled: 126990\n",
      "  num_agent_steps_trained: 126990\n",
      "  num_steps_sampled: 63495\n",
      "  num_steps_trained: 63495\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 156\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.655102040816324\n",
      "  ram_util_percent: 37.173469387755105\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.38734999999999076\n",
      "  blue_1: 0.23467999999999298\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5761818929827779\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.54243025189552\n",
      "  mean_inference_ms: 2.8270556518698102\n",
      "  mean_raw_obs_processing_ms: 28.86237281636363\n",
      "time_since_restore: 9175.456979990005\n",
      "time_this_iter_s: 65.25030446052551\n",
      "time_total_s: 9175.456979990005\n",
      "timers:\n",
      "  learn_throughput: 837.621\n",
      "  learn_time_ms: 515.03\n",
      "  load_throughput: 615785.035\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.842\n",
      "  sample_time_ms: 63050.598\n",
      "timestamp: 1638992236\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 63495\n",
      "training_iteration: 156\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:156 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 239 0.005 1.1949999999999965\n",
      "blue_1 True True 239 -2.001 -0.8150000000000028\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 191 0.005 0.9550000000000007\n",
      "blue_1 True True 191 -1.998 -1.0419999999999994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 127850\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-38-22\n",
      "done: false\n",
      "episode_len_mean: 259.32\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6366599999999677\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 313\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.080831527709961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011028318665921688\n",
      "        model: {}\n",
      "        policy_loss: -0.2562921941280365\n",
      "        total_loss: -0.007282448932528496\n",
      "        vf_explained_var: -0.337415486574173\n",
      "        vf_loss: 0.23016683757305145\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.441625952720642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.923246383666992\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014388855546712875\n",
      "        model: {}\n",
      "        policy_loss: -0.25913381576538086\n",
      "        total_loss: 0.03869170695543289\n",
      "        vf_explained_var: 0.49184921383857727\n",
      "        vf_loss: 0.2770822048187256\n",
      "  num_agent_steps_sampled: 127850\n",
      "  num_agent_steps_trained: 127850\n",
      "  num_steps_sampled: 63925\n",
      "  num_steps_trained: 63925\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 157\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.548351648351655\n",
      "  ram_util_percent: 37.18571428571428\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.1949999999999963\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.41469999999999074\n",
      "  blue_1: 0.22195999999999294\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5761276493168045\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.47081751557751\n",
      "  mean_inference_ms: 2.8263996015415573\n",
      "  mean_raw_obs_processing_ms: 28.783750718122747\n",
      "time_since_restore: 9235.943639993668\n",
      "time_this_iter_s: 60.48666000366211\n",
      "time_total_s: 9235.943639993668\n",
      "timers:\n",
      "  learn_throughput: 834.421\n",
      "  learn_time_ms: 533.424\n",
      "  load_throughput: 558061.971\n",
      "  load_time_ms: 0.798\n",
      "  sample_throughput: 6.82\n",
      "  sample_time_ms: 65266.36\n",
      "timestamp: 1638992302\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 63925\n",
      "training_iteration: 157\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:157 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 536 -2.0 0.6749999999999652\n",
      "blue_1 True True 536 0.005 3.2939999999999676\n",
      "agent_timesteps_total: 128922\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-39-31\n",
      "done: false\n",
      "episode_len_mean: 258.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.636189999999968\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 314\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.65750503540039\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0059539335779845715\n",
      "        model: {}\n",
      "        policy_loss: -0.16772031784057617\n",
      "        total_loss: 0.03352265805006027\n",
      "        vf_explained_var: -0.5141927599906921\n",
      "        vf_loss: 0.19107013940811157\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.441625952720642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.339249134063721\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021996308118104935\n",
      "        model: {}\n",
      "        policy_loss: -0.14978477358818054\n",
      "        total_loss: 0.1257835179567337\n",
      "        vf_explained_var: -0.4591585099697113\n",
      "        vf_loss: 0.24385784566402435\n",
      "  num_agent_steps_sampled: 128922\n",
      "  num_agent_steps_trained: 128922\n",
      "  num_steps_sampled: 64461\n",
      "  num_steps_trained: 64461\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 158\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.33368421052631\n",
      "  ram_util_percent: 37.21789473684209\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.4014499999999907\n",
      "  blue_1: 0.23473999999999304\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5760975725546911\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.43823237227923\n",
      "  mean_inference_ms: 2.8260781331674023\n",
      "  mean_raw_obs_processing_ms: 28.746430157799605\n",
      "time_since_restore: 9299.536393404007\n",
      "time_this_iter_s: 63.592753410339355\n",
      "time_total_s: 9299.536393404007\n",
      "timers:\n",
      "  learn_throughput: 825.21\n",
      "  learn_time_ms: 551.375\n",
      "  load_throughput: 653833.192\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 6.825\n",
      "  sample_time_ms: 66669.824\n",
      "timestamp: 1638992371\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 64461\n",
      "training_iteration: 158\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:158 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 247 0.005 1.2349999999999957\n",
      "blue_1 True True 247 -2.0 -0.7700000000000042\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 197 0.005 0.9850000000000008\n",
      "blue_1 True True 197 -2.0 -1.0199999999999991\n",
      "agent_timesteps_total: 129810\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-40-31\n",
      "done: false\n",
      "episode_len_mean: 255.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5977299999999687\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 316\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.494442939758301\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007167784497141838\n",
      "        model: {}\n",
      "        policy_loss: -0.2658136487007141\n",
      "        total_loss: -0.24124495685100555\n",
      "        vf_explained_var: -0.08340174704790115\n",
      "        vf_loss: 0.012321882881224155\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.1624388694763184\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.5731401443481445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020086020231246948\n",
      "        model: {}\n",
      "        policy_loss: -0.2689283788204193\n",
      "        total_loss: 0.26947933435440063\n",
      "        vf_explained_var: 0.028937984257936478\n",
      "        vf_loss: 0.49497294425964355\n",
      "  num_agent_steps_sampled: 129810\n",
      "  num_agent_steps_trained: 129810\n",
      "  num_steps_sampled: 64905\n",
      "  num_steps_trained: 64905\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 159\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.551219512195125\n",
      "  ram_util_percent: 37.207317073170735\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.40409999999999124\n",
      "  blue_1: 0.1936299999999934\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5760387186210584\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.37481133753943\n",
      "  mean_inference_ms: 2.8255053871861873\n",
      "  mean_raw_obs_processing_ms: 28.678157679190246\n",
      "time_since_restore: 9353.54671883583\n",
      "time_this_iter_s: 54.01032543182373\n",
      "time_total_s: 9353.54671883583\n",
      "timers:\n",
      "  learn_throughput: 823.123\n",
      "  learn_time_ms: 569.538\n",
      "  load_throughput: 673663.737\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 7.115\n",
      "  sample_time_ms: 65892.849\n",
      "timestamp: 1638992431\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 64905\n",
      "training_iteration: 159\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:159 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 167 0.005 0.8350000000000006\n",
      "blue_1 True True 167 -2.0 -1.1699999999999995\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 159 -2.0 -1.2099999999999995\n",
      "blue_1 True True 159 0.005 0.7950000000000006\n",
      "agent_timesteps_total: 130462\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-41-23\n",
      "done: false\n",
      "episode_len_mean: 256.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6108299999999685\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 318\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.649564266204834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009240971878170967\n",
      "        model: {}\n",
      "        policy_loss: -0.4833656847476959\n",
      "        total_loss: -0.35121822357177734\n",
      "        vf_explained_var: 0.016213243827223778\n",
      "        vf_loss: 0.11635841429233551\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.090672016143799\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010725529864430428\n",
      "        model: {}\n",
      "        policy_loss: 0.0996890515089035\n",
      "        total_loss: 0.5753411054611206\n",
      "        vf_explained_var: 0.20119363069534302\n",
      "        vf_loss: 0.4408620595932007\n",
      "  num_agent_steps_sampled: 130462\n",
      "  num_agent_steps_trained: 130462\n",
      "  num_steps_sampled: 65231\n",
      "  num_steps_trained: 65231\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 160\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.451388888888886\n",
      "  ram_util_percent: 37.21944444444445\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.39059999999999123\n",
      "  blue_1: 0.22022999999999338\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5759737553838442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.3068041588319\n",
      "  mean_inference_ms: 2.824964058160705\n",
      "  mean_raw_obs_processing_ms: 28.60838051721237\n",
      "time_since_restore: 9399.969103574753\n",
      "time_this_iter_s: 46.42238473892212\n",
      "time_total_s: 9399.969103574753\n",
      "timers:\n",
      "  learn_throughput: 827.297\n",
      "  learn_time_ms: 568.84\n",
      "  load_throughput: 676505.282\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 7.024\n",
      "  sample_time_ms: 66998.126\n",
      "timestamp: 1638992483\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 65231\n",
      "training_iteration: 160\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:160 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 109 -2.0 -1.4599999999999995\n",
      "blue_1 True True 109 0.005 0.5450000000000004\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 460 0.005 2.299999999999973\n",
      "blue_1 True True 460 -2.0 0.3309999999999724\n",
      "agent_timesteps_total: 131600\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-42-44\n",
      "done: false\n",
      "episode_len_mean: 257.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6242499999999681\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 320\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.658992290496826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01572975516319275\n",
      "        model: {}\n",
      "        policy_loss: -0.0968562588095665\n",
      "        total_loss: 0.1649019718170166\n",
      "        vf_explained_var: -0.5548828840255737\n",
      "        vf_loss: 0.23488245904445648\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.901267051696777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003981777932494879\n",
      "        model: {}\n",
      "        policy_loss: -0.3072160482406616\n",
      "        total_loss: -0.12926168739795685\n",
      "        vf_explained_var: -0.35465362668037415\n",
      "        vf_loss: 0.1650388389825821\n",
      "  num_agent_steps_sampled: 131600\n",
      "  num_agent_steps_trained: 131600\n",
      "  num_steps_sampled: 65800\n",
      "  num_steps_trained: 65800\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 161\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.591150442477876\n",
      "  ram_util_percent: 37.22743362831858\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.417099999999991\n",
      "  blue_1: 0.20714999999999312\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5759084771520139\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.23972967563361\n",
      "  mean_inference_ms: 2.8244180483651835\n",
      "  mean_raw_obs_processing_ms: 28.53873556959458\n",
      "time_since_restore: 9475.92770409584\n",
      "time_this_iter_s: 75.95860052108765\n",
      "time_total_s: 9475.92770409584\n",
      "timers:\n",
      "  learn_throughput: 815.801\n",
      "  learn_time_ms: 554.179\n",
      "  load_throughput: 649866.287\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 6.91\n",
      "  sample_time_ms: 65431.13\n",
      "timestamp: 1638992564\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 65800\n",
      "training_iteration: 161\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:160 starting ! -----------------\n",
      "agent_timesteps_total: 131600\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-42-44\n",
      "done: false\n",
      "episode_len_mean: 257.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6242499999999681\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 320\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.658992290496826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01572975516319275\n",
      "        model: {}\n",
      "        policy_loss: -0.0968562588095665\n",
      "        total_loss: 0.1649019718170166\n",
      "        vf_explained_var: -0.5548828840255737\n",
      "        vf_loss: 0.23488245904445648\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.901267051696777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003981777932494879\n",
      "        model: {}\n",
      "        policy_loss: -0.3072160482406616\n",
      "        total_loss: -0.12926168739795685\n",
      "        vf_explained_var: -0.35465362668037415\n",
      "        vf_loss: 0.1650388389825821\n",
      "  num_agent_steps_sampled: 131600\n",
      "  num_agent_steps_trained: 131600\n",
      "  num_steps_sampled: 65800\n",
      "  num_steps_trained: 65800\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 161\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.591150442477876\n",
      "  ram_util_percent: 37.22743362831858\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.417099999999991\n",
      "  blue_1: 0.20714999999999312\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5759084771520139\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.23972967563361\n",
      "  mean_inference_ms: 2.8244180483651835\n",
      "  mean_raw_obs_processing_ms: 28.53873556959458\n",
      "time_since_restore: 9475.92770409584\n",
      "time_this_iter_s: 75.95860052108765\n",
      "time_total_s: 9475.92770409584\n",
      "timers:\n",
      "  learn_throughput: 815.801\n",
      "  learn_time_ms: 554.179\n",
      "  load_throughput: 649866.287\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 6.91\n",
      "  sample_time_ms: 65431.13\n",
      "timestamp: 1638992564\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 65800\n",
      "training_iteration: 161\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:161 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 118 -2.0 -1.4149999999999996\n",
      "blue_1 True True 118 0.005 0.5900000000000004\n",
      "LOSE\n",
      "blue_0 False True 465 -0.995 1.3249999999999726\n",
      "blue_1 False True 465 -0.996 1.1330000000000013\n",
      "agent_timesteps_total: 132766\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-44-06\n",
      "done: false\n",
      "episode_len_mean: 258.65\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6304499999999681\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 322\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.04359245300293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016233965754508972\n",
      "        model: {}\n",
      "        policy_loss: -0.19115915894508362\n",
      "        total_loss: 0.220207080245018\n",
      "        vf_explained_var: -0.5112202167510986\n",
      "        vf_loss: 0.3836289644241333\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6218292713165283\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.494536399841309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0068505387753248215\n",
      "        model: {}\n",
      "        policy_loss: -0.3544286787509918\n",
      "        total_loss: -0.31450170278549194\n",
      "        vf_explained_var: -0.10576321184635162\n",
      "        vf_loss: 0.02881655842065811\n",
      "  num_agent_steps_sampled: 132766\n",
      "  num_agent_steps_trained: 132766\n",
      "  num_steps_sampled: 66383\n",
      "  num_steps_trained: 66383\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 162\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.54910714285713\n",
      "  ram_util_percent: 37.191964285714285\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.4113499999999909\n",
      "  blue_1: 0.21909999999999322\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5758441233564063\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.17479605644547\n",
      "  mean_inference_ms: 2.823880940798268\n",
      "  mean_raw_obs_processing_ms: 28.469768729056515\n",
      "time_since_restore: 9551.993997573853\n",
      "time_this_iter_s: 76.06629347801208\n",
      "time_total_s: 9551.993997573853\n",
      "timers:\n",
      "  learn_throughput: 814.64\n",
      "  learn_time_ms: 552.882\n",
      "  load_throughput: 755827.207\n",
      "  load_time_ms: 0.596\n",
      "  sample_throughput: 6.829\n",
      "  sample_time_ms: 65953.117\n",
      "timestamp: 1638992646\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 66383\n",
      "training_iteration: 162\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:162 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 322 -2.0 -0.39500000000001223\n",
      "blue_1 True True 322 0.004 1.5839999999999936\n",
      "agent_timesteps_total: 133410\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-44-51\n",
      "done: false\n",
      "episode_len_mean: 259.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.6354899999999681\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 323\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.48894214630127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005666452459990978\n",
      "        model: {}\n",
      "        policy_loss: -0.4783528447151184\n",
      "        total_loss: -0.40121394395828247\n",
      "        vf_explained_var: 0.5154000520706177\n",
      "        vf_loss: 0.06745728850364685\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6218292713165283\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.103537559509277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011026043444871902\n",
      "        model: {}\n",
      "        policy_loss: -0.3528195023536682\n",
      "        total_loss: -0.327600359916687\n",
      "        vf_explained_var: -0.5596209168434143\n",
      "        vf_loss: 0.007336762268096209\n",
      "  num_agent_steps_sampled: 133410\n",
      "  num_agent_steps_trained: 133410\n",
      "  num_steps_sampled: 66705\n",
      "  num_steps_trained: 66705\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 163\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.12222222222222\n",
      "  ram_util_percent: 37.09047619047618\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.4140499999999909\n",
      "  blue_1: 0.22143999999999317\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5758123074106871\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.1426042428278\n",
      "  mean_inference_ms: 2.8236112668813833\n",
      "  mean_raw_obs_processing_ms: 28.435130729847984\n",
      "time_since_restore: 9591.401705503464\n",
      "time_this_iter_s: 39.407707929611206\n",
      "time_total_s: 9591.401705503464\n",
      "timers:\n",
      "  learn_throughput: 812.064\n",
      "  learn_time_ms: 547.494\n",
      "  load_throughput: 896101.662\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 6.902\n",
      "  sample_time_ms: 64420.175\n",
      "timestamp: 1638992691\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 66705\n",
      "training_iteration: 163\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:163 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 72 -2.0 -1.6449999999999998\n",
      "blue_1 True True 72 0.005 0.3600000000000002\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 159 -2.0 -1.2099999999999995\n",
      "blue_1 True True 159 0.005 0.7950000000000006\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 102 0.005 0.5100000000000003\n",
      "blue_1 True True 102 -2.0 -1.4949999999999997\n",
      "agent_timesteps_total: 134076\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-46-02\n",
      "done: false\n",
      "episode_len_mean: 251.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5554299999999696\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 326\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.78400993347168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01265870314091444\n",
      "        model: {}\n",
      "        policy_loss: 0.3166790008544922\n",
      "        total_loss: 0.7162134051322937\n",
      "        vf_explained_var: 0.42721855640411377\n",
      "        vf_loss: 0.37790584564208984\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6218292713165283\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.203489303588867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00718542467802763\n",
      "        model: {}\n",
      "        policy_loss: -0.5306991338729858\n",
      "        total_loss: -0.42576897144317627\n",
      "        vf_explained_var: 0.00875839963555336\n",
      "        vf_loss: 0.09327661991119385\n",
      "  num_agent_steps_sampled: 134076\n",
      "  num_agent_steps_trained: 134076\n",
      "  num_steps_sampled: 67038\n",
      "  num_steps_trained: 67038\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 164\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.25\n",
      "  ram_util_percent: 37.10714285714286\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3538499999999914\n",
      "  blue_1: 0.20157999999999368\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5757121785420085\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.06197936842547\n",
      "  mean_inference_ms: 2.822843792602033\n",
      "  mean_raw_obs_processing_ms: 28.34706321229891\n",
      "time_since_restore: 9657.145018577576\n",
      "time_this_iter_s: 65.74331307411194\n",
      "time_total_s: 9657.145018577576\n",
      "timers:\n",
      "  learn_throughput: 817.828\n",
      "  learn_time_ms: 515.879\n",
      "  load_throughput: 845756.755\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.619\n",
      "  sample_time_ms: 63738.243\n",
      "timestamp: 1638992762\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 67038\n",
      "training_iteration: 164\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:164 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 324 -2.0 -0.38500000000001244\n",
      "blue_1 True True 324 0.006 1.6409999999999885\n",
      "agent_timesteps_total: 134724\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-46-50\n",
      "done: false\n",
      "episode_len_mean: 249.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.54024999999997\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 327\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.476715087890625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005954984109848738\n",
      "        model: {}\n",
      "        policy_loss: -0.49471980333328247\n",
      "        total_loss: -0.3049571216106415\n",
      "        vf_explained_var: -0.24133141338825226\n",
      "        vf_loss: 0.1795879751443863\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6218292713165283\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.561030864715576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008246783167123795\n",
      "        model: {}\n",
      "        policy_loss: -0.4765467941761017\n",
      "        total_loss: -0.4379235506057739\n",
      "        vf_explained_var: -0.6306884288787842\n",
      "        vf_loss: 0.02524835616350174\n",
      "  num_agent_steps_sampled: 134724\n",
      "  num_agent_steps_trained: 134724\n",
      "  num_steps_sampled: 67362\n",
      "  num_steps_trained: 67362\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 165\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.915151515151514\n",
      "  ram_util_percent: 37.124242424242425\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3457999999999915\n",
      "  blue_1: 0.1944499999999938\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5756780137158671\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 113.03647600635614\n",
      "  mean_inference_ms: 2.8225908922092167\n",
      "  mean_raw_obs_processing_ms: 28.319418063438935\n",
      "time_since_restore: 9699.426620960236\n",
      "time_this_iter_s: 42.28160238265991\n",
      "time_total_s: 9699.426620960236\n",
      "timers:\n",
      "  learn_throughput: 827.678\n",
      "  learn_time_ms: 516.143\n",
      "  load_throughput: 856381.336\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.625\n",
      "  sample_time_ms: 64484.368\n",
      "timestamp: 1638992810\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 67362\n",
      "training_iteration: 165\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:165 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 147 0.005 0.7350000000000005\n",
      "blue_1 True True 147 -2.0 -1.2699999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 337 0.005 1.684999999999986\n",
      "blue_1 True True 337 -1.998 -0.1300000000000121\n",
      "agent_timesteps_total: 135692\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-48-01\n",
      "done: false\n",
      "episode_len_mean: 248.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5335299999999704\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 329\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.931109428405762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015142064541578293\n",
      "        model: {}\n",
      "        policy_loss: -0.08643193542957306\n",
      "        total_loss: 0.022025173529982567\n",
      "        vf_explained_var: -0.7011480331420898\n",
      "        vf_loss: 0.08258549124002457\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6218292713165283\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.632457256317139\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014293694868683815\n",
      "        model: {}\n",
      "        policy_loss: -0.31575024127960205\n",
      "        total_loss: 0.16287757456302643\n",
      "        vf_explained_var: 0.006104240193963051\n",
      "        vf_loss: 0.4554458558559418\n",
      "  num_agent_steps_sampled: 135692\n",
      "  num_agent_steps_trained: 135692\n",
      "  num_steps_sampled: 67846\n",
      "  num_steps_trained: 67846\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 166\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.341414141414134\n",
      "  ram_util_percent: 37.180808080808085\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.36169999999999164\n",
      "  blue_1: 0.17182999999999388\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5756134941158833\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.98793352941122\n",
      "  mean_inference_ms: 2.8221028577818412\n",
      "  mean_raw_obs_processing_ms: 28.26686402168807\n",
      "time_since_restore: 9765.111462593079\n",
      "time_this_iter_s: 65.68484163284302\n",
      "time_total_s: 9765.111462593079\n",
      "timers:\n",
      "  learn_throughput: 842.571\n",
      "  learn_time_ms: 516.396\n",
      "  load_throughput: 1090298.525\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 6.743\n",
      "  sample_time_ms: 64528.869\n",
      "timestamp: 1638992881\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 67846\n",
      "training_iteration: 166\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:166 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 588 0.005 2.9399999999999595\n",
      "blue_1 True True 588 -2.0 1.030999999999961\n",
      "agent_timesteps_total: 136868\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-49-18\n",
      "done: false\n",
      "episode_len_mean: 252.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5689199999999696\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 330\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.773920059204102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009592163376510143\n",
      "        model: {}\n",
      "        policy_loss: -0.31257399916648865\n",
      "        total_loss: -0.2820853888988495\n",
      "        vf_explained_var: -0.7353910803794861\n",
      "        vf_loss: 0.014099528081715107\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6218292713165283\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.610831260681152\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004690682049840689\n",
      "        model: {}\n",
      "        policy_loss: -0.3592689633369446\n",
      "        total_loss: -0.23732154071331024\n",
      "        vf_explained_var: -0.3573835492134094\n",
      "        vf_loss: 0.11433995515108109\n",
      "  num_agent_steps_sampled: 136868\n",
      "  num_agent_steps_trained: 136868\n",
      "  num_steps_sampled: 68434\n",
      "  num_steps_trained: 68434\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 167\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.92285714285714\n",
      "  ram_util_percent: 37.22380952380952\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3791999999999913\n",
      "  blue_1: 0.18971999999999356\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5755750385054925\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.96366743488494\n",
      "  mean_inference_ms: 2.8218577964888785\n",
      "  mean_raw_obs_processing_ms: 28.239108456818236\n",
      "time_since_restore: 9836.268060684204\n",
      "time_this_iter_s: 71.15659809112549\n",
      "time_total_s: 9836.268060684204\n",
      "timers:\n",
      "  learn_throughput: 851.449\n",
      "  learn_time_ms: 529.568\n",
      "  load_throughput: 1122247.611\n",
      "  load_time_ms: 0.402\n",
      "  sample_throughput: 6.875\n",
      "  sample_time_ms: 65583.276\n",
      "timestamp: 1638992958\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 68434\n",
      "training_iteration: 167\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:167 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 89 0.005 0.4450000000000003\n",
      "blue_1 True True 89 -2.0 -1.5599999999999996\n",
      "LOSE\n",
      "blue_0 False True 453 -0.995 1.264999999999974\n",
      "blue_1 False True 453 -0.993 1.5199999999999938\n",
      "agent_timesteps_total: 137952\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-50-37\n",
      "done: false\n",
      "episode_len_mean: 247.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5170799999999709\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 332\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.899351119995117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008404167369008064\n",
      "        model: {}\n",
      "        policy_loss: -0.22350244224071503\n",
      "        total_loss: -0.15626958012580872\n",
      "        vf_explained_var: -0.3232928514480591\n",
      "        vf_loss: 0.052873533219099045\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.098676681518555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02035430818796158\n",
      "        model: {}\n",
      "        policy_loss: -0.12774862349033356\n",
      "        total_loss: 0.2536719739437103\n",
      "        vf_explained_var: -0.4752412736415863\n",
      "        vf_loss: 0.3649149537086487\n",
      "  num_agent_steps_sampled: 137952\n",
      "  num_agent_steps_trained: 137952\n",
      "  num_steps_sampled: 68976\n",
      "  num_steps_trained: 68976\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 168\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.1138888888889\n",
      "  ram_util_percent: 37.26388888888888\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.37684999999999164\n",
      "  blue_1: 0.14022999999999367\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5755027692915442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.91888340003793\n",
      "  mean_inference_ms: 2.8213914841222087\n",
      "  mean_raw_obs_processing_ms: 28.18825407625566\n",
      "time_since_restore: 9909.595390319824\n",
      "time_this_iter_s: 73.32732963562012\n",
      "time_total_s: 9909.595390319824\n",
      "timers:\n",
      "  learn_throughput: 856.669\n",
      "  learn_time_ms: 527.041\n",
      "  load_throughput: 900274.902\n",
      "  load_time_ms: 0.502\n",
      "  sample_throughput: 6.782\n",
      "  sample_time_ms: 66568.588\n",
      "timestamp: 1638993037\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 68976\n",
      "training_iteration: 168\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:168 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 148 -2.0 -1.2649999999999995\n",
      "blue_1 True True 148 0.005 0.7400000000000005\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 588 -2.0 0.9349999999999596\n",
      "blue_1 True True 588 0.004 2.7679999999999874\n",
      "agent_timesteps_total: 139424\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-52-12\n",
      "done: false\n",
      "episode_len_mean: 250.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5466399999999696\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 334\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.490670680999756\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012296716682612896\n",
      "        model: {}\n",
      "        policy_loss: -0.28017669916152954\n",
      "        total_loss: 0.0320376381278038\n",
      "        vf_explained_var: -0.266724556684494\n",
      "        vf_loss: 0.29120421409606934\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.246458530426025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01427985355257988\n",
      "        model: {}\n",
      "        policy_loss: -0.21904484927654266\n",
      "        total_loss: -0.17165222764015198\n",
      "        vf_explained_var: -0.9134160280227661\n",
      "        vf_loss: 0.03002302721142769\n",
      "  num_agent_steps_sampled: 139424\n",
      "  num_agent_steps_trained: 139424\n",
      "  num_steps_sampled: 69712\n",
      "  num_steps_trained: 69712\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 169\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.27424242424242\n",
      "  ram_util_percent: 37.212878787878786\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3926499999999912\n",
      "  blue_1: 0.15398999999999352\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.575435593041946\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.8733419323212\n",
      "  mean_inference_ms: 2.820948236415808\n",
      "  mean_raw_obs_processing_ms: 28.13800498907761\n",
      "time_since_restore: 9999.39849805832\n",
      "time_this_iter_s: 89.80310773849487\n",
      "time_total_s: 9999.39849805832\n",
      "timers:\n",
      "  learn_throughput: 863.071\n",
      "  learn_time_ms: 556.964\n",
      "  load_throughput: 799413.954\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 6.856\n",
      "  sample_time_ms: 70113.126\n",
      "timestamp: 1638993132\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 69712\n",
      "training_iteration: 169\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:169 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 537 0.005 2.684999999999965\n",
      "blue_1 True True 537 -2.0 0.7029999999999661\n",
      "agent_timesteps_total: 140498\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-53-15\n",
      "done: false\n",
      "episode_len_mean: 253.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5733699999999691\n",
      "episode_reward_min: -1.5349999999999997\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 335\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.357239723205566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010315114632248878\n",
      "        model: {}\n",
      "        policy_loss: -0.16156679391860962\n",
      "        total_loss: -0.09858209639787674\n",
      "        vf_explained_var: -0.8361747860908508\n",
      "        vf_loss: 0.045360371470451355\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.997889041900635\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0125719690695405\n",
      "        model: {}\n",
      "        policy_loss: -0.20029741525650024\n",
      "        total_loss: -0.07942982017993927\n",
      "        vf_explained_var: -0.6233125329017639\n",
      "        vf_loss: 0.10557541251182556\n",
      "  num_agent_steps_sampled: 140498\n",
      "  num_agent_steps_trained: 140498\n",
      "  num_steps_sampled: 70249\n",
      "  num_steps_trained: 70249\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 170\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.08620689655174\n",
      "  ram_util_percent: 37.22413793103447\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.40589999999999093\n",
      "  blue_1: 0.16746999999999324\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7699999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5754016967521881\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.8499730447681\n",
      "  mean_inference_ms: 2.8207256399476033\n",
      "  mean_raw_obs_processing_ms: 28.112173119252837\n",
      "time_since_restore: 10057.073297023773\n",
      "time_this_iter_s: 57.6747989654541\n",
      "time_total_s: 10057.073297023773\n",
      "timers:\n",
      "  learn_throughput: 850.831\n",
      "  learn_time_ms: 589.776\n",
      "  load_throughput: 834338.281\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.044\n",
      "  sample_time_ms: 71235.706\n",
      "timestamp: 1638993195\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 70249\n",
      "training_iteration: 170\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:170 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 112 -2.0 -1.4449999999999996\n",
      "blue_1 True True 112 0.005 0.5600000000000004\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 291 -2.0 -0.5500000000000089\n",
      "blue_1 True True 291 0.005 1.472999999999992\n",
      "agent_timesteps_total: 141304\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-54-18\n",
      "done: false\n",
      "episode_len_mean: 254.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.584749999999969\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 337\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.853395462036133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01634403131902218\n",
      "        model: {}\n",
      "        policy_loss: -0.164089173078537\n",
      "        total_loss: 0.3246172368526459\n",
      "        vf_explained_var: 0.2045954018831253\n",
      "        vf_loss: 0.46078112721443176\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.55751895904541\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014382243156433105\n",
      "        model: {}\n",
      "        policy_loss: -0.16020467877388\n",
      "        total_loss: -0.12571358680725098\n",
      "        vf_explained_var: -0.2925797700881958\n",
      "        vf_loss: 0.01699693128466606\n",
      "  num_agent_steps_sampled: 141304\n",
      "  num_agent_steps_trained: 141304\n",
      "  num_steps_sampled: 70652\n",
      "  num_steps_trained: 70652\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 171\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.65465116279069\n",
      "  ram_util_percent: 37.21162790697674\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.4114999999999908\n",
      "  blue_1: 0.17324999999999316\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753370949912884\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.80203075119269\n",
      "  mean_inference_ms: 2.820280089918951\n",
      "  mean_raw_obs_processing_ms: 28.059501176369935\n",
      "time_since_restore: 10113.798171520233\n",
      "time_this_iter_s: 56.72487449645996\n",
      "time_total_s: 10113.798171520233\n",
      "timers:\n",
      "  learn_throughput: 845.411\n",
      "  learn_time_ms: 573.922\n",
      "  load_throughput: 967701.522\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 6.995\n",
      "  sample_time_ms: 69360.293\n",
      "timestamp: 1638993258\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 70652\n",
      "training_iteration: 171\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:170 starting ! -----------------\n",
      "agent_timesteps_total: 141304\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-54-18\n",
      "done: false\n",
      "episode_len_mean: 254.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.584749999999969\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 337\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.853395462036133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01634403131902218\n",
      "        model: {}\n",
      "        policy_loss: -0.164089173078537\n",
      "        total_loss: 0.3246172368526459\n",
      "        vf_explained_var: 0.2045954018831253\n",
      "        vf_loss: 0.46078112721443176\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.55751895904541\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014382243156433105\n",
      "        model: {}\n",
      "        policy_loss: -0.16020467877388\n",
      "        total_loss: -0.12571358680725098\n",
      "        vf_explained_var: -0.2925797700881958\n",
      "        vf_loss: 0.01699693128466606\n",
      "  num_agent_steps_sampled: 141304\n",
      "  num_agent_steps_trained: 141304\n",
      "  num_steps_sampled: 70652\n",
      "  num_steps_trained: 70652\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 171\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.65465116279069\n",
      "  ram_util_percent: 37.21162790697674\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.4114999999999908\n",
      "  blue_1: 0.17324999999999316\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753370949912884\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.80203075119269\n",
      "  mean_inference_ms: 2.820280089918951\n",
      "  mean_raw_obs_processing_ms: 28.059501176369935\n",
      "time_since_restore: 10113.798171520233\n",
      "time_this_iter_s: 56.72487449645996\n",
      "time_total_s: 10113.798171520233\n",
      "timers:\n",
      "  learn_throughput: 845.411\n",
      "  learn_time_ms: 573.922\n",
      "  load_throughput: 967701.522\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 6.995\n",
      "  sample_time_ms: 69360.293\n",
      "timestamp: 1638993258\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 70652\n",
      "training_iteration: 171\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:171 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 226 -2.0 -0.875000000000002\n",
      "blue_1 True True 226 0.005 1.1399999999999986\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 133 0.005 0.6650000000000005\n",
      "blue_1 True True 133 -2.0 -1.3399999999999994\n",
      "agent_timesteps_total: 142022\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-55-17\n",
      "done: false\n",
      "episode_len_mean: 255.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5927499999999689\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 339\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.341311931610107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011803094297647476\n",
      "        model: {}\n",
      "        policy_loss: 0.28550732135772705\n",
      "        total_loss: 0.5072978734970093\n",
      "        vf_explained_var: 0.6677976250648499\n",
      "        vf_loss: 0.20162388682365417\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.186651229858398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014254874549806118\n",
      "        model: {}\n",
      "        policy_loss: -0.6016190052032471\n",
      "        total_loss: -0.5204535126686096\n",
      "        vf_explained_var: -0.8212862014770508\n",
      "        vf_loss: 0.0638263076543808\n",
      "  num_agent_steps_sampled: 142022\n",
      "  num_agent_steps_trained: 142022\n",
      "  num_steps_sampled: 71011\n",
      "  num_steps_trained: 71011\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 172\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.168674698795186\n",
      "  ram_util_percent: 37.18433734939759\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3953999999999908\n",
      "  blue_1: 0.19734999999999311\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752746445778218\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.7536959144544\n",
      "  mean_inference_ms: 2.8198508464534693\n",
      "  mean_raw_obs_processing_ms: 28.006012855624412\n",
      "time_since_restore: 10168.08827662468\n",
      "time_this_iter_s: 54.29010510444641\n",
      "time_total_s: 10168.08827662468\n",
      "timers:\n",
      "  learn_throughput: 855.122\n",
      "  learn_time_ms: 541.21\n",
      "  load_throughput: 769981.71\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 6.887\n",
      "  sample_time_ms: 67197.372\n",
      "timestamp: 1638993317\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 71011\n",
      "training_iteration: 172\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:172 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 210 -2.0 -0.9550000000000003\n",
      "blue_1 True True 210 0.005 1.0499999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 169 0.005 0.8450000000000006\n",
      "blue_1 True True 169 -2.0 -1.1599999999999993\n",
      "agent_timesteps_total: 142780\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-56-14\n",
      "done: false\n",
      "episode_len_mean: 255.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.500999999999903\n",
      "episode_reward_mean: 0.5887899999999691\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 341\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.018975257873535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01852436736226082\n",
      "        model: {}\n",
      "        policy_loss: 0.34900009632110596\n",
      "        total_loss: 0.4955994486808777\n",
      "        vf_explained_var: 0.6706802845001221\n",
      "        vf_loss: 0.11494874954223633\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.0523271560668945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0115700364112854\n",
      "        model: {}\n",
      "        policy_loss: -0.6551874279975891\n",
      "        total_loss: -0.6025339365005493\n",
      "        vf_explained_var: -0.191618412733078\n",
      "        vf_loss: 0.03857998922467232\n",
      "  num_agent_steps_sampled: 142780\n",
      "  num_agent_steps_trained: 142780\n",
      "  num_steps_sampled: 71390\n",
      "  num_steps_trained: 71390\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 173\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.741772151898736\n",
      "  ram_util_percent: 37.18860759493669\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.37369999999999104\n",
      "  blue_1: 0.21508999999999326\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752133379668853\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.70411371822257\n",
      "  mean_inference_ms: 2.819440698110891\n",
      "  mean_raw_obs_processing_ms: 27.953215512991083\n",
      "time_since_restore: 10219.200890541077\n",
      "time_this_iter_s: 51.112613916397095\n",
      "time_total_s: 10219.200890541077\n",
      "timers:\n",
      "  learn_throughput: 866.607\n",
      "  learn_time_ms: 540.614\n",
      "  load_throughput: 779465.063\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 6.856\n",
      "  sample_time_ms: 68334.348\n",
      "timestamp: 1638993374\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 71390\n",
      "training_iteration: 173\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:173 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 218 0.005 1.0899999999999987\n",
      "blue_1 True True 218 -2.0 -0.9150000000000011\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 165 -2.0 -1.1799999999999993\n",
      "blue_1 True True 165 0.005 0.8250000000000006\n",
      "agent_timesteps_total: 143546\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-57-12\n",
      "done: false\n",
      "episode_len_mean: 246.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5015799999999707\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 343\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.013428688049316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011183316819369793\n",
      "        model: {}\n",
      "        policy_loss: -0.6449263095855713\n",
      "        total_loss: -0.4072796106338501\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.21853895485401154\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.787978172302246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02119079791009426\n",
      "        model: {}\n",
      "        policy_loss: 0.21651405096054077\n",
      "        total_loss: 0.5365499258041382\n",
      "        vf_explained_var: 0.5326659679412842\n",
      "        vf_loss: 0.2942599356174469\n",
      "  num_agent_steps_sampled: 143546\n",
      "  num_agent_steps_trained: 143546\n",
      "  num_steps_sampled: 71773\n",
      "  num_steps_trained: 71773\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 174\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.29625\n",
      "  ram_util_percent: 37.191250000000004\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.33279999999999177\n",
      "  blue_1: 0.16877999999999407\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751645786021795\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.6588377951908\n",
      "  mean_inference_ms: 2.8190597823885417\n",
      "  mean_raw_obs_processing_ms: 27.90825561146903\n",
      "time_since_restore: 10271.895748138428\n",
      "time_this_iter_s: 52.694857597351074\n",
      "time_total_s: 10271.895748138428\n",
      "timers:\n",
      "  learn_throughput: 874.722\n",
      "  learn_time_ms: 541.315\n",
      "  load_throughput: 794051.795\n",
      "  load_time_ms: 0.596\n",
      "  sample_throughput: 7.065\n",
      "  sample_time_ms: 67023.944\n",
      "timestamp: 1638993432\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 71773\n",
      "training_iteration: 174\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:174 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 107 -2.0 -1.4699999999999998\n",
      "blue_1 True True 107 0.005 0.5350000000000004\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 139 -2.0 -1.3099999999999996\n",
      "blue_1 True True 139 0.005 0.6950000000000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False True 359 -0.995 0.7949999999999838\n",
      "blue_1 False True 359 -0.993 1.033999999999987\n",
      "agent_timesteps_total: 144756\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-58-42\n",
      "done: false\n",
      "episode_len_mean: 247.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5090799999999708\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 346\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.14013671875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01760896109044552\n",
      "        model: {}\n",
      "        policy_loss: -0.10945994406938553\n",
      "        total_loss: 0.24921827018260956\n",
      "        vf_explained_var: -0.21663837134838104\n",
      "        vf_loss: 0.3285916745662689\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.686709403991699\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016102584078907967\n",
      "        model: {}\n",
      "        policy_loss: -0.17177729308605194\n",
      "        total_loss: -0.062424186617136\n",
      "        vf_explained_var: -0.1852143257856369\n",
      "        vf_loss: 0.0799730122089386\n",
      "  num_agent_steps_sampled: 144756\n",
      "  num_agent_steps_trained: 144756\n",
      "  num_steps_sampled: 72378\n",
      "  num_steps_trained: 72378\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 175\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.9824\n",
      "  ram_util_percent: 37.2016\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3052999999999917\n",
      "  blue_1: 0.203779999999994\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.575105511653212\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.59053879272926\n",
      "  mean_inference_ms: 2.8185052324270754\n",
      "  mean_raw_obs_processing_ms: 27.84534944301108\n",
      "time_since_restore: 10356.397881507874\n",
      "time_this_iter_s: 84.5021333694458\n",
      "time_total_s: 10356.397881507874\n",
      "timers:\n",
      "  learn_throughput: 876.258\n",
      "  learn_time_ms: 572.434\n",
      "  load_throughput: 841175.038\n",
      "  load_time_ms: 0.596\n",
      "  sample_throughput: 7.043\n",
      "  sample_time_ms: 71218.053\n",
      "timestamp: 1638993522\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 72378\n",
      "training_iteration: 175\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:175 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 147 0.005 0.7350000000000005\n",
      "blue_1 True True 147 -2.0 -1.2699999999999996\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 133 -2.0 -1.3399999999999994\n",
      "blue_1 True True 133 0.005 0.6650000000000005\n",
      "agent_timesteps_total: 145316\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_04-59-32\n",
      "done: false\n",
      "episode_len_mean: 246.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.4956199999999709\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 348\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.313530921936035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010645133443176746\n",
      "        model: {}\n",
      "        policy_loss: -0.2634027302265167\n",
      "        total_loss: -0.16959303617477417\n",
      "        vf_explained_var: 0.41949012875556946\n",
      "        vf_loss: 0.07562149316072464\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.136144161224365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014053886756300926\n",
      "        model: {}\n",
      "        policy_loss: -0.019258689135313034\n",
      "        total_loss: 0.5612735748291016\n",
      "        vf_explained_var: 0.22706669569015503\n",
      "        vf_loss: 0.5548900961875916\n",
      "  num_agent_steps_sampled: 145316\n",
      "  num_agent_steps_trained: 145316\n",
      "  num_steps_sampled: 72658\n",
      "  num_steps_trained: 72658\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 176\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.43088235294118\n",
      "  ram_util_percent: 37.249999999999986\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.27899999999999175\n",
      "  blue_1: 0.2166199999999941\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750760323922418\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.54601663870791\n",
      "  mean_inference_ms: 2.8181570384583745\n",
      "  mean_raw_obs_processing_ms: 27.806905098600527\n",
      "time_since_restore: 10400.374785900116\n",
      "time_this_iter_s: 43.97690439224243\n",
      "time_total_s: 10400.374785900116\n",
      "timers:\n",
      "  learn_throughput: 868.305\n",
      "  learn_time_ms: 554.183\n",
      "  load_throughput: 694026.713\n",
      "  load_time_ms: 0.693\n",
      "  sample_throughput: 6.965\n",
      "  sample_time_ms: 69092.189\n",
      "timestamp: 1638993572\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 72658\n",
      "training_iteration: 176\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:176 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 68 -2.0 -1.6649999999999998\n",
      "blue_1 True True 68 0.005 0.3400000000000002\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 267 -2.0 -0.6700000000000064\n",
      "blue_1 True True 267 0.005 1.3349999999999935\n",
      "agent_timesteps_total: 145986\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-00-29\n",
      "done: false\n",
      "episode_len_mean: 241.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.45026999999997175\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 350\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.5460638999938965\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01478035282343626\n",
      "        model: {}\n",
      "        policy_loss: -0.20929670333862305\n",
      "        total_loss: 0.11042917519807816\n",
      "        vf_explained_var: 0.15536843240261078\n",
      "        vf_loss: 0.29447224736213684\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.862947940826416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018383055925369263\n",
      "        model: {}\n",
      "        policy_loss: -0.35827919840812683\n",
      "        total_loss: -0.1857217401266098\n",
      "        vf_explained_var: 0.027248751372098923\n",
      "        vf_loss: 0.13901646435260773\n",
      "  num_agent_steps_sampled: 145986\n",
      "  num_agent_steps_trained: 145986\n",
      "  num_steps_sampled: 72993\n",
      "  num_steps_trained: 72993\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 177\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.981012658227854\n",
      "  ram_util_percent: 37.2632911392405\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.22639999999999216\n",
      "  blue_1: 0.22386999999999443\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750494780170459\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.50207748307572\n",
      "  mean_inference_ms: 2.8178163543691106\n",
      "  mean_raw_obs_processing_ms: 27.77071323591469\n",
      "time_since_restore: 10451.59688425064\n",
      "time_this_iter_s: 51.2220983505249\n",
      "time_total_s: 10451.59688425064\n",
      "timers:\n",
      "  learn_throughput: 872.94\n",
      "  learn_time_ms: 522.258\n",
      "  load_throughput: 654700.309\n",
      "  load_time_ms: 0.696\n",
      "  sample_throughput: 6.793\n",
      "  sample_time_ms: 67115.246\n",
      "timestamp: 1638993629\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 72993\n",
      "training_iteration: 177\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:177 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 495 -0.995 1.4749999999999694\n",
      "blue_1 False True 495 -0.993 1.7719999999999958\n",
      "agent_timesteps_total: 146976\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-01-31\n",
      "done: false\n",
      "episode_len_mean: 244.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.485089999999971\n",
      "episode_reward_min: -1.4949999999999997\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 351\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.96988582611084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011775132268667221\n",
      "        model: {}\n",
      "        policy_loss: -0.3623584806919098\n",
      "        total_loss: -0.23172488808631897\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.11051472276449203\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.905216217041016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003721271874383092\n",
      "        model: {}\n",
      "        policy_loss: -0.47072890400886536\n",
      "        total_loss: -0.4335811138153076\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.030358152464032173\n",
      "  num_agent_steps_sampled: 146976\n",
      "  num_agent_steps_trained: 146976\n",
      "  num_steps_sampled: 73488\n",
      "  num_steps_trained: 73488\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 178\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.18953488372093\n",
      "  ram_util_percent: 37.281395348837194\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.25234999999999186\n",
      "  blue_1: 0.2327399999999944\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.75\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750341459382997\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.47909728828974\n",
      "  mean_inference_ms: 2.8176440780468837\n",
      "  mean_raw_obs_processing_ms: 27.751506231290982\n",
      "time_since_restore: 10508.369906663895\n",
      "time_this_iter_s: 56.773022413253784\n",
      "time_total_s: 10508.369906663895\n",
      "timers:\n",
      "  learn_throughput: 889.005\n",
      "  learn_time_ms: 507.534\n",
      "  load_throughput: 756261.974\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 6.894\n",
      "  sample_time_ms: 65447.125\n",
      "timestamp: 1638993691\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 73488\n",
      "training_iteration: 178\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:178 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False True 310 -0.995 0.549999999999989\n",
      "blue_1 False True 310 -0.993 0.8159999999999886\n",
      "agent_timesteps_total: 147596\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-02-18\n",
      "done: false\n",
      "episode_len_mean: 247.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5136999999999707\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 352\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.114702224731445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006724438164383173\n",
      "        model: {}\n",
      "        policy_loss: -0.4372192323207855\n",
      "        total_loss: -0.3885626792907715\n",
      "        vf_explained_var: -0.17868542671203613\n",
      "        vf_loss: 0.037167228758335114\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.912278950214386\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.589370727539062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00948147103190422\n",
      "        model: {}\n",
      "        policy_loss: -0.44007980823516846\n",
      "        total_loss: -0.41589102149009705\n",
      "        vf_explained_var: -0.28649836778640747\n",
      "        vf_loss: 0.015539051033556461\n",
      "  num_agent_steps_sampled: 147596\n",
      "  num_agent_steps_trained: 147596\n",
      "  num_steps_sampled: 73798\n",
      "  num_steps_trained: 73798\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 179\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.11692307692307\n",
      "  ram_util_percent: 37.26923076923077\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.25529999999999176\n",
      "  blue_1: 0.2583999999999943\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750203284090307\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.45612217673313\n",
      "  mean_inference_ms: 2.817480470848855\n",
      "  mean_raw_obs_processing_ms: 27.73246713110179\n",
      "time_since_restore: 10549.9776699543\n",
      "time_this_iter_s: 41.60776329040527\n",
      "time_total_s: 10549.9776699543\n",
      "timers:\n",
      "  learn_throughput: 886.795\n",
      "  learn_time_ms: 460.76\n",
      "  load_throughput: 684914.321\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 6.736\n",
      "  sample_time_ms: 60661.802\n",
      "timestamp: 1638993738\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 73798\n",
      "training_iteration: 179\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:179 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 160 -2.0 -1.2049999999999994\n",
      "blue_1 True True 160 0.005 0.8000000000000006\n",
      "LOSE\n",
      "blue_0 False True 555 -0.995 1.774999999999963\n",
      "blue_1 False True 555 -0.993 2.09\n",
      "agent_timesteps_total: 149026\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-03-53\n",
      "done: false\n",
      "episode_len_mean: 249.21\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5344799999999701\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 354\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.04488754272461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009874014183878899\n",
      "        model: {}\n",
      "        policy_loss: -0.1643822193145752\n",
      "        total_loss: 0.09779845923185349\n",
      "        vf_explained_var: -0.19004638493061066\n",
      "        vf_loss: 0.2453100085258484\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.912278950214386\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.460769653320312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015208075754344463\n",
      "        model: {}\n",
      "        policy_loss: -0.2495696246623993\n",
      "        total_loss: -0.194613978266716\n",
      "        vf_explained_var: -0.577000617980957\n",
      "        vf_loss: 0.04108164459466934\n",
      "  num_agent_steps_sampled: 149026\n",
      "  num_agent_steps_trained: 149026\n",
      "  num_steps_sampled: 74513\n",
      "  num_steps_trained: 74513\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 180\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.878461538461536\n",
      "  ram_util_percent: 37.283076923076926\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2538999999999916\n",
      "  blue_1: 0.2805799999999945\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749896786889199\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.40815543372247\n",
      "  mean_inference_ms: 2.8171547864361957\n",
      "  mean_raw_obs_processing_ms: 27.69256032983297\n",
      "time_since_restore: 10639.1896276474\n",
      "time_this_iter_s: 89.21195769309998\n",
      "time_total_s: 10639.1896276474\n",
      "timers:\n",
      "  learn_throughput: 885.919\n",
      "  learn_time_ms: 481.308\n",
      "  load_throughput: 858181.97\n",
      "  load_time_ms: 0.497\n",
      "  sample_throughput: 6.689\n",
      "  sample_time_ms: 63748.573\n",
      "timestamp: 1638993833\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 74513\n",
      "training_iteration: 180\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:180 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 129 0.005 0.6450000000000005\n",
      "blue_1 True True 129 -2.0 -1.3599999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 427 0.005 2.1349999999999767\n",
      "blue_1 True True 427 -2.0 0.1599999999999766\n",
      "agent_timesteps_total: 150138\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-05-08\n",
      "done: false\n",
      "episode_len_mean: 248.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5236899999999703\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 356\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.019627571105957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0169613528996706\n",
      "        model: {}\n",
      "        policy_loss: -0.013668998144567013\n",
      "        total_loss: 0.06199575215578079\n",
      "        vf_explained_var: -0.44058531522750854\n",
      "        vf_loss: 0.046684686094522476\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.912278950214386\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.802069187164307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021249091252684593\n",
      "        model: {}\n",
      "        policy_loss: -0.2291279137134552\n",
      "        total_loss: 0.17354542016983032\n",
      "        vf_explained_var: 0.14342206716537476\n",
      "        vf_loss: 0.38328826427459717\n",
      "  num_agent_steps_sampled: 150138\n",
      "  num_agent_steps_trained: 150138\n",
      "  num_steps_sampled: 75069\n",
      "  num_steps_trained: 75069\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 181\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.9875\n",
      "  ram_util_percent: 37.300000000000004\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.26899999999999163\n",
      "  blue_1: 0.2546899999999945\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749659275694419\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.35819114007751\n",
      "  mean_inference_ms: 2.816840170089589\n",
      "  mean_raw_obs_processing_ms: 27.652723588618027\n",
      "time_since_restore: 10708.53634762764\n",
      "time_this_iter_s: 69.34671998023987\n",
      "time_total_s: 10708.53634762764\n",
      "timers:\n",
      "  learn_throughput: 888.636\n",
      "  learn_time_ms: 497.054\n",
      "  load_throughput: 740427.671\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 6.794\n",
      "  sample_time_ms: 65014.802\n",
      "timestamp: 1638993908\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 75069\n",
      "training_iteration: 181\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:180 starting ! -----------------\n",
      "agent_timesteps_total: 150138\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-05-08\n",
      "done: false\n",
      "episode_len_mean: 248.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5236899999999703\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 356\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.019627571105957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0169613528996706\n",
      "        model: {}\n",
      "        policy_loss: -0.013668998144567013\n",
      "        total_loss: 0.06199575215578079\n",
      "        vf_explained_var: -0.44058531522750854\n",
      "        vf_loss: 0.046684686094522476\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.912278950214386\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.802069187164307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021249091252684593\n",
      "        model: {}\n",
      "        policy_loss: -0.2291279137134552\n",
      "        total_loss: 0.17354542016983032\n",
      "        vf_explained_var: 0.14342206716537476\n",
      "        vf_loss: 0.38328826427459717\n",
      "  num_agent_steps_sampled: 150138\n",
      "  num_agent_steps_trained: 150138\n",
      "  num_steps_sampled: 75069\n",
      "  num_steps_trained: 75069\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 181\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.9875\n",
      "  ram_util_percent: 37.300000000000004\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.26899999999999163\n",
      "  blue_1: 0.2546899999999945\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749659275694419\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.35819114007751\n",
      "  mean_inference_ms: 2.816840170089589\n",
      "  mean_raw_obs_processing_ms: 27.652723588618027\n",
      "time_since_restore: 10708.53634762764\n",
      "time_this_iter_s: 69.34671998023987\n",
      "time_total_s: 10708.53634762764\n",
      "timers:\n",
      "  learn_throughput: 888.636\n",
      "  learn_time_ms: 497.054\n",
      "  load_throughput: 740427.671\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 6.794\n",
      "  sample_time_ms: 65014.802\n",
      "timestamp: 1638993908\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 75069\n",
      "training_iteration: 181\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:181 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 80 0.005 0.40000000000000024\n",
      "blue_1 True True 80 -2.0 -1.6049999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 375 -2.0 -0.13000000000001788\n",
      "blue_1 True True 375 0.004 1.8469999999999887\n",
      "agent_timesteps_total: 151048\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-06-19\n",
      "done: false\n",
      "episode_len_mean: 246.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5046699999999709\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 358\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.124970436096191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006236369721591473\n",
      "        model: {}\n",
      "        policy_loss: -0.3868063688278198\n",
      "        total_loss: -0.3294978141784668\n",
      "        vf_explained_var: 0.04249412566423416\n",
      "        vf_loss: 0.0466531440615654\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.464882850646973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015758072957396507\n",
      "        model: {}\n",
      "        policy_loss: -0.05385594815015793\n",
      "        total_loss: 0.4836491346359253\n",
      "        vf_explained_var: -0.26066675782203674\n",
      "        vf_loss: 0.5159415602684021\n",
      "  num_agent_steps_sampled: 151048\n",
      "  num_agent_steps_trained: 151048\n",
      "  num_steps_sampled: 75524\n",
      "  num_steps_trained: 75524\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 182\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.53092783505155\n",
      "  ram_util_percent: 37.31134020618558\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.25114999999999177\n",
      "  blue_1: 0.2535199999999943\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749457468035617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.3109657993524\n",
      "  mean_inference_ms: 2.816541803505865\n",
      "  mean_raw_obs_processing_ms: 27.61581281783702\n",
      "time_since_restore: 10773.756224632263\n",
      "time_this_iter_s: 65.21987700462341\n",
      "time_total_s: 10773.756224632263\n",
      "timers:\n",
      "  learn_throughput: 877.321\n",
      "  learn_time_ms: 514.407\n",
      "  load_throughput: 756097.222\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 6.827\n",
      "  sample_time_ms: 66106.981\n",
      "timestamp: 1638993979\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 75524\n",
      "training_iteration: 182\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:182 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 340 -2.0 -0.30500000000001415\n",
      "blue_1 True True 340 0.007 1.8709999999999873\n",
      "agent_timesteps_total: 151728\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-07-06\n",
      "done: false\n",
      "episode_len_mean: 248.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5265799999999706\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 359\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.823369979858398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009462783113121986\n",
      "        model: {}\n",
      "        policy_loss: -0.5711609125137329\n",
      "        total_loss: -0.4870379865169525\n",
      "        vf_explained_var: -0.609178900718689\n",
      "        vf_loss: 0.0679548904299736\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.856994152069092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009676428511738777\n",
      "        model: {}\n",
      "        policy_loss: -0.5511038899421692\n",
      "        total_loss: -0.47198331356048584\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.06587925553321838\n",
      "  num_agent_steps_sampled: 151728\n",
      "  num_agent_steps_trained: 151728\n",
      "  num_steps_sampled: 75864\n",
      "  num_steps_trained: 75864\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 183\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.89846153846154\n",
      "  ram_util_percent: 37.29538461538462\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2612499999999916\n",
      "  blue_1: 0.26532999999999424\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749314946028244\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.28935226463955\n",
      "  mean_inference_ms: 2.8163938708318867\n",
      "  mean_raw_obs_processing_ms: 27.597727783813887\n",
      "time_since_restore: 10815.507571935654\n",
      "time_this_iter_s: 41.7513473033905\n",
      "time_total_s: 10815.507571935654\n",
      "timers:\n",
      "  learn_throughput: 871.259\n",
      "  learn_time_ms: 513.51\n",
      "  load_throughput: 642251.903\n",
      "  load_time_ms: 0.697\n",
      "  sample_throughput: 6.863\n",
      "  sample_time_ms: 65188.788\n",
      "timestamp: 1638994026\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 75864\n",
      "training_iteration: 183\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:183 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 218 0.005 1.0899999999999987\n",
      "blue_1 True True 218 -1.998 -0.8679999999999997\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 332 0.005 1.6599999999999866\n",
      "blue_1 True True 332 -1.998 -0.22000000000001152\n",
      "agent_timesteps_total: 152828\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-08-22\n",
      "done: false\n",
      "episode_len_mean: 248.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5241199999999708\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 361\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.461318969726562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014574695378541946\n",
      "        model: {}\n",
      "        policy_loss: -0.18671557307243347\n",
      "        total_loss: -0.14404675364494324\n",
      "        vf_explained_var: -0.6156397461891174\n",
      "        vf_loss: 0.017766578122973442\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.329357147216797\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012703944928944111\n",
      "        model: {}\n",
      "        policy_loss: -0.196124866604805\n",
      "        total_loss: 0.21306762099266052\n",
      "        vf_explained_var: -0.03278356045484543\n",
      "        vf_loss: 0.3918081820011139\n",
      "  num_agent_steps_sampled: 152828\n",
      "  num_agent_steps_trained: 152828\n",
      "  num_steps_sampled: 76414\n",
      "  num_steps_trained: 76414\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 184\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.491428571428564\n",
      "  ram_util_percent: 37.280952380952385\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2595499999999916\n",
      "  blue_1: 0.26456999999999425\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749082939147464\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.24593334217121\n",
      "  mean_inference_ms: 2.816096895217723\n",
      "  mean_raw_obs_processing_ms: 27.561045710154062\n",
      "time_since_restore: 10885.72955274582\n",
      "time_this_iter_s: 70.2219808101654\n",
      "time_total_s: 10885.72955274582\n",
      "timers:\n",
      "  learn_throughput: 852.12\n",
      "  learn_time_ms: 544.641\n",
      "  load_throughput: 664269.89\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 6.936\n",
      "  sample_time_ms: 66910.866\n",
      "timestamp: 1638994102\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 76414\n",
      "training_iteration: 184\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:184 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 594 -2.0 0.964999999999959\n",
      "blue_1 True True 594 0.005 2.9859999999999616\n",
      "agent_timesteps_total: 154016\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-09-34\n",
      "done: false\n",
      "episode_len_mean: 251.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5553399999999701\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 362\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.817248344421387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006537978071719408\n",
      "        model: {}\n",
      "        policy_loss: -0.38316723704338074\n",
      "        total_loss: -0.3223817050457001\n",
      "        vf_explained_var: -0.5982748866081238\n",
      "        vf_loss: 0.04961477965116501\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.196222305297852\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009618250653147697\n",
      "        model: {}\n",
      "        policy_loss: -0.3564077913761139\n",
      "        total_loss: -0.2664991617202759\n",
      "        vf_explained_var: -0.8504589796066284\n",
      "        vf_loss: 0.0767468586564064\n",
      "  num_agent_steps_sampled: 154016\n",
      "  num_agent_steps_trained: 154016\n",
      "  num_steps_sampled: 77008\n",
      "  num_steps_trained: 77008\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 185\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.80510204081633\n",
      "  ram_util_percent: 37.28979591836735\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.27514999999999135\n",
      "  blue_1: 0.280189999999994\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748978498909845\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.22368210171076\n",
      "  mean_inference_ms: 2.8159489920452723\n",
      "  mean_raw_obs_processing_ms: 27.541585190983803\n",
      "time_since_restore: 10951.95664525032\n",
      "time_this_iter_s: 66.22709250450134\n",
      "time_total_s: 10951.95664525032\n",
      "timers:\n",
      "  learn_throughput: 849.185\n",
      "  learn_time_ms: 545.229\n",
      "  load_throughput: 662695.452\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 7.111\n",
      "  sample_time_ms: 65109.422\n",
      "timestamp: 1638994174\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 77008\n",
      "training_iteration: 185\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:185 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 82 0.005 0.41000000000000025\n",
      "blue_1 True True 82 -2.0 -1.5949999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 101 0.005 0.5050000000000003\n",
      "blue_1 True True 101 -2.0 -1.4999999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 116 0.005 0.5800000000000004\n",
      "blue_1 True True 116 -2.0 -1.4249999999999996\n",
      "agent_timesteps_total: 154614\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-10-37\n",
      "done: false\n",
      "episode_len_mean: 250.15\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5422399999999703\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 365\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.134803295135498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01242332998663187\n",
      "        model: {}\n",
      "        policy_loss: -0.2591117322444916\n",
      "        total_loss: -0.2139115035533905\n",
      "        vf_explained_var: -0.9002413153648376\n",
      "        vf_loss: 0.023973830044269562\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.438535213470459\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01559726893901825\n",
      "        model: {}\n",
      "        policy_loss: -0.26113465428352356\n",
      "        total_loss: 0.6811151504516602\n",
      "        vf_explained_var: -0.04472428187727928\n",
      "        vf_loss: 0.920906126499176\n",
      "  num_agent_steps_sampled: 154614\n",
      "  num_agent_steps_trained: 154614\n",
      "  num_steps_sampled: 77307\n",
      "  num_steps_trained: 77307\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 186\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.17045454545455\n",
      "  ram_util_percent: 37.253409090909095\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2886499999999913\n",
      "  blue_1: 0.253589999999994\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748593472477697\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.15959916093483\n",
      "  mean_inference_ms: 2.8155258993663335\n",
      "  mean_raw_obs_processing_ms: 27.484004073220557\n",
      "time_since_restore: 11009.866777420044\n",
      "time_this_iter_s: 57.91013216972351\n",
      "time_total_s: 11009.866777420044\n",
      "timers:\n",
      "  learn_throughput: 851.891\n",
      "  learn_time_ms: 545.727\n",
      "  load_throughput: 772739.926\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 6.99\n",
      "  sample_time_ms: 66513.877\n",
      "timestamp: 1638994237\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 77307\n",
      "training_iteration: 186\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:186 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.993 2.0539999999999647\n",
      "blue_1 False False 600 -0.996 1.973999999999962\n",
      "agent_timesteps_total: 155814\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-11-49\n",
      "done: false\n",
      "episode_len_mean: 254.65\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5875699999999692\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 366\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.204283714294434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012281030416488647\n",
      "        model: {}\n",
      "        policy_loss: -0.36839568614959717\n",
      "        total_loss: -0.3266822099685669\n",
      "        vf_explained_var: -0.6506741642951965\n",
      "        vf_loss: 0.020730188116431236\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.280277729034424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018128467723727226\n",
      "        model: {}\n",
      "        policy_loss: -0.38220757246017456\n",
      "        total_loss: -0.05092437565326691\n",
      "        vf_explained_var: -0.6264272332191467\n",
      "        vf_loss: 0.3064759075641632\n",
      "  num_agent_steps_sampled: 155814\n",
      "  num_agent_steps_trained: 155814\n",
      "  num_steps_sampled: 77907\n",
      "  num_steps_trained: 77907\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 187\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.52525252525253\n",
      "  ram_util_percent: 37.296969696969704\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.301689999999991\n",
      "  blue_1: 0.28587999999999364\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748519376302693\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.13778138871055\n",
      "  mean_inference_ms: 2.8153734961951447\n",
      "  mean_raw_obs_processing_ms: 27.464512746947904\n",
      "time_since_restore: 11076.216069459915\n",
      "time_this_iter_s: 66.34929203987122\n",
      "time_total_s: 11076.216069459915\n",
      "timers:\n",
      "  learn_throughput: 846.457\n",
      "  learn_time_ms: 580.537\n",
      "  load_throughput: 984984.939\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.228\n",
      "  sample_time_ms: 67990.209\n",
      "timestamp: 1638994309\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 77907\n",
      "training_iteration: 187\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:187 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 101 -2.0 -1.4999999999999996\n",
      "blue_1 True True 101 0.005 0.5050000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 216 0.005 1.079999999999999\n",
      "blue_1 True True 216 -2.001 -0.9290000000000005\n",
      "agent_timesteps_total: 156448\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-12-42\n",
      "done: false\n",
      "episode_len_mean: 252.43\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.5616399999999695\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 368\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.698366641998291\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015147625468671322\n",
      "        model: {}\n",
      "        policy_loss: 0.02136251889169216\n",
      "        total_loss: 0.7340167760848999\n",
      "        vf_explained_var: -0.14409556984901428\n",
      "        vf_loss: 0.6867732405662537\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.081071376800537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011911232955753803\n",
      "        model: {}\n",
      "        policy_loss: -0.47554251551628113\n",
      "        total_loss: -0.31845083832740784\n",
      "        vf_explained_var: 0.22668375074863434\n",
      "        vf_loss: 0.14079219102859497\n",
      "  num_agent_steps_sampled: 156448\n",
      "  num_agent_steps_trained: 156448\n",
      "  num_steps_sampled: 78224\n",
      "  num_steps_trained: 78224\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 188\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.97534246575343\n",
      "  ram_util_percent: 37.28356164383562\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3005899999999913\n",
      "  blue_1: 0.2610499999999936\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.574840429366682\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.09460141268117\n",
      "  mean_inference_ms: 2.81507955352513\n",
      "  mean_raw_obs_processing_ms: 27.42732007021761\n",
      "time_since_restore: 11123.271833896637\n",
      "time_this_iter_s: 47.0557644367218\n",
      "time_total_s: 11123.271833896637\n",
      "timers:\n",
      "  learn_throughput: 842.579\n",
      "  learn_time_ms: 562.084\n",
      "  load_throughput: 787481.615\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.061\n",
      "  sample_time_ms: 67070.42\n",
      "timestamp: 1638994362\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 78224\n",
      "training_iteration: 188\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:188 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 517 -2.0 0.5799999999999672\n",
      "blue_1 True True 517 0.005 2.608999999999967\n",
      "agent_timesteps_total: 157482\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-13-48\n",
      "done: false\n",
      "episode_len_mean: 256.6\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.069999999999903\n",
      "episode_reward_mean: 0.6035799999999687\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 369\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.184597969055176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011296132579445839\n",
      "        model: {}\n",
      "        policy_loss: -0.10562543570995331\n",
      "        total_loss: 0.15343843400478363\n",
      "        vf_explained_var: -0.42866820096969604\n",
      "        vf_loss: 0.23976339399814606\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.607141494750977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014764074236154556\n",
      "        model: {}\n",
      "        policy_loss: -0.10389140248298645\n",
      "        total_loss: -0.03642437607049942\n",
      "        vf_explained_var: -0.7283343076705933\n",
      "        vf_loss: 0.04726361483335495\n",
      "  num_agent_steps_sampled: 157482\n",
      "  num_agent_steps_trained: 157482\n",
      "  num_steps_sampled: 78741\n",
      "  num_steps_trained: 78741\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 189\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.26263736263737\n",
      "  ram_util_percent: 37.2945054945055\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.32143999999999096\n",
      "  blue_1: 0.2821399999999933\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.574836422201235\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.0726994145848\n",
      "  mean_inference_ms: 2.814935277816393\n",
      "  mean_raw_obs_processing_ms: 27.408260583031847\n",
      "time_since_restore: 11183.900840044022\n",
      "time_this_iter_s: 60.629006147384644\n",
      "time_total_s: 11183.900840044022\n",
      "timers:\n",
      "  learn_throughput: 832.452\n",
      "  learn_time_ms: 593.788\n",
      "  load_throughput: 818235.246\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 7.172\n",
      "  sample_time_ms: 68922.544\n",
      "timestamp: 1638994428\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 78741\n",
      "training_iteration: 189\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:189 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 113 -2.0 -1.4399999999999995\n",
      "blue_1 True True 113 0.005 0.5650000000000004\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 549 -2.0 0.7399999999999638\n",
      "blue_1 True True 549 0.005 2.8039999999999643\n",
      "agent_timesteps_total: 158806\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-15-10\n",
      "done: false\n",
      "episode_len_mean: 255.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5925199999999688\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 371\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.222594261169434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012309655547142029\n",
      "        model: {}\n",
      "        policy_loss: -0.15992164611816406\n",
      "        total_loss: 0.177594393491745\n",
      "        vf_explained_var: -0.3489828407764435\n",
      "        vf_loss: 0.31648385524749756\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.584745407104492\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015961389988660812\n",
      "        model: {}\n",
      "        policy_loss: -0.1623787134885788\n",
      "        total_loss: -0.12065157294273376\n",
      "        vf_explained_var: -0.545030951499939\n",
      "        vf_loss: 0.019885284826159477\n",
      "  num_agent_steps_sampled: 158806\n",
      "  num_agent_steps_trained: 158806\n",
      "  num_steps_sampled: 79403\n",
      "  num_steps_trained: 79403\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 190\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.13826086956523\n",
      "  ram_util_percent: 37.39304347826088\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.28588999999999093\n",
      "  blue_1: 0.3066299999999934\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748319195026453\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 112.02732533426271\n",
      "  mean_inference_ms: 2.8146595163125316\n",
      "  mean_raw_obs_processing_ms: 27.369570739425203\n",
      "time_since_restore: 11261.0504052639\n",
      "time_this_iter_s: 77.14956521987915\n",
      "time_total_s: 11261.0504052639\n",
      "timers:\n",
      "  learn_throughput: 827.696\n",
      "  learn_time_ms: 590.796\n",
      "  load_throughput: 809461.937\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 7.218\n",
      "  sample_time_ms: 67751.69\n",
      "timestamp: 1638994510\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 79403\n",
      "training_iteration: 190\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:190 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 208 -2.0 -0.9650000000000001\n",
      "blue_1 True True 208 0.004 1.0470000000000008\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 157 -2.0 -1.2199999999999993\n",
      "blue_1 True True 157 0.005 0.7850000000000006\n",
      "agent_timesteps_total: 159536\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-16-09\n",
      "done: false\n",
      "episode_len_mean: 253.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5679899999999692\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 373\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.313755989074707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013839607127010822\n",
      "        model: {}\n",
      "        policy_loss: -0.2538723349571228\n",
      "        total_loss: 0.02680281363427639\n",
      "        vf_explained_var: 0.4849502742290497\n",
      "        vf_loss: 0.2570289075374603\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.166903495788574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013934357091784477\n",
      "        model: {}\n",
      "        policy_loss: -0.3496502637863159\n",
      "        total_loss: -0.31457215547561646\n",
      "        vf_explained_var: 0.3611336350440979\n",
      "        vf_loss: 0.016010118648409843\n",
      "  num_agent_steps_sampled: 159536\n",
      "  num_agent_steps_trained: 159536\n",
      "  num_steps_sampled: 79768\n",
      "  num_steps_trained: 79768\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 191\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.40493827160494\n",
      "  ram_util_percent: 37.834567901234564\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.26493999999999124\n",
      "  blue_1: 0.30304999999999355\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748255647845331\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.98314355985433\n",
      "  mean_inference_ms: 2.814393355122789\n",
      "  mean_raw_obs_processing_ms: 27.332699391150165\n",
      "time_since_restore: 11314.478407621384\n",
      "time_this_iter_s: 53.42800235748291\n",
      "time_total_s: 11314.478407621384\n",
      "timers:\n",
      "  learn_throughput: 841.669\n",
      "  learn_time_ms: 558.296\n",
      "  load_throughput: 777783.524\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 7.099\n",
      "  sample_time_ms: 66190.0\n",
      "timestamp: 1638994569\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 79768\n",
      "training_iteration: 191\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:190 starting ! -----------------\n",
      "agent_timesteps_total: 159536\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-16-09\n",
      "done: false\n",
      "episode_len_mean: 253.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5679899999999692\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 373\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.313755989074707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013839607127010822\n",
      "        model: {}\n",
      "        policy_loss: -0.2538723349571228\n",
      "        total_loss: 0.02680281363427639\n",
      "        vf_explained_var: 0.4849502742290497\n",
      "        vf_loss: 0.2570289075374603\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.166903495788574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013934357091784477\n",
      "        model: {}\n",
      "        policy_loss: -0.3496502637863159\n",
      "        total_loss: -0.31457215547561646\n",
      "        vf_explained_var: 0.3611336350440979\n",
      "        vf_loss: 0.016010118648409843\n",
      "  num_agent_steps_sampled: 159536\n",
      "  num_agent_steps_trained: 159536\n",
      "  num_steps_sampled: 79768\n",
      "  num_steps_trained: 79768\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 191\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.40493827160494\n",
      "  ram_util_percent: 37.834567901234564\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.26493999999999124\n",
      "  blue_1: 0.30304999999999355\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748255647845331\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.98314355985433\n",
      "  mean_inference_ms: 2.814393355122789\n",
      "  mean_raw_obs_processing_ms: 27.332699391150165\n",
      "time_since_restore: 11314.478407621384\n",
      "time_this_iter_s: 53.42800235748291\n",
      "time_total_s: 11314.478407621384\n",
      "timers:\n",
      "  learn_throughput: 841.669\n",
      "  learn_time_ms: 558.296\n",
      "  load_throughput: 777783.524\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 7.099\n",
      "  sample_time_ms: 66190.0\n",
      "timestamp: 1638994569\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 79768\n",
      "training_iteration: 191\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:191 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 437 -1.998 0.3019999999999874\n",
      "blue_1 True True 437 0.005 2.785999999999977\n",
      "agent_timesteps_total: 160410\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-17-05\n",
      "done: false\n",
      "episode_len_mean: 254.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5845499999999687\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 374\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.606213569641113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009780418127775192\n",
      "        model: {}\n",
      "        policy_loss: -0.3384953737258911\n",
      "        total_loss: -0.014576027169823647\n",
      "        vf_explained_var: -0.7435606122016907\n",
      "        vf_loss: 0.3072085976600647\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.054757118225098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009881076402962208\n",
      "        model: {}\n",
      "        policy_loss: -0.33438777923583984\n",
      "        total_loss: -0.29751595854759216\n",
      "        vf_explained_var: -0.3061387240886688\n",
      "        vf_loss: 0.023350369185209274\n",
      "  num_agent_steps_sampled: 160410\n",
      "  num_agent_steps_trained: 160410\n",
      "  num_steps_sampled: 80205\n",
      "  num_steps_trained: 80205\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 192\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.34025974025974\n",
      "  ram_util_percent: 37.853246753246744\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2519099999999912\n",
      "  blue_1: 0.33263999999999344\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748235883965366\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.96023295314933\n",
      "  mean_inference_ms: 2.814262649078286\n",
      "  mean_raw_obs_processing_ms: 27.313779397053544\n",
      "time_since_restore: 11364.241755008698\n",
      "time_this_iter_s: 49.76334738731384\n",
      "time_total_s: 11364.241755008698\n",
      "timers:\n",
      "  learn_throughput: 838.349\n",
      "  learn_time_ms: 558.359\n",
      "  load_throughput: 775140.63\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 7.245\n",
      "  sample_time_ms: 64613.99\n",
      "timestamp: 1638994625\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 80205\n",
      "training_iteration: 192\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:192 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 64 0.005 0.3200000000000002\n",
      "blue_1 True True 64 -2.0 -1.6849999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 231 0.005 1.1549999999999974\n",
      "blue_1 True True 231 -2.001 -0.8509999999999991\n",
      "agent_timesteps_total: 161000\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-17-58\n",
      "done: false\n",
      "episode_len_mean: 250.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5439399999999694\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 376\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.00387954711914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018078971654176712\n",
      "        model: {}\n",
      "        policy_loss: -0.06730633229017258\n",
      "        total_loss: 0.0010339011205360293\n",
      "        vf_explained_var: -0.2317465841770172\n",
      "        vf_loss: 0.03745062276721001\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.126235008239746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013882715255022049\n",
      "        model: {}\n",
      "        policy_loss: -0.27092650532722473\n",
      "        total_loss: 0.5890112519264221\n",
      "        vf_explained_var: 0.004539817571640015\n",
      "        vf_loss: 0.8409404754638672\n",
      "  num_agent_steps_sampled: 161000\n",
      "  num_agent_steps_trained: 161000\n",
      "  num_steps_sampled: 80500\n",
      "  num_steps_trained: 80500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 193\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.31621621621621\n",
      "  ram_util_percent: 37.683783783783774\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2519599999999916\n",
      "  blue_1: 0.2919799999999938\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748251509055661\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.9169065304649\n",
      "  mean_inference_ms: 2.8140141205511373\n",
      "  mean_raw_obs_processing_ms: 27.27972679810736\n",
      "time_since_restore: 11412.230149745941\n",
      "time_this_iter_s: 47.98839473724365\n",
      "time_total_s: 11412.230149745941\n",
      "timers:\n",
      "  learn_throughput: 829.405\n",
      "  learn_time_ms: 558.955\n",
      "  load_throughput: 767901.167\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 7.106\n",
      "  sample_time_ms: 65236.363\n",
      "timestamp: 1638994678\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 80500\n",
      "training_iteration: 193\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:193 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 191 0.005 0.9550000000000007\n",
      "blue_1 True True 191 -2.0 -1.0499999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 74 0.005 0.3700000000000002\n",
      "blue_1 True True 74 -2.0 -1.6349999999999998\n",
      "agent_timesteps_total: 161530\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-18-47\n",
      "done: false\n",
      "episode_len_mean: 249.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5301199999999695\n",
      "episode_reward_min: -1.4449999999999996\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 378\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.619764804840088\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01839495450258255\n",
      "        model: {}\n",
      "        policy_loss: -0.13361094892024994\n",
      "        total_loss: -0.09521039575338364\n",
      "        vf_explained_var: 0.33373260498046875\n",
      "        vf_loss: 0.006971040274947882\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.556211471557617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018282242119312286\n",
      "        model: {}\n",
      "        policy_loss: -0.14891143143177032\n",
      "        total_loss: 0.2941446006298065\n",
      "        vf_explained_var: 0.1446317881345749\n",
      "        vf_loss: 0.4180383086204529\n",
      "  num_agent_steps_sampled: 161530\n",
      "  num_agent_steps_trained: 161530\n",
      "  num_steps_sampled: 80765\n",
      "  num_steps_trained: 80765\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 194\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.56716417910449\n",
      "  ram_util_percent: 37.686567164179095\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2656599999999917\n",
      "  blue_1: 0.2644599999999939\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7049999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748292856396147\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.87495061169764\n",
      "  mean_inference_ms: 2.813773055520612\n",
      "  mean_raw_obs_processing_ms: 27.247404730525876\n",
      "time_since_restore: 11454.997326850891\n",
      "time_this_iter_s: 42.76717710494995\n",
      "time_total_s: 11454.997326850891\n",
      "timers:\n",
      "  learn_throughput: 826.231\n",
      "  learn_time_ms: 526.608\n",
      "  load_throughput: 720637.21\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 6.959\n",
      "  sample_time_ms: 62523.091\n",
      "timestamp: 1638994727\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 80765\n",
      "training_iteration: 194\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:194 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 41 -2.0 -1.7999999999999998\n",
      "blue_1 True True 41 0.005 0.2050000000000001\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 50 -2.0 -1.755\n",
      "blue_1 True True 50 0.005 0.2500000000000001\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 304 -2.0 -0.4850000000000103\n",
      "blue_1 True True 304 0.005 1.50499999999999\n",
      "agent_timesteps_total: 162320\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-20-02\n",
      "done: false\n",
      "episode_len_mean: 248.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5255999999999694\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 381\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.117215156555176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01585160382091999\n",
      "        model: {}\n",
      "        policy_loss: -0.1450621783733368\n",
      "        total_loss: 0.39516937732696533\n",
      "        vf_explained_var: 0.20814134180545807\n",
      "        vf_loss: 0.5131475925445557\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.508050441741943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01656302437186241\n",
      "        model: {}\n",
      "        policy_loss: -0.13229522109031677\n",
      "        total_loss: 0.17862175405025482\n",
      "        vf_explained_var: -0.5582029819488525\n",
      "        vf_loss: 0.2882518172264099\n",
      "  num_agent_steps_sampled: 162320\n",
      "  num_agent_steps_trained: 162320\n",
      "  num_steps_sampled: 81160\n",
      "  num_steps_trained: 81160\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 195\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.208653846153844\n",
      "  ram_util_percent: 37.69615384615384\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.22330999999999165\n",
      "  blue_1: 0.30228999999999384\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748247449857269\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.81351700095169\n",
      "  mean_inference_ms: 2.813422353432735\n",
      "  mean_raw_obs_processing_ms: 27.199397900176354\n",
      "time_since_restore: 11525.117482662201\n",
      "time_this_iter_s: 70.12015581130981\n",
      "time_total_s: 11525.117482662201\n",
      "timers:\n",
      "  learn_throughput: 807.539\n",
      "  learn_time_ms: 514.155\n",
      "  load_throughput: 687677.705\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 6.601\n",
      "  sample_time_ms: 62897.056\n",
      "timestamp: 1638994802\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 81160\n",
      "training_iteration: 195\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:195 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 272 0.005 1.359999999999993\n",
      "blue_1 True True 272 -2.0 -0.6410000000000053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 162864\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-20-42\n",
      "done: false\n",
      "episode_len_mean: 248.68\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5245999999999694\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 382\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.432516098022461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017274873331189156\n",
      "        model: {}\n",
      "        policy_loss: -0.08451561629772186\n",
      "        total_loss: 0.12663300335407257\n",
      "        vf_explained_var: -0.651197612285614\n",
      "        vf_loss: 0.1816328912973404\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.679295063018799\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011942301876842976\n",
      "        model: {}\n",
      "        policy_loss: -0.20773062109947205\n",
      "        total_loss: 0.0819418802857399\n",
      "        vf_explained_var: 0.3122480511665344\n",
      "        vf_loss: 0.273330420255661\n",
      "  num_agent_steps_sampled: 162864\n",
      "  num_agent_steps_trained: 162864\n",
      "  num_steps_sampled: 81432\n",
      "  num_steps_trained: 81432\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 196\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.96296296296296\n",
      "  ram_util_percent: 37.67962962962963\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.24290999999999163\n",
      "  blue_1: 0.28168999999999383\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.7249999999999999\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748253563803093\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.79261037562013\n",
      "  mean_inference_ms: 2.813302390629123\n",
      "  mean_raw_obs_processing_ms: 27.183039071742705\n",
      "time_since_restore: 11558.89809846878\n",
      "time_this_iter_s: 33.78061580657959\n",
      "time_total_s: 11558.89809846878\n",
      "timers:\n",
      "  learn_throughput: 802.132\n",
      "  learn_time_ms: 514.255\n",
      "  load_throughput: 683205.813\n",
      "  load_time_ms: 0.604\n",
      "  sample_throughput: 6.823\n",
      "  sample_time_ms: 60460.846\n",
      "timestamp: 1638994842\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 81432\n",
      "training_iteration: 196\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:196 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 128 -2.0 -1.3649999999999995\n",
      "blue_1 True True 128 0.005 0.6400000000000005\n",
      "LOSE\n",
      "blue_0 False True 348 -0.995 0.739999999999985\n",
      "blue_1 False True 348 -0.993 1.0039999999999853\n",
      "agent_timesteps_total: 163816\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-21-54\n",
      "done: false\n",
      "episode_len_mean: 250.92\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5496899999999694\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 384\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.099916458129883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017369011417031288\n",
      "        model: {}\n",
      "        policy_loss: -0.1865890771150589\n",
      "        total_loss: 0.31389299035072327\n",
      "        vf_explained_var: -0.22312672436237335\n",
      "        vf_loss: 0.47080549597740173\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.789580821990967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010032136924564838\n",
      "        model: {}\n",
      "        policy_loss: -0.4095064401626587\n",
      "        total_loss: -0.31081122159957886\n",
      "        vf_explained_var: -0.31465333700180054\n",
      "        vf_loss: 0.08496701717376709\n",
      "  num_agent_steps_sampled: 163816\n",
      "  num_agent_steps_trained: 163816\n",
      "  num_steps_sampled: 81908\n",
      "  num_steps_trained: 81908\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 197\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.522000000000006\n",
      "  ram_util_percent: 37.701\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.22405999999999146\n",
      "  blue_1: 0.32562999999999365\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748257469181017\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.7493656045445\n",
      "  mean_inference_ms: 2.813057000353664\n",
      "  mean_raw_obs_processing_ms: 27.147876288199345\n",
      "time_since_restore: 11625.375071048737\n",
      "time_this_iter_s: 66.47697257995605\n",
      "time_total_s: 11625.375071048737\n",
      "timers:\n",
      "  learn_throughput: 806.647\n",
      "  learn_time_ms: 496.004\n",
      "  load_throughput: 568745.689\n",
      "  load_time_ms: 0.703\n",
      "  sample_throughput: 6.614\n",
      "  sample_time_ms: 60492.745\n",
      "timestamp: 1638994914\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 81908\n",
      "training_iteration: 197\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:197 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 205 -2.0 -0.9799999999999998\n",
      "blue_1 True True 205 0.005 1.0250000000000001\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 209 -2.0 -0.9600000000000002\n",
      "blue_1 True True 209 0.005 1.0449999999999997\n",
      "agent_timesteps_total: 164644\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-22-51\n",
      "done: false\n",
      "episode_len_mean: 253.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.571189999999969\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 386\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.475052356719971\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012312541715800762\n",
      "        model: {}\n",
      "        policy_loss: -0.2049609273672104\n",
      "        total_loss: 0.062264494597911835\n",
      "        vf_explained_var: 0.1073986366391182\n",
      "        vf_loss: 0.24618831276893616\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.723571300506592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017898663878440857\n",
      "        model: {}\n",
      "        policy_loss: -0.1325019896030426\n",
      "        total_loss: -0.10469114780426025\n",
      "        vf_explained_var: 0.591340959072113\n",
      "        vf_loss: 0.0033179759047925472\n",
      "  num_agent_steps_sampled: 164644\n",
      "  num_agent_steps_trained: 164644\n",
      "  num_steps_sampled: 82322\n",
      "  num_steps_trained: 82322\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 198\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.416250000000005\n",
      "  ram_util_percent: 37.73125\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.23480999999999141\n",
      "  blue_1: 0.3363799999999937\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748288363315188\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.70487477938572\n",
      "  mean_inference_ms: 2.8128102944732167\n",
      "  mean_raw_obs_processing_ms: 27.11222348808235\n",
      "time_since_restore: 11677.209330558777\n",
      "time_this_iter_s: 51.83425951004028\n",
      "time_total_s: 11677.209330558777\n",
      "timers:\n",
      "  learn_throughput: 797.655\n",
      "  learn_time_ms: 513.756\n",
      "  load_throughput: 584753.956\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.725\n",
      "  sample_time_ms: 60932.419\n",
      "timestamp: 1638994971\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 82322\n",
      "training_iteration: 198\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:198 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 270 0.005 1.3499999999999932\n",
      "blue_1 True True 270 -1.998 -0.6340000000000052\n",
      "agent_timesteps_total: 165184\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-23-30\n",
      "done: false\n",
      "episode_len_mean: 253.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.577099999999969\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 387\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.9395623207092285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013429208658635616\n",
      "        model: {}\n",
      "        policy_loss: -0.1852555274963379\n",
      "        total_loss: -0.039137933403253555\n",
      "        vf_explained_var: -0.3492666482925415\n",
      "        vf_loss: 0.12317253649234772\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.960042953491211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01235221792012453\n",
      "        model: {}\n",
      "        policy_loss: -0.18974965810775757\n",
      "        total_loss: 0.09397483617067337\n",
      "        vf_explained_var: 0.36205214262008667\n",
      "        vf_loss: 0.2668214738368988\n",
      "  num_agent_steps_sampled: 165184\n",
      "  num_agent_steps_trained: 165184\n",
      "  num_steps_sampled: 82592\n",
      "  num_steps_trained: 82592\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 199\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.37777777777778\n",
      "  ram_util_percent: 37.74444444444445\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.23765999999999138\n",
      "  blue_1: 0.33943999999999364\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748287031608236\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.68249993366958\n",
      "  mean_inference_ms: 2.8126812230602773\n",
      "  mean_raw_obs_processing_ms: 27.093939965337064\n",
      "time_since_restore: 11710.827430725098\n",
      "time_this_iter_s: 33.6181001663208\n",
      "time_total_s: 11710.827430725098\n",
      "timers:\n",
      "  learn_throughput: 799.381\n",
      "  learn_time_ms: 481.748\n",
      "  load_throughput: 549247.304\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 6.607\n",
      "  sample_time_ms: 58285.624\n",
      "timestamp: 1638995010\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 82592\n",
      "training_iteration: 199\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:199 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 220 -2.0 -0.9050000000000014\n",
      "blue_1 True True 220 0.005 1.133999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 81 0.005 0.40500000000000025\n",
      "blue_1 True True 81 -2.0 -1.5999999999999996\n",
      "agent_timesteps_total: 165786\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-24-26\n",
      "done: false\n",
      "episode_len_mean: 251.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5565399999999694\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 389\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.116236686706543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016636788845062256\n",
      "        model: {}\n",
      "        policy_loss: 0.03921627998352051\n",
      "        total_loss: 0.2834567129611969\n",
      "        vf_explained_var: 0.32302871346473694\n",
      "        vf_loss: 0.21581488847732544\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.317645072937012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01199678611010313\n",
      "        model: {}\n",
      "        policy_loss: -0.3939797580242157\n",
      "        total_loss: -0.06769774109125137\n",
      "        vf_explained_var: -0.7100732922554016\n",
      "        vf_loss: 0.30986541509628296\n",
      "  num_agent_steps_sampled: 165786\n",
      "  num_agent_steps_trained: 165786\n",
      "  num_steps_sampled: 82893\n",
      "  num_steps_trained: 82893\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 200\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.142857142857146\n",
      "  ram_util_percent: 37.76883116883117\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2272099999999915\n",
      "  blue_1: 0.32932999999999374\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.574829620790387\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.64147066645617\n",
      "  mean_inference_ms: 2.8124307197892033\n",
      "  mean_raw_obs_processing_ms: 27.059612904499062\n",
      "time_since_restore: 11760.992548704147\n",
      "time_this_iter_s: 50.16511797904968\n",
      "time_total_s: 11760.992548704147\n",
      "timers:\n",
      "  learn_throughput: 808.472\n",
      "  learn_time_ms: 431.679\n",
      "  load_throughput: 434249.635\n",
      "  load_time_ms: 0.804\n",
      "  sample_throughput: 6.277\n",
      "  sample_time_ms: 55603.587\n",
      "timestamp: 1638995066\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 82893\n",
      "training_iteration: 200\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:200 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 189 0.005 0.9450000000000007\n",
      "blue_1 True True 189 -2.0 -1.0599999999999992\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 126 -2.0 -1.3749999999999996\n",
      "blue_1 True True 126 0.005 0.6300000000000004\n",
      "agent_timesteps_total: 166416\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-25-15\n",
      "done: false\n",
      "episode_len_mean: 247.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5089699999999703\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 391\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.363744258880615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01010535191744566\n",
      "        model: {}\n",
      "        policy_loss: -0.4409194588661194\n",
      "        total_loss: -0.23655398190021515\n",
      "        vf_explained_var: -0.3894825875759125\n",
      "        vf_loss: 0.18709951639175415\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.512527942657471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0219112541526556\n",
      "        model: {}\n",
      "        policy_loss: 0.07436125725507736\n",
      "        total_loss: 0.5409969091415405\n",
      "        vf_explained_var: 0.2600378394126892\n",
      "        vf_loss: 0.4366520047187805\n",
      "  num_agent_steps_sampled: 166416\n",
      "  num_agent_steps_trained: 166416\n",
      "  num_steps_sampled: 83208\n",
      "  num_steps_trained: 83208\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 201\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.79402985074627\n",
      "  ram_util_percent: 37.740298507462676\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19475999999999183\n",
      "  blue_1: 0.3142099999999939\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748299859232234\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.60137054613244\n",
      "  mean_inference_ms: 2.812186407896906\n",
      "  mean_raw_obs_processing_ms: 27.027909794314255\n",
      "time_since_restore: 11803.975167751312\n",
      "time_this_iter_s: 42.98261904716492\n",
      "time_total_s: 11803.975167751312\n",
      "timers:\n",
      "  learn_throughput: 793.411\n",
      "  learn_time_ms: 433.571\n",
      "  load_throughput: 488667.81\n",
      "  load_time_ms: 0.704\n",
      "  sample_throughput: 6.311\n",
      "  sample_time_ms: 54506.477\n",
      "timestamp: 1638995115\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 83208\n",
      "training_iteration: 201\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:200 starting ! -----------------\n",
      "agent_timesteps_total: 166416\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-25-15\n",
      "done: false\n",
      "episode_len_mean: 247.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5089699999999703\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 391\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.363744258880615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01010535191744566\n",
      "        model: {}\n",
      "        policy_loss: -0.4409194588661194\n",
      "        total_loss: -0.23655398190021515\n",
      "        vf_explained_var: -0.3894825875759125\n",
      "        vf_loss: 0.18709951639175415\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.512527942657471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0219112541526556\n",
      "        model: {}\n",
      "        policy_loss: 0.07436125725507736\n",
      "        total_loss: 0.5409969091415405\n",
      "        vf_explained_var: 0.2600378394126892\n",
      "        vf_loss: 0.4366520047187805\n",
      "  num_agent_steps_sampled: 166416\n",
      "  num_agent_steps_trained: 166416\n",
      "  num_steps_sampled: 83208\n",
      "  num_steps_trained: 83208\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 201\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.79402985074627\n",
      "  ram_util_percent: 37.740298507462676\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19475999999999183\n",
      "  blue_1: 0.3142099999999939\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748299859232234\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.60137054613244\n",
      "  mean_inference_ms: 2.812186407896906\n",
      "  mean_raw_obs_processing_ms: 27.027909794314255\n",
      "time_since_restore: 11803.975167751312\n",
      "time_this_iter_s: 42.98261904716492\n",
      "time_total_s: 11803.975167751312\n",
      "timers:\n",
      "  learn_throughput: 793.411\n",
      "  learn_time_ms: 433.571\n",
      "  load_throughput: 488667.81\n",
      "  load_time_ms: 0.704\n",
      "  sample_throughput: 6.311\n",
      "  sample_time_ms: 54506.477\n",
      "timestamp: 1638995115\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 83208\n",
      "training_iteration: 201\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:201 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 324 -2.0 -0.38500000000001244\n",
      "blue_1 True True 324 0.006 1.6259999999999886\n",
      "agent_timesteps_total: 167064\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-25-58\n",
      "done: false\n",
      "episode_len_mean: 249.01\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5284299999999698\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 392\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.2945780754089355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01160773728042841\n",
      "        model: {}\n",
      "        policy_loss: -0.49262598156929016\n",
      "        total_loss: -0.4232478737831116\n",
      "        vf_explained_var: -0.3490854501724243\n",
      "        vf_loss: 0.049545153975486755\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.0526275634765625\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.711694240570068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01111031137406826\n",
      "        model: {}\n",
      "        policy_loss: -0.3500422239303589\n",
      "        total_loss: -0.1472666710615158\n",
      "        vf_explained_var: -0.523070752620697\n",
      "        vf_loss: 0.17997024953365326\n",
      "  num_agent_steps_sampled: 167064\n",
      "  num_agent_steps_trained: 167064\n",
      "  num_steps_sampled: 83532\n",
      "  num_steps_trained: 83532\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 202\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.059322033898304\n",
      "  ram_util_percent: 37.69661016949152\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.18440999999999172\n",
      "  blue_1: 0.3440199999999938\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748284338663686\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.58070096335409\n",
      "  mean_inference_ms: 2.8120619812948386\n",
      "  mean_raw_obs_processing_ms: 27.011623575845455\n",
      "time_since_restore: 11841.319633722305\n",
      "time_this_iter_s: 37.34446597099304\n",
      "time_total_s: 11841.319633722305\n",
      "timers:\n",
      "  learn_throughput: 799.089\n",
      "  learn_time_ms: 416.349\n",
      "  load_throughput: 472503.62\n",
      "  load_time_ms: 0.704\n",
      "  sample_throughput: 6.244\n",
      "  sample_time_ms: 53284.684\n",
      "timestamp: 1638995158\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 83532\n",
      "training_iteration: 202\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:202 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 332 0.005 1.6599999999999866\n",
      "blue_1 True True 332 -1.999 -0.34600000000000875\n",
      "agent_timesteps_total: 167728\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-26-49\n",
      "done: false\n",
      "episode_len_mean: 251.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5488199999999693\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 393\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.8133463859558105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016735954210162163\n",
      "        model: {}\n",
      "        policy_loss: -0.175999253988266\n",
      "        total_loss: -0.0710650160908699\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.07633931934833527\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.0526275634765625\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.281618595123291\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007026176434010267\n",
      "        model: {}\n",
      "        policy_loss: -0.5292096734046936\n",
      "        total_loss: -0.4588002562522888\n",
      "        vf_explained_var: 0.30311718583106995\n",
      "        vf_loss: 0.055987294763326645\n",
      "  num_agent_steps_sampled: 167728\n",
      "  num_agent_steps_trained: 167728\n",
      "  num_steps_sampled: 83864\n",
      "  num_steps_trained: 83864\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 203\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.319718309859155\n",
      "  ram_util_percent: 37.85492957746479\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19460999999999157\n",
      "  blue_1: 0.3542099999999937\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748266993346216\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.56024344603775\n",
      "  mean_inference_ms: 2.8119374305613616\n",
      "  mean_raw_obs_processing_ms: 26.994976353676297\n",
      "time_since_restore: 11887.09415102005\n",
      "time_this_iter_s: 45.77451729774475\n",
      "time_total_s: 11887.09415102005\n",
      "timers:\n",
      "  learn_throughput: 805.66\n",
      "  learn_time_ms: 417.546\n",
      "  load_throughput: 556439.589\n",
      "  load_time_ms: 0.605\n",
      "  sample_throughput: 6.341\n",
      "  sample_time_ms: 53048.687\n",
      "timestamp: 1638995209\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 83864\n",
      "training_iteration: 203\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:203 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 191 -2.0 -1.0499999999999994\n",
      "blue_1 True True 191 0.005 0.9550000000000007\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 381 0.005 1.9049999999999814\n",
      "blue_1 True True 381 -2.001 -0.11500000000001331\n",
      "agent_timesteps_total: 168872\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-28-03\n",
      "done: false\n",
      "episode_len_mean: 249.62\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5312899999999695\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 395\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.167816162109375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013963667675852776\n",
      "        model: {}\n",
      "        policy_loss: 0.11049889773130417\n",
      "        total_loss: 0.5547930002212524\n",
      "        vf_explained_var: -0.17829500138759613\n",
      "        vf_loss: 0.4204358458518982\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.0526275634765625\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.318324565887451\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0067868828773498535\n",
      "        model: {}\n",
      "        policy_loss: -0.3295079171657562\n",
      "        total_loss: -0.2764763832092285\n",
      "        vf_explained_var: 0.14577817916870117\n",
      "        vf_loss: 0.03910057991743088\n",
      "  num_agent_steps_sampled: 168872\n",
      "  num_agent_steps_trained: 168872\n",
      "  num_steps_sampled: 84436\n",
      "  num_steps_trained: 84436\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 204\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.31747572815533\n",
      "  ram_util_percent: 37.961165048543705\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.20750999999999167\n",
      "  blue_1: 0.32377999999999363\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748220692206831\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.51883040487039\n",
      "  mean_inference_ms: 2.8116853652818254\n",
      "  mean_raw_obs_processing_ms: 26.963246361242096\n",
      "time_since_restore: 11955.795799255371\n",
      "time_this_iter_s: 68.70164823532104\n",
      "time_total_s: 11955.795799255371\n",
      "timers:\n",
      "  learn_throughput: 815.981\n",
      "  learn_time_ms: 449.888\n",
      "  load_throughput: 607268.388\n",
      "  load_time_ms: 0.605\n",
      "  sample_throughput: 6.601\n",
      "  sample_time_ms: 55614.538\n",
      "timestamp: 1638995283\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 84436\n",
      "training_iteration: 204\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:204 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 82 -2.0 -1.5949999999999998\n",
      "blue_1 True True 82 0.005 0.41000000000000025\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 61 -2.0 -1.6999999999999997\n",
      "blue_1 True True 61 0.005 0.30500000000000016\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 310 -2.0 -0.45500000000001095\n",
      "blue_1 True True 310 0.005 1.549999999999989\n",
      "agent_timesteps_total: 169778\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-29-17\n",
      "done: false\n",
      "episode_len_mean: 248.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5144099999999696\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 398\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.437074184417725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015013051219284534\n",
      "        model: {}\n",
      "        policy_loss: -0.27109500765800476\n",
      "        total_loss: 0.25501614809036255\n",
      "        vf_explained_var: 0.29456284642219543\n",
      "        vf_loss: 0.5004600286483765\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.0526275634765625\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.899234771728516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02001345530152321\n",
      "        model: {}\n",
      "        policy_loss: 0.20063868165016174\n",
      "        total_loss: 0.2598177194595337\n",
      "        vf_explained_var: -0.06668417155742645\n",
      "        vf_loss: 0.018098872154951096\n",
      "  num_agent_steps_sampled: 169778\n",
      "  num_agent_steps_trained: 169778\n",
      "  num_steps_sampled: 84889\n",
      "  num_steps_trained: 84889\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 205\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.92038834951455\n",
      "  ram_util_percent: 37.81262135922331\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1705099999999918\n",
      "  blue_1: 0.3438999999999936\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748170048056382\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.45745580830653\n",
      "  mean_inference_ms: 2.8113250475123515\n",
      "  mean_raw_obs_processing_ms: 26.918527112075687\n",
      "time_since_restore: 12023.763100385666\n",
      "time_this_iter_s: 67.9673011302948\n",
      "time_total_s: 12023.763100385666\n",
      "timers:\n",
      "  learn_throughput: 830.909\n",
      "  learn_time_ms: 448.785\n",
      "  load_throughput: 616862.931\n",
      "  load_time_ms: 0.605\n",
      "  sample_throughput: 6.727\n",
      "  sample_time_ms: 55431.238\n",
      "timestamp: 1638995357\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 84889\n",
      "training_iteration: 205\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:205 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 347 -0.995 0.7349999999999851\n",
      "blue_1 False True 347 -0.996 0.6359999999999981\n",
      "agent_timesteps_total: 170472\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-30-01\n",
      "done: false\n",
      "episode_len_mean: 250.19\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.5329699999999693\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 399\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.12963581085205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008476674556732178\n",
      "        model: {}\n",
      "        policy_loss: -0.5589501857757568\n",
      "        total_loss: -0.37794479727745056\n",
      "        vf_explained_var: -0.3731054961681366\n",
      "        vf_loss: 0.16652220487594604\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.0789413452148438\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.338075637817383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004208140540868044\n",
      "        model: {}\n",
      "        policy_loss: -0.515490710735321\n",
      "        total_loss: -0.48371216654777527\n",
      "        vf_explained_var: -0.44467440247535706\n",
      "        vf_loss: 0.018821915611624718\n",
      "  num_agent_steps_sampled: 170472\n",
      "  num_agent_steps_trained: 170472\n",
      "  num_steps_sampled: 85236\n",
      "  num_steps_trained: 85236\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 206\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.63833333333333\n",
      "  ram_util_percent: 37.84000000000001\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9799999999999587\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19030999999999168\n",
      "  blue_1: 0.3426599999999936\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748131515059548\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.43564022073242\n",
      "  mean_inference_ms: 2.8112026970639494\n",
      "  mean_raw_obs_processing_ms: 26.90276663486934\n",
      "time_since_restore: 12062.186486959457\n",
      "time_this_iter_s: 38.423386573791504\n",
      "time_total_s: 12062.186486959457\n",
      "timers:\n",
      "  learn_throughput: 847.062\n",
      "  learn_time_ms: 449.081\n",
      "  load_throughput: 629269.667\n",
      "  load_time_ms: 0.605\n",
      "  sample_throughput: 6.805\n",
      "  sample_time_ms: 55896.899\n",
      "timestamp: 1638995401\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 85236\n",
      "training_iteration: 206\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:206 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 133 -2.0 -1.3399999999999994\n",
      "blue_1 True True 133 0.005 0.6650000000000005\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 71 -2.0 -1.65\n",
      "blue_1 True True 71 0.005 0.3550000000000002\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 52 -2.0 -1.7449999999999999\n",
      "blue_1 True True 52 0.005 0.2600000000000001\n",
      "agent_timesteps_total: 170984\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-31-00\n",
      "done: false\n",
      "episode_len_mean: 242.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.041999999999899\n",
      "episode_reward_mean: 0.4520499999999709\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 402\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.30948543548584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017203493043780327\n",
      "        model: {}\n",
      "        policy_loss: -0.10310036689043045\n",
      "        total_loss: 0.7678269147872925\n",
      "        vf_explained_var: 0.20508025586605072\n",
      "        vf_loss: 0.841533362865448\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5394706726074219\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.628938674926758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020061174407601357\n",
      "        model: {}\n",
      "        policy_loss: -0.10983412712812424\n",
      "        total_loss: -0.055899981409311295\n",
      "        vf_explained_var: 0.5251780152320862\n",
      "        vf_loss: 0.023050546646118164\n",
      "  num_agent_steps_sampled: 170984\n",
      "  num_agent_steps_trained: 170984\n",
      "  num_steps_sampled: 85492\n",
      "  num_steps_trained: 85492\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 207\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.225301204819274\n",
      "  ram_util_percent: 37.82650602409639\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08985999999999222\n",
      "  blue_1: 0.362189999999994\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748026422790636\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.37864180635097\n",
      "  mean_inference_ms: 2.810837106110156\n",
      "  mean_raw_obs_processing_ms: 26.863539783912234\n",
      "time_since_restore: 12116.19913649559\n",
      "time_this_iter_s: 54.01264953613281\n",
      "time_total_s: 12116.19913649559\n",
      "timers:\n",
      "  learn_throughput: 827.477\n",
      "  learn_time_ms: 433.124\n",
      "  load_throughput: 595577.874\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 6.556\n",
      "  sample_time_ms: 54669.485\n",
      "timestamp: 1638995460\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 85492\n",
      "training_iteration: 207\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:207 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 522 -2.0 0.6049999999999667\n",
      "blue_1 True True 522 0.005 2.620999999999968\n",
      "agent_timesteps_total: 172028\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-32-05\n",
      "done: false\n",
      "episode_len_mean: 241.35\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.027999999999878\n",
      "episode_reward_mean: 0.443889999999971\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 403\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.959425926208496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013245650567114353\n",
      "        model: {}\n",
      "        policy_loss: -0.11108198016881943\n",
      "        total_loss: 0.40020594000816345\n",
      "        vf_explained_var: -0.6469249725341797\n",
      "        vf_loss: 0.48865649104118347\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.309206008911133\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.0704984664917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011452823877334595\n",
      "        model: {}\n",
      "        policy_loss: -0.10050158947706223\n",
      "        total_loss: -0.042774152010679245\n",
      "        vf_explained_var: -0.7790281772613525\n",
      "        vf_loss: 0.0312805101275444\n",
      "  num_agent_steps_sampled: 172028\n",
      "  num_agent_steps_trained: 172028\n",
      "  num_steps_sampled: 86014\n",
      "  num_steps_trained: 86014\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 208\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.24444444444445\n",
      "  ram_util_percent: 37.83222222222222\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.07590999999999228\n",
      "  blue_1: 0.36797999999999415\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748051689444735\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.3601677940961\n",
      "  mean_inference_ms: 2.8107239535469444\n",
      "  mean_raw_obs_processing_ms: 26.851795742004757\n",
      "time_since_restore: 12175.657995462418\n",
      "time_this_iter_s: 59.45885896682739\n",
      "time_total_s: 12175.657995462418\n",
      "timers:\n",
      "  learn_throughput: 821.941\n",
      "  learn_time_ms: 449.181\n",
      "  load_throughput: 613549.284\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 6.664\n",
      "  sample_time_ms: 55402.163\n",
      "timestamp: 1638995525\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 86014\n",
      "training_iteration: 208\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:208 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 309 -2.0 -0.46000000000001084\n",
      "blue_1 True True 309 0.006 1.5479999999999905\n",
      "agent_timesteps_total: 172646\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-32-43\n",
      "done: false\n",
      "episode_len_mean: 242.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.027999999999878\n",
      "episode_reward_mean: 0.4557199999999708\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 404\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.164541721343994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005879970267415047\n",
      "        model: {}\n",
      "        policy_loss: -0.4164285361766815\n",
      "        total_loss: -0.333127498626709\n",
      "        vf_explained_var: 0.25940147042274475\n",
      "        vf_loss: 0.0732545331120491\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.309206008911133\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.399267196655273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0062794373370707035\n",
      "        model: {}\n",
      "        policy_loss: -0.41632089018821716\n",
      "        total_loss: -0.3997388482093811\n",
      "        vf_explained_var: 0.31962695717811584\n",
      "        vf_loss: 0.002081583719700575\n",
      "  num_agent_steps_sampled: 172646\n",
      "  num_agent_steps_trained: 172646\n",
      "  num_steps_sampled: 86323\n",
      "  num_steps_trained: 86323\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 209\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.65283018867923\n",
      "  ram_util_percent: 37.867924528301884\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08180999999999218\n",
      "  blue_1: 0.373909999999994\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748084599218476\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.34082806672455\n",
      "  mean_inference_ms: 2.8106115525731856\n",
      "  mean_raw_obs_processing_ms: 26.839651862478473\n",
      "time_since_restore: 12208.354975700378\n",
      "time_this_iter_s: 32.696980237960815\n",
      "time_total_s: 12208.354975700378\n",
      "timers:\n",
      "  learn_throughput: 830.944\n",
      "  learn_time_ms: 449.008\n",
      "  load_throughput: 747751.731\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 6.745\n",
      "  sample_time_ms: 55317.917\n",
      "timestamp: 1638995563\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 86323\n",
      "training_iteration: 209\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:209 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.0129999999999595\n",
      "agent_timesteps_total: 173846\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-33-58\n",
      "done: false\n",
      "episode_len_mean: 246.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.027999999999878\n",
      "episode_reward_mean: 0.49708999999996983\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 405\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.777103424072266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0046027908101677895\n",
      "        model: {}\n",
      "        policy_loss: -0.36366745829582214\n",
      "        total_loss: -0.27999916672706604\n",
      "        vf_explained_var: -0.8822402954101562\n",
      "        vf_loss: 0.07580399513244629\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.309206008911133\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.849065780639648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0037771358620375395\n",
      "        model: {}\n",
      "        policy_loss: -0.381168395280838\n",
      "        total_loss: -0.35098856687545776\n",
      "        vf_explained_var: -0.47871384024620056\n",
      "        vf_loss: 0.02145766094326973\n",
      "  num_agent_steps_sampled: 173846\n",
      "  num_agent_steps_trained: 173846\n",
      "  num_steps_sampled: 86923\n",
      "  num_steps_trained: 86923\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 210\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.47254901960785\n",
      "  ram_util_percent: 37.90490196078431\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.9399999999999595\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.09235999999999177\n",
      "  blue_1: 0.40472999999999365\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748120772853003\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.32085287504404\n",
      "  mean_inference_ms: 2.810494486288228\n",
      "  mean_raw_obs_processing_ms: 26.826374928358394\n",
      "time_since_restore: 12277.760600090027\n",
      "time_this_iter_s: 69.40562438964844\n",
      "time_total_s: 12277.760600090027\n",
      "timers:\n",
      "  learn_throughput: 833.329\n",
      "  learn_time_ms: 483.603\n",
      "  load_throughput: 812216.862\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 7.045\n",
      "  sample_time_ms: 57205.541\n",
      "timestamp: 1638995638\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 86923\n",
      "training_iteration: 210\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:210 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 593 0.005 2.964999999999959\n",
      "blue_1 True True 593 -1.999 1.001999999999958\n",
      "agent_timesteps_total: 175032\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-35-12\n",
      "done: false\n",
      "episode_len_mean: 250.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.027999999999878\n",
      "episode_reward_mean: 0.5410099999999691\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 406\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.970685005187988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013545670546591282\n",
      "        model: {}\n",
      "        policy_loss: -0.26760751008987427\n",
      "        total_loss: -0.25166305899620056\n",
      "        vf_explained_var: -0.870343029499054\n",
      "        vf_loss: 0.004372436087578535\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.073025703430176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007003158330917358\n",
      "        model: {}\n",
      "        policy_loss: -0.3784805238246918\n",
      "        total_loss: -0.29877907037734985\n",
      "        vf_explained_var: -0.4937354624271393\n",
      "        vf_loss: 0.07161559164524078\n",
      "  num_agent_steps_sampled: 175032\n",
      "  num_agent_steps_trained: 175032\n",
      "  num_steps_sampled: 87516\n",
      "  num_steps_trained: 87516\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 211\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.06138613861386\n",
      "  ram_util_percent: 37.94950495049505\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.964999999999959\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11410999999999133\n",
      "  blue_1: 0.4268999999999933\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748179914024181\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.30077980703149\n",
      "  mean_inference_ms: 2.8103855422541244\n",
      "  mean_raw_obs_processing_ms: 26.812648910221906\n",
      "time_since_restore: 12345.457023143768\n",
      "time_this_iter_s: 67.69642305374146\n",
      "time_total_s: 12345.457023143768\n",
      "timers:\n",
      "  learn_throughput: 836.313\n",
      "  learn_time_ms: 515.118\n",
      "  load_throughput: 868245.718\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 7.219\n",
      "  sample_time_ms: 59678.522\n",
      "timestamp: 1638995712\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 87516\n",
      "training_iteration: 211\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:210 starting ! -----------------\n",
      "agent_timesteps_total: 175032\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-35-12\n",
      "done: false\n",
      "episode_len_mean: 250.99\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.027999999999878\n",
      "episode_reward_mean: 0.5410099999999691\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 406\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.970685005187988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013545670546591282\n",
      "        model: {}\n",
      "        policy_loss: -0.26760751008987427\n",
      "        total_loss: -0.25166305899620056\n",
      "        vf_explained_var: -0.870343029499054\n",
      "        vf_loss: 0.004372436087578535\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.073025703430176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007003158330917358\n",
      "        model: {}\n",
      "        policy_loss: -0.3784805238246918\n",
      "        total_loss: -0.29877907037734985\n",
      "        vf_explained_var: -0.4937354624271393\n",
      "        vf_loss: 0.07161559164524078\n",
      "  num_agent_steps_sampled: 175032\n",
      "  num_agent_steps_trained: 175032\n",
      "  num_steps_sampled: 87516\n",
      "  num_steps_trained: 87516\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 211\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.06138613861386\n",
      "  ram_util_percent: 37.94950495049505\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.964999999999959\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11410999999999133\n",
      "  blue_1: 0.4268999999999933\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748179914024181\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.30077980703149\n",
      "  mean_inference_ms: 2.8103855422541244\n",
      "  mean_raw_obs_processing_ms: 26.812648910221906\n",
      "time_since_restore: 12345.457023143768\n",
      "time_this_iter_s: 67.69642305374146\n",
      "time_total_s: 12345.457023143768\n",
      "timers:\n",
      "  learn_throughput: 836.313\n",
      "  learn_time_ms: 515.118\n",
      "  load_throughput: 868245.718\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 7.219\n",
      "  sample_time_ms: 59678.522\n",
      "timestamp: 1638995712\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 87516\n",
      "training_iteration: 211\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:211 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 121 -2.0 -1.3999999999999995\n",
      "blue_1 True True 121 0.005 0.6050000000000004\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 283 0.005 1.4149999999999918\n",
      "blue_1 True True 283 -2.0 -0.5900000000000081\n",
      "agent_timesteps_total: 175840\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-36-10\n",
      "done: false\n",
      "episode_len_mean: 248.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.027999999999878\n",
      "episode_reward_mean: 0.5130799999999696\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 408\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.613134860992432\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021902699023485184\n",
      "        model: {}\n",
      "        policy_loss: -0.0859309509396553\n",
      "        total_loss: 0.14178651571273804\n",
      "        vf_explained_var: 0.43577733635902405\n",
      "        vf_loss: 0.20900605618953705\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.420668601989746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01949997805058956\n",
      "        model: {}\n",
      "        policy_loss: -0.217924103140831\n",
      "        total_loss: 0.0035285777412354946\n",
      "        vf_explained_var: -0.11927173286676407\n",
      "        vf_loss: 0.19893792271614075\n",
      "  num_agent_steps_sampled: 175840\n",
      "  num_agent_steps_trained: 175840\n",
      "  num_steps_sampled: 87920\n",
      "  num_steps_trained: 87920\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 212\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.701249999999995\n",
      "  ram_util_percent: 37.897499999999994\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 2.964999999999959\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08060999999999152\n",
      "  blue_1: 0.4324699999999934\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748284223963953\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.25995655126047\n",
      "  mean_inference_ms: 2.8101698364261334\n",
      "  mean_raw_obs_processing_ms: 26.78563208820224\n",
      "time_since_restore: 12398.15708899498\n",
      "time_this_iter_s: 52.70006585121155\n",
      "time_total_s: 12398.15708899498\n",
      "timers:\n",
      "  learn_throughput: 824.101\n",
      "  learn_time_ms: 532.459\n",
      "  load_throughput: 1107310.388\n",
      "  load_time_ms: 0.396\n",
      "  sample_throughput: 7.167\n",
      "  sample_time_ms: 61229.257\n",
      "timestamp: 1638995770\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 87920\n",
      "training_iteration: 212\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:212 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 421 -1.98 3.480000000000008\n",
      "blue_1 True True 421 0.005 2.1049999999999773\n",
      "agent_timesteps_total: 176682\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-37-06\n",
      "done: false\n",
      "episode_len_mean: 251.91\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5829799999999694\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 409\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.980248928070068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012408453971147537\n",
      "        model: {}\n",
      "        policy_loss: -0.2594161629676819\n",
      "        total_loss: 0.44360172748565674\n",
      "        vf_explained_var: -0.6977640390396118\n",
      "        vf_loss: 0.6871172189712524\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.381976127624512\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011278774589300156\n",
      "        model: {}\n",
      "        policy_loss: -0.301229864358902\n",
      "        total_loss: -0.2611979842185974\n",
      "        vf_explained_var: -0.6382802128791809\n",
      "        vf_loss: 0.0270093884319067\n",
      "  num_agent_steps_sampled: 176682\n",
      "  num_agent_steps_trained: 176682\n",
      "  num_steps_sampled: 88341\n",
      "  num_steps_trained: 88341\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 213\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.84230769230768\n",
      "  ram_util_percent: 37.915384615384625\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1324599999999916\n",
      "  blue_1: 0.45051999999999315\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748341223094762\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.2392145478907\n",
      "  mean_inference_ms: 2.810067969536898\n",
      "  mean_raw_obs_processing_ms: 26.770792941821767\n",
      "time_since_restore: 12449.032821178436\n",
      "time_this_iter_s: 50.87573218345642\n",
      "time_total_s: 12449.032821178436\n",
      "timers:\n",
      "  learn_throughput: 814.38\n",
      "  learn_time_ms: 549.743\n",
      "  load_throughput: 1129769.509\n",
      "  load_time_ms: 0.396\n",
      "  sample_throughput: 7.252\n",
      "  sample_time_ms: 61736.59\n",
      "timestamp: 1638995826\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 88341\n",
      "training_iteration: 213\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:213 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 73 -2.0 -1.6399999999999997\n",
      "blue_1 True True 73 0.005 0.3650000000000002\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 151 -2.0 -1.2499999999999996\n",
      "blue_1 True True 151 0.005 0.7550000000000006\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 57 -2.0 -1.7199999999999998\n",
      "blue_1 True True 57 0.005 0.28500000000000014\n",
      "agent_timesteps_total: 177244\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-38-07\n",
      "done: false\n",
      "episode_len_mean: 248.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5527299999999697\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 412\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.261061668395996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019026458263397217\n",
      "        model: {}\n",
      "        policy_loss: -0.19157372415065765\n",
      "        total_loss: 1.3509117364883423\n",
      "        vf_explained_var: 0.12702932953834534\n",
      "        vf_loss: 1.5181039571762085\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1546030044555664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.833183288574219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02116585150361061\n",
      "        model: {}\n",
      "        policy_loss: -0.24581946432590485\n",
      "        total_loss: -0.21130500733852386\n",
      "        vf_explained_var: 0.25126487016677856\n",
      "        vf_loss: 0.010076298378407955\n",
      "  num_agent_steps_sampled: 177244\n",
      "  num_agent_steps_trained: 177244\n",
      "  num_steps_sampled: 88622\n",
      "  num_steps_trained: 88622\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 214\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.73571428571428\n",
      "  ram_util_percent: 37.9\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.2939999999999676\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.09725999999999164\n",
      "  blue_1: 0.45546999999999327\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748505096143119\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.17918758293241\n",
      "  mean_inference_ms: 2.8097615909744524\n",
      "  mean_raw_obs_processing_ms: 26.729499862532112\n",
      "time_since_restore: 12503.895269870758\n",
      "time_this_iter_s: 54.86244869232178\n",
      "time_total_s: 12503.895269870758\n",
      "timers:\n",
      "  learn_throughput: 808.722\n",
      "  learn_time_ms: 517.607\n",
      "  load_throughput: 1056272.202\n",
      "  load_time_ms: 0.396\n",
      "  sample_throughput: 6.931\n",
      "  sample_time_ms: 60397.378\n",
      "timestamp: 1638995887\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 88622\n",
      "training_iteration: 214\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:214 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 103 -2.0 -1.4899999999999998\n",
      "blue_1 True True 103 0.005 0.5150000000000003\n",
      "LOSE\n",
      "blue_0 False False 600 -0.996 1.9629999999999623\n",
      "blue_1 False False 600 -0.995 1.9999999999999583\n",
      "agent_timesteps_total: 178650\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-39-41\n",
      "done: false\n",
      "episode_len_mean: 248.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5437899999999696\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 414\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.829692840576172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017219137400388718\n",
      "        model: {}\n",
      "        policy_loss: -0.16778019070625305\n",
      "        total_loss: 0.2836667001247406\n",
      "        vf_explained_var: -0.4905441403388977\n",
      "        vf_loss: 0.42938145995140076\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.642925262451172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007075813598930836\n",
      "        model: {}\n",
      "        policy_loss: -0.2753150463104248\n",
      "        total_loss: -0.22046081721782684\n",
      "        vf_explained_var: -0.30307742953300476\n",
      "        vf_loss: 0.04259956628084183\n",
      "  num_agent_steps_sampled: 178650\n",
      "  num_agent_steps_trained: 178650\n",
      "  num_steps_sampled: 89325\n",
      "  num_steps_trained: 89325\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 215\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.84384615384616\n",
      "  ram_util_percent: 37.988461538461536\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08568999999999162\n",
      "  blue_1: 0.4580999999999932\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748608462696039\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.13944671560861\n",
      "  mean_inference_ms: 2.8095571536636244\n",
      "  mean_raw_obs_processing_ms: 26.701916373392372\n",
      "time_since_restore: 12592.885210752487\n",
      "time_this_iter_s: 88.98994088172913\n",
      "time_total_s: 12592.885210752487\n",
      "timers:\n",
      "  learn_throughput: 810.796\n",
      "  learn_time_ms: 547.116\n",
      "  load_throughput: 894301.011\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 7.105\n",
      "  sample_time_ms: 62436.173\n",
      "timestamp: 1638995981\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 89325\n",
      "training_iteration: 215\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:215 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 234 0.005 1.169999999999997\n",
      "blue_1 True True 234 -2.0 -0.8350000000000029\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 107 -2.0 -1.4699999999999998\n",
      "blue_1 True True 107 0.005 0.5350000000000004\n",
      "agent_timesteps_total: 179332\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-40-32\n",
      "done: false\n",
      "episode_len_mean: 247.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5334899999999698\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 416\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.8954877853393555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008573793806135654\n",
      "        model: {}\n",
      "        policy_loss: -0.5467206239700317\n",
      "        total_loss: -0.46185439825057983\n",
      "        vf_explained_var: -0.14729692041873932\n",
      "        vf_loss: 0.07387945801019669\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.124878406524658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018117491155862808\n",
      "        model: {}\n",
      "        policy_loss: 0.11756008118391037\n",
      "        total_loss: 0.5365840792655945\n",
      "        vf_explained_var: 0.3984828591346741\n",
      "        vf_loss: 0.3876461386680603\n",
      "  num_agent_steps_sampled: 179332\n",
      "  num_agent_steps_trained: 179332\n",
      "  num_steps_sampled: 89666\n",
      "  num_steps_trained: 89666\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 216\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.90428571428571\n",
      "  ram_util_percent: 37.95857142857143\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.06048999999999163\n",
      "  blue_1: 0.47299999999999315\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748676377370151\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.102095863057\n",
      "  mean_inference_ms: 2.809320532864997\n",
      "  mean_raw_obs_processing_ms: 26.676523605326096\n",
      "time_since_restore: 12637.795063495636\n",
      "time_this_iter_s: 44.909852743148804\n",
      "time_total_s: 12637.795063495636\n",
      "timers:\n",
      "  learn_throughput: 808.065\n",
      "  learn_time_ms: 548.223\n",
      "  load_throughput: 743647.111\n",
      "  load_time_ms: 0.596\n",
      "  sample_throughput: 7.019\n",
      "  sample_time_ms: 63111.778\n",
      "timestamp: 1638996032\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 89666\n",
      "training_iteration: 216\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:216 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 178 0.005 0.8900000000000007\n",
      "blue_1 True True 178 -2.0 -1.1149999999999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False True 420 -0.995 1.0999999999999774\n",
      "blue_1 False True 420 -0.993 1.4199999999999906\n",
      "agent_timesteps_total: 180528\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-41-54\n",
      "done: false\n",
      "episode_len_mean: 250.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5639399999999695\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 418\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.140296459197998\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012738869525492191\n",
      "        model: {}\n",
      "        policy_loss: -0.3576769530773163\n",
      "        total_loss: -0.31736478209495544\n",
      "        vf_explained_var: -0.4006914794445038\n",
      "        vf_loss: 0.023987986147403717\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.234739303588867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017975354567170143\n",
      "        model: {}\n",
      "        policy_loss: -0.11856646835803986\n",
      "        total_loss: 0.35125359892845154\n",
      "        vf_explained_var: -0.12731680274009705\n",
      "        vf_loss: 0.4386884570121765\n",
      "  num_agent_steps_sampled: 180528\n",
      "  num_agent_steps_trained: 180528\n",
      "  num_steps_sampled: 90264\n",
      "  num_steps_trained: 90264\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 217\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.72456140350877\n",
      "  ram_util_percent: 37.95614035087719\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.08413999999999139\n",
      "  blue_1: 0.479799999999993\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748745062997906\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.06494050071154\n",
      "  mean_inference_ms: 2.8090848763525016\n",
      "  mean_raw_obs_processing_ms: 26.64947367528313\n",
      "time_since_restore: 12714.721709728241\n",
      "time_this_iter_s: 76.92664623260498\n",
      "time_total_s: 12714.721709728241\n",
      "timers:\n",
      "  learn_throughput: 823.558\n",
      "  learn_time_ms: 579.437\n",
      "  load_throughput: 956796.151\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.299\n",
      "  sample_time_ms: 65376.885\n",
      "timestamp: 1638996114\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 90264\n",
      "training_iteration: 217\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:217 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 420 -0.995 1.2999999999999776\n",
      "blue_1 False True 420 -0.996 0.9789999999999965\n",
      "agent_timesteps_total: 181368\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-42-54\n",
      "done: false\n",
      "episode_len_mean: 253.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.595879999999969\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 419\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.872180938720703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009051978588104248\n",
      "        model: {}\n",
      "        policy_loss: -0.3002333641052246\n",
      "        total_loss: -0.253473699092865\n",
      "        vf_explained_var: -0.8049923181533813\n",
      "        vf_loss: 0.035160064697265625\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.521133422851562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012117416597902775\n",
      "        model: {}\n",
      "        policy_loss: -0.2697175443172455\n",
      "        total_loss: -0.17212264239788055\n",
      "        vf_explained_var: -0.3321289122104645\n",
      "        vf_loss: 0.07660873234272003\n",
      "  num_agent_steps_sampled: 181368\n",
      "  num_agent_steps_trained: 181368\n",
      "  num_steps_sampled: 90684\n",
      "  num_steps_trained: 90684\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 218\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.08414634146341\n",
      "  ram_util_percent: 37.96829268292683\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11173999999999117\n",
      "  blue_1: 0.48413999999999296\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.574878174306218\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.04663942332614\n",
      "  mean_inference_ms: 2.808967560854881\n",
      "  mean_raw_obs_processing_ms: 26.636021746112288\n",
      "time_since_restore: 12768.798776626587\n",
      "time_this_iter_s: 54.07706689834595\n",
      "time_total_s: 12768.798776626587\n",
      "timers:\n",
      "  learn_throughput: 828.32\n",
      "  learn_time_ms: 563.792\n",
      "  load_throughput: 1170655.013\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.197\n",
      "  sample_time_ms: 64883.746\n",
      "timestamp: 1638996174\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 90684\n",
      "training_iteration: 218\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:218 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 221 0.005 1.1049999999999984\n",
      "blue_1 True True 221 -2.0 -0.9000000000000015\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 331 -2.0 -0.3500000000000132\n",
      "blue_1 True True 331 0.005 1.6399999999999884\n",
      "agent_timesteps_total: 182472\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-44-08\n",
      "done: false\n",
      "episode_len_mean: 253.18\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5927699999999692\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 421\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.620432376861572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017817605286836624\n",
      "        model: {}\n",
      "        policy_loss: -0.2541719377040863\n",
      "        total_loss: -0.08488927036523819\n",
      "        vf_explained_var: -0.28424790501594543\n",
      "        vf_loss: 0.14645041525363922\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.888662815093994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015353942289948463\n",
      "        model: {}\n",
      "        policy_loss: -0.06527959555387497\n",
      "        total_loss: 0.2813997268676758\n",
      "        vf_explained_var: -0.22417594492435455\n",
      "        vf_loss: 0.3200877606868744\n",
      "  num_agent_steps_sampled: 182472\n",
      "  num_agent_steps_trained: 182472\n",
      "  num_steps_sampled: 91236\n",
      "  num_steps_trained: 91236\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 219\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.40980392156862\n",
      "  ram_util_percent: 37.97254901960785\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11043999999999127\n",
      "  blue_1: 0.48232999999999315\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748861723721428\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 111.00914182557855\n",
      "  mean_inference_ms: 2.8087361247171185\n",
      "  mean_raw_obs_processing_ms: 26.609138858485174\n",
      "time_since_restore: 12837.223130226135\n",
      "time_this_iter_s: 68.42435359954834\n",
      "time_total_s: 12837.223130226135\n",
      "timers:\n",
      "  learn_throughput: 816.293\n",
      "  learn_time_ms: 601.867\n",
      "  load_throughput: 985255.346\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.182\n",
      "  sample_time_ms: 68407.959\n",
      "timestamp: 1638996248\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 91236\n",
      "training_iteration: 219\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:219 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 328 0.005 1.639999999999987\n",
      "blue_1 True True 328 -2.0 -0.33900000000001285\n",
      "agent_timesteps_total: 183128\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-44-55\n",
      "done: false\n",
      "episode_len_mean: 251.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5811999999999694\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 422\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.696544647216797\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011195073835551739\n",
      "        model: {}\n",
      "        policy_loss: -0.48326820135116577\n",
      "        total_loss: -0.4298495948314667\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.03907278552651405\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.128776550292969\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008865782991051674\n",
      "        model: {}\n",
      "        policy_loss: -0.514417827129364\n",
      "        total_loss: -0.39464041590690613\n",
      "        vf_explained_var: -0.609762966632843\n",
      "        vf_loss: 0.10442274063825607\n",
      "  num_agent_steps_sampled: 183128\n",
      "  num_agent_steps_trained: 183128\n",
      "  num_steps_sampled: 91564\n",
      "  num_steps_trained: 91564\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 220\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.93230769230768\n",
      "  ram_util_percent: 37.983076923076915\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1135899999999914\n",
      "  blue_1: 0.4676099999999931\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748921662874491\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.99044046080716\n",
      "  mean_inference_ms: 2.808617630153181\n",
      "  mean_raw_obs_processing_ms: 26.595732020966935\n",
      "time_since_restore: 12878.670766830444\n",
      "time_this_iter_s: 41.44763660430908\n",
      "time_total_s: 12878.670766830444\n",
      "timers:\n",
      "  learn_throughput: 817.408\n",
      "  learn_time_ms: 567.771\n",
      "  load_throughput: 931153.545\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 7.065\n",
      "  sample_time_ms: 65690.281\n",
      "timestamp: 1638996295\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 91564\n",
      "training_iteration: 220\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:220 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 363 -2.0 -0.1900000000000166\n",
      "blue_1 True True 363 0.005 1.8319999999999848\n",
      "agent_timesteps_total: 183854\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-45-45\n",
      "done: false\n",
      "episode_len_mean: 252.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5857299999999692\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 423\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.466643810272217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01202976331114769\n",
      "        model: {}\n",
      "        policy_loss: -0.5705822110176086\n",
      "        total_loss: -0.5036413073539734\n",
      "        vf_explained_var: 0.298409640789032\n",
      "        vf_loss: 0.051525361835956573\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.880544662475586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009285923093557358\n",
      "        model: {}\n",
      "        policy_loss: -0.6327904462814331\n",
      "        total_loss: -0.5851022601127625\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.03160573169589043\n",
      "  num_agent_steps_sampled: 183854\n",
      "  num_agent_steps_trained: 183854\n",
      "  num_steps_sampled: 91927\n",
      "  num_steps_trained: 91927\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 221\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.45294117647059\n",
      "  ram_util_percent: 37.99117647058823\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11563999999999136\n",
      "  blue_1: 0.4700899999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748976586026442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.97195201188084\n",
      "  mean_inference_ms: 2.8084982052264977\n",
      "  mean_raw_obs_processing_ms: 26.582390148278886\n",
      "time_since_restore: 12922.68857216835\n",
      "time_this_iter_s: 44.017805337905884\n",
      "time_total_s: 12922.68857216835\n",
      "timers:\n",
      "  learn_throughput: 824.55\n",
      "  learn_time_ms: 534.958\n",
      "  load_throughput: 885007.173\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 6.966\n",
      "  sample_time_ms: 63323.939\n",
      "timestamp: 1638996345\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 91927\n",
      "training_iteration: 221\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:220 starting ! -----------------\n",
      "agent_timesteps_total: 183854\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-45-45\n",
      "done: false\n",
      "episode_len_mean: 252.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.5857299999999692\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 423\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.466643810272217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01202976331114769\n",
      "        model: {}\n",
      "        policy_loss: -0.5705822110176086\n",
      "        total_loss: -0.5036413073539734\n",
      "        vf_explained_var: 0.298409640789032\n",
      "        vf_loss: 0.051525361835956573\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.880544662475586\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009285923093557358\n",
      "        model: {}\n",
      "        policy_loss: -0.6327904462814331\n",
      "        total_loss: -0.5851022601127625\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.03160573169589043\n",
      "  num_agent_steps_sampled: 183854\n",
      "  num_agent_steps_trained: 183854\n",
      "  num_steps_sampled: 91927\n",
      "  num_steps_trained: 91927\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 221\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.45294117647059\n",
      "  ram_util_percent: 37.99117647058823\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.11563999999999136\n",
      "  blue_1: 0.4700899999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748976586026442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.97195201188084\n",
      "  mean_inference_ms: 2.8084982052264977\n",
      "  mean_raw_obs_processing_ms: 26.582390148278886\n",
      "time_since_restore: 12922.68857216835\n",
      "time_this_iter_s: 44.017805337905884\n",
      "time_total_s: 12922.68857216835\n",
      "timers:\n",
      "  learn_throughput: 824.55\n",
      "  learn_time_ms: 534.958\n",
      "  load_throughput: 885007.173\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 6.966\n",
      "  sample_time_ms: 63323.939\n",
      "timestamp: 1638996345\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 91927\n",
      "training_iteration: 221\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:221 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 352 -0.995 0.7599999999999846\n",
      "blue_1 False True 352 -0.993 1.0019999999999865\n",
      "agent_timesteps_total: 184558\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-46-33\n",
      "done: false\n",
      "episode_len_mean: 255.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6161999999999689\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 424\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.77519416809082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011041880585253239\n",
      "        model: {}\n",
      "        policy_loss: -0.5633223056793213\n",
      "        total_loss: -0.5363190770149231\n",
      "        vf_explained_var: -0.06003092601895332\n",
      "        vf_loss: 0.012853736989200115\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.322321891784668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011454555206000805\n",
      "        model: {}\n",
      "        policy_loss: -0.6002143025398254\n",
      "        total_loss: -0.56523197889328\n",
      "        vf_explained_var: -0.542532742023468\n",
      "        vf_loss: 0.015144141390919685\n",
      "  num_agent_steps_sampled: 184558\n",
      "  num_agent_steps_trained: 184558\n",
      "  num_steps_sampled: 92279\n",
      "  num_steps_trained: 92279\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 222\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.69253731343283\n",
      "  ram_util_percent: 37.9955223880597\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1396899999999912\n",
      "  blue_1: 0.4765099999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749045142649223\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.95160268160828\n",
      "  mean_inference_ms: 2.8083799459696492\n",
      "  mean_raw_obs_processing_ms: 26.567549819883425\n",
      "time_since_restore: 12965.785024404526\n",
      "time_this_iter_s: 43.09645223617554\n",
      "time_total_s: 12965.785024404526\n",
      "timers:\n",
      "  learn_throughput: 840.826\n",
      "  learn_time_ms: 518.419\n",
      "  load_throughput: 874574.08\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 6.992\n",
      "  sample_time_ms: 62343.417\n",
      "timestamp: 1638996393\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 92279\n",
      "training_iteration: 222\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:222 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 149 -2.0 -1.2599999999999993\n",
      "blue_1 True True 149 0.005 0.7450000000000006\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 325 0.005 1.6249999999999873\n",
      "blue_1 True True 325 -2.001 -0.478999999999999\n",
      "agent_timesteps_total: 185506\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-47-42\n",
      "done: false\n",
      "episode_len_mean: 257.15\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6365099999999686\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 426\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2814452648162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.189797401428223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022062156349420547\n",
      "        model: {}\n",
      "        policy_loss: 0.028110159561038017\n",
      "        total_loss: 0.5733954310417175\n",
      "        vf_explained_var: -0.3050103187561035\n",
      "        vf_loss: 0.5170137882232666\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.11797571182251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0061432248912751675\n",
      "        model: {}\n",
      "        policy_loss: -0.4353639483451843\n",
      "        total_loss: -0.4031515419483185\n",
      "        vf_explained_var: 0.5645522475242615\n",
      "        vf_loss: 0.021572941914200783\n",
      "  num_agent_steps_sampled: 185506\n",
      "  num_agent_steps_trained: 185506\n",
      "  num_steps_sampled: 92753\n",
      "  num_steps_trained: 92753\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 223\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.976041666666674\n",
      "  ram_util_percent: 37.9875\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.15033999999999106\n",
      "  blue_1: 0.4861699999999928\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749180981445831\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.91047007718987\n",
      "  mean_inference_ms: 2.808137317099969\n",
      "  mean_raw_obs_processing_ms: 26.537656108183164\n",
      "time_since_restore: 13029.30189871788\n",
      "time_this_iter_s: 63.51687431335449\n",
      "time_total_s: 13029.30189871788\n",
      "timers:\n",
      "  learn_throughput: 853.421\n",
      "  learn_time_ms: 516.978\n",
      "  load_throughput: 737643.77\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 6.938\n",
      "  sample_time_ms: 63593.465\n",
      "timestamp: 1638996462\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 92753\n",
      "training_iteration: 223\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:223 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 286 -2.0 -0.5750000000000084\n",
      "blue_1 True True 286 0.004 1.4009999999999978\n",
      "agent_timesteps_total: 186078\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-48-28\n",
      "done: false\n",
      "episode_len_mean: 256.77\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6322099999999686\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 427\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.3959479331970215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010215270332992077\n",
      "        model: {}\n",
      "        policy_loss: -0.29811903834342957\n",
      "        total_loss: -0.0637461468577385\n",
      "        vf_explained_var: 0.25666946172714233\n",
      "        vf_loss: 0.21473738551139832\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.495424270629883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007988657802343369\n",
      "        model: {}\n",
      "        policy_loss: 0.22443169355392456\n",
      "        total_loss: 0.2444598227739334\n",
      "        vf_explained_var: 0.2975304424762726\n",
      "        vf_loss: 0.006192517466843128\n",
      "  num_agent_steps_sampled: 186078\n",
      "  num_agent_steps_trained: 186078\n",
      "  num_steps_sampled: 93039\n",
      "  num_steps_trained: 93039\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 224\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.59047619047619\n",
      "  ram_util_percent: 37.990476190476194\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.14843999999999113\n",
      "  blue_1: 0.4837699999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749246788690783\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.89023067028893\n",
      "  mean_inference_ms: 2.8080202899142233\n",
      "  mean_raw_obs_processing_ms: 26.522992673097168\n",
      "time_since_restore: 13069.180977582932\n",
      "time_this_iter_s: 39.87907886505127\n",
      "time_total_s: 13069.180977582932\n",
      "timers:\n",
      "  learn_throughput: 854.914\n",
      "  learn_time_ms: 516.66\n",
      "  load_throughput: 886295.784\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 7.113\n",
      "  sample_time_ms: 62097.069\n",
      "timestamp: 1638996508\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 93039\n",
      "training_iteration: 224\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:224 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 129 0.005 0.6450000000000005\n",
      "blue_1 True True 129 -2.0 -1.3599999999999994\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.993 2.1349999999999625\n",
      "agent_timesteps_total: 187536\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-50-10\n",
      "done: false\n",
      "episode_len_mean: 259.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.656209999999968\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 429\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.411279678344727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0060519143007695675\n",
      "        model: {}\n",
      "        policy_loss: -0.3471769690513611\n",
      "        total_loss: -0.27346479892730713\n",
      "        vf_explained_var: -0.5829282402992249\n",
      "        vf_loss: 0.06207937374711037\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.1471586227417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010381042025983334\n",
      "        model: {}\n",
      "        policy_loss: -0.20190632343292236\n",
      "        total_loss: 0.16395772993564606\n",
      "        vf_explained_var: -0.3036833703517914\n",
      "        vf_loss: 0.34788504242897034\n",
      "  num_agent_steps_sampled: 187536\n",
      "  num_agent_steps_trained: 187536\n",
      "  num_steps_sampled: 93768\n",
      "  num_steps_trained: 93768\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 225\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.965\n",
      "  ram_util_percent: 38.00714285714286\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.15068999999999086\n",
      "  blue_1: 0.5055199999999926\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749379354645372\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.85062417287637\n",
      "  mean_inference_ms: 2.8077819136837467\n",
      "  mean_raw_obs_processing_ms: 26.4926643382487\n",
      "time_since_restore: 13165.455263137817\n",
      "time_this_iter_s: 96.27428555488586\n",
      "time_total_s: 13165.455263137817\n",
      "timers:\n",
      "  learn_throughput: 861.253\n",
      "  learn_time_ms: 515.876\n",
      "  load_throughput: 1114550.997\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.072\n",
      "  sample_time_ms: 62826.197\n",
      "timestamp: 1638996610\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 93768\n",
      "training_iteration: 225\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:225 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 292 -0.995 0.45999999999999097\n",
      "blue_1 False True 292 -0.996 0.370000000000001\n",
      "agent_timesteps_total: 188120\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-50-52\n",
      "done: false\n",
      "episode_len_mean: 256.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6247999999999686\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 430\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.358897686004639\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01700335182249546\n",
      "        model: {}\n",
      "        policy_loss: -0.33272019028663635\n",
      "        total_loss: -0.2468099445104599\n",
      "        vf_explained_var: 0.3516201972961426\n",
      "        vf_loss: 0.05322697013616562\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.863885879516602\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007120024878531694\n",
      "        model: {}\n",
      "        policy_loss: -0.346749871969223\n",
      "        total_loss: -0.3120954930782318\n",
      "        vf_explained_var: 0.5988536477088928\n",
      "        vf_loss: 0.022323183715343475\n",
      "  num_agent_steps_sampled: 188120\n",
      "  num_agent_steps_trained: 188120\n",
      "  num_steps_sampled: 94060\n",
      "  num_steps_trained: 94060\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 226\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.984482758620686\n",
      "  ram_util_percent: 37.977586206896554\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.12588999999999115\n",
      "  blue_1: 0.4989099999999931\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749511490343462\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.8310265876464\n",
      "  mean_inference_ms: 2.8076667767083494\n",
      "  mean_raw_obs_processing_ms: 26.478798577813624\n",
      "time_since_restore: 13202.291053533554\n",
      "time_this_iter_s: 36.835790395736694\n",
      "time_total_s: 13202.291053533554\n",
      "timers:\n",
      "  learn_throughput: 848.812\n",
      "  learn_time_ms: 517.665\n",
      "  load_throughput: 1101929.553\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.085\n",
      "  sample_time_ms: 62016.347\n",
      "timestamp: 1638996652\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 94060\n",
      "training_iteration: 226\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:226 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 401 -0.995 1.0049999999999795\n",
      "blue_1 False True 401 -0.993 1.2599999999999896\n",
      "agent_timesteps_total: 188922\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-51-51\n",
      "done: false\n",
      "episode_len_mean: 259.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6585999999999682\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 431\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.265543937683105\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01436846237629652\n",
      "        model: {}\n",
      "        policy_loss: -0.17002694308757782\n",
      "        total_loss: -0.06107276678085327\n",
      "        vf_explained_var: -0.18605586886405945\n",
      "        vf_loss: 0.08133558928966522\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.529157638549805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0077574485912919044\n",
      "        model: {}\n",
      "        policy_loss: -0.17432600259780884\n",
      "        total_loss: -0.07874902337789536\n",
      "        vf_explained_var: -0.3037976324558258\n",
      "        vf_loss: 0.08214180916547775\n",
      "  num_agent_steps_sampled: 188922\n",
      "  num_agent_steps_trained: 188922\n",
      "  num_steps_sampled: 94461\n",
      "  num_steps_trained: 94461\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 227\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.17777777777777\n",
      "  ram_util_percent: 37.983950617283945\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.13148999999999092\n",
      "  blue_1: 0.527109999999993\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749633350059501\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.81178827429869\n",
      "  mean_inference_ms: 2.807551508470687\n",
      "  mean_raw_obs_processing_ms: 26.46493267648188\n",
      "time_since_restore: 13255.729617595673\n",
      "time_this_iter_s: 53.43856406211853\n",
      "time_total_s: 13255.729617595673\n",
      "timers:\n",
      "  learn_throughput: 833.061\n",
      "  learn_time_ms: 503.805\n",
      "  load_throughput: 1052525.793\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.033\n",
      "  sample_time_ms: 59678.983\n",
      "timestamp: 1638996711\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 94461\n",
      "training_iteration: 227\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:227 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 403 -2.0 0.009999999999979359\n",
      "blue_1 True True 403 0.007 2.0409999999999817\n",
      "agent_timesteps_total: 189728\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-52-37\n",
      "done: false\n",
      "episode_len_mean: 258.88\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6512599999999682\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 432\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.19296407699585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014063163660466671\n",
      "        model: {}\n",
      "        policy_loss: -0.1801186203956604\n",
      "        total_loss: 0.03621001914143562\n",
      "        vf_explained_var: 0.4773595631122589\n",
      "        vf_loss: 0.1892968863248825\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.274971961975098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00989642646163702\n",
      "        model: {}\n",
      "        policy_loss: -0.1709337681531906\n",
      "        total_loss: -0.14832209050655365\n",
      "        vf_explained_var: -0.7357755899429321\n",
      "        vf_loss: 0.005472010467201471\n",
      "  num_agent_steps_sampled: 189728\n",
      "  num_agent_steps_trained: 189728\n",
      "  num_steps_sampled: 94864\n",
      "  num_steps_trained: 94864\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 228\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.2546875\n",
      "  ram_util_percent: 37.984375\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.118939999999991\n",
      "  blue_1: 0.5323199999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749748394561757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.79133682741416\n",
      "  mean_inference_ms: 2.8074392255666476\n",
      "  mean_raw_obs_processing_ms: 26.450577449346266\n",
      "time_since_restore: 13296.21076464653\n",
      "time_this_iter_s: 40.481147050857544\n",
      "time_total_s: 13296.21076464653\n",
      "timers:\n",
      "  learn_throughput: 826.269\n",
      "  learn_time_ms: 505.889\n",
      "  load_throughput: 843136.997\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 7.169\n",
      "  sample_time_ms: 58305.967\n",
      "timestamp: 1638996757\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 94864\n",
      "training_iteration: 228\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:228 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 191 -2.0 -1.0499999999999994\n",
      "blue_1 True True 191 0.005 0.9550000000000007\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 241 -2.0 -0.8000000000000036\n",
      "blue_1 True True 241 0.007 1.2089999999999963\n",
      "agent_timesteps_total: 190592\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-53-37\n",
      "done: false\n",
      "episode_len_mean: 255.84\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6226199999999694\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 434\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.298540115356445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013748450204730034\n",
      "        model: {}\n",
      "        policy_loss: -0.22749048471450806\n",
      "        total_loss: 0.006341289263218641\n",
      "        vf_explained_var: 0.5497255921363831\n",
      "        vf_loss: 0.20740492641925812\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7319045066833496\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.188296794891357\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020792169496417046\n",
      "        model: {}\n",
      "        policy_loss: -0.2379520684480667\n",
      "        total_loss: -0.19631095230579376\n",
      "        vf_explained_var: 0.2659531831741333\n",
      "        vf_loss: 0.005631052888929844\n",
      "  num_agent_steps_sampled: 190592\n",
      "  num_agent_steps_trained: 190592\n",
      "  num_steps_sampled: 95296\n",
      "  num_steps_trained: 95296\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 229\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.966265060240964\n",
      "  ram_util_percent: 37.98313253012048\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.10373999999999137\n",
      "  blue_1: 0.5188799999999929\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750054508107624\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.75105497924679\n",
      "  mean_inference_ms: 2.807224364383613\n",
      "  mean_raw_obs_processing_ms: 26.424372770448628\n",
      "time_since_restore: 13350.121317386627\n",
      "time_this_iter_s: 53.910552740097046\n",
      "time_total_s: 13350.121317386627\n",
      "timers:\n",
      "  learn_throughput: 830.213\n",
      "  learn_time_ms: 489.031\n",
      "  load_throughput: 818932.107\n",
      "  load_time_ms: 0.496\n",
      "  sample_throughput: 7.139\n",
      "  sample_time_ms: 56872.128\n",
      "timestamp: 1638996817\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 95296\n",
      "training_iteration: 229\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:229 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 211 -2.0 -0.9500000000000004\n",
      "blue_1 True True 211 0.005 1.0549999999999995\n",
      "LOSE\n",
      "blue_0 False True 404 -0.995 1.0199999999999791\n",
      "blue_1 False True 404 -0.993 1.2949999999999902\n",
      "agent_timesteps_total: 191822\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-55-02\n",
      "done: false\n",
      "episode_len_mean: 255.5\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6217899999999696\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 436\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.64249038696289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014169556088745594\n",
      "        model: {}\n",
      "        policy_loss: -0.1244371235370636\n",
      "        total_loss: 0.24429747462272644\n",
      "        vf_explained_var: 0.1327468752861023\n",
      "        vf_loss: 0.34149831533432007\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5978567600250244\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.66986608505249\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005154075101017952\n",
      "        model: {}\n",
      "        policy_loss: -0.39114856719970703\n",
      "        total_loss: -0.3676002323627472\n",
      "        vf_explained_var: -0.1354890763759613\n",
      "        vf_loss: 0.010158825665712357\n",
      "  num_agent_steps_sampled: 191822\n",
      "  num_agent_steps_trained: 191822\n",
      "  num_steps_sampled: 95911\n",
      "  num_steps_trained: 95911\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 230\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.94871794871795\n",
      "  ram_util_percent: 37.985470085470084\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.09203999999999152\n",
      "  blue_1: 0.5297499999999932\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750344692480606\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.71355208370656\n",
      "  mean_inference_ms: 2.807019640412485\n",
      "  mean_raw_obs_processing_ms: 26.399654535704713\n",
      "time_since_restore: 13429.587237358093\n",
      "time_this_iter_s: 79.46591997146606\n",
      "time_total_s: 13429.587237358093\n",
      "timers:\n",
      "  learn_throughput: 833.805\n",
      "  learn_time_ms: 521.345\n",
      "  load_throughput: 871541.085\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.171\n",
      "  sample_time_ms: 60622.095\n",
      "timestamp: 1638996902\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 95911\n",
      "training_iteration: 230\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:230 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 575 0.005 2.874999999999961\n",
      "blue_1 True True 575 -2.0 0.8609999999999629\n",
      "agent_timesteps_total: 192972\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-56-15\n",
      "done: false\n",
      "episode_len_mean: 258.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6499199999999691\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 437\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.030696868896484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008496553637087345\n",
      "        model: {}\n",
      "        policy_loss: -0.27039778232574463\n",
      "        total_loss: -0.2071126252412796\n",
      "        vf_explained_var: -0.7545642256736755\n",
      "        vf_loss: 0.04695337265729904\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5978567600250244\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.77448558807373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00641671335324645\n",
      "        model: {}\n",
      "        policy_loss: -0.322896271944046\n",
      "        total_loss: -0.23811620473861694\n",
      "        vf_explained_var: -0.5493001937866211\n",
      "        vf_loss: 0.0681103765964508\n",
      "  num_agent_steps_sampled: 192972\n",
      "  num_agent_steps_trained: 192972\n",
      "  num_steps_sampled: 96486\n",
      "  num_steps_trained: 96486\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 231\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.79207920792077\n",
      "  ram_util_percent: 37.88910891089108\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.12628999999999121\n",
      "  blue_1: 0.5236299999999928\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750469164619365\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.6944897370358\n",
      "  mean_inference_ms: 2.80691324783631\n",
      "  mean_raw_obs_processing_ms: 26.386351974755016\n",
      "time_since_restore: 13497.261169433594\n",
      "time_this_iter_s: 67.67393207550049\n",
      "time_total_s: 13497.261169433594\n",
      "timers:\n",
      "  learn_throughput: 819.594\n",
      "  learn_time_ms: 556.251\n",
      "  load_throughput: 761249.729\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 7.238\n",
      "  sample_time_ms: 62989.16\n",
      "timestamp: 1638996975\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 96486\n",
      "training_iteration: 231\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:230 starting ! -----------------\n",
      "agent_timesteps_total: 192972\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-56-15\n",
      "done: false\n",
      "episode_len_mean: 258.34\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6499199999999691\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 437\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.030696868896484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008496553637087345\n",
      "        model: {}\n",
      "        policy_loss: -0.27039778232574463\n",
      "        total_loss: -0.2071126252412796\n",
      "        vf_explained_var: -0.7545642256736755\n",
      "        vf_loss: 0.04695337265729904\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5978567600250244\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.77448558807373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00641671335324645\n",
      "        model: {}\n",
      "        policy_loss: -0.322896271944046\n",
      "        total_loss: -0.23811620473861694\n",
      "        vf_explained_var: -0.5493001937866211\n",
      "        vf_loss: 0.0681103765964508\n",
      "  num_agent_steps_sampled: 192972\n",
      "  num_agent_steps_trained: 192972\n",
      "  num_steps_sampled: 96486\n",
      "  num_steps_trained: 96486\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 231\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.79207920792077\n",
      "  ram_util_percent: 37.88910891089108\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.12628999999999121\n",
      "  blue_1: 0.5236299999999928\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750469164619365\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.6944897370358\n",
      "  mean_inference_ms: 2.80691324783631\n",
      "  mean_raw_obs_processing_ms: 26.386351974755016\n",
      "time_since_restore: 13497.261169433594\n",
      "time_this_iter_s: 67.67393207550049\n",
      "time_total_s: 13497.261169433594\n",
      "timers:\n",
      "  learn_throughput: 819.594\n",
      "  learn_time_ms: 556.251\n",
      "  load_throughput: 761249.729\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 7.238\n",
      "  sample_time_ms: 62989.16\n",
      "timestamp: 1638996975\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 96486\n",
      "training_iteration: 231\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:231 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 209 -2.0 -0.9600000000000002\n",
      "blue_1 True True 209 0.005 1.0449999999999997\n",
      "LOSE\n",
      "blue_0 False True 406 -0.995 1.029999999999979\n",
      "blue_1 False True 406 -0.993 1.3149999999999902\n",
      "agent_timesteps_total: 194202\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-57-37\n",
      "done: false\n",
      "episode_len_mean: 260.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6783199999999688\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 439\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.806046485900879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013394455425441265\n",
      "        model: {}\n",
      "        policy_loss: -0.20756909251213074\n",
      "        total_loss: 0.10951853543519974\n",
      "        vf_explained_var: -0.07726705819368362\n",
      "        vf_loss: 0.29134121537208557\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5978567600250244\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.209827899932861\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008486449718475342\n",
      "        model: {}\n",
      "        policy_loss: -0.3582032024860382\n",
      "        total_loss: -0.32469457387924194\n",
      "        vf_explained_var: 0.20216652750968933\n",
      "        vf_loss: 0.011462080292403698\n",
      "  num_agent_steps_sampled: 194202\n",
      "  num_agent_steps_trained: 194202\n",
      "  num_steps_sampled: 97101\n",
      "  num_steps_trained: 97101\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 232\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.454385964912284\n",
      "  ram_util_percent: 37.99473684210526\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.129089999999991\n",
      "  blue_1: 0.5492299999999928\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750665180731146\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.65504818463161\n",
      "  mean_inference_ms: 2.806687592385984\n",
      "  mean_raw_obs_processing_ms: 26.358408182793237\n",
      "time_since_restore: 13574.320172071457\n",
      "time_this_iter_s: 77.05900263786316\n",
      "time_total_s: 13574.320172071457\n",
      "timers:\n",
      "  learn_throughput: 820.408\n",
      "  learn_time_ms: 587.757\n",
      "  load_throughput: 689846.984\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 7.263\n",
      "  sample_time_ms: 66388.63\n",
      "timestamp: 1638997057\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 97101\n",
      "training_iteration: 232\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:232 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 386 -0.995 0.929999999999981\n",
      "blue_1 False True 386 -0.993 1.2039999999999882\n",
      "agent_timesteps_total: 194974\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-58-28\n",
      "done: false\n",
      "episode_len_mean: 262.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.6987099999999689\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 440\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.154740333557129\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012388326227664948\n",
      "        model: {}\n",
      "        policy_loss: -0.08798737823963165\n",
      "        total_loss: 0.03347262740135193\n",
      "        vf_explained_var: -0.5262910723686218\n",
      "        vf_loss: 0.09764757007360458\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5978567600250244\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.336865425109863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007996922358870506\n",
      "        model: {}\n",
      "        policy_loss: -0.0752330794930458\n",
      "        total_loss: 0.006909354589879513\n",
      "        vf_explained_var: -0.31275510787963867\n",
      "        vf_loss: 0.06136756390333176\n",
      "  num_agent_steps_sampled: 194974\n",
      "  num_agent_steps_trained: 194974\n",
      "  num_steps_sampled: 97487\n",
      "  num_steps_trained: 97487\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 233\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.342857142857135\n",
      "  ram_util_percent: 37.99285714285714\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.14793999999999083\n",
      "  blue_1: 0.5507699999999925\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750760081319995\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.63542902877062\n",
      "  mean_inference_ms: 2.8065739293008347\n",
      "  mean_raw_obs_processing_ms: 26.34387384989038\n",
      "time_since_restore: 13619.673678159714\n",
      "time_this_iter_s: 45.353506088256836\n",
      "time_total_s: 13619.673678159714\n",
      "timers:\n",
      "  learn_throughput: 805.111\n",
      "  learn_time_ms: 587.994\n",
      "  load_throughput: 789936.153\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 7.328\n",
      "  sample_time_ms: 64601.655\n",
      "timestamp: 1638997108\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 97487\n",
      "training_iteration: 233\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:233 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 507 -2.0 0.5299999999999683\n",
      "blue_1 True True 507 0.005 2.5499999999999696\n",
      "agent_timesteps_total: 195988\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_05-59-28\n",
      "done: false\n",
      "episode_len_mean: 266.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7326599999999679\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 441\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.687657833099365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005959297064691782\n",
      "        model: {}\n",
      "        policy_loss: -0.5077276229858398\n",
      "        total_loss: -0.47197455167770386\n",
      "        vf_explained_var: -0.48081666231155396\n",
      "        vf_loss: 0.024298308417201042\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5978567600250244\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.6503682136535645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011183182708919048\n",
      "        model: {}\n",
      "        policy_loss: 0.02619987539947033\n",
      "        total_loss: 0.07061365246772766\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.015361466445028782\n",
      "  num_agent_steps_sampled: 195988\n",
      "  num_agent_steps_trained: 195988\n",
      "  num_steps_sampled: 97994\n",
      "  num_steps_trained: 97994\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 234\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.94024390243903\n",
      "  ram_util_percent: 38.00365853658537\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.14478999999999048\n",
      "  blue_1: 0.5878699999999923\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750866784642701\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.61481469056906\n",
      "  mean_inference_ms: 2.8064601144484165\n",
      "  mean_raw_obs_processing_ms: 26.328617499823903\n",
      "time_since_restore: 13673.609336853027\n",
      "time_this_iter_s: 53.9356586933136\n",
      "time_total_s: 13673.609336853027\n",
      "timers:\n",
      "  learn_throughput: 816.198\n",
      "  learn_time_ms: 607.083\n",
      "  load_throughput: 706272.559\n",
      "  load_time_ms: 0.702\n",
      "  sample_throughput: 7.509\n",
      "  sample_time_ms: 65984.459\n",
      "timestamp: 1638997168\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 97994\n",
      "training_iteration: 234\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:234 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 1.9869999999999588\n",
      "agent_timesteps_total: 197188\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-00-39\n",
      "done: false\n",
      "episode_len_mean: 269.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7707799999999672\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 442\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9221680164337158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.806272506713867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0036659189499914646\n",
      "        model: {}\n",
      "        policy_loss: -0.4015587270259857\n",
      "        total_loss: -0.37146836519241333\n",
      "        vf_explained_var: -0.8220580816268921\n",
      "        vf_loss: 0.023043880239129066\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5978567600250244\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.131802558898926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0043607354164123535\n",
      "        model: {}\n",
      "        policy_loss: -0.3638763427734375\n",
      "        total_loss: -0.3286358714103699\n",
      "        vf_explained_var: -0.6543079614639282\n",
      "        vf_loss: 0.023911865428090096\n",
      "  num_agent_steps_sampled: 197188\n",
      "  num_agent_steps_trained: 197188\n",
      "  num_steps_sampled: 98594\n",
      "  num_steps_trained: 98594\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 235\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.89090909090909\n",
      "  ram_util_percent: 38.0070707070707\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1538899999999901\n",
      "  blue_1: 0.616889999999992\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750963544193359\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.59370835198685\n",
      "  mean_inference_ms: 2.8063450064878044\n",
      "  mean_raw_obs_processing_ms: 26.312280427117248\n",
      "time_since_restore: 13738.958975315094\n",
      "time_this_iter_s: 65.34963846206665\n",
      "time_total_s: 13738.958975315094\n",
      "timers:\n",
      "  learn_throughput: 815.724\n",
      "  learn_time_ms: 591.621\n",
      "  load_throughput: 687885.241\n",
      "  load_time_ms: 0.702\n",
      "  sample_throughput: 7.669\n",
      "  sample_time_ms: 62931.154\n",
      "timestamp: 1638997239\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 98594\n",
      "training_iteration: 235\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:235 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 89 0.005 0.4450000000000003\n",
      "blue_1 True True 89 -2.0 -1.5599999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 172 0.005 0.8600000000000007\n",
      "blue_1 True True 172 -2.0 -1.1449999999999994\n",
      "agent_timesteps_total: 197710\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-01-26\n",
      "done: false\n",
      "episode_len_mean: 269.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7696799999999672\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 444\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9610840082168579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.892664432525635\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017032625153660774\n",
      "        model: {}\n",
      "        policy_loss: -0.10833708941936493\n",
      "        total_loss: -0.05411849915981293\n",
      "        vf_explained_var: -0.4824533760547638\n",
      "        vf_loss: 0.0378488227725029\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2989283800125122\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.600149154663086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020426979288458824\n",
      "        model: {}\n",
      "        policy_loss: -0.13447260856628418\n",
      "        total_loss: 0.6002302765846252\n",
      "        vf_explained_var: -0.05709399655461311\n",
      "        vf_loss: 0.7081697583198547\n",
      "  num_agent_steps_sampled: 197710\n",
      "  num_agent_steps_trained: 197710\n",
      "  num_steps_sampled: 98855\n",
      "  num_steps_trained: 98855\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 236\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.060937499999994\n",
      "  ram_util_percent: 38.0375\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19343999999999006\n",
      "  blue_1: 0.5762399999999919\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751148161882229\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.55194836057431\n",
      "  mean_inference_ms: 2.806129962876642\n",
      "  mean_raw_obs_processing_ms: 26.280438007883227\n",
      "time_since_restore: 13780.420532941818\n",
      "time_this_iter_s: 41.46155762672424\n",
      "time_total_s: 13780.420532941818\n",
      "timers:\n",
      "  learn_throughput: 814.559\n",
      "  learn_time_ms: 588.662\n",
      "  load_throughput: 796818.054\n",
      "  load_time_ms: 0.602\n",
      "  sample_throughput: 7.565\n",
      "  sample_time_ms: 63382.254\n",
      "timestamp: 1638997286\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 98855\n",
      "training_iteration: 236\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:236 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 228 -2.0 -0.8650000000000022\n",
      "blue_1 True True 228 0.004 1.127999999999999\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 298 0.005 1.4899999999999902\n",
      "blue_1 True True 298 -2.001 -0.5500000000000027\n",
      "agent_timesteps_total: 198762\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-02-40\n",
      "done: false\n",
      "episode_len_mean: 270.03\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.769569999999967\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 446\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9610840082168579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.070097923278809\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017629731446504593\n",
      "        model: {}\n",
      "        policy_loss: -0.06684238463640213\n",
      "        total_loss: 0.2927015423774719\n",
      "        vf_explained_var: 0.08619581907987595\n",
      "        vf_loss: 0.3426002860069275\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.623533248901367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01198283489793539\n",
      "        model: {}\n",
      "        policy_loss: -0.14655965566635132\n",
      "        total_loss: 0.16090266406536102\n",
      "        vf_explained_var: 0.19494056701660156\n",
      "        vf_loss: 0.2841150164604187\n",
      "  num_agent_steps_sampled: 198762\n",
      "  num_agent_steps_trained: 198762\n",
      "  num_steps_sampled: 99381\n",
      "  num_steps_trained: 99381\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 237\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.36310679611651\n",
      "  ram_util_percent: 38.0252427184466\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2048399999999901\n",
      "  blue_1: 0.5647299999999921\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751289905752835\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.50993276239738\n",
      "  mean_inference_ms: 2.805926330522666\n",
      "  mean_raw_obs_processing_ms: 26.248105733422825\n",
      "time_since_restore: 13848.738218069077\n",
      "time_this_iter_s: 68.3176851272583\n",
      "time_total_s: 13848.738218069077\n",
      "timers:\n",
      "  learn_throughput: 815.216\n",
      "  learn_time_ms: 603.521\n",
      "  load_throughput: 704155.316\n",
      "  load_time_ms: 0.699\n",
      "  sample_throughput: 7.587\n",
      "  sample_time_ms: 64849.442\n",
      "timestamp: 1638997360\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 99381\n",
      "training_iteration: 237\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:237 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 256 -2.0 -0.7250000000000052\n",
      "blue_1 True True 256 0.004 1.271999999999999\n",
      "agent_timesteps_total: 199274\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-03-21\n",
      "done: false\n",
      "episode_len_mean: 271.12\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7803899999999667\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 447\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9610840082168579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.057940483093262\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019075827673077583\n",
      "        model: {}\n",
      "        policy_loss: -0.09927447140216827\n",
      "        total_loss: 0.19919146597385406\n",
      "        vf_explained_var: 0.2545510530471802\n",
      "        vf_loss: 0.2801324725151062\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.518228530883789\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016037799417972565\n",
      "        model: {}\n",
      "        policy_loss: -0.09702405333518982\n",
      "        total_loss: -0.04553442820906639\n",
      "        vf_explained_var: -0.4429953992366791\n",
      "        vf_loss: 0.020241690799593925\n",
      "  num_agent_steps_sampled: 199274\n",
      "  num_agent_steps_trained: 199274\n",
      "  num_steps_sampled: 99637\n",
      "  num_steps_trained: 99637\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 238\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.75614035087719\n",
      "  ram_util_percent: 37.99298245614034\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19023999999999006\n",
      "  blue_1: 0.5901499999999921\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751343583645111\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.4890124628453\n",
      "  mean_inference_ms: 2.8058224474860394\n",
      "  mean_raw_obs_processing_ms: 26.231360752720857\n",
      "time_since_restore: 13884.884105682373\n",
      "time_this_iter_s: 36.14588761329651\n",
      "time_total_s: 13884.884105682373\n",
      "timers:\n",
      "  learn_throughput: 815.992\n",
      "  learn_time_ms: 584.933\n",
      "  load_throughput: 677774.08\n",
      "  load_time_ms: 0.704\n",
      "  sample_throughput: 7.406\n",
      "  sample_time_ms: 64451.913\n",
      "timestamp: 1638997401\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 99637\n",
      "training_iteration: 238\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:238 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.029999999999959\n",
      "agent_timesteps_total: 200474\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-04-35\n",
      "done: false\n",
      "episode_len_mean: 275.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8274399999999658\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 448\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9610840082168579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.444373607635498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016880476847290993\n",
      "        model: {}\n",
      "        policy_loss: -0.3152972161769867\n",
      "        total_loss: -0.18524417281150818\n",
      "        vf_explained_var: -0.7632178068161011\n",
      "        vf_loss: 0.11382949352264404\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.991576671600342\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005422281567007303\n",
      "        model: {}\n",
      "        policy_loss: -0.3355264961719513\n",
      "        total_loss: -0.3122251331806183\n",
      "        vf_explained_var: -0.689610481262207\n",
      "        vf_loss: 0.012736642733216286\n",
      "  num_agent_steps_sampled: 200474\n",
      "  num_agent_steps_trained: 200474\n",
      "  num_steps_sampled: 100237\n",
      "  num_steps_trained: 100237\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 239\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.25148514851484\n",
      "  ram_util_percent: 38.04059405940595\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.22363999999998968\n",
      "  blue_1: 0.6037999999999916\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751423103956\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.4674600975512\n",
      "  mean_inference_ms: 2.8057164304697033\n",
      "  mean_raw_obs_processing_ms: 26.213690169266183\n",
      "time_since_restore: 13952.833223581314\n",
      "time_this_iter_s: 67.94911789894104\n",
      "time_total_s: 13952.833223581314\n",
      "timers:\n",
      "  learn_throughput: 827.915\n",
      "  learn_time_ms: 596.801\n",
      "  load_throughput: 701582.859\n",
      "  load_time_ms: 0.704\n",
      "  sample_throughput: 7.506\n",
      "  sample_time_ms: 65826.776\n",
      "timestamp: 1638997475\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 100237\n",
      "training_iteration: 239\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:239 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 73 -2.0 -1.6399999999999997\n",
      "blue_1 True True 73 0.005 0.3650000000000002\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 385 -2.0 -0.08000000000001894\n",
      "blue_1 True True 385 0.005 1.9099999999999826\n",
      "agent_timesteps_total: 201390\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-05-38\n",
      "done: false\n",
      "episode_len_mean: 277.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8395899999999653\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 450\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9610840082168579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.649712562561035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02497069537639618\n",
      "        model: {}\n",
      "        policy_loss: -0.3229932487010956\n",
      "        total_loss: 0.15994703769683838\n",
      "        vf_explained_var: 0.23836757242679596\n",
      "        vf_loss: 0.4589413106441498\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.8686699867248535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017950985580682755\n",
      "        model: {}\n",
      "        policy_loss: -0.24662525951862335\n",
      "        total_loss: -0.19530312716960907\n",
      "        vf_explained_var: -0.16073542833328247\n",
      "        vf_loss: 0.01634654961526394\n",
      "  num_agent_steps_sampled: 201390\n",
      "  num_agent_steps_trained: 201390\n",
      "  num_steps_sampled: 100695\n",
      "  num_steps_trained: 100695\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 240\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.583908045977026\n",
      "  ram_util_percent: 38.02988505747127\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 2.9859999999999616\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2297899999999895\n",
      "  blue_1: 0.6097999999999915\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751581360289952\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.42281552272581\n",
      "  mean_inference_ms: 2.8055144112436623\n",
      "  mean_raw_obs_processing_ms: 26.177695610300265\n",
      "time_since_restore: 14010.153340101242\n",
      "time_this_iter_s: 57.32011651992798\n",
      "time_total_s: 14010.153340101242\n",
      "timers:\n",
      "  learn_throughput: 822.073\n",
      "  learn_time_ms: 581.943\n",
      "  load_throughput: 682014.559\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 7.518\n",
      "  sample_time_ms: 63638.086\n",
      "timestamp: 1638997538\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 100695\n",
      "training_iteration: 240\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:240 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 158 0.005 0.7900000000000006\n",
      "blue_1 True True 158 -2.0 -1.2149999999999994\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 592 -2.0 0.9549999999999592\n",
      "blue_1 True True 592 0.005 3.023999999999959\n",
      "agent_timesteps_total: 202890\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-07-13\n",
      "done: false\n",
      "episode_len_mean: 276.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8289999999999654\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 452\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.441625952720642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.3331880569458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00873575359582901\n",
      "        model: {}\n",
      "        policy_loss: -0.391611248254776\n",
      "        total_loss: -0.3167027235031128\n",
      "        vf_explained_var: -0.07652497291564941\n",
      "        vf_loss: 0.062314826995134354\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.990135192871094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012466266751289368\n",
      "        model: {}\n",
      "        policy_loss: -0.04756728559732437\n",
      "        total_loss: 0.30371901392936707\n",
      "        vf_explained_var: -0.45409801602363586\n",
      "        vf_loss: 0.3269971013069153\n",
      "  num_agent_steps_sampled: 202890\n",
      "  num_agent_steps_trained: 202890\n",
      "  num_steps_sampled: 101445\n",
      "  num_steps_trained: 101445\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 241\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.61145038167939\n",
      "  ram_util_percent: 38.04274809160306\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2269899999999895\n",
      "  blue_1: 0.6020099999999913\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751818190906961\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.37846667262929\n",
      "  mean_inference_ms: 2.8053181532342717\n",
      "  mean_raw_obs_processing_ms: 26.142654051173302\n",
      "time_since_restore: 14099.634219884872\n",
      "time_this_iter_s: 89.48087978363037\n",
      "time_total_s: 14099.634219884872\n",
      "timers:\n",
      "  learn_throughput: 828.849\n",
      "  learn_time_ms: 598.3\n",
      "  load_throughput: 824659.168\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.538\n",
      "  sample_time_ms: 65782.682\n",
      "timestamp: 1638997633\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 101445\n",
      "training_iteration: 241\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:240 starting ! -----------------\n",
      "agent_timesteps_total: 202890\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-07-13\n",
      "done: false\n",
      "episode_len_mean: 276.47\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8289999999999654\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 452\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.441625952720642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.3331880569458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00873575359582901\n",
      "        model: {}\n",
      "        policy_loss: -0.391611248254776\n",
      "        total_loss: -0.3167027235031128\n",
      "        vf_explained_var: -0.07652497291564941\n",
      "        vf_loss: 0.062314826995134354\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.990135192871094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012466266751289368\n",
      "        model: {}\n",
      "        policy_loss: -0.04756728559732437\n",
      "        total_loss: 0.30371901392936707\n",
      "        vf_explained_var: -0.45409801602363586\n",
      "        vf_loss: 0.3269971013069153\n",
      "  num_agent_steps_sampled: 202890\n",
      "  num_agent_steps_trained: 202890\n",
      "  num_steps_sampled: 101445\n",
      "  num_steps_trained: 101445\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 241\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.61145038167939\n",
      "  ram_util_percent: 38.04274809160306\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2269899999999895\n",
      "  blue_1: 0.6020099999999913\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751818190906961\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.37846667262929\n",
      "  mean_inference_ms: 2.8053181532342717\n",
      "  mean_raw_obs_processing_ms: 26.142654051173302\n",
      "time_since_restore: 14099.634219884872\n",
      "time_this_iter_s: 89.48087978363037\n",
      "time_total_s: 14099.634219884872\n",
      "timers:\n",
      "  learn_throughput: 828.849\n",
      "  learn_time_ms: 598.3\n",
      "  load_throughput: 824659.168\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.538\n",
      "  sample_time_ms: 65782.682\n",
      "timestamp: 1638997633\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 101445\n",
      "training_iteration: 241\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:241 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 66 0.005 0.3300000000000002\n",
      "blue_1 True True 66 -2.0 -1.6749999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 139 -2.0 -1.3099999999999996\n",
      "blue_1 True True 139 0.005 0.6950000000000005\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 262 -2.0 -0.6950000000000058\n",
      "blue_1 True True 262 0.005 1.309999999999994\n",
      "agent_timesteps_total: 203824\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-08-26\n",
      "done: false\n",
      "episode_len_mean: 272.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7880999999999664\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 455\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.441625952720642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.901700973510742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021119624376296997\n",
      "        model: {}\n",
      "        policy_loss: -0.31783580780029297\n",
      "        total_loss: 0.05185674503445625\n",
      "        vf_explained_var: 0.21660113334655762\n",
      "        vf_loss: 0.3392459452152252\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.2846808433532715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013019015081226826\n",
      "        model: {}\n",
      "        policy_loss: -0.07646161317825317\n",
      "        total_loss: 0.363088995218277\n",
      "        vf_explained_var: 0.32994386553764343\n",
      "        vf_loss: 0.41418445110321045\n",
      "  num_agent_steps_sampled: 203824\n",
      "  num_agent_steps_trained: 203824\n",
      "  num_steps_sampled: 101912\n",
      "  num_steps_trained: 101912\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 242\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 45.59504950495049\n",
      "  ram_util_percent: 38.054455445544555\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19808999999998986\n",
      "  blue_1: 0.5900099999999913\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752216341902977\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.31408019332333\n",
      "  mean_inference_ms: 2.8050419693368376\n",
      "  mean_raw_obs_processing_ms: 26.09574009029412\n",
      "time_since_restore: 14166.867961645126\n",
      "time_this_iter_s: 67.2337417602539\n",
      "time_total_s: 14166.867961645126\n",
      "timers:\n",
      "  learn_throughput: 819.534\n",
      "  learn_time_ms: 587.041\n",
      "  load_throughput: 800174.342\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.421\n",
      "  sample_time_ms: 64830.801\n",
      "timestamp: 1638997706\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 101912\n",
      "training_iteration: 242\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:242 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 97 0.005 0.4850000000000003\n",
      "blue_1 True True 97 -2.0 -1.5199999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 99 0.005 0.49500000000000033\n",
      "blue_1 True True 99 -2.0 -1.5099999999999998\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 165 -2.0 -1.1799999999999993\n",
      "blue_1 True True 165 0.005 0.8250000000000006\n",
      "agent_timesteps_total: 204546\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-09-36\n",
      "done: false\n",
      "episode_len_mean: 267.49\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7359799999999674\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 458\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.1624388694763184\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.826557636260986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008989772759377956\n",
      "        model: {}\n",
      "        policy_loss: -0.6166762113571167\n",
      "        total_loss: -0.4920414984226227\n",
      "        vf_explained_var: -0.3158564269542694\n",
      "        vf_loss: 0.10519485920667648\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.465652942657471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014286387711763382\n",
      "        model: {}\n",
      "        policy_loss: 0.31054750084877014\n",
      "        total_loss: 0.8791266083717346\n",
      "        vf_explained_var: 0.40211278200149536\n",
      "        vf_loss: 0.5407436490058899\n",
      "  num_agent_steps_sampled: 204546\n",
      "  num_agent_steps_trained: 204546\n",
      "  num_steps_sampled: 102273\n",
      "  num_steps_trained: 102273\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 243\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.872448979591844\n",
      "  ram_util_percent: 38.183673469387756\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.17203999999999028\n",
      "  blue_1: 0.5639399999999916\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752508928342045\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.25323382407709\n",
      "  mean_inference_ms: 2.804776791943704\n",
      "  mean_raw_obs_processing_ms: 26.05222481206048\n",
      "time_since_restore: 14231.85802602768\n",
      "time_this_iter_s: 64.9900643825531\n",
      "time_total_s: 14231.85802602768\n",
      "timers:\n",
      "  learn_throughput: 835.46\n",
      "  learn_time_ms: 572.858\n",
      "  load_throughput: 796016.296\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.165\n",
      "  sample_time_ms: 66799.271\n",
      "timestamp: 1638997776\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 102273\n",
      "training_iteration: 243\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:243 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.1259999999999617\n",
      "agent_timesteps_total: 205746\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-10-37\n",
      "done: false\n",
      "episode_len_mean: 270.09\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7615799999999667\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 459\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.1624388694763184\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.3995747566223145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024545008316636086\n",
      "        model: {}\n",
      "        policy_loss: -0.3606979548931122\n",
      "        total_loss: -0.29827195405960083\n",
      "        vf_explained_var: 0.06281015276908875\n",
      "        vf_loss: 0.009348948486149311\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.337619304656982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0092893922701478\n",
      "        model: {}\n",
      "        policy_loss: -0.32562369108200073\n",
      "        total_loss: -0.20030778646469116\n",
      "        vf_explained_var: -0.9732359647750854\n",
      "        vf_loss: 0.10721652954816818\n",
      "  num_agent_steps_sampled: 205746\n",
      "  num_agent_steps_trained: 205746\n",
      "  num_steps_sampled: 102873\n",
      "  num_steps_trained: 102873\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 244\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.19523809523808\n",
      "  ram_util_percent: 38.19642857142856\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19508999999999\n",
      "  blue_1: 0.5664899999999914\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752634814513066\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.2314634152536\n",
      "  mean_inference_ms: 2.8046938555307936\n",
      "  mean_raw_obs_processing_ms: 26.037303813254102\n",
      "time_since_restore: 14286.892226934433\n",
      "time_this_iter_s: 55.03420090675354\n",
      "time_total_s: 14286.892226934433\n",
      "timers:\n",
      "  learn_throughput: 831.306\n",
      "  learn_time_ms: 586.908\n",
      "  load_throughput: 977829.187\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.294\n",
      "  sample_time_ms: 66886.701\n",
      "timestamp: 1638997837\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 102873\n",
      "training_iteration: 244\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:244 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 250 0.005 1.2499999999999953\n",
      "blue_1 True True 250 -2.0 -0.7550000000000046\n",
      "LOSE\n",
      "blue_0 False True 339 -0.995 0.694999999999986\n",
      "blue_1 False True 339 -0.993 0.9579999999999865\n",
      "agent_timesteps_total: 206924\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-11-57\n",
      "done: false\n",
      "episode_len_mean: 270.48\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7664399999999667\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 461\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.644601345062256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014818738214671612\n",
      "        model: {}\n",
      "        policy_loss: -0.337431937456131\n",
      "        total_loss: -0.2712039351463318\n",
      "        vf_explained_var: 0.06650569289922714\n",
      "        vf_loss: 0.018161077052354813\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.926177978515625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01875135861337185\n",
      "        model: {}\n",
      "        policy_loss: -0.08132448047399521\n",
      "        total_loss: 0.27621522545814514\n",
      "        vf_explained_var: -0.33827483654022217\n",
      "        vf_loss: 0.3210046887397766\n",
      "  num_agent_steps_sampled: 206924\n",
      "  num_agent_steps_trained: 206924\n",
      "  num_steps_sampled: 103462\n",
      "  num_steps_trained: 103462\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 245\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.01711711711714\n",
      "  ram_util_percent: 38.19369369369369\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.18703999999998996\n",
      "  blue_1: 0.5793999999999913\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752767272262143\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.18848907864205\n",
      "  mean_inference_ms: 2.804532327863358\n",
      "  mean_raw_obs_processing_ms: 26.007659897695962\n",
      "time_since_restore: 14361.482353925705\n",
      "time_this_iter_s: 74.59012699127197\n",
      "time_total_s: 14361.482353925705\n",
      "timers:\n",
      "  learn_throughput: 829.149\n",
      "  learn_time_ms: 587.108\n",
      "  load_throughput: 975624.612\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.178\n",
      "  sample_time_ms: 67821.641\n",
      "timestamp: 1638997917\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 103462\n",
      "training_iteration: 245\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:245 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 348 0.005 1.739999999999985\n",
      "blue_1 True True 348 -2.0 -0.24500000000001387\n",
      "agent_timesteps_total: 207620\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-12-47\n",
      "done: false\n",
      "episode_len_mean: 268.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7418799999999673\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 462\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.843243598937988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008784313686192036\n",
      "        model: {}\n",
      "        policy_loss: -0.016388259828090668\n",
      "        total_loss: 0.024872949346899986\n",
      "        vf_explained_var: -0.8718453049659729\n",
      "        vf_loss: 0.012767896056175232\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.830480575561523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006125783082097769\n",
      "        model: {}\n",
      "        policy_loss: -0.47390279173851013\n",
      "        total_loss: -0.4407420754432678\n",
      "        vf_explained_var: 0.4819394052028656\n",
      "        vf_loss: 0.02122526802122593\n",
      "  num_agent_steps_sampled: 207620\n",
      "  num_agent_steps_trained: 207620\n",
      "  num_steps_sampled: 103810\n",
      "  num_steps_trained: 103810\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 246\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.457971014492756\n",
      "  ram_util_percent: 38.18840579710144\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19478999999999022\n",
      "  blue_1: 0.5470899999999915\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752827553388012\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.16802131713905\n",
      "  mean_inference_ms: 2.804450148794501\n",
      "  mean_raw_obs_processing_ms: 25.993834754264736\n",
      "time_since_restore: 14405.909143447876\n",
      "time_this_iter_s: 44.42678952217102\n",
      "time_total_s: 14405.909143447876\n",
      "timers:\n",
      "  learn_throughput: 844.448\n",
      "  learn_time_ms: 586.774\n",
      "  load_throughput: 827867.126\n",
      "  load_time_ms: 0.599\n",
      "  sample_throughput: 7.274\n",
      "  sample_time_ms: 68118.76\n",
      "timestamp: 1638997967\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 103810\n",
      "training_iteration: 246\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:246 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 180 -2.0 -1.1049999999999993\n",
      "blue_1 True True 180 0.006 0.8970000000000007\n",
      "LOSE\n",
      "blue_0 False True 345 -0.995 0.7249999999999853\n",
      "blue_1 False True 345 -0.993 0.9629999999999871\n",
      "agent_timesteps_total: 208670\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-14-05\n",
      "done: false\n",
      "episode_len_mean: 271.44\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.778479999999967\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 464\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.637691497802734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007940794341266155\n",
      "        model: {}\n",
      "        policy_loss: -0.0763024240732193\n",
      "        total_loss: 0.2535322904586792\n",
      "        vf_explained_var: 0.22557395696640015\n",
      "        vf_loss: 0.30407750606536865\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.171018600463867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007800258230417967\n",
      "        model: {}\n",
      "        policy_loss: -0.12857268750667572\n",
      "        total_loss: -0.032317325472831726\n",
      "        vf_explained_var: 0.02793017216026783\n",
      "        vf_loss: 0.08105739951133728\n",
      "  num_agent_steps_sampled: 208670\n",
      "  num_agent_steps_trained: 208670\n",
      "  num_steps_sampled: 104335\n",
      "  num_steps_trained: 104335\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 247\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.242990654205606\n",
      "  ram_util_percent: 38.17757009345794\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.18183999999999004\n",
      "  blue_1: 0.5966399999999913\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752979796347528\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.12560633281612\n",
      "  mean_inference_ms: 2.8042818801444955\n",
      "  mean_raw_obs_processing_ms: 25.963571427890102\n",
      "time_since_restore: 14478.226054430008\n",
      "time_this_iter_s: 72.31691098213196\n",
      "time_total_s: 14478.226054430008\n",
      "timers:\n",
      "  learn_throughput: 845.572\n",
      "  learn_time_ms: 585.876\n",
      "  load_throughput: 987669.076\n",
      "  load_time_ms: 0.502\n",
      "  sample_throughput: 7.23\n",
      "  sample_time_ms: 68520.189\n",
      "timestamp: 1638998045\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 104335\n",
      "training_iteration: 247\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:247 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 283 -2.0 -0.5900000000000081\n",
      "blue_1 True True 283 0.005 1.4149999999999918\n",
      "agent_timesteps_total: 209236\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-14-46\n",
      "done: false\n",
      "episode_len_mean: 273.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.7951799999999666\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 465\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.1214213371276855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007837877608835697\n",
      "        model: {}\n",
      "        policy_loss: -0.28720203042030334\n",
      "        total_loss: -0.16527482867240906\n",
      "        vf_explained_var: 0.38850137591362\n",
      "        vf_loss: 0.09650377184152603\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.5633625984191895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010821337811648846\n",
      "        model: {}\n",
      "        policy_loss: -0.19384093582630157\n",
      "        total_loss: -0.16504618525505066\n",
      "        vf_explained_var: 0.18102286756038666\n",
      "        vf_loss: 0.007710545789450407\n",
      "  num_agent_steps_sampled: 209236\n",
      "  num_agent_steps_trained: 209236\n",
      "  num_steps_sampled: 104618\n",
      "  num_steps_trained: 104618\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 248\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.508620689655174\n",
      "  ram_util_percent: 38.18275862068965\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.17013999999998997\n",
      "  blue_1: 0.6250399999999913\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753048111948044\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.10421785321135\n",
      "  mean_inference_ms: 2.8041969269629075\n",
      "  mean_raw_obs_processing_ms: 25.948302626067356\n",
      "time_since_restore: 14514.182231664658\n",
      "time_this_iter_s: 35.95617723464966\n",
      "time_total_s: 14514.182231664658\n",
      "timers:\n",
      "  learn_throughput: 848.717\n",
      "  learn_time_ms: 586.886\n",
      "  load_throughput: 1248167.536\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.272\n",
      "  sample_time_ms: 68496.738\n",
      "timestamp: 1638998086\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 104618\n",
      "training_iteration: 248\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:248 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 456 -0.995 1.2799999999999736\n",
      "blue_1 False True 456 -0.993 1.5399999999999925\n",
      "agent_timesteps_total: 210148\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-15-41\n",
      "done: false\n",
      "episode_len_mean: 271.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.783099999999967\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 466\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2436585426330566\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.375481605529785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004540000576525927\n",
      "        model: {}\n",
      "        policy_loss: -0.3497071862220764\n",
      "        total_loss: -0.2637956738471985\n",
      "        vf_explained_var: -0.267795592546463\n",
      "        vf_loss: 0.07118533551692963\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.04189395904541\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011561598628759384\n",
      "        model: {}\n",
      "        policy_loss: -0.41124099493026733\n",
      "        total_loss: -0.38361668586730957\n",
      "        vf_explained_var: -0.4904336631298065\n",
      "        vf_loss: 0.005097753833979368\n",
      "  num_agent_steps_sampled: 210148\n",
      "  num_agent_steps_trained: 210148\n",
      "  num_steps_sampled: 105074\n",
      "  num_steps_trained: 105074\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 249\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.10266666666668\n",
      "  ram_util_percent: 38.096000000000004\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.16239999999999008\n",
      "  blue_1: 0.6206999999999916\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.575309012206936\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.08318630813562\n",
      "  mean_inference_ms: 2.8041160125903364\n",
      "  mean_raw_obs_processing_ms: 25.933792466947793\n",
      "time_since_restore: 14563.43956232071\n",
      "time_this_iter_s: 49.257330656051636\n",
      "time_total_s: 14563.43956232071\n",
      "timers:\n",
      "  learn_throughput: 846.326\n",
      "  learn_time_ms: 571.529\n",
      "  load_throughput: 1212228.038\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.258\n",
      "  sample_time_ms: 66646.705\n",
      "timestamp: 1638998141\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 105074\n",
      "training_iteration: 249\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:249 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.994 2.1469999999999425\n",
      "blue_1 False False 600 -0.995 1.9999999999999583\n",
      "agent_timesteps_total: 211348\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-16-55\n",
      "done: false\n",
      "episode_len_mean: 276.66\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8345199999999664\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 467\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6218292713165283\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.024764060974121\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004772817250341177\n",
      "        model: {}\n",
      "        policy_loss: -0.38093045353889465\n",
      "        total_loss: -0.35323184728622437\n",
      "        vf_explained_var: -0.705079972743988\n",
      "        vf_loss: 0.019957903772592545\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.948392629623413\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.21010971069336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004628471564501524\n",
      "        model: {}\n",
      "        policy_loss: -0.3860434591770172\n",
      "        total_loss: -0.3584219813346863\n",
      "        vf_explained_var: -0.6931498646736145\n",
      "        vf_loss: 0.0186033733189106\n",
      "  num_agent_steps_sampled: 211348\n",
      "  num_agent_steps_trained: 211348\n",
      "  num_steps_sampled: 105674\n",
      "  num_steps_trained: 105674\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 250\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.140196078431366\n",
      "  ram_util_percent: 38.030392156862746\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.19886999999998942\n",
      "  blue_1: 0.635649999999991\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.575312028981758\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.06171635181549\n",
      "  mean_inference_ms: 2.8040335558511855\n",
      "  mean_raw_obs_processing_ms: 25.918060520091636\n",
      "time_since_restore: 14631.71275472641\n",
      "time_this_iter_s: 68.27319240570068\n",
      "time_total_s: 14631.71275472641\n",
      "timers:\n",
      "  learn_throughput: 849.817\n",
      "  learn_time_ms: 585.891\n",
      "  load_throughput: 1247890.028\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.353\n",
      "  sample_time_ms: 67711.565\n",
      "timestamp: 1638998215\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 105674\n",
      "training_iteration: 250\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:250 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 77 -2.0 -1.6199999999999997\n",
      "blue_1 True True 77 0.005 0.38500000000000023\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 469 -2.0 0.3399999999999723\n",
      "blue_1 True True 469 0.005 2.4829999999999743\n",
      "agent_timesteps_total: 212440\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-18-04\n",
      "done: false\n",
      "episode_len_mean: 274.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8169999999999668\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 469\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.348794460296631\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027079539373517036\n",
      "        model: {}\n",
      "        policy_loss: -0.19423630833625793\n",
      "        total_loss: 0.22299952805042267\n",
      "        vf_explained_var: 0.11767376959323883\n",
      "        vf_loss: 0.39527663588523865\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9741963148117065\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.21638298034668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02940899319946766\n",
      "        model: {}\n",
      "        policy_loss: -0.1499677300453186\n",
      "        total_loss: -0.10575249791145325\n",
      "        vf_explained_var: -0.33715343475341797\n",
      "        vf_loss: 0.015565096400678158\n",
      "  num_agent_steps_sampled: 212440\n",
      "  num_agent_steps_trained: 212440\n",
      "  num_steps_sampled: 106220\n",
      "  num_steps_trained: 106220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 251\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.48020833333334\n",
      "  ram_util_percent: 38.09062500000001\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.16946999999998952\n",
      "  blue_1: 0.6475299999999912\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753170151453978\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.01759245945148\n",
      "  mean_inference_ms: 2.8038826933913823\n",
      "  mean_raw_obs_processing_ms: 25.887164491769443\n",
      "time_since_restore: 14695.400379180908\n",
      "time_this_iter_s: 63.68762445449829\n",
      "time_total_s: 14695.400379180908\n",
      "timers:\n",
      "  learn_throughput: 843.124\n",
      "  learn_time_ms: 566.346\n",
      "  load_throughput: 957489.2\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.327\n",
      "  sample_time_ms: 65169.067\n",
      "timestamp: 1638998284\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 106220\n",
      "training_iteration: 251\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:250 starting ! -----------------\n",
      "agent_timesteps_total: 212440\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-18-04\n",
      "done: false\n",
      "episode_len_mean: 274.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8169999999999668\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 469\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.8109146356582642\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.348794460296631\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027079539373517036\n",
      "        model: {}\n",
      "        policy_loss: -0.19423630833625793\n",
      "        total_loss: 0.22299952805042267\n",
      "        vf_explained_var: 0.11767376959323883\n",
      "        vf_loss: 0.39527663588523865\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.9741963148117065\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.21638298034668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02940899319946766\n",
      "        model: {}\n",
      "        policy_loss: -0.1499677300453186\n",
      "        total_loss: -0.10575249791145325\n",
      "        vf_explained_var: -0.33715343475341797\n",
      "        vf_loss: 0.015565096400678158\n",
      "  num_agent_steps_sampled: 212440\n",
      "  num_agent_steps_trained: 212440\n",
      "  num_steps_sampled: 106220\n",
      "  num_steps_trained: 106220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 251\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.48020833333334\n",
      "  ram_util_percent: 38.09062500000001\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.16946999999998952\n",
      "  blue_1: 0.6475299999999912\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753170151453978\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 110.01759245945148\n",
      "  mean_inference_ms: 2.8038826933913823\n",
      "  mean_raw_obs_processing_ms: 25.887164491769443\n",
      "time_since_restore: 14695.400379180908\n",
      "time_this_iter_s: 63.68762445449829\n",
      "time_total_s: 14695.400379180908\n",
      "timers:\n",
      "  learn_throughput: 843.124\n",
      "  learn_time_ms: 566.346\n",
      "  load_throughput: 957489.2\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.327\n",
      "  sample_time_ms: 65169.067\n",
      "timestamp: 1638998284\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 106220\n",
      "training_iteration: 251\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:251 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 363 -0.995 0.8149999999999834\n",
      "blue_1 False True 363 -0.993 1.066999999999986\n",
      "agent_timesteps_total: 213166\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-18-55\n",
      "done: false\n",
      "episode_len_mean: 277.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8445699999999667\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 470\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.339005470275879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00853798259049654\n",
      "        model: {}\n",
      "        policy_loss: -0.5524747371673584\n",
      "        total_loss: -0.40936166048049927\n",
      "        vf_explained_var: -0.47908687591552734\n",
      "        vf_loss: 0.1327276974916458\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.31220817565918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005934367422014475\n",
      "        model: {}\n",
      "        policy_loss: -0.5371895432472229\n",
      "        total_loss: -0.5156076550483704\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.012910031713545322\n",
      "  num_agent_steps_sampled: 213166\n",
      "  num_agent_steps_trained: 213166\n",
      "  num_steps_sampled: 106583\n",
      "  num_steps_trained: 106583\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 252\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.2536231884058\n",
      "  ram_util_percent: 38.11014492753623\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.1920199999999893\n",
      "  blue_1: 0.6525499999999909\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753168764375353\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.9970225214488\n",
      "  mean_inference_ms: 2.803804365180615\n",
      "  mean_raw_obs_processing_ms: 25.87266584797041\n",
      "time_since_restore: 14740.058297395706\n",
      "time_this_iter_s: 44.657918214797974\n",
      "time_total_s: 14740.058297395706\n",
      "timers:\n",
      "  learn_throughput: 855.048\n",
      "  learn_time_ms: 546.285\n",
      "  load_throughput: 937217.47\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 7.424\n",
      "  sample_time_ms: 62913.85\n",
      "timestamp: 1638998335\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 106583\n",
      "training_iteration: 252\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:252 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 377 0.005 1.8849999999999818\n",
      "blue_1 True True 377 -2.0 -0.0760000000000185\n",
      "agent_timesteps_total: 213920\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-19-45\n",
      "done: false\n",
      "episode_len_mean: 275.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.827219999999967\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 471\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.230310440063477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01097729429602623\n",
      "        model: {}\n",
      "        policy_loss: -0.542435348033905\n",
      "        total_loss: -0.5185039043426514\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.010578914545476437\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.241702079772949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008837464265525341\n",
      "        model: {}\n",
      "        policy_loss: -0.6280383467674255\n",
      "        total_loss: -0.5840447545051575\n",
      "        vf_explained_var: 0.4267180263996124\n",
      "        vf_loss: 0.031079387292265892\n",
      "  num_agent_steps_sampled: 213920\n",
      "  num_agent_steps_trained: 213920\n",
      "  num_steps_sampled: 106960\n",
      "  num_steps_trained: 106960\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 253\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.142857142857146\n",
      "  ram_util_percent: 38.19714285714285\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2034699999999895\n",
      "  blue_1: 0.6237499999999911\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753174538111144\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.97611366967338\n",
      "  mean_inference_ms: 2.8037269163750973\n",
      "  mean_raw_obs_processing_ms: 25.85784272097672\n",
      "time_since_restore: 14785.018873929977\n",
      "time_this_iter_s: 44.96057653427124\n",
      "time_total_s: 14785.018873929977\n",
      "timers:\n",
      "  learn_throughput: 862.069\n",
      "  learn_time_ms: 543.692\n",
      "  load_throughput: 783621.112\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.697\n",
      "  sample_time_ms: 60894.74\n",
      "timestamp: 1638998385\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 106960\n",
      "training_iteration: 253\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:253 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 404 0.005 2.019999999999979\n",
      "blue_1 True True 404 -2.0 0.03199999999998049\n",
      "agent_timesteps_total: 214728\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-20-36\n",
      "done: false\n",
      "episode_len_mean: 277.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8469199999999664\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 472\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.581292629241943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013656528666615486\n",
      "        model: {}\n",
      "        policy_loss: -0.20826175808906555\n",
      "        total_loss: -0.18901990354061127\n",
      "        vf_explained_var: -0.37273842096328735\n",
      "        vf_loss: 0.002630434464663267\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.206258773803711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01679721102118492\n",
      "        model: {}\n",
      "        policy_loss: -0.19977478682994843\n",
      "        total_loss: 0.055577050894498825\n",
      "        vf_explained_var: -0.01926150918006897\n",
      "        vf_loss: 0.23080618679523468\n",
      "  num_agent_steps_sampled: 214728\n",
      "  num_agent_steps_trained: 214728\n",
      "  num_steps_sampled: 107364\n",
      "  num_steps_trained: 107364\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 254\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.55285714285714\n",
      "  ram_util_percent: 38.1842857142857\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2333199999999893\n",
      "  blue_1: 0.6135999999999909\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753169358104625\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.95456515838954\n",
      "  mean_inference_ms: 2.803653034304464\n",
      "  mean_raw_obs_processing_ms: 25.842420637958465\n",
      "time_since_restore: 14829.999073505402\n",
      "time_this_iter_s: 44.980199575424194\n",
      "time_total_s: 14829.999073505402\n",
      "timers:\n",
      "  learn_throughput: 849.71\n",
      "  learn_time_ms: 528.533\n",
      "  load_throughput: 643678.898\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 7.497\n",
      "  sample_time_ms: 59904.005\n",
      "timestamp: 1638998436\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 107364\n",
      "training_iteration: 254\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:254 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 205 -2.0 -0.9799999999999998\n",
      "blue_1 True True 205 0.005 1.0250000000000001\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 371 -2.0 -0.15000000000001745\n",
      "blue_1 True True 371 0.007 1.9729999999999843\n",
      "agent_timesteps_total: 215880\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-21-51\n",
      "done: false\n",
      "episode_len_mean: 277.35\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8390699999999666\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 474\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.525919437408447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016694949939846992\n",
      "        model: {}\n",
      "        policy_loss: -0.24470822513103485\n",
      "        total_loss: 0.03308963403105736\n",
      "        vf_explained_var: 0.35037556290626526\n",
      "        vf_loss: 0.2574906051158905\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.375739097595215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015397475101053715\n",
      "        model: {}\n",
      "        policy_loss: -0.10015369206666946\n",
      "        total_loss: -0.051792461425065994\n",
      "        vf_explained_var: -0.38078048825263977\n",
      "        vf_loss: 0.02586098574101925\n",
      "  num_agent_steps_sampled: 215880\n",
      "  num_agent_steps_trained: 215880\n",
      "  num_steps_sampled: 107940\n",
      "  num_steps_trained: 107940\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 255\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.0\n",
      "  ram_util_percent: 38.1757281553398\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.23119999999998925\n",
      "  blue_1: 0.607869999999991\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6849999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.575311494624176\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.9109645755547\n",
      "  mean_inference_ms: 2.803509978364555\n",
      "  mean_raw_obs_processing_ms: 25.81174728633082\n",
      "time_since_restore: 14899.52249956131\n",
      "time_this_iter_s: 69.5234260559082\n",
      "time_total_s: 14899.52249956131\n",
      "timers:\n",
      "  learn_throughput: 848.207\n",
      "  learn_time_ms: 527.937\n",
      "  load_throughput: 641815.654\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 7.541\n",
      "  sample_time_ms: 59381.366\n",
      "timestamp: 1638998511\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 107940\n",
      "training_iteration: 255\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:255 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 264 -2.0 -0.685000000000006\n",
      "blue_1 True True 264 0.005 1.4559999999999944\n",
      "agent_timesteps_total: 216408\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-22-30\n",
      "done: false\n",
      "episode_len_mean: 279.35\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8604299999999664\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 475\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.958362579345703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016105853021144867\n",
      "        model: {}\n",
      "        policy_loss: -0.14765389263629913\n",
      "        total_loss: 0.09103188663721085\n",
      "        vf_explained_var: 0.2778659164905548\n",
      "        vf_loss: 0.21909509599208832\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.254683494567871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008474401198327541\n",
      "        model: {}\n",
      "        policy_loss: -0.12144185602664948\n",
      "        total_loss: -0.10005326569080353\n",
      "        vf_explained_var: -0.22166453301906586\n",
      "        vf_loss: 0.009004993364214897\n",
      "  num_agent_steps_sampled: 216408\n",
      "  num_agent_steps_trained: 216408\n",
      "  num_steps_sampled: 108204\n",
      "  num_steps_trained: 108204\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 256\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.07818181818183\n",
      "  ram_util_percent: 38.11090909090908\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.22114999999998922\n",
      "  blue_1: 0.639279999999991\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6749999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753073398563309\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.88905076107172\n",
      "  mean_inference_ms: 2.803441858421327\n",
      "  mean_raw_obs_processing_ms: 25.79626502268856\n",
      "time_since_restore: 14933.314806699753\n",
      "time_this_iter_s: 33.79230713844299\n",
      "time_total_s: 14933.314806699753\n",
      "timers:\n",
      "  learn_throughput: 831.824\n",
      "  learn_time_ms: 528.236\n",
      "  load_throughput: 734605.061\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.534\n",
      "  sample_time_ms: 58322.9\n",
      "timestamp: 1638998550\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 108204\n",
      "training_iteration: 256\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:256 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 323 -0.995 0.6149999999999877\n",
      "blue_1 False True 323 -0.996 0.5180000000000011\n",
      "agent_timesteps_total: 217054\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-23-17\n",
      "done: false\n",
      "episode_len_mean: 280.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.8687199999999663\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 476\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.549028396606445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012679164297878742\n",
      "        model: {}\n",
      "        policy_loss: -0.44953152537345886\n",
      "        total_loss: -0.21428006887435913\n",
      "        vf_explained_var: -0.8791665434837341\n",
      "        vf_loss: 0.21982890367507935\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.00204086303711\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008677719160914421\n",
      "        model: {}\n",
      "        policy_loss: -0.45892074704170227\n",
      "        total_loss: -0.4303196370601654\n",
      "        vf_explained_var: -0.0036790231242775917\n",
      "        vf_loss: 0.015920409932732582\n",
      "  num_agent_steps_sampled: 217054\n",
      "  num_agent_steps_trained: 217054\n",
      "  num_steps_sampled: 108527\n",
      "  num_steps_trained: 108527\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 257\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.984375\n",
      "  ram_util_percent: 38.1\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2157499999999892\n",
      "  blue_1: 0.6529699999999911\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6749999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753047307250959\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.86700219565567\n",
      "  mean_inference_ms: 2.8033704708472738\n",
      "  mean_raw_obs_processing_ms: 25.78057298976613\n",
      "time_since_restore: 14974.098621368408\n",
      "time_this_iter_s: 40.783814668655396\n",
      "time_total_s: 14974.098621368408\n",
      "timers:\n",
      "  learn_throughput: 844.61\n",
      "  learn_time_ms: 496.324\n",
      "  load_throughput: 600660.097\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 7.594\n",
      "  sample_time_ms: 55203.943\n",
      "timestamp: 1638998597\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 108527\n",
      "training_iteration: 257\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:257 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 210 0.005 1.0499999999999996\n",
      "blue_1 True True 210 -2.0 -0.9550000000000003\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 555 0.005 2.774999999999963\n",
      "blue_1 True True 555 -2.0 0.795999999999963\n",
      "agent_timesteps_total: 218584\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-24-51\n",
      "done: false\n",
      "episode_len_mean: 285.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.9189799999999653\n",
      "episode_reward_min: -1.5949999999999998\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 478\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2163718938827515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.116233825683594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02547386847436428\n",
      "        model: {}\n",
      "        policy_loss: -0.08147978037595749\n",
      "        total_loss: -0.024697907269001007\n",
      "        vf_explained_var: -0.4282563328742981\n",
      "        vf_loss: 0.02579616941511631\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.213115692138672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015468258410692215\n",
      "        model: {}\n",
      "        policy_loss: -0.25279471278190613\n",
      "        total_loss: 0.07962774485349655\n",
      "        vf_explained_var: -0.4184471070766449\n",
      "        vf_loss: 0.30981871485710144\n",
      "  num_agent_steps_sampled: 218584\n",
      "  num_agent_steps_trained: 218584\n",
      "  num_steps_sampled: 109292\n",
      "  num_steps_trained: 109292\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 258\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.52461538461538\n",
      "  ram_util_percent: 38.060769230769225\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.24074999999998878\n",
      "  blue_1: 0.6782299999999906\n",
      "policy_reward_min:\n",
      "  blue_0: -1.7999999999999998\n",
      "  blue_1: -1.6749999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5753006890251503\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.820918130014\n",
      "  mean_inference_ms: 2.8032394611600875\n",
      "  mean_raw_obs_processing_ms: 25.74679588079339\n",
      "time_since_restore: 15062.690561294556\n",
      "time_this_iter_s: 88.59193992614746\n",
      "time_total_s: 15062.690561294556\n",
      "timers:\n",
      "  learn_throughput: 858.37\n",
      "  learn_time_ms: 544.52\n",
      "  load_throughput: 669724.546\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 7.74\n",
      "  sample_time_ms: 60385.732\n",
      "timestamp: 1638998691\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 109292\n",
      "training_iteration: 258\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:258 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 36 -2.0 -1.825\n",
      "blue_1 True True 36 0.005 0.18000000000000008\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 53 0.005 0.2650000000000001\n",
      "blue_1 True True 53 -2.0 -1.7399999999999998\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 146 0.005 0.7300000000000005\n",
      "blue_1 True True 146 -2.0 -1.2749999999999995\n",
      "LOSE\n",
      "blue_0 False True 380 -0.995 0.8999999999999816\n",
      "blue_1 False True 380 -0.993 1.1519999999999881\n",
      "agent_timesteps_total: 219814\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-26-36\n",
      "done: false\n",
      "episode_len_mean: 284.75\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.9164599999999656\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 482\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.560498237609863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010733733884990215\n",
      "        model: {}\n",
      "        policy_loss: -0.2334950715303421\n",
      "        total_loss: 0.009937990456819534\n",
      "        vf_explained_var: 0.17673009634017944\n",
      "        vf_loss: 0.22384876012802124\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.461294412612915\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.837876319885254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02248409576714039\n",
      "        model: {}\n",
      "        policy_loss: -0.09782657772302628\n",
      "        total_loss: 0.44730013608932495\n",
      "        vf_explained_var: -0.21405769884586334\n",
      "        vf_loss: 0.5122708678245544\n",
      "  num_agent_steps_sampled: 219814\n",
      "  num_agent_steps_trained: 219814\n",
      "  num_steps_sampled: 109907\n",
      "  num_steps_trained: 109907\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 259\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.18896551724138\n",
      "  ram_util_percent: 38.21172413793104\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2682499999999888\n",
      "  blue_1: 0.6482099999999908\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752979946670383\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.7283171845828\n",
      "  mean_inference_ms: 2.8029765810039593\n",
      "  mean_raw_obs_processing_ms: 25.67895966338894\n",
      "time_since_restore: 15162.797790288925\n",
      "time_this_iter_s: 100.1072289943695\n",
      "time_total_s: 15162.797790288925\n",
      "timers:\n",
      "  learn_throughput: 859.704\n",
      "  learn_time_ms: 562.17\n",
      "  load_throughput: 807966.489\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.379\n",
      "  sample_time_ms: 65496.83\n",
      "timestamp: 1638998796\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 109907\n",
      "training_iteration: 259\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:259 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 440 -0.995 1.1999999999999753\n",
      "blue_1 False True 440 -0.993 1.4729999999999923\n",
      "agent_timesteps_total: 220694\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-27-34\n",
      "done: false\n",
      "episode_len_mean: 287.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.950439999999965\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 483\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.206046104431152\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01029957365244627\n",
      "        model: {}\n",
      "        policy_loss: -0.3601993918418884\n",
      "        total_loss: -0.30550336837768555\n",
      "        vf_explained_var: -0.3788878917694092\n",
      "        vf_loss: 0.03590388223528862\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.59084701538086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00894676148891449\n",
      "        model: {}\n",
      "        policy_loss: -0.3428313434123993\n",
      "        total_loss: -0.23561276495456696\n",
      "        vf_explained_var: -0.517073929309845\n",
      "        vf_loss: 0.08760783076286316\n",
      "  num_agent_steps_sampled: 220694\n",
      "  num_agent_steps_trained: 220694\n",
      "  num_steps_sampled: 110347\n",
      "  num_steps_trained: 110347\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 260\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.11749999999999\n",
      "  ram_util_percent: 38.34125\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.2938999999999885\n",
      "  blue_1: 0.6565399999999907\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752984771638376\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.70509129009972\n",
      "  mean_inference_ms: 2.802916110412155\n",
      "  mean_raw_obs_processing_ms: 25.661844140907128\n",
      "time_since_restore: 15214.677340269089\n",
      "time_this_iter_s: 51.879549980163574\n",
      "time_total_s: 15214.677340269089\n",
      "timers:\n",
      "  learn_throughput: 853.645\n",
      "  learn_time_ms: 547.417\n",
      "  load_throughput: 781436.193\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.313\n",
      "  sample_time_ms: 63897.291\n",
      "timestamp: 1638998854\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 110347\n",
      "training_iteration: 260\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:260 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 385 -0.995 0.9249999999999811\n",
      "blue_1 False True 385 -0.993 1.1809999999999885\n",
      "agent_timesteps_total: 221464\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-28-25\n",
      "done: false\n",
      "episode_len_mean: 288.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.9540599999999648\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 484\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.839361190795898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010125352069735527\n",
      "        model: {}\n",
      "        policy_loss: -0.08188726752996445\n",
      "        total_loss: 0.011515315622091293\n",
      "        vf_explained_var: -0.2545067369937897\n",
      "        vf_loss: 0.07492829114198685\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.105401992797852\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011621581390500069\n",
      "        model: {}\n",
      "        policy_loss: -0.08753087371587753\n",
      "        total_loss: 0.025134626775979996\n",
      "        vf_explained_var: -0.6897404193878174\n",
      "        vf_loss: 0.08719167858362198\n",
      "  num_agent_steps_sampled: 221464\n",
      "  num_agent_steps_trained: 221464\n",
      "  num_steps_sampled: 110732\n",
      "  num_steps_trained: 110732\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 261\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.52285714285714\n",
      "  ram_util_percent: 38.28857142857142\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.29574999999998847\n",
      "  blue_1: 0.6583099999999905\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752986051028899\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.68150958173551\n",
      "  mean_inference_ms: 2.8028566368635794\n",
      "  mean_raw_obs_processing_ms: 25.644391235002132\n",
      "time_since_restore: 15260.144229412079\n",
      "time_this_iter_s: 45.46688914299011\n",
      "time_total_s: 15260.144229412079\n",
      "timers:\n",
      "  learn_throughput: 848.509\n",
      "  learn_time_ms: 531.757\n",
      "  load_throughput: 754422.948\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.269\n",
      "  sample_time_ms: 62075.806\n",
      "timestamp: 1638998905\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 110732\n",
      "training_iteration: 261\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:260 starting ! -----------------\n",
      "agent_timesteps_total: 221464\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-28-25\n",
      "done: false\n",
      "episode_len_mean: 288.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.9540599999999648\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 484\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.839361190795898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010125352069735527\n",
      "        model: {}\n",
      "        policy_loss: -0.08188726752996445\n",
      "        total_loss: 0.011515315622091293\n",
      "        vf_explained_var: -0.2545067369937897\n",
      "        vf_loss: 0.07492829114198685\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.105401992797852\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011621581390500069\n",
      "        model: {}\n",
      "        policy_loss: -0.08753087371587753\n",
      "        total_loss: 0.025134626775979996\n",
      "        vf_explained_var: -0.6897404193878174\n",
      "        vf_loss: 0.08719167858362198\n",
      "  num_agent_steps_sampled: 221464\n",
      "  num_agent_steps_trained: 221464\n",
      "  num_steps_sampled: 110732\n",
      "  num_steps_trained: 110732\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 261\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.52285714285714\n",
      "  ram_util_percent: 38.28857142857142\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.29574999999998847\n",
      "  blue_1: 0.6583099999999905\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752986051028899\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.68150958173551\n",
      "  mean_inference_ms: 2.8028566368635794\n",
      "  mean_raw_obs_processing_ms: 25.644391235002132\n",
      "time_since_restore: 15260.144229412079\n",
      "time_this_iter_s: 45.46688914299011\n",
      "time_total_s: 15260.144229412079\n",
      "timers:\n",
      "  learn_throughput: 848.509\n",
      "  learn_time_ms: 531.757\n",
      "  load_throughput: 754422.948\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.269\n",
      "  sample_time_ms: 62075.806\n",
      "timestamp: 1638998905\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 110732\n",
      "training_iteration: 261\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:261 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 263 0.005 1.314999999999994\n",
      "blue_1 True True 263 -2.001 -0.6740000000000042\n",
      "agent_timesteps_total: 221990\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-29-02\n",
      "done: false\n",
      "episode_len_mean: 288.82\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.9600199999999649\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 485\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.090588569641113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010203276760876179\n",
      "        model: {}\n",
      "        policy_loss: -0.06135612726211548\n",
      "        total_loss: -0.034979064017534256\n",
      "        vf_explained_var: -0.562175452709198\n",
      "        vf_loss: 0.007760592736303806\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.661424160003662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015553776174783707\n",
      "        model: {}\n",
      "        policy_loss: -0.12826433777809143\n",
      "        total_loss: 0.23856176435947418\n",
      "        vf_explained_var: 0.3968740403652191\n",
      "        vf_loss: 0.3327331244945526\n",
      "  num_agent_steps_sampled: 221990\n",
      "  num_agent_steps_trained: 221990\n",
      "  num_steps_sampled: 110995\n",
      "  num_steps_trained: 110995\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 262\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.68653846153846\n",
      "  ram_util_percent: 38.24230769230769\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3186999999999884\n",
      "  blue_1: 0.6413199999999906\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752965772250779\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.65848489026561\n",
      "  mean_inference_ms: 2.8027953576912013\n",
      "  mean_raw_obs_processing_ms: 25.62683624342605\n",
      "time_since_restore: 15292.088846683502\n",
      "time_this_iter_s: 31.94461727142334\n",
      "time_total_s: 15292.088846683502\n",
      "timers:\n",
      "  learn_throughput: 828.432\n",
      "  learn_time_ms: 532.572\n",
      "  load_throughput: 737702.581\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.258\n",
      "  sample_time_ms: 60786.474\n",
      "timestamp: 1638998942\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 110995\n",
      "training_iteration: 262\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:262 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.994 1.89099999999997\n",
      "agent_timesteps_total: 223190\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-30-16\n",
      "done: false\n",
      "episode_len_mean: 292.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 0.998079999999964\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 486\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.30198860168457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0069046346470713615\n",
      "        model: {}\n",
      "        policy_loss: -0.38572224974632263\n",
      "        total_loss: -0.3444831669330597\n",
      "        vf_explained_var: -0.7186916470527649\n",
      "        vf_loss: 0.028641192242503166\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.8362202644348145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006082681939005852\n",
      "        model: {}\n",
      "        policy_loss: -0.35342803597450256\n",
      "        total_loss: -0.27678677439689636\n",
      "        vf_explained_var: -0.5303844213485718\n",
      "        vf_loss: 0.06330840289592743\n",
      "  num_agent_steps_sampled: 223190\n",
      "  num_agent_steps_trained: 223190\n",
      "  num_steps_sampled: 111595\n",
      "  num_steps_trained: 111595\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 263\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.54158415841585\n",
      "  ram_util_percent: 38.21386138613861\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.34829999999998795\n",
      "  blue_1: 0.6497799999999903\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752924136217813\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.63495817176346\n",
      "  mean_inference_ms: 2.8027307174714946\n",
      "  mean_raw_obs_processing_ms: 25.608485402572697\n",
      "time_since_restore: 15360.151355266571\n",
      "time_this_iter_s: 68.06250858306885\n",
      "time_total_s: 15360.151355266571\n",
      "timers:\n",
      "  learn_throughput: 820.813\n",
      "  learn_time_ms: 564.684\n",
      "  load_throughput: 930083.2\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 7.349\n",
      "  sample_time_ms: 63066.546\n",
      "timestamp: 1638999016\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 111595\n",
      "training_iteration: 263\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:263 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 310 -2.0 -0.45500000000001095\n",
      "blue_1 True True 310 0.007 1.6489999999999905\n",
      "agent_timesteps_total: 223810\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-31-03\n",
      "done: false\n",
      "episode_len_mean: 293.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.002859999999964\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 487\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.847254753112793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010823773220181465\n",
      "        model: {}\n",
      "        policy_loss: -0.4174444079399109\n",
      "        total_loss: -0.26868852972984314\n",
      "        vf_explained_var: -0.10651682317256927\n",
      "        vf_loss: 0.1290072649717331\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.351582527160645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008840382099151611\n",
      "        model: {}\n",
      "        policy_loss: -0.2094118297100067\n",
      "        total_loss: -0.177644744515419\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.012389514595270157\n",
      "  num_agent_steps_sampled: 223810\n",
      "  num_agent_steps_trained: 223810\n",
      "  num_steps_sampled: 111905\n",
      "  num_steps_trained: 111905\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 264\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.37384615384616\n",
      "  ram_util_percent: 38.207692307692305\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.330249999999988\n",
      "  blue_1: 0.6726099999999903\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752866199147135\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.61184837657848\n",
      "  mean_inference_ms: 2.8026698857728904\n",
      "  mean_raw_obs_processing_ms: 25.59014563886563\n",
      "time_since_restore: 15401.23149394989\n",
      "time_this_iter_s: 41.08013868331909\n",
      "time_total_s: 15401.23149394989\n",
      "timers:\n",
      "  learn_throughput: 831.116\n",
      "  learn_time_ms: 546.374\n",
      "  load_throughput: 1138794.288\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.24\n",
      "  sample_time_ms: 62722.706\n",
      "timestamp: 1638999063\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 111905\n",
      "training_iteration: 264\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:264 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 236 -2.0 -0.8250000000000031\n",
      "blue_1 True True 236 0.005 1.2159999999999969\n",
      "red_0 Splash :1\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 548 0.005 3.159999999999964\n",
      "blue_1 True True 548 -3.0 -0.256000000000022\n",
      "agent_timesteps_total: 225378\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-32-45\n",
      "done: false\n",
      "episode_len_mean: 297.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.0454699999999628\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 489\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.619336128234863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011479650624096394\n",
      "        model: {}\n",
      "        policy_loss: -0.04927673190832138\n",
      "        total_loss: 0.19370636343955994\n",
      "        vf_explained_var: -0.25834256410598755\n",
      "        vf_loss: 0.22203780710697174\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.627995491027832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010348008945584297\n",
      "        model: {}\n",
      "        policy_loss: -0.1392192840576172\n",
      "        total_loss: 0.1307862550020218\n",
      "        vf_explained_var: -0.22120897471904755\n",
      "        vf_loss: 0.24732330441474915\n",
      "  num_agent_steps_sampled: 225378\n",
      "  num_agent_steps_trained: 225378\n",
      "  num_steps_sampled: 112689\n",
      "  num_steps_trained: 112689\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 265\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.75985915492957\n",
      "  ram_util_percent: 38.203521126760556\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.35859999999998754\n",
      "  blue_1: 0.6868699999999899\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752725534097397\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.56407774139011\n",
      "  mean_inference_ms: 2.8025343728830774\n",
      "  mean_raw_obs_processing_ms: 25.55131249692371\n",
      "time_since_restore: 15498.393675088882\n",
      "time_this_iter_s: 97.16218113899231\n",
      "time_total_s: 15498.393675088882\n",
      "timers:\n",
      "  learn_throughput: 820.23\n",
      "  learn_time_ms: 578.984\n",
      "  load_throughput: 1190956.634\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.257\n",
      "  sample_time_ms: 65438.189\n",
      "timestamp: 1638999165\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 112689\n",
      "training_iteration: 265\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:265 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False True 517 -0.995 1.584999999999967\n",
      "blue_1 False True 517 -0.996 1.419999999999988\n",
      "agent_timesteps_total: 226412\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-33-52\n",
      "done: false\n",
      "episode_len_mean: 301.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.0766699999999618\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 490\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.209187507629395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012577062472701073\n",
      "        model: {}\n",
      "        policy_loss: -0.08159233629703522\n",
      "        total_loss: -0.01580698974430561\n",
      "        vf_explained_var: -0.5566800236701965\n",
      "        vf_loss: 0.042837779968976974\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.195908546447754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009019416756927967\n",
      "        model: {}\n",
      "        policy_loss: -0.08131759613752365\n",
      "        total_loss: 0.008067439310252666\n",
      "        vf_explained_var: -0.3355158269405365\n",
      "        vf_loss: 0.06961501389741898\n",
      "  num_agent_steps_sampled: 226412\n",
      "  num_agent_steps_trained: 226412\n",
      "  num_steps_sampled: 113206\n",
      "  num_steps_trained: 113206\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 266\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.05714285714286\n",
      "  ram_util_percent: 38.18791208791209\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.3649999999999872\n",
      "  blue_1: 0.7116699999999898\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752626371662886\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.54044737286169\n",
      "  mean_inference_ms: 2.802465934642764\n",
      "  mean_raw_obs_processing_ms: 25.530945204668843\n",
      "time_since_restore: 15559.445834159851\n",
      "time_this_iter_s: 61.05215907096863\n",
      "time_total_s: 15559.445834159851\n",
      "timers:\n",
      "  learn_throughput: 801.906\n",
      "  learn_time_ms: 623.764\n",
      "  load_throughput: 1254404.102\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.34\n",
      "  sample_time_ms: 68147.612\n",
      "timestamp: 1638999232\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 113206\n",
      "training_iteration: 266\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:266 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 208 -2.0 -0.9650000000000001\n",
      "blue_1 True True 208 0.005 1.0679999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 345 0.005 1.7249999999999852\n",
      "blue_1 True True 345 -2.0 -0.29300000000001325\n",
      "agent_timesteps_total: 227518\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-35-08\n",
      "done: false\n",
      "episode_len_mean: 302.27\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.0870599999999617\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 492\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.902980327606201\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010791419073939323\n",
      "        model: {}\n",
      "        policy_loss: 0.005618453025817871\n",
      "        total_loss: 0.2592932879924774\n",
      "        vf_explained_var: -0.038521599024534225\n",
      "        vf_loss: 0.23398524522781372\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.758717060089111\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009851248003542423\n",
      "        model: {}\n",
      "        policy_loss: -0.23237958550453186\n",
      "        total_loss: -0.05960947275161743\n",
      "        vf_explained_var: -0.34720703959465027\n",
      "        vf_loss: 0.15117670595645905\n",
      "  num_agent_steps_sampled: 227518\n",
      "  num_agent_steps_trained: 227518\n",
      "  num_steps_sampled: 113759\n",
      "  num_steps_trained: 113759\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 267\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.323076923076925\n",
      "  ram_util_percent: 38.19423076923077\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.39019999999998717\n",
      "  blue_1: 0.6968599999999898\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752485100886439\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.49317494849974\n",
      "  mean_inference_ms: 2.8023333962595136\n",
      "  mean_raw_obs_processing_ms: 25.490173410793215\n",
      "time_since_restore: 15629.39313340187\n",
      "time_this_iter_s: 69.94729924201965\n",
      "time_total_s: 15629.39313340187\n",
      "timers:\n",
      "  learn_throughput: 797.89\n",
      "  learn_time_ms: 655.73\n",
      "  load_throughput: 1311769.892\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.361\n",
      "  sample_time_ms: 71080.733\n",
      "timestamp: 1638999308\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 113759\n",
      "training_iteration: 267\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:267 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 92 0.005 0.4600000000000003\n",
      "blue_1 True True 92 -2.0 -1.5449999999999997\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 1.9859999999999598\n",
      "agent_timesteps_total: 228902\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-36-34\n",
      "done: false\n",
      "episode_len_mean: 303.96\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.1038799999999613\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 494\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.040234565734863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016599686816334724\n",
      "        model: {}\n",
      "        policy_loss: -0.2021206170320511\n",
      "        total_loss: -0.12866410613059998\n",
      "        vf_explained_var: -0.5667726397514343\n",
      "        vf_loss: 0.04316940903663635\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.31875467300415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010706039145588875\n",
      "        model: {}\n",
      "        policy_loss: -0.1408020406961441\n",
      "        total_loss: 0.18176770210266113\n",
      "        vf_explained_var: -0.5889036059379578\n",
      "        vf_loss: 0.2991027235984802\n",
      "  num_agent_steps_sampled: 228902\n",
      "  num_agent_steps_trained: 228902\n",
      "  num_steps_sampled: 114451\n",
      "  num_steps_trained: 114451\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 268\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.53666666666666\n",
      "  ram_util_percent: 38.192499999999995\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.40869999999998696\n",
      "  blue_1: 0.6951799999999896\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752298233662905\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.44571649787675\n",
      "  mean_inference_ms: 2.8022068237762348\n",
      "  mean_raw_obs_processing_ms: 25.44995091537078\n",
      "time_since_restore: 15710.575484991074\n",
      "time_this_iter_s: 81.18235158920288\n",
      "time_total_s: 15710.575484991074\n",
      "timers:\n",
      "  learn_throughput: 790.488\n",
      "  learn_time_ms: 652.635\n",
      "  load_throughput: 1293467.292\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.331\n",
      "  sample_time_ms: 70372.879\n",
      "timestamp: 1638999394\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 114451\n",
      "training_iteration: 268\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:268 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.017999999999959\n",
      "agent_timesteps_total: 230102\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-37-49\n",
      "done: false\n",
      "episode_len_mean: 306.15\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.126159999999961\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 495\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.526902198791504\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00766418781131506\n",
      "        model: {}\n",
      "        policy_loss: -0.28850746154785156\n",
      "        total_loss: -0.22553326189517975\n",
      "        vf_explained_var: -0.7145359516143799\n",
      "        vf_loss: 0.048990461975336075\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.191941738128662\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.432751655578613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004708176013082266\n",
      "        model: {}\n",
      "        policy_loss: -0.36039960384368896\n",
      "        total_loss: -0.30854102969169617\n",
      "        vf_explained_var: -0.7360535860061646\n",
      "        vf_loss: 0.04153849184513092\n",
      "  num_agent_steps_sampled: 230102\n",
      "  num_agent_steps_trained: 230102\n",
      "  num_steps_sampled: 115051\n",
      "  num_steps_trained: 115051\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 269\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.882524271844666\n",
      "  ram_util_percent: 38.200970873786396\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.40964999999998675\n",
      "  blue_1: 0.7165099999999893\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752169972650876\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.42212765243025\n",
      "  mean_inference_ms: 2.8021419914566623\n",
      "  mean_raw_obs_processing_ms: 25.429303241983472\n",
      "time_since_restore: 15779.85547041893\n",
      "time_this_iter_s: 69.27998542785645\n",
      "time_total_s: 15779.85547041893\n",
      "timers:\n",
      "  learn_throughput: 791.694\n",
      "  learn_time_ms: 649.746\n",
      "  load_throughput: 1031777.523\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.645\n",
      "  sample_time_ms: 67287.76\n",
      "timestamp: 1638999469\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 115051\n",
      "training_iteration: 269\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:269 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 437 -2.0 0.17999999999997573\n",
      "blue_1 True True 437 0.005 2.1849999999999756\n",
      "agent_timesteps_total: 230976\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-38-40\n",
      "done: false\n",
      "episode_len_mean: 309.7\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.16165999999996\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 496\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.717077732086182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01403795462101698\n",
      "        model: {}\n",
      "        policy_loss: -0.33661508560180664\n",
      "        total_loss: -0.1499425321817398\n",
      "        vf_explained_var: 0.2517496645450592\n",
      "        vf_loss: 0.16105954349040985\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.095970869064331\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.033936977386475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02424090914428234\n",
      "        model: {}\n",
      "        policy_loss: -0.29391881823539734\n",
      "        total_loss: -0.25242742896080017\n",
      "        vf_explained_var: -0.9087026119232178\n",
      "        vf_loss: 0.014924070797860622\n",
      "  num_agent_steps_sampled: 230976\n",
      "  num_agent_steps_trained: 230976\n",
      "  num_steps_sampled: 115488\n",
      "  num_steps_trained: 115488\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 270\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.72142857142856\n",
      "  ram_util_percent: 38.231428571428566\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.42739999999998646\n",
      "  blue_1: 0.7342599999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5752054545113647\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.39778239357383\n",
      "  mean_inference_ms: 2.802074879866687\n",
      "  mean_raw_obs_processing_ms: 25.407668694020625\n",
      "time_since_restore: 15824.878967761993\n",
      "time_this_iter_s: 45.023497343063354\n",
      "time_total_s: 15824.878967761993\n",
      "timers:\n",
      "  learn_throughput: 789.186\n",
      "  learn_time_ms: 651.431\n",
      "  load_throughput: 1288338.225\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.72\n",
      "  sample_time_ms: 66592.235\n",
      "timestamp: 1638999520\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 115488\n",
      "training_iteration: 270\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:270 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 336 0.005 1.6799999999999862\n",
      "blue_1 True True 336 -2.0 -0.3250000000000137\n",
      "agent_timesteps_total: 231648\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-39-24\n",
      "done: false\n",
      "episode_len_mean: 312.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.1891599999999594\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 497\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.560063362121582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007929261773824692\n",
      "        model: {}\n",
      "        policy_loss: -0.5354229807853699\n",
      "        total_loss: -0.5026797652244568\n",
      "        vf_explained_var: 0.3150903284549713\n",
      "        vf_loss: 0.018275748938322067\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.851576805114746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009724753908813\n",
      "        model: {}\n",
      "        policy_loss: -0.5225165486335754\n",
      "        total_loss: -0.4441664516925812\n",
      "        vf_explained_var: 0.07264577597379684\n",
      "        vf_loss: 0.06236308440566063\n",
      "  num_agent_steps_sampled: 231648\n",
      "  num_agent_steps_trained: 231648\n",
      "  num_steps_sampled: 115824\n",
      "  num_steps_trained: 115824\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 271\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.52295081967213\n",
      "  ram_util_percent: 38.240983606557386\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.46119999999998634\n",
      "  blue_1: 0.7279599999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751934380734189\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.37298200709438\n",
      "  mean_inference_ms: 2.802008967971467\n",
      "  mean_raw_obs_processing_ms: 25.385827685486806\n",
      "time_since_restore: 15863.212232112885\n",
      "time_this_iter_s: 38.33326435089111\n",
      "time_total_s: 15863.212232112885\n",
      "timers:\n",
      "  learn_throughput: 802.047\n",
      "  learn_time_ms: 634.875\n",
      "  load_throughput: 1701513.382\n",
      "  load_time_ms: 0.299\n",
      "  sample_throughput: 7.727\n",
      "  sample_time_ms: 65899.561\n",
      "timestamp: 1638999564\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 115824\n",
      "training_iteration: 271\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:270 starting ! -----------------\n",
      "agent_timesteps_total: 231648\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-39-24\n",
      "done: false\n",
      "episode_len_mean: 312.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.1891599999999594\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 497\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.560063362121582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007929261773824692\n",
      "        model: {}\n",
      "        policy_loss: -0.5354229807853699\n",
      "        total_loss: -0.5026797652244568\n",
      "        vf_explained_var: 0.3150903284549713\n",
      "        vf_loss: 0.018275748938322067\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.851576805114746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009724753908813\n",
      "        model: {}\n",
      "        policy_loss: -0.5225165486335754\n",
      "        total_loss: -0.4441664516925812\n",
      "        vf_explained_var: 0.07264577597379684\n",
      "        vf_loss: 0.06236308440566063\n",
      "  num_agent_steps_sampled: 231648\n",
      "  num_agent_steps_trained: 231648\n",
      "  num_steps_sampled: 115824\n",
      "  num_steps_trained: 115824\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 271\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.52295081967213\n",
      "  ram_util_percent: 38.240983606557386\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.46119999999998634\n",
      "  blue_1: 0.7279599999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751934380734189\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.37298200709438\n",
      "  mean_inference_ms: 2.802008967971467\n",
      "  mean_raw_obs_processing_ms: 25.385827685486806\n",
      "time_since_restore: 15863.212232112885\n",
      "time_this_iter_s: 38.33326435089111\n",
      "time_total_s: 15863.212232112885\n",
      "timers:\n",
      "  learn_throughput: 802.047\n",
      "  learn_time_ms: 634.875\n",
      "  load_throughput: 1701513.382\n",
      "  load_time_ms: 0.299\n",
      "  sample_throughput: 7.727\n",
      "  sample_time_ms: 65899.561\n",
      "timestamp: 1638999564\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 115824\n",
      "training_iteration: 271\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:271 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 498 0.005 2.489999999999969\n",
      "blue_1 True True 498 -2.0 0.4719999999999702\n",
      "agent_timesteps_total: 232644\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-40-28\n",
      "done: false\n",
      "episode_len_mean: 314.33\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.2078299999999593\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 498\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.372096538543701\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010975589044392109\n",
      "        model: {}\n",
      "        policy_loss: -0.3195164203643799\n",
      "        total_loss: -0.2880210876464844\n",
      "        vf_explained_var: -0.9563897252082825\n",
      "        vf_loss: 0.011469737626612186\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.341567993164062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007361344061791897\n",
      "        model: {}\n",
      "        policy_loss: -0.5162503719329834\n",
      "        total_loss: -0.4595564007759094\n",
      "        vf_explained_var: -0.7373331785202026\n",
      "        vf_loss: 0.044592227786779404\n",
      "  num_agent_steps_sampled: 232644\n",
      "  num_agent_steps_trained: 232644\n",
      "  num_steps_sampled: 116322\n",
      "  num_steps_trained: 116322\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 272\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.054545454545455\n",
      "  ram_util_percent: 37.839772727272724\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.4906499999999861\n",
      "  blue_1: 0.7171799999999885\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751804177084159\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.34792050387875\n",
      "  mean_inference_ms: 2.8019405801974164\n",
      "  mean_raw_obs_processing_ms: 25.36344772503379\n",
      "time_since_restore: 15921.989608764648\n",
      "time_this_iter_s: 58.777376651763916\n",
      "time_total_s: 15921.989608764648\n",
      "timers:\n",
      "  learn_throughput: 818.867\n",
      "  learn_time_ms: 650.533\n",
      "  load_throughput: 2669421.435\n",
      "  load_time_ms: 0.2\n",
      "  sample_throughput: 7.771\n",
      "  sample_time_ms: 68547.798\n",
      "timestamp: 1638999628\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 116322\n",
      "training_iteration: 272\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:272 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 185 0.005 0.9250000000000007\n",
      "blue_1 True True 185 -2.0 -1.0959999999999992\n",
      "LOSE\n",
      "blue_0 False True 384 -0.995 0.9199999999999812\n",
      "blue_1 False True 384 -0.993 1.2319999999999887\n",
      "agent_timesteps_total: 233782\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-41-47\n",
      "done: false\n",
      "episode_len_mean: 315.22\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.2206799999999591\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 500\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.385233879089355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006313846912235022\n",
      "        model: {}\n",
      "        policy_loss: -0.3213863670825958\n",
      "        total_loss: -0.286522775888443\n",
      "        vf_explained_var: 0.31990617513656616\n",
      "        vf_loss: 0.023343626409769058\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.866164207458496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011059585958719254\n",
      "        model: {}\n",
      "        policy_loss: -0.1530785858631134\n",
      "        total_loss: 0.1380757987499237\n",
      "        vf_explained_var: -0.03413204848766327\n",
      "        vf_loss: 0.27297288179397583\n",
      "  num_agent_steps_sampled: 233782\n",
      "  num_agent_steps_trained: 233782\n",
      "  num_steps_sampled: 116891\n",
      "  num_steps_trained: 116891\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 273\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.71009174311927\n",
      "  ram_util_percent: 37.81926605504588\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.515149999999986\n",
      "  blue_1: 0.7055299999999883\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751563432459527\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.29815866798624\n",
      "  mean_inference_ms: 2.801811262770919\n",
      "  mean_raw_obs_processing_ms: 25.317921125994612\n",
      "time_since_restore: 15995.042058229446\n",
      "time_this_iter_s: 73.05244946479797\n",
      "time_total_s: 15995.042058229446\n",
      "timers:\n",
      "  learn_throughput: 814.598\n",
      "  learn_time_ms: 650.137\n",
      "  load_throughput: 2653886.975\n",
      "  load_time_ms: 0.2\n",
      "  sample_throughput: 7.669\n",
      "  sample_time_ms: 69060.729\n",
      "timestamp: 1638999707\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 116891\n",
      "training_iteration: 273\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:273 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.0129999999999595\n",
      "agent_timesteps_total: 234982\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-42-59\n",
      "done: false\n",
      "episode_len_mean: 320.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.2737599999999583\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 501\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.351503372192383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008343303576111794\n",
      "        model: {}\n",
      "        policy_loss: -0.38828524947166443\n",
      "        total_loss: -0.35952264070510864\n",
      "        vf_explained_var: -0.6988811492919922\n",
      "        vf_loss: 0.01353981252759695\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.014728546142578\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011352943256497383\n",
      "        model: {}\n",
      "        policy_loss: -0.2757110297679901\n",
      "        total_loss: -0.18020503222942352\n",
      "        vf_explained_var: -0.7254888415336609\n",
      "        vf_loss: 0.07684222608804703\n",
      "  num_agent_steps_sampled: 234982\n",
      "  num_agent_steps_trained: 234982\n",
      "  num_steps_sampled: 117491\n",
      "  num_steps_trained: 117491\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 274\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.61414141414142\n",
      "  ram_util_percent: 37.80303030303031\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5516499999999857\n",
      "  blue_1: 0.722109999999988\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751428672495666\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.27223633012447\n",
      "  mean_inference_ms: 2.8017451964443616\n",
      "  mean_raw_obs_processing_ms: 25.293846963630386\n",
      "time_since_restore: 16061.63123178482\n",
      "time_this_iter_s: 66.58917355537415\n",
      "time_total_s: 16061.63123178482\n",
      "timers:\n",
      "  learn_throughput: 814.045\n",
      "  learn_time_ms: 686.203\n",
      "  load_throughput: 2799209.336\n",
      "  load_time_ms: 0.2\n",
      "  sample_throughput: 7.805\n",
      "  sample_time_ms: 71572.779\n",
      "timestamp: 1638999779\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 117491\n",
      "training_iteration: 274\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:274 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 120 0.005 0.6000000000000004\n",
      "blue_1 True True 120 -2.0 -1.4049999999999996\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 309 -2.0 -0.46000000000001084\n",
      "blue_1 True True 309 0.005 1.544999999999989\n",
      "agent_timesteps_total: 235840\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-43-57\n",
      "done: false\n",
      "episode_len_mean: 319.06\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.2591499999999585\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 503\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.281267166137695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008979668840765953\n",
      "        model: {}\n",
      "        policy_loss: -0.31689199805259705\n",
      "        total_loss: -0.1521161049604416\n",
      "        vf_explained_var: 0.356156587600708\n",
      "        vf_loss: 0.148391991853714\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.3390069007873535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018658695742487907\n",
      "        model: {}\n",
      "        policy_loss: -0.07185932248830795\n",
      "        total_loss: 0.35895615816116333\n",
      "        vf_explained_var: 0.06400176882743835\n",
      "        vf_loss: 0.40014129877090454\n",
      "  num_agent_steps_sampled: 235840\n",
      "  num_agent_steps_trained: 235840\n",
      "  num_steps_sampled: 117920\n",
      "  num_steps_trained: 117920\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 275\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.847500000000004\n",
      "  ram_util_percent: 37.785000000000004\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5644499999999859\n",
      "  blue_1: 0.6946999999999883\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5751155626441744\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.21996106806512\n",
      "  mean_inference_ms: 2.8016144549207023\n",
      "  mean_raw_obs_processing_ms: 25.246801357588993\n",
      "time_since_restore: 16114.28046989441\n",
      "time_this_iter_s: 52.64923810958862\n",
      "time_total_s: 16114.28046989441\n",
      "timers:\n",
      "  learn_throughput: 817.943\n",
      "  learn_time_ms: 639.531\n",
      "  load_throughput: 1747821.574\n",
      "  load_time_ms: 0.299\n",
      "  sample_throughput: 7.784\n",
      "  sample_time_ms: 67202.877\n",
      "timestamp: 1638999837\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 117920\n",
      "training_iteration: 275\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:275 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 199 0.005 0.9950000000000008\n",
      "blue_1 True True 199 -1.998 -0.9889999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 202 0.005 1.0100000000000005\n",
      "blue_1 True True 202 -2.0 -0.9949999999999994\n",
      "agent_timesteps_total: 236642\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-45-00\n",
      "done: false\n",
      "episode_len_mean: 313.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.2083499999999596\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 505\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.933020114898682\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009458769112825394\n",
      "        model: {}\n",
      "        policy_loss: -0.1493651121854782\n",
      "        total_loss: -0.10874409228563309\n",
      "        vf_explained_var: 0.4975660443305969\n",
      "        vf_loss: 0.02336294576525688\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.367951393127441\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012167256325483322\n",
      "        model: {}\n",
      "        policy_loss: -0.15329505503177643\n",
      "        total_loss: 0.07131054252386093\n",
      "        vf_explained_var: 0.5054363012313843\n",
      "        vf_loss: 0.2046031504869461\n",
      "  num_agent_steps_sampled: 236642\n",
      "  num_agent_steps_trained: 236642\n",
      "  num_steps_sampled: 118321\n",
      "  num_steps_trained: 118321\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 276\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.17727272727273\n",
      "  ram_util_percent: 37.79204545454545\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5690999999999864\n",
      "  blue_1: 0.6392499999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750875033392662\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.1712760135769\n",
      "  mean_inference_ms: 2.8014856736228055\n",
      "  mean_raw_obs_processing_ms: 25.20266191690804\n",
      "time_since_restore: 16171.951177120209\n",
      "time_this_iter_s: 57.67070722579956\n",
      "time_total_s: 16171.951177120209\n",
      "timers:\n",
      "  learn_throughput: 836.604\n",
      "  learn_time_ms: 611.4\n",
      "  load_throughput: 1281286.727\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.652\n",
      "  sample_time_ms: 66844.337\n",
      "timestamp: 1638999900\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 118321\n",
      "training_iteration: 276\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:276 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 217 -2.0 -0.920000000000001\n",
      "blue_1 True True 217 0.005 1.0849999999999989\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 275 -2.0 -0.6300000000000072\n",
      "blue_1 True True 275 0.004 1.3679999999999934\n",
      "agent_timesteps_total: 237626\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-46-05\n",
      "done: false\n",
      "episode_len_mean: 311.76\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.1856599999999604\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 507\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.921632289886475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012864689342677593\n",
      "        model: {}\n",
      "        policy_loss: -0.32412585616111755\n",
      "        total_loss: -0.008444759994745255\n",
      "        vf_explained_var: 0.4558350741863251\n",
      "        vf_loss: 0.2922087013721466\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6439563035964966\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.656376361846924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02400490641593933\n",
      "        model: {}\n",
      "        policy_loss: 0.24313054978847504\n",
      "        total_loss: 0.40031275153160095\n",
      "        vf_explained_var: -0.6607373952865601\n",
      "        vf_loss: 0.1177191212773323\n",
      "  num_agent_steps_sampled: 237626\n",
      "  num_agent_steps_trained: 237626\n",
      "  num_steps_sampled: 118813\n",
      "  num_steps_trained: 118813\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 277\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.99666666666666\n",
      "  ram_util_percent: 37.79444444444445\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5379499999999866\n",
      "  blue_1: 0.6477099999999893\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750600942257889\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.12401423412193\n",
      "  mean_inference_ms: 2.8013560123204093\n",
      "  mean_raw_obs_processing_ms: 25.161701784061552\n",
      "time_since_restore: 16231.264895915985\n",
      "time_this_iter_s: 59.31371879577637\n",
      "time_total_s: 16231.264895915985\n",
      "timers:\n",
      "  learn_throughput: 846.73\n",
      "  learn_time_ms: 596.884\n",
      "  load_throughput: 1266687.327\n",
      "  load_time_ms: 0.399\n",
      "  sample_throughput: 7.685\n",
      "  sample_time_ms: 65761.78\n",
      "timestamp: 1638999965\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 118813\n",
      "training_iteration: 277\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:277 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 405 -0.995 1.024999999999979\n",
      "blue_1 False True 405 -0.993 1.3129999999999895\n",
      "agent_timesteps_total: 238436\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-47-03\n",
      "done: false\n",
      "episode_len_mean: 312.98\n",
      "episode_media: {}\n",
      "episode_reward_max: 5.584999999999989\n",
      "episode_reward_mean: 1.2007899999999603\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 508\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.184106826782227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011242756620049477\n",
      "        model: {}\n",
      "        policy_loss: -0.19871936738491058\n",
      "        total_loss: -0.06741761416196823\n",
      "        vf_explained_var: -0.28892093896865845\n",
      "        vf_loss: 0.11078870296478271\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 10.148982048034668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00698196841403842\n",
      "        model: {}\n",
      "        policy_loss: -0.20751354098320007\n",
      "        total_loss: -0.11069507896900177\n",
      "        vf_explained_var: -0.7389767169952393\n",
      "        vf_loss: 0.07960138469934464\n",
      "  num_agent_steps_sampled: 238436\n",
      "  num_agent_steps_trained: 238436\n",
      "  num_steps_sampled: 119218\n",
      "  num_steps_trained: 119218\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 278\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.14102564102565\n",
      "  ram_util_percent: 37.8025641025641\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.480000000000008\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5340499999999866\n",
      "  blue_1: 0.6667399999999892\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750469825433432\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.10073059814049\n",
      "  mean_inference_ms: 2.801286486964902\n",
      "  mean_raw_obs_processing_ms: 25.140849351519286\n",
      "time_since_restore: 16283.351315021515\n",
      "time_this_iter_s: 52.086419105529785\n",
      "time_total_s: 16283.351315021515\n",
      "timers:\n",
      "  learn_throughput: 840.793\n",
      "  learn_time_ms: 566.965\n",
      "  load_throughput: 950613.187\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.582\n",
      "  sample_time_ms: 62869.449\n",
      "timestamp: 1639000023\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 119218\n",
      "training_iteration: 278\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:278 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 236 -2.0 -0.8250000000000031\n",
      "blue_1 True True 236 0.005 1.1799999999999968\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 398 -2.0 -0.01500000000002033\n",
      "blue_1 True True 398 0.007 2.101999999999984\n",
      "agent_timesteps_total: 239704\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-48-31\n",
      "done: false\n",
      "episode_len_mean: 314.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.1821099999999596\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 510\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.599828243255615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010554548352956772\n",
      "        model: {}\n",
      "        policy_loss: -0.2857367694377899\n",
      "        total_loss: 0.013782477006316185\n",
      "        vf_explained_var: -0.022964708507061005\n",
      "        vf_loss: 0.28026193380355835\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.295107841491699\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013903096318244934\n",
      "        model: {}\n",
      "        policy_loss: 0.11093178391456604\n",
      "        total_loss: 0.17603039741516113\n",
      "        vf_explained_var: -0.30550211668014526\n",
      "        vf_loss: 0.030814481899142265\n",
      "  num_agent_steps_sampled: 239704\n",
      "  num_agent_steps_trained: 239704\n",
      "  num_steps_sampled: 119852\n",
      "  num_steps_trained: 119852\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 279\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.99669421487604\n",
      "  ram_util_percent: 37.714049586776866\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5072499999999863\n",
      "  blue_1: 0.6748599999999894\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5750153308968747\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.05416821036508\n",
      "  mean_inference_ms: 2.801142434255137\n",
      "  mean_raw_obs_processing_ms: 25.098645466522218\n",
      "time_since_restore: 16365.59687948227\n",
      "time_this_iter_s: 82.2455644607544\n",
      "time_total_s: 16365.59687948227\n",
      "timers:\n",
      "  learn_throughput: 846.195\n",
      "  learn_time_ms: 567.363\n",
      "  load_throughput: 1194994.57\n",
      "  load_time_ms: 0.402\n",
      "  sample_throughput: 7.485\n",
      "  sample_time_ms: 64138.946\n",
      "timestamp: 1639000111\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 119852\n",
      "training_iteration: 279\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:279 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 204 0.005 1.0200000000000002\n",
      "blue_1 True True 204 -2.0 -0.9849999999999997\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 111 0.005 0.5550000000000004\n",
      "blue_1 True True 111 -2.0 -1.4499999999999997\n",
      "agent_timesteps_total: 240334\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-49-22\n",
      "done: false\n",
      "episode_len_mean: 315.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.1928099999999595\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 512\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.648139953613281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015820574015378952\n",
      "        model: {}\n",
      "        policy_loss: -0.3568713366985321\n",
      "        total_loss: -0.2811346650123596\n",
      "        vf_explained_var: 0.4028967022895813\n",
      "        vf_loss: 0.04687114059925079\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.170544147491455\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014691831544041634\n",
      "        model: {}\n",
      "        policy_loss: -0.3332415819168091\n",
      "        total_loss: 0.25105151534080505\n",
      "        vf_explained_var: 0.030730562284588814\n",
      "        vf_loss: 0.5480639934539795\n",
      "  num_agent_steps_sampled: 240334\n",
      "  num_agent_steps_trained: 240334\n",
      "  num_steps_sampled: 120167\n",
      "  num_steps_trained: 120167\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 280\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.541666666666664\n",
      "  ram_util_percent: 37.672222222222224\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5526999999999863\n",
      "  blue_1: 0.6401099999999893\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749830880079085\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 109.0068618262857\n",
      "  mean_inference_ms: 2.8009999377104733\n",
      "  mean_raw_obs_processing_ms: 25.055993005592715\n",
      "time_since_restore: 16411.763454675674\n",
      "time_this_iter_s: 46.16657519340515\n",
      "time_total_s: 16411.763454675674\n",
      "timers:\n",
      "  learn_throughput: 853.493\n",
      "  learn_time_ms: 548.218\n",
      "  load_throughput: 933064.633\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.28\n",
      "  sample_time_ms: 64270.944\n",
      "timestamp: 1639000162\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 120167\n",
      "training_iteration: 280\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:280 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 69 -2.0 -1.6599999999999997\n",
      "blue_1 True True 69 0.005 0.3450000000000002\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 135 -2.0 -1.3299999999999996\n",
      "blue_1 True True 135 0.005 0.6750000000000005\n",
      "LOSE\n",
      "blue_0 False True 375 -0.995 0.8749999999999821\n",
      "blue_1 False True 375 -0.993 1.0939999999999865\n",
      "agent_timesteps_total: 241492\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-50-50\n",
      "done: false\n",
      "episode_len_mean: 311.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.1595699999999605\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 515\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.9461283683776855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01846078224480152\n",
      "        model: {}\n",
      "        policy_loss: -0.12430548667907715\n",
      "        total_loss: 0.4228059649467468\n",
      "        vf_explained_var: -0.04149628430604935\n",
      "        vf_loss: 0.5134286880493164\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.113080978393555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006995724979788065\n",
      "        model: {}\n",
      "        policy_loss: -0.3206694424152374\n",
      "        total_loss: -0.18435941636562347\n",
      "        vf_explained_var: -0.11374975740909576\n",
      "        vf_loss: 0.11905904114246368\n",
      "  num_agent_steps_sampled: 241492\n",
      "  num_agent_steps_trained: 241492\n",
      "  num_steps_sampled: 120746\n",
      "  num_steps_trained: 120746\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 281\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.61157024793389\n",
      "  ram_util_percent: 37.62148760330579\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5151199999999866\n",
      "  blue_1: 0.6444499999999894\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749257624755506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.9375581117808\n",
      "  mean_inference_ms: 2.800805947975191\n",
      "  mean_raw_obs_processing_ms: 24.994823460551125\n",
      "time_since_restore: 16493.67337870598\n",
      "time_this_iter_s: 81.90992403030396\n",
      "time_total_s: 16493.67337870598\n",
      "timers:\n",
      "  learn_throughput: 848.245\n",
      "  learn_time_ms: 580.257\n",
      "  load_throughput: 981522.573\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.177\n",
      "  sample_time_ms: 68576.188\n",
      "timestamp: 1639000250\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 120746\n",
      "training_iteration: 281\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:280 starting ! -----------------\n",
      "agent_timesteps_total: 241492\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-50-50\n",
      "done: false\n",
      "episode_len_mean: 311.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.1595699999999605\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 515\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.9461283683776855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01846078224480152\n",
      "        model: {}\n",
      "        policy_loss: -0.12430548667907715\n",
      "        total_loss: 0.4228059649467468\n",
      "        vf_explained_var: -0.04149628430604935\n",
      "        vf_loss: 0.5134286880493164\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.113080978393555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006995724979788065\n",
      "        model: {}\n",
      "        policy_loss: -0.3206694424152374\n",
      "        total_loss: -0.18435941636562347\n",
      "        vf_explained_var: -0.11374975740909576\n",
      "        vf_loss: 0.11905904114246368\n",
      "  num_agent_steps_sampled: 241492\n",
      "  num_agent_steps_trained: 241492\n",
      "  num_steps_sampled: 120746\n",
      "  num_steps_trained: 120746\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 281\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.61157024793389\n",
      "  ram_util_percent: 37.62148760330579\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5151199999999866\n",
      "  blue_1: 0.6444499999999894\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749257624755506\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.9375581117808\n",
      "  mean_inference_ms: 2.800805947975191\n",
      "  mean_raw_obs_processing_ms: 24.994823460551125\n",
      "time_since_restore: 16493.67337870598\n",
      "time_this_iter_s: 81.90992403030396\n",
      "time_total_s: 16493.67337870598\n",
      "timers:\n",
      "  learn_throughput: 848.245\n",
      "  learn_time_ms: 580.257\n",
      "  load_throughput: 981522.573\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.177\n",
      "  sample_time_ms: 68576.188\n",
      "timestamp: 1639000250\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 120746\n",
      "training_iteration: 281\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:281 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.994 2.074999999999963\n",
      "blue_1 False False 600 -0.995 1.9999999999999583\n",
      "agent_timesteps_total: 242692\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-51-46\n",
      "done: false\n",
      "episode_len_mean: 316.8\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.2096699999999594\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 516\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.8902008533477783\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013190020807087421\n",
      "        model: {}\n",
      "        policy_loss: -0.3686811327934265\n",
      "        total_loss: -0.2946804165840149\n",
      "        vf_explained_var: -0.051095087081193924\n",
      "        vf_loss: 0.04993476718664169\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.356216907501221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00835605151951313\n",
      "        model: {}\n",
      "        policy_loss: -0.3823873698711395\n",
      "        total_loss: -0.3423057794570923\n",
      "        vf_explained_var: -0.11679451167583466\n",
      "        vf_loss: 0.01947612315416336\n",
      "  num_agent_steps_sampled: 242692\n",
      "  num_agent_steps_trained: 242692\n",
      "  num_steps_sampled: 121346\n",
      "  num_steps_trained: 121346\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 282\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.04675324675324\n",
      "  ram_util_percent: 37.672727272727265\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5505699999999861\n",
      "  blue_1: 0.6590999999999891\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5749056385673808\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.91295563629815\n",
      "  mean_inference_ms: 2.8007410839014857\n",
      "  mean_raw_obs_processing_ms: 24.973555711501504\n",
      "time_since_restore: 16543.88909482956\n",
      "time_this_iter_s: 50.21571612358093\n",
      "time_total_s: 16543.88909482956\n",
      "timers:\n",
      "  learn_throughput: 842.936\n",
      "  learn_time_ms: 596.012\n",
      "  load_throughput: 1001862.944\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.417\n",
      "  sample_time_ms: 67736.306\n",
      "timestamp: 1639000306\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 121346\n",
      "training_iteration: 282\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:282 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 359 -2.0 -0.21000000000001617\n",
      "blue_1 True True 359 0.004 1.7639999999999914\n",
      "agent_timesteps_total: 243410\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-52-37\n",
      "done: false\n",
      "episode_len_mean: 318.61\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.2274599999999591\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 517\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.734566688537598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009558110497891903\n",
      "        model: {}\n",
      "        policy_loss: -0.5210269689559937\n",
      "        total_loss: -0.4837454557418823\n",
      "        vf_explained_var: 0.1396021693944931\n",
      "        vf_loss: 0.019842276349663734\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.886641502380371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007053140085190535\n",
      "        model: {}\n",
      "        policy_loss: 0.40628403425216675\n",
      "        total_loss: 0.4412803053855896\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.017603624612092972\n",
      "  num_agent_steps_sampled: 243410\n",
      "  num_agent_steps_trained: 243410\n",
      "  num_steps_sampled: 121705\n",
      "  num_steps_trained: 121705\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 283\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.99577464788732\n",
      "  ram_util_percent: 37.684507042253514\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.539569999999986\n",
      "  blue_1: 0.6878899999999891\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748861657648403\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.88870255376266\n",
      "  mean_inference_ms: 2.8006751399855006\n",
      "  mean_raw_obs_processing_ms: 24.952525655139638\n",
      "time_since_restore: 16590.057809591293\n",
      "time_this_iter_s: 46.16871476173401\n",
      "time_total_s: 16590.057809591293\n",
      "timers:\n",
      "  learn_throughput: 853.914\n",
      "  learn_time_ms: 563.757\n",
      "  load_throughput: 959985.711\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.395\n",
      "  sample_time_ms: 65094.303\n",
      "timestamp: 1639000357\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 121705\n",
      "training_iteration: 283\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:283 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 155 0.005 0.7750000000000006\n",
      "blue_1 True True 155 -2.0 -1.2299999999999995\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 175 0.005 0.8750000000000007\n",
      "blue_1 True True 175 -2.0 -1.1299999999999994\n",
      "agent_timesteps_total: 244070\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-53-29\n",
      "done: false\n",
      "episode_len_mean: 313.51\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.17236999999996\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 519\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.734434604644775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008168928325176239\n",
      "        model: {}\n",
      "        policy_loss: 0.31565505266189575\n",
      "        total_loss: 0.3314588963985443\n",
      "        vf_explained_var: 0.9290408492088318\n",
      "        vf_loss: 0.0008992197108455002\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.992743492126465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009895765222609043\n",
      "        model: {}\n",
      "        policy_loss: -0.35459959506988525\n",
      "        total_loss: 0.19651006162166595\n",
      "        vf_explained_var: 0.12363883852958679\n",
      "        vf_loss: 0.5267073512077332\n",
      "  num_agent_steps_sampled: 244070\n",
      "  num_agent_steps_trained: 244070\n",
      "  num_steps_sampled: 122035\n",
      "  num_steps_trained: 122035\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 284\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.06805555555556\n",
      "  ram_util_percent: 37.68333333333333\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5320699999999864\n",
      "  blue_1: 0.6402999999999893\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748502443709114\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.83989038988842\n",
      "  mean_inference_ms: 2.8005494227588303\n",
      "  mean_raw_obs_processing_ms: 24.911573671231505\n",
      "time_since_restore: 16636.526801347733\n",
      "time_this_iter_s: 46.46899175643921\n",
      "time_total_s: 16636.526801347733\n",
      "timers:\n",
      "  learn_throughput: 858.589\n",
      "  learn_time_ms: 529.241\n",
      "  load_throughput: 906143.554\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.203\n",
      "  sample_time_ms: 63085.081\n",
      "timestamp: 1639000409\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 122035\n",
      "training_iteration: 284\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:284 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 495 -0.995 1.4749999999999694\n",
      "blue_1 False True 495 -0.993 1.9089999999999994\n",
      "agent_timesteps_total: 245060\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-54-30\n",
      "done: false\n",
      "episode_len_mean: 316.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.2041599999999593\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 520\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.87873363494873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0065480247139930725\n",
      "        model: {}\n",
      "        policy_loss: -0.4685717821121216\n",
      "        total_loss: -0.4293575584888458\n",
      "        vf_explained_var: -0.9496122598648071\n",
      "        vf_loss: 0.027266910299658775\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.674317359924316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006902946159243584\n",
      "        model: {}\n",
      "        policy_loss: -0.4962952136993408\n",
      "        total_loss: -0.28352800011634827\n",
      "        vf_explained_var: -0.5370245575904846\n",
      "        vf_loss: 0.195745050907135\n",
      "  num_agent_steps_sampled: 245060\n",
      "  num_agent_steps_trained: 245060\n",
      "  num_steps_sampled: 122530\n",
      "  num_steps_trained: 122530\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 285\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.98333333333333\n",
      "  ram_util_percent: 37.73809523809525\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5357699999999862\n",
      "  blue_1: 0.6683899999999894\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5748321409557109\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.81566253143345\n",
      "  mean_inference_ms: 2.800491041472962\n",
      "  mean_raw_obs_processing_ms: 24.891222731633093\n",
      "time_since_restore: 16691.687644720078\n",
      "time_this_iter_s: 55.16084337234497\n",
      "time_total_s: 16691.687644720078\n",
      "timers:\n",
      "  learn_throughput: 872.705\n",
      "  learn_time_ms: 528.243\n",
      "  load_throughput: 919348.68\n",
      "  load_time_ms: 0.501\n",
      "  sample_throughput: 7.283\n",
      "  sample_time_ms: 63301.062\n",
      "timestamp: 1639000470\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 122530\n",
      "training_iteration: 285\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:285 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 101 0.005 0.5050000000000003\n",
      "blue_1 True True 101 -2.0 -1.4999999999999996\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 122 0.005 0.6100000000000004\n",
      "blue_1 True True 122 -2.0 -1.3949999999999996\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.028999999999959\n",
      "agent_timesteps_total: 246706\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-56-18\n",
      "done: false\n",
      "episode_len_mean: 314.26\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.146999999999943\n",
      "episode_reward_mean: 1.1843199999999596\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 523\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.238444805145264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012280894443392754\n",
      "        model: {}\n",
      "        policy_loss: -0.24897319078445435\n",
      "        total_loss: -0.20816633105278015\n",
      "        vf_explained_var: -0.30471330881118774\n",
      "        vf_loss: 0.01839962974190712\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.289395332336426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014517826959490776\n",
      "        model: {}\n",
      "        policy_loss: -0.12233170121908188\n",
      "        total_loss: 0.3281383514404297\n",
      "        vf_explained_var: -0.340131938457489\n",
      "        vf_loss: 0.41467005014419556\n",
      "  num_agent_steps_sampled: 246706\n",
      "  num_agent_steps_trained: 246706\n",
      "  num_steps_sampled: 123353\n",
      "  num_steps_trained: 123353\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 286\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.225503355704696\n",
      "  ram_util_percent: 37.78724832214766\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5559199999999861\n",
      "  blue_1: 0.6283999999999893\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5747736778994519\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.74206901651952\n",
      "  mean_inference_ms: 2.8003252186261074\n",
      "  mean_raw_obs_processing_ms: 24.83051654802154\n",
      "time_since_restore: 16794.177162885666\n",
      "time_this_iter_s: 102.48951816558838\n",
      "time_total_s: 16794.177162885666\n",
      "timers:\n",
      "  learn_throughput: 877.678\n",
      "  learn_time_ms: 573.331\n",
      "  load_throughput: 1253235.421\n",
      "  load_time_ms: 0.402\n",
      "  sample_throughput: 7.429\n",
      "  sample_time_ms: 67736.523\n",
      "timestamp: 1639000578\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 123353\n",
      "training_iteration: 286\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:286 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.1759999999999597\n",
      "agent_timesteps_total: 247906\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-57-28\n",
      "done: false\n",
      "episode_len_mean: 316.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2084599999999588\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 524\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.275984764099121\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005047151818871498\n",
      "        model: {}\n",
      "        policy_loss: -0.37502172589302063\n",
      "        total_loss: -0.347885400056839\n",
      "        vf_explained_var: -0.6666148900985718\n",
      "        vf_loss: 0.017927521839737892\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.314681053161621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00513265747576952\n",
      "        model: {}\n",
      "        policy_loss: -0.36047235131263733\n",
      "        total_loss: -0.28655242919921875\n",
      "        vf_explained_var: -0.6910431981086731\n",
      "        vf_loss: 0.06126316264271736\n",
      "  num_agent_steps_sampled: 247906\n",
      "  num_agent_steps_trained: 247906\n",
      "  num_steps_sampled: 123953\n",
      "  num_steps_trained: 123953\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 287\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.65625\n",
      "  ram_util_percent: 37.79583333333334\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.568319999999986\n",
      "  blue_1: 0.640139999999989\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5747536877166608\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.71743035384272\n",
      "  mean_inference_ms: 2.8002754870685975\n",
      "  mean_raw_obs_processing_ms: 24.810333851001218\n",
      "time_since_restore: 16858.357029676437\n",
      "time_this_iter_s: 64.17986679077148\n",
      "time_total_s: 16858.357029676437\n",
      "timers:\n",
      "  learn_throughput: 874.158\n",
      "  learn_time_ms: 587.994\n",
      "  load_throughput: 1702631.698\n",
      "  load_time_ms: 0.302\n",
      "  sample_throughput: 7.531\n",
      "  sample_time_ms: 68249.977\n",
      "timestamp: 1639000648\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 123953\n",
      "training_iteration: 287\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:287 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 159 0.005 0.7950000000000006\n",
      "blue_1 True True 159 -2.0 -1.2099999999999995\n",
      "LOSE\n",
      "blue_0 False True 345 -0.995 0.7249999999999853\n",
      "blue_1 False True 345 -0.993 0.9769999999999869\n",
      "agent_timesteps_total: 248914\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-58-33\n",
      "done: false\n",
      "episode_len_mean: 317.04\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2150199999999591\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 526\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.305724620819092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010680785402655602\n",
      "        model: {}\n",
      "        policy_loss: -0.4170686602592468\n",
      "        total_loss: -0.37242719531059265\n",
      "        vf_explained_var: -0.09346547722816467\n",
      "        vf_loss: 0.025153813883662224\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.987786769866943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016361093148589134\n",
      "        model: {}\n",
      "        policy_loss: -0.07603824883699417\n",
      "        total_loss: 0.34710922837257385\n",
      "        vf_explained_var: -0.0610329695045948\n",
      "        vf_loss: 0.3828020691871643\n",
      "  num_agent_steps_sampled: 248914\n",
      "  num_agent_steps_trained: 248914\n",
      "  num_steps_sampled: 124457\n",
      "  num_steps_trained: 124457\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 288\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.674157303370784\n",
      "  ram_util_percent: 37.78764044943821\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5798699999999859\n",
      "  blue_1: 0.6351499999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5747116128296862\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.66721428431342\n",
      "  mean_inference_ms: 2.80018402076896\n",
      "  mean_raw_obs_processing_ms: 24.77006723472537\n",
      "time_since_restore: 16917.556344509125\n",
      "time_this_iter_s: 59.19931483268738\n",
      "time_total_s: 16917.556344509125\n",
      "timers:\n",
      "  learn_throughput: 891.298\n",
      "  learn_time_ms: 587.795\n",
      "  load_throughput: 1733919.25\n",
      "  load_time_ms: 0.302\n",
      "  sample_throughput: 7.596\n",
      "  sample_time_ms: 68974.822\n",
      "timestamp: 1639000713\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 124457\n",
      "training_iteration: 288\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:288 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 319 -2.0 -0.4100000000000119\n",
      "blue_1 True True 319 0.007 1.7279999999999895\n",
      "agent_timesteps_total: 249552\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_06-59-19\n",
      "done: false\n",
      "episode_len_mean: 317.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2199399999999592\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 527\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.129362106323242\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007344953715801239\n",
      "        model: {}\n",
      "        policy_loss: -0.458387553691864\n",
      "        total_loss: -0.3455914258956909\n",
      "        vf_explained_var: 0.20285609364509583\n",
      "        vf_loss: 0.09939486533403397\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.959296226501465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007901234552264214\n",
      "        model: {}\n",
      "        policy_loss: -0.2506404519081116\n",
      "        total_loss: -0.15216627717018127\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.07899026572704315\n",
      "  num_agent_steps_sampled: 249552\n",
      "  num_agent_steps_trained: 249552\n",
      "  num_steps_sampled: 124776\n",
      "  num_steps_trained: 124776\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 289\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.3984375\n",
      "  ram_util_percent: 37.787499999999994\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5815199999999858\n",
      "  blue_1: 0.6384199999999887\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.57469062699676\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.6419084305077\n",
      "  mean_inference_ms: 2.8001410236119195\n",
      "  mean_raw_obs_processing_ms: 24.749965076259578\n",
      "time_since_restore: 16958.441854715347\n",
      "time_this_iter_s: 40.885510206222534\n",
      "time_total_s: 16958.441854715347\n",
      "timers:\n",
      "  learn_throughput: 886.602\n",
      "  learn_time_ms: 555.379\n",
      "  load_throughput: 1225173.69\n",
      "  load_time_ms: 0.402\n",
      "  sample_throughput: 7.591\n",
      "  sample_time_ms: 64868.348\n",
      "timestamp: 1639000759\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 124776\n",
      "training_iteration: 289\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:289 start! -----------------\n",
      "LOSE\n",
      "blue_0 False True 500 -0.995 1.499999999999969\n",
      "blue_1 False True 500 -0.996 1.4109999999999818\n",
      "agent_timesteps_total: 250552\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-00-16\n",
      "done: false\n",
      "episode_len_mean: 321.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.256199999999958\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 528\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.824557900428772\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.831727981567383\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021150363609194756\n",
      "        model: {}\n",
      "        policy_loss: -0.46083107590675354\n",
      "        total_loss: -0.4012775123119354\n",
      "        vf_explained_var: -0.22599908709526062\n",
      "        vf_loss: 0.0209635142236948\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4659345149993896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.902740001678467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004971505608409643\n",
      "        model: {}\n",
      "        policy_loss: -0.48652732372283936\n",
      "        total_loss: -0.4412330687046051\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.03303491696715355\n",
      "  num_agent_steps_sampled: 250552\n",
      "  num_agent_steps_trained: 250552\n",
      "  num_steps_sampled: 125276\n",
      "  num_steps_trained: 125276\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 290\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.55875000000001\n",
      "  ram_util_percent: 37.77875\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5900699999999857\n",
      "  blue_1: 0.6661299999999887\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5746703143458077\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.61564836275707\n",
      "  mean_inference_ms: 2.8001180209815804\n",
      "  mean_raw_obs_processing_ms: 24.730160197001116\n",
      "time_since_restore: 17010.296286582947\n",
      "time_this_iter_s: 51.85443186759949\n",
      "time_total_s: 17010.296286582947\n",
      "timers:\n",
      "  learn_throughput: 883.582\n",
      "  learn_time_ms: 578.215\n",
      "  load_throughput: 1690627.151\n",
      "  load_time_ms: 0.302\n",
      "  sample_throughput: 7.814\n",
      "  sample_time_ms: 65385.186\n",
      "timestamp: 1639000816\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 125276\n",
      "training_iteration: 290\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:290 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 270 -2.0 -0.6550000000000067\n",
      "blue_1 True True 270 0.005 1.3499999999999932\n",
      "agent_timesteps_total: 251092\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-00-53\n",
      "done: false\n",
      "episode_len_mean: 317.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2217999999999587\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 529\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.53518533706665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009463573805987835\n",
      "        model: {}\n",
      "        policy_loss: -0.1741533726453781\n",
      "        total_loss: 0.2161707729101181\n",
      "        vf_explained_var: 0.22093729674816132\n",
      "        vf_loss: 0.36442387104034424\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2329672574996948\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.594637870788574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015405146405100822\n",
      "        model: {}\n",
      "        policy_loss: -0.16085702180862427\n",
      "        total_loss: -0.11300725489854813\n",
      "        vf_explained_var: -0.20648357272148132\n",
      "        vf_loss: 0.028855739161372185\n",
      "  num_agent_steps_sampled: 251092\n",
      "  num_agent_steps_trained: 251092\n",
      "  num_steps_sampled: 125546\n",
      "  num_steps_trained: 125546\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 291\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.922\n",
      "  ram_util_percent: 37.668\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5635199999999859\n",
      "  blue_1: 0.6582799999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5746497449180539\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.58896486280952\n",
      "  mean_inference_ms: 2.800093982773271\n",
      "  mean_raw_obs_processing_ms: 24.71029400200871\n",
      "time_since_restore: 17040.897435188293\n",
      "time_this_iter_s: 30.60114860534668\n",
      "time_total_s: 17040.897435188293\n",
      "timers:\n",
      "  learn_throughput: 877.074\n",
      "  learn_time_ms: 547.274\n",
      "  load_throughput: 1200731.15\n",
      "  load_time_ms: 0.4\n",
      "  sample_throughput: 7.959\n",
      "  sample_time_ms: 60306.499\n",
      "timestamp: 1639000853\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 125546\n",
      "training_iteration: 291\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:290 starting ! -----------------\n",
      "agent_timesteps_total: 251092\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-00-53\n",
      "done: false\n",
      "episode_len_mean: 317.78\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2217999999999587\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 529\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.53518533706665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009463573805987835\n",
      "        model: {}\n",
      "        policy_loss: -0.1741533726453781\n",
      "        total_loss: 0.2161707729101181\n",
      "        vf_explained_var: 0.22093729674816132\n",
      "        vf_loss: 0.36442387104034424\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2329672574996948\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.594637870788574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015405146405100822\n",
      "        model: {}\n",
      "        policy_loss: -0.16085702180862427\n",
      "        total_loss: -0.11300725489854813\n",
      "        vf_explained_var: -0.20648357272148132\n",
      "        vf_loss: 0.028855739161372185\n",
      "  num_agent_steps_sampled: 251092\n",
      "  num_agent_steps_trained: 251092\n",
      "  num_steps_sampled: 125546\n",
      "  num_steps_trained: 125546\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 291\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.922\n",
      "  ram_util_percent: 37.668\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5635199999999859\n",
      "  blue_1: 0.6582799999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5746497449180539\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.58896486280952\n",
      "  mean_inference_ms: 2.800093982773271\n",
      "  mean_raw_obs_processing_ms: 24.71029400200871\n",
      "time_since_restore: 17040.897435188293\n",
      "time_this_iter_s: 30.60114860534668\n",
      "time_total_s: 17040.897435188293\n",
      "timers:\n",
      "  learn_throughput: 877.074\n",
      "  learn_time_ms: 547.274\n",
      "  load_throughput: 1200731.15\n",
      "  load_time_ms: 0.4\n",
      "  sample_throughput: 7.959\n",
      "  sample_time_ms: 60306.499\n",
      "timestamp: 1639000853\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 125546\n",
      "training_iteration: 291\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:291 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 98 0.005 0.4900000000000003\n",
      "blue_1 True True 98 -2.0 -1.5149999999999997\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 234 -2.0 -0.8350000000000029\n",
      "blue_1 True True 234 0.005 1.169999999999997\n",
      "agent_timesteps_total: 251756\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-01-42\n",
      "done: false\n",
      "episode_len_mean: 314.17\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.183949999999959\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 531\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.0000715255737305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009500646032392979\n",
      "        model: {}\n",
      "        policy_loss: -0.49622586369514465\n",
      "        total_loss: -0.38682839274406433\n",
      "        vf_explained_var: 0.020460238680243492\n",
      "        vf_loss: 0.08339575678110123\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2329672574996948\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.170572280883789\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028775909915566444\n",
      "        model: {}\n",
      "        policy_loss: -0.005643160082399845\n",
      "        total_loss: 0.428298681974411\n",
      "        vf_explained_var: -0.2755318284034729\n",
      "        vf_loss: 0.3984621465206146\n",
      "  num_agent_steps_sampled: 251756\n",
      "  num_agent_steps_trained: 251756\n",
      "  num_steps_sampled: 125878\n",
      "  num_steps_trained: 125878\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 292\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.076811594202894\n",
      "  ram_util_percent: 37.657971014492745\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5454199999999861\n",
      "  blue_1: 0.6385299999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5746108295903442\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.53526134482743\n",
      "  mean_inference_ms: 2.800054025397375\n",
      "  mean_raw_obs_processing_ms: 24.671962474776937\n",
      "time_since_restore: 17085.144426107407\n",
      "time_this_iter_s: 44.24699091911316\n",
      "time_total_s: 17085.144426107407\n",
      "timers:\n",
      "  learn_throughput: 880.404\n",
      "  learn_time_ms: 514.764\n",
      "  load_throughput: 911420.489\n",
      "  load_time_ms: 0.497\n",
      "  sample_throughput: 7.589\n",
      "  sample_time_ms: 59718.294\n",
      "timestamp: 1639000902\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 125878\n",
      "training_iteration: 292\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:292 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 443 0.005 2.214999999999975\n",
      "blue_1 True True 443 -2.0 0.22199999999997688\n",
      "agent_timesteps_total: 252642\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-02-31\n",
      "done: false\n",
      "episode_len_mean: 314.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.187809999999959\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 532\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.298635482788086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01275633554905653\n",
      "        model: {}\n",
      "        policy_loss: -0.2760840058326721\n",
      "        total_loss: -0.21293483674526215\n",
      "        vf_explained_var: -0.41144001483917236\n",
      "        vf_loss: 0.02823721617460251\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.837757110595703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006478714291006327\n",
      "        model: {}\n",
      "        policy_loss: -0.3527871370315552\n",
      "        total_loss: -0.2816811203956604\n",
      "        vf_explained_var: -0.5155162215232849\n",
      "        vf_loss: 0.0591239258646965\n",
      "  num_agent_steps_sampled: 252642\n",
      "  num_agent_steps_trained: 252642\n",
      "  num_steps_sampled: 126321\n",
      "  num_steps_trained: 126321\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 293\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.22537313432835\n",
      "  ram_util_percent: 37.685074626865664\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5674699999999862\n",
      "  blue_1: 0.6203399999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5745941239225841\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.50856088173725\n",
      "  mean_inference_ms: 2.800035832130087\n",
      "  mean_raw_obs_processing_ms: 24.653154210480032\n",
      "time_since_restore: 17128.341287374496\n",
      "time_this_iter_s: 43.196861267089844\n",
      "time_total_s: 17128.341287374496\n",
      "timers:\n",
      "  learn_throughput: 866.436\n",
      "  learn_time_ms: 532.757\n",
      "  load_throughput: 928313.544\n",
      "  load_time_ms: 0.497\n",
      "  sample_throughput: 7.775\n",
      "  sample_time_ms: 59370.637\n",
      "timestamp: 1639000951\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 126321\n",
      "training_iteration: 293\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:293 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 264 -2.0 -0.685000000000006\n",
      "blue_1 True True 264 0.005 1.3039999999999956\n",
      "agent_timesteps_total: 253170\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-03-08\n",
      "done: false\n",
      "episode_len_mean: 315.3\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1949499999999589\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 533\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.414913177490234\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011186237446963787\n",
      "        model: {}\n",
      "        policy_loss: -0.12773574888706207\n",
      "        total_loss: 0.1660051941871643\n",
      "        vf_explained_var: 0.355170875787735\n",
      "        vf_loss: 0.263126015663147\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.580180168151855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013641802594065666\n",
      "        model: {}\n",
      "        policy_loss: -0.11375417560338974\n",
      "        total_loss: -0.03817969933152199\n",
      "        vf_explained_var: -0.6308044791221619\n",
      "        vf_loss: 0.05034463852643967\n",
      "  num_agent_steps_sampled: 253170\n",
      "  num_agent_steps_trained: 253170\n",
      "  num_steps_sampled: 126585\n",
      "  num_steps_trained: 126585\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 294\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.130769230769246\n",
      "  ram_util_percent: 37.67884615384616\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5711199999999861\n",
      "  blue_1: 0.6238299999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5745762283517455\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.48228431765757\n",
      "  mean_inference_ms: 2.8000187461311508\n",
      "  mean_raw_obs_processing_ms: 24.634271920017845\n",
      "time_since_restore: 17159.73624229431\n",
      "time_this_iter_s: 31.394954919815063\n",
      "time_total_s: 17159.73624229431\n",
      "timers:\n",
      "  learn_throughput: 855.48\n",
      "  learn_time_ms: 531.865\n",
      "  load_throughput: 761961.319\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 7.861\n",
      "  sample_time_ms: 57882.774\n",
      "timestamp: 1639000988\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 126585\n",
      "training_iteration: 294\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:294 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 278 0.005 1.3899999999999924\n",
      "blue_1 True True 278 -2.0 -0.6150000000000075\n",
      "agent_timesteps_total: 253726\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-03-50\n",
      "done: false\n",
      "episode_len_mean: 315.67\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1986099999999589\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 534\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.239967346191406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007741844281554222\n",
      "        model: {}\n",
      "        policy_loss: 0.09855977445840836\n",
      "        total_loss: 0.1703041046857834\n",
      "        vf_explained_var: -0.9881367087364197\n",
      "        vf_loss: 0.05055614933371544\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.6501665115356445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011323562823235989\n",
      "        model: {}\n",
      "        policy_loss: -0.26380154490470886\n",
      "        total_loss: -0.05426058545708656\n",
      "        vf_explained_var: -0.1607864648103714\n",
      "        vf_loss: 0.1885986030101776\n",
      "  num_agent_steps_sampled: 253726\n",
      "  num_agent_steps_trained: 253726\n",
      "  num_steps_sampled: 126863\n",
      "  num_steps_trained: 126863\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 295\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.05689655172414\n",
      "  ram_util_percent: 37.68275862068965\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5930199999999861\n",
      "  blue_1: 0.6055899999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.574557630368882\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.4560103857097\n",
      "  mean_inference_ms: 2.8000000446446642\n",
      "  mean_raw_obs_processing_ms: 24.61531491388104\n",
      "time_since_restore: 17196.500766277313\n",
      "time_this_iter_s: 36.76452398300171\n",
      "time_total_s: 17196.500766277313\n",
      "timers:\n",
      "  learn_throughput: 841.021\n",
      "  learn_time_ms: 515.207\n",
      "  load_throughput: 721989.482\n",
      "  load_time_ms: 0.6\n",
      "  sample_throughput: 7.729\n",
      "  sample_time_ms: 56060.038\n",
      "timestamp: 1639001030\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 126863\n",
      "training_iteration: 295\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:295 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 158 -2.0 -1.2149999999999994\n",
      "blue_1 True True 158 0.005 0.7900000000000006\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 203 -2.0 -0.9899999999999995\n",
      "blue_1 True True 203 0.005 1.0150000000000003\n",
      "agent_timesteps_total: 254448\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-04-48\n",
      "done: false\n",
      "episode_len_mean: 313.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.170409999999959\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 536\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.609232425689697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012094969861209393\n",
      "        model: {}\n",
      "        policy_loss: -0.33883148431777954\n",
      "        total_loss: 0.14000444114208221\n",
      "        vf_explained_var: 0.3173360526561737\n",
      "        vf_loss: 0.4457339942455292\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.039743423461914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01317854318767786\n",
      "        model: {}\n",
      "        policy_loss: 0.36702024936676025\n",
      "        total_loss: 0.41366201639175415\n",
      "        vf_explained_var: 0.4306449592113495\n",
      "        vf_loss: 0.022268762812018394\n",
      "  num_agent_steps_sampled: 254448\n",
      "  num_agent_steps_trained: 254448\n",
      "  num_steps_sampled: 127224\n",
      "  num_steps_trained: 127224\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 296\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 47.5\n",
      "  ram_util_percent: 37.735\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5702699999999862\n",
      "  blue_1: 0.6001399999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5745265152525447\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.40394462861141\n",
      "  mean_inference_ms: 2.799961894398729\n",
      "  mean_raw_obs_processing_ms: 24.578737872861492\n",
      "time_since_restore: 17248.196128368378\n",
      "time_this_iter_s: 51.69536209106445\n",
      "time_total_s: 17248.196128368378\n",
      "timers:\n",
      "  learn_throughput: 852.329\n",
      "  learn_time_ms: 454.167\n",
      "  load_throughput: 553153.134\n",
      "  load_time_ms: 0.7\n",
      "  sample_throughput: 7.586\n",
      "  sample_time_ms: 51030.899\n",
      "timestamp: 1639001088\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 127224\n",
      "training_iteration: 296\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:296 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 1.99699999999996\n",
      "agent_timesteps_total: 255648\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-05-54\n",
      "done: false\n",
      "episode_len_mean: 313.38\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.173019999999959\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 537\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.021697998046875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0052619860507547855\n",
      "        model: {}\n",
      "        policy_loss: -0.37816277146339417\n",
      "        total_loss: -0.21072295308113098\n",
      "        vf_explained_var: -0.7673917412757874\n",
      "        vf_loss: 0.15303859114646912\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.2850661277771\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018444376066327095\n",
      "        model: {}\n",
      "        policy_loss: -0.30199819803237915\n",
      "        total_loss: -0.23932787775993347\n",
      "        vf_explained_var: -0.7566909790039062\n",
      "        vf_loss: 0.02855837531387806\n",
      "  num_agent_steps_sampled: 255648\n",
      "  num_agent_steps_trained: 255648\n",
      "  num_steps_sampled: 127824\n",
      "  num_steps_trained: 127824\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 297\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.305555555555564\n",
      "  ram_util_percent: 37.73444444444444\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5615199999999863\n",
      "  blue_1: 0.6114999999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5745134547440585\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.37731695131248\n",
      "  mean_inference_ms: 2.7999490046079614\n",
      "  mean_raw_obs_processing_ms: 24.560710642579906\n",
      "time_since_restore: 17308.328607082367\n",
      "time_this_iter_s: 60.13247871398926\n",
      "time_total_s: 17308.328607082367\n",
      "timers:\n",
      "  learn_throughput: 851.207\n",
      "  learn_time_ms: 454.766\n",
      "  load_throughput: 553153.134\n",
      "  load_time_ms: 0.7\n",
      "  sample_throughput: 7.655\n",
      "  sample_time_ms: 50570.929\n",
      "timestamp: 1639001154\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 127824\n",
      "training_iteration: 297\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:297 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 113 -2.0 -1.4399999999999995\n",
      "blue_1 True True 113 0.005 0.5650000000000004\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 166 -2.0 -1.1749999999999994\n",
      "blue_1 True True 166 0.005 0.8300000000000006\n",
      "agent_timesteps_total: 256206\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-06-45\n",
      "done: false\n",
      "episode_len_mean: 310.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1365199999999596\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 539\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.25570297241211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010559632442891598\n",
      "        model: {}\n",
      "        policy_loss: -0.207437664270401\n",
      "        total_loss: 0.26816511154174805\n",
      "        vf_explained_var: 0.17391043901443481\n",
      "        vf_loss: 0.44670265913009644\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.281399726867676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009732498787343502\n",
      "        model: {}\n",
      "        policy_loss: -0.13417145609855652\n",
      "        total_loss: -0.09372314065694809\n",
      "        vf_explained_var: -0.26077789068222046\n",
      "        vf_loss: 0.02244853600859642\n",
      "  num_agent_steps_sampled: 256206\n",
      "  num_agent_steps_trained: 256206\n",
      "  num_steps_sampled: 128103\n",
      "  num_steps_trained: 128103\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 298\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 43.32777777777778\n",
      "  ram_util_percent: 37.75972222222222\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5346699999999864\n",
      "  blue_1: 0.6018499999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5744857225902057\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.32553638628657\n",
      "  mean_inference_ms: 2.7999339543723227\n",
      "  mean_raw_obs_processing_ms: 24.52625864837917\n",
      "time_since_restore: 17354.16346359253\n",
      "time_this_iter_s: 45.83485651016235\n",
      "time_total_s: 17354.16346359253\n",
      "timers:\n",
      "  learn_throughput: 833.343\n",
      "  learn_time_ms: 437.515\n",
      "  load_throughput: 610646.983\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 7.403\n",
      "  sample_time_ms: 49253.271\n",
      "timestamp: 1639001205\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 128103\n",
      "training_iteration: 298\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:298 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 404 0.005 2.019999999999979\n",
      "blue_1 True True 404 -2.0 0.03799999999997983\n",
      "agent_timesteps_total: 257014\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-07-36\n",
      "done: false\n",
      "episode_len_mean: 310.2\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1357599999999592\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 540\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.418153285980225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008225502446293831\n",
      "        model: {}\n",
      "        policy_loss: -0.15666157007217407\n",
      "        total_loss: 0.01580606773495674\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.14995576441287994\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.978312015533447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011015130206942558\n",
      "        model: {}\n",
      "        policy_loss: -0.20084315538406372\n",
      "        total_loss: 0.005036421585828066\n",
      "        vf_explained_var: -0.01888864114880562\n",
      "        vf_loss: 0.185507670044899\n",
      "  num_agent_steps_sampled: 257014\n",
      "  num_agent_steps_trained: 257014\n",
      "  num_steps_sampled: 128507\n",
      "  num_steps_trained: 128507\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 299\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 55.64714285714287\n",
      "  ram_util_percent: 37.77857142857143\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5455699999999866\n",
      "  blue_1: 0.5901899999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5744721987709493\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.29967735473663\n",
      "  mean_inference_ms: 2.799922932519987\n",
      "  mean_raw_obs_processing_ms: 24.5091515183467\n",
      "time_since_restore: 17399.40299487114\n",
      "time_this_iter_s: 45.23953127861023\n",
      "time_total_s: 17399.40299487114\n",
      "timers:\n",
      "  learn_throughput: 820.979\n",
      "  learn_time_ms: 454.458\n",
      "  load_throughput: 750225.237\n",
      "  load_time_ms: 0.497\n",
      "  sample_throughput: 7.514\n",
      "  sample_time_ms: 49654.96\n",
      "timestamp: 1639001256\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 128507\n",
      "training_iteration: 299\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:299 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.0349999999999615\n",
      "agent_timesteps_total: 258214\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-08-49\n",
      "done: false\n",
      "episode_len_mean: 311.13\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1453099999999592\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 541\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7368369102478027\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.7867431640625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0029009999707341194\n",
      "        model: {}\n",
      "        policy_loss: -0.38154640793800354\n",
      "        total_loss: -0.3630065619945526\n",
      "        vf_explained_var: -0.408381849527359\n",
      "        vf_loss: 0.010600285604596138\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 9.055736541748047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007500631734728813\n",
      "        model: {}\n",
      "        policy_loss: -0.3437028229236603\n",
      "        total_loss: -0.2725982964038849\n",
      "        vf_explained_var: -0.7650853395462036\n",
      "        vf_loss: 0.05723251402378082\n",
      "  num_agent_steps_sampled: 258214\n",
      "  num_agent_steps_trained: 258214\n",
      "  num_steps_sampled: 129107\n",
      "  num_steps_trained: 129107\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 300\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.285148514851485\n",
      "  ram_util_percent: 37.80693069306932\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5602699999999864\n",
      "  blue_1: 0.5850399999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5744566287743006\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.27441206494048\n",
      "  mean_inference_ms: 2.7999117930286332\n",
      "  mean_raw_obs_processing_ms: 24.492120201616284\n",
      "time_since_restore: 17467.054057121277\n",
      "time_this_iter_s: 67.65106225013733\n",
      "time_total_s: 17467.054057121277\n",
      "timers:\n",
      "  learn_throughput: 824.345\n",
      "  learn_time_ms: 464.733\n",
      "  load_throughput: 641657.161\n",
      "  load_time_ms: 0.597\n",
      "  sample_throughput: 7.477\n",
      "  sample_time_ms: 51235.54\n",
      "timestamp: 1639001329\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 129107\n",
      "training_iteration: 300\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:300 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 495 -2.0 0.46999999999996955\n",
      "blue_1 True True 495 0.005 2.5069999999999695\n",
      "agent_timesteps_total: 259204\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-09-44\n",
      "done: false\n",
      "episode_len_mean: 310.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1352099999999592\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 542\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.539212703704834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009349610656499863\n",
      "        model: {}\n",
      "        policy_loss: -0.4974251985549927\n",
      "        total_loss: -0.4617847502231598\n",
      "        vf_explained_var: 0.06094198301434517\n",
      "        vf_loss: 0.022846288979053497\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.034824848175049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005052263382822275\n",
      "        model: {}\n",
      "        policy_loss: -0.4057183563709259\n",
      "        total_loss: -0.3936924338340759\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.0026819882914423943\n",
      "  num_agent_steps_sampled: 259204\n",
      "  num_agent_steps_trained: 259204\n",
      "  num_steps_sampled: 129602\n",
      "  num_steps_trained: 129602\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 301\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.74025974025975\n",
      "  ram_util_percent: 37.827272727272735\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5449699999999865\n",
      "  blue_1: 0.5902399999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5744403277226904\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.24918518006506\n",
      "  mean_inference_ms: 2.799905586060163\n",
      "  mean_raw_obs_processing_ms: 24.475580125369653\n",
      "time_since_restore: 17516.79296350479\n",
      "time_this_iter_s: 49.738906383514404\n",
      "time_total_s: 17516.79296350479\n",
      "timers:\n",
      "  learn_throughput: 843.961\n",
      "  learn_time_ms: 480.591\n",
      "  load_throughput: 812033.271\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.632\n",
      "  sample_time_ms: 53141.554\n",
      "timestamp: 1639001384\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 129602\n",
      "training_iteration: 301\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:300 starting ! -----------------\n",
      "agent_timesteps_total: 259204\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-09-44\n",
      "done: false\n",
      "episode_len_mean: 310.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1352099999999592\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 542\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.539212703704834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009349610656499863\n",
      "        model: {}\n",
      "        policy_loss: -0.4974251985549927\n",
      "        total_loss: -0.4617847502231598\n",
      "        vf_explained_var: 0.06094198301434517\n",
      "        vf_loss: 0.022846288979053497\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.034824848175049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005052263382822275\n",
      "        model: {}\n",
      "        policy_loss: -0.4057183563709259\n",
      "        total_loss: -0.3936924338340759\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.0026819882914423943\n",
      "  num_agent_steps_sampled: 259204\n",
      "  num_agent_steps_trained: 259204\n",
      "  num_steps_sampled: 129602\n",
      "  num_steps_trained: 129602\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 301\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.74025974025975\n",
      "  ram_util_percent: 37.827272727272735\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5449699999999865\n",
      "  blue_1: 0.5902399999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5744403277226904\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.24918518006506\n",
      "  mean_inference_ms: 2.799905586060163\n",
      "  mean_raw_obs_processing_ms: 24.475580125369653\n",
      "time_since_restore: 17516.79296350479\n",
      "time_this_iter_s: 49.738906383514404\n",
      "time_total_s: 17516.79296350479\n",
      "timers:\n",
      "  learn_throughput: 843.961\n",
      "  learn_time_ms: 480.591\n",
      "  load_throughput: 812033.271\n",
      "  load_time_ms: 0.499\n",
      "  sample_throughput: 7.632\n",
      "  sample_time_ms: 53141.554\n",
      "timestamp: 1639001384\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 129602\n",
      "training_iteration: 301\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:301 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 296 -2.0 -0.5250000000000095\n",
      "blue_1 True True 296 0.005 1.492999999999992\n",
      "agent_timesteps_total: 259796\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-10-27\n",
      "done: false\n",
      "episode_len_mean: 312.15\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1560399999999589\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 543\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.137304782867432\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013245508074760437\n",
      "        model: {}\n",
      "        policy_loss: -0.3796914219856262\n",
      "        total_loss: -0.2751532196998596\n",
      "        vf_explained_var: 0.17128592729568481\n",
      "        vf_loss: 0.08641279488801956\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.589720726013184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010510274209082127\n",
      "        model: {}\n",
      "        policy_loss: -0.24772946536540985\n",
      "        total_loss: -0.21879474818706512\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 0.009496496059000492\n",
      "  num_agent_steps_sampled: 259796\n",
      "  num_agent_steps_trained: 259796\n",
      "  num_steps_sampled: 129898\n",
      "  num_steps_trained: 129898\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 302\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.95862068965517\n",
      "  ram_util_percent: 37.81034482758621\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5352699999999864\n",
      "  blue_1: 0.6207699999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5744232464448146\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.22377255709571\n",
      "  mean_inference_ms: 2.7998968090284366\n",
      "  mean_raw_obs_processing_ms: 24.458479921279707\n",
      "time_since_restore: 17553.706816911697\n",
      "time_this_iter_s: 36.91385340690613\n",
      "time_total_s: 17553.706816911697\n",
      "timers:\n",
      "  learn_throughput: 835.95\n",
      "  learn_time_ms: 480.89\n",
      "  load_throughput: 801192.781\n",
      "  load_time_ms: 0.502\n",
      "  sample_throughput: 7.67\n",
      "  sample_time_ms: 52414.563\n",
      "timestamp: 1639001427\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 129898\n",
      "training_iteration: 302\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:302 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 182 -2.0 -1.0949999999999993\n",
      "blue_1 True True 182 0.005 0.9100000000000007\n",
      "LOSE\n",
      "blue_0 False True 389 -0.995 0.9449999999999806\n",
      "blue_1 False True 389 -0.993 1.1929999999999872\n",
      "agent_timesteps_total: 260938\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-11-41\n",
      "done: false\n",
      "episode_len_mean: 313.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1757899999999588\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 545\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.207497596740723\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01744518056511879\n",
      "        model: {}\n",
      "        policy_loss: -0.12725695967674255\n",
      "        total_loss: 0.2606959939002991\n",
      "        vf_explained_var: -0.12363968044519424\n",
      "        vf_loss: 0.36408063769340515\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.56355094909668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011522876098752022\n",
      "        model: {}\n",
      "        policy_loss: -0.31283631920814514\n",
      "        total_loss: -0.2698071300983429\n",
      "        vf_explained_var: -0.29412922263145447\n",
      "        vf_loss: 0.021718209609389305\n",
      "  num_agent_steps_sampled: 260938\n",
      "  num_agent_steps_trained: 260938\n",
      "  num_steps_sampled: 130469\n",
      "  num_steps_trained: 130469\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 303\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.59902912621359\n",
      "  ram_util_percent: 37.7980582524272\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5338199999999861\n",
      "  blue_1: 0.6419699999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.574393017433538\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.17228264141319\n",
      "  mean_inference_ms: 2.7998744215266833\n",
      "  mean_raw_obs_processing_ms: 24.424163714713114\n",
      "time_since_restore: 17622.453954696655\n",
      "time_this_iter_s: 68.74713778495789\n",
      "time_total_s: 17622.453954696655\n",
      "timers:\n",
      "  learn_throughput: 835.036\n",
      "  learn_time_ms: 496.745\n",
      "  load_throughput: 689820.903\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.548\n",
      "  sample_time_ms: 54953.612\n",
      "timestamp: 1639001501\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 130469\n",
      "training_iteration: 303\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:303 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 113 -2.0 -1.4399999999999995\n",
      "blue_1 True True 113 0.005 0.5650000000000004\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 162 0.005 0.8100000000000006\n",
      "blue_1 True True 162 -2.0 -1.1949999999999994\n",
      "agent_timesteps_total: 261488\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-12-31\n",
      "done: false\n",
      "episode_len_mean: 311.07\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.148319999999959\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 547\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3684184551239014\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.2967352867126465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023839302361011505\n",
      "        model: {}\n",
      "        policy_loss: -0.0422588512301445\n",
      "        total_loss: 0.3982211649417877\n",
      "        vf_explained_var: 0.5459414720535278\n",
      "        vf_loss: 0.40785789489746094\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 5.802920341491699\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013306105509400368\n",
      "        model: {}\n",
      "        policy_loss: -0.22763170301914215\n",
      "        total_loss: 0.14698651432991028\n",
      "        vf_explained_var: -0.10593607276678085\n",
      "        vf_loss: 0.35000917315483093\n",
      "  num_agent_steps_sampled: 261488\n",
      "  num_agent_steps_trained: 261488\n",
      "  num_steps_sampled: 130744\n",
      "  num_steps_trained: 130744\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 304\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.58985507246378\n",
      "  ram_util_percent: 37.85217391304349\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5198699999999864\n",
      "  blue_1: 0.6284499999999887\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5743677256863997\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.12137758037689\n",
      "  mean_inference_ms: 2.799852534624055\n",
      "  mean_raw_obs_processing_ms: 24.39084290458496\n",
      "time_since_restore: 17666.945437908173\n",
      "time_this_iter_s: 44.491483211517334\n",
      "time_total_s: 17666.945437908173\n",
      "timers:\n",
      "  learn_throughput: 837.755\n",
      "  learn_time_ms: 496.446\n",
      "  load_throughput: 691952.017\n",
      "  load_time_ms: 0.601\n",
      "  sample_throughput: 7.39\n",
      "  sample_time_ms: 56277.491\n",
      "timestamp: 1639001551\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 130744\n",
      "training_iteration: 304\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:304 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 179 0.005 0.8950000000000007\n",
      "blue_1 True True 179 -2.0 -1.1099999999999994\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 224 0.005 1.119999999999998\n",
      "blue_1 True True 224 -2.0 -0.8570000000000022\n",
      "agent_timesteps_total: 262294\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-13-32\n",
      "done: false\n",
      "episode_len_mean: 308.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.12124999999996\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 549\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.0526275634765625\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.6355109214782715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012637590989470482\n",
      "        model: {}\n",
      "        policy_loss: -0.1718367338180542\n",
      "        total_loss: -0.015515976585447788\n",
      "        vf_explained_var: -0.10263457894325256\n",
      "        vf_loss: 0.13038048148155212\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.248795509338379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013484910130500793\n",
      "        model: {}\n",
      "        policy_loss: -0.1783309131860733\n",
      "        total_loss: 0.14591963589191437\n",
      "        vf_explained_var: 0.15790650248527527\n",
      "        vf_loss: 0.2993108630180359\n",
      "  num_agent_steps_sampled: 262294\n",
      "  num_agent_steps_trained: 262294\n",
      "  num_steps_sampled: 131147\n",
      "  num_steps_trained: 131147\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 305\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 48.88941176470588\n",
      "  ram_util_percent: 37.87411764705883\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5364199999999868\n",
      "  blue_1: 0.5848299999999891\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5743372692346517\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.07209042398372\n",
      "  mean_inference_ms: 2.7998335579727103\n",
      "  mean_raw_obs_processing_ms: 24.359748791152715\n",
      "time_since_restore: 17722.335344552994\n",
      "time_this_iter_s: 55.38990664482117\n",
      "time_total_s: 17722.335344552994\n",
      "timers:\n",
      "  learn_throughput: 831.682\n",
      "  learn_time_ms: 515.1\n",
      "  load_throughput: 859649.715\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 7.371\n",
      "  sample_time_ms: 58123.242\n",
      "timestamp: 1639001612\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 131147\n",
      "training_iteration: 305\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:305 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.993 2.0229999999999637\n",
      "agent_timesteps_total: 263494\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-14-29\n",
      "done: false\n",
      "episode_len_mean: 310.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.143179999999959\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 550\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.0526275634765625\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.600203990936279\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02719847671687603\n",
      "        model: {}\n",
      "        policy_loss: -0.3672730028629303\n",
      "        total_loss: -0.3035105764865875\n",
      "        vf_explained_var: 0.09016355127096176\n",
      "        vf_loss: 0.007934064604341984\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.476279258728027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010452144779264927\n",
      "        model: {}\n",
      "        policy_loss: -0.38385143876075745\n",
      "        total_loss: -0.30779165029525757\n",
      "        vf_explained_var: 0.054154135286808014\n",
      "        vf_loss: 0.05672907829284668\n",
      "  num_agent_steps_sampled: 263494\n",
      "  num_agent_steps_trained: 263494\n",
      "  num_steps_sampled: 131747\n",
      "  num_steps_trained: 131747\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 306\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.949367088607595\n",
      "  ram_util_percent: 37.86329113924052\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5572199999999865\n",
      "  blue_1: 0.5859599999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5743235787784168\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.04615545055557\n",
      "  mean_inference_ms: 2.7998293060783874\n",
      "  mean_raw_obs_processing_ms: 24.343587184765752\n",
      "time_since_restore: 17774.160461187363\n",
      "time_this_iter_s: 51.8251166343689\n",
      "time_total_s: 17774.160461187363\n",
      "timers:\n",
      "  learn_throughput: 826.696\n",
      "  learn_time_ms: 547.118\n",
      "  load_throughput: 907478.45\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 7.782\n",
      "  sample_time_ms: 58120.067\n",
      "timestamp: 1639001669\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 131747\n",
      "training_iteration: 306\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:306 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSE\n",
      "blue_0 False True 475 -0.995 1.3749999999999716\n",
      "blue_1 False True 475 -0.996 1.6979999999999955\n",
      "agent_timesteps_total: 264444\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-15-29\n",
      "done: false\n",
      "episode_len_mean: 313.69\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1781599999999586\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 551\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.0789413452148438\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.688404560089111\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003574493806809187\n",
      "        model: {}\n",
      "        policy_loss: -0.44694870710372925\n",
      "        total_loss: -0.4114289879798889\n",
      "        vf_explained_var: -0.8193509578704834\n",
      "        vf_loss: 0.024514008313417435\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.42509937286377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009190949611365795\n",
      "        model: {}\n",
      "        policy_loss: -0.37825772166252136\n",
      "        total_loss: -0.32093554735183716\n",
      "        vf_explained_var: -0.6980610489845276\n",
      "        vf_loss: 0.04032395780086517\n",
      "  num_agent_steps_sampled: 264444\n",
      "  num_agent_steps_trained: 264444\n",
      "  num_steps_sampled: 132222\n",
      "  num_steps_trained: 132222\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 307\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.33902439024391\n",
      "  ram_util_percent: 38.131707317073165\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 3.023999999999959\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5630699999999862\n",
      "  blue_1: 0.6150899999999888\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5743100201197673\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 108.02056347274338\n",
      "  mean_inference_ms: 2.7998299206051964\n",
      "  mean_raw_obs_processing_ms: 24.327754673920666\n",
      "time_since_restore: 17827.869129419327\n",
      "time_this_iter_s: 53.70866823196411\n",
      "time_total_s: 17827.869129419327\n",
      "timers:\n",
      "  learn_throughput: 822.593\n",
      "  learn_time_ms: 534.651\n",
      "  load_throughput: 882398.899\n",
      "  load_time_ms: 0.498\n",
      "  sample_throughput: 7.646\n",
      "  sample_time_ms: 57517.916\n",
      "timestamp: 1639001729\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 132222\n",
      "training_iteration: 307\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:307 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 145 -2.0 -1.2799999999999994\n",
      "blue_1 True True 145 0.005 0.7250000000000005\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 430 -2.0 0.14499999999997648\n",
      "blue_1 True True 430 0.004 2.3109999999999826\n",
      "agent_timesteps_total: 265594\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-16-49\n",
      "done: false\n",
      "episode_len_mean: 312.86\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.1708299999999587\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 553\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5394706726074219\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.51667594909668\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012619216926395893\n",
      "        model: {}\n",
      "        policy_loss: -0.24180631339550018\n",
      "        total_loss: 0.12654049694538116\n",
      "        vf_explained_var: 0.13683119416236877\n",
      "        vf_loss: 0.34891992807388306\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.633489608764648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012731382623314857\n",
      "        model: {}\n",
      "        policy_loss: -0.19817201793193817\n",
      "        total_loss: -0.14133456349372864\n",
      "        vf_explained_var: -0.3684818148612976\n",
      "        vf_loss: 0.03329137712717056\n",
      "  num_agent_steps_sampled: 265594\n",
      "  num_agent_steps_trained: 265594\n",
      "  num_steps_sampled: 132797\n",
      "  num_steps_trained: 132797\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 308\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.450450450450454\n",
      "  ram_util_percent: 38.36306306306306\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5069999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5388699999999864\n",
      "  blue_1: 0.6319599999999891\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742836164658855\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.96953270743826\n",
      "  mean_inference_ms: 2.799821695785766\n",
      "  mean_raw_obs_processing_ms: 24.295267915285653\n",
      "time_since_restore: 17902.447780370712\n",
      "time_this_iter_s: 74.5786509513855\n",
      "time_total_s: 17902.447780370712\n",
      "timers:\n",
      "  learn_throughput: 821.272\n",
      "  learn_time_ms: 571.552\n",
      "  load_throughput: 784666.334\n",
      "  load_time_ms: 0.598\n",
      "  sample_throughput: 7.779\n",
      "  sample_time_ms: 60340.592\n",
      "timestamp: 1639001809\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 132797\n",
      "training_iteration: 308\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:308 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 461 0.005 2.304999999999973\n",
      "blue_1 True True 461 -2.0 0.31399999999997297\n",
      "agent_timesteps_total: 266516\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-17-47\n",
      "done: false\n",
      "episode_len_mean: 316.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.203169999999958\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 554\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5394706726074219\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.077055931091309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008868345059454441\n",
      "        model: {}\n",
      "        policy_loss: -0.3530677556991577\n",
      "        total_loss: -0.2777010202407837\n",
      "        vf_explained_var: -0.7532804012298584\n",
      "        vf_loss: 0.06171419471502304\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.735495567321777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006793742533773184\n",
      "        model: {}\n",
      "        policy_loss: -0.4330775737762451\n",
      "        total_loss: -0.3718910217285156\n",
      "        vf_explained_var: -0.3962332308292389\n",
      "        vf_loss: 0.048621829599142075\n",
      "  num_agent_steps_sampled: 266516\n",
      "  num_agent_steps_trained: 266516\n",
      "  num_steps_sampled: 133258\n",
      "  num_steps_trained: 133258\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 309\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.43827160493826\n",
      "  ram_util_percent: 38.38395061728395\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5069999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.575019999999986\n",
      "  blue_1: 0.6281499999999889\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742703653110616\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.94385833083503\n",
      "  mean_inference_ms: 2.7998142616101065\n",
      "  mean_raw_obs_processing_ms: 24.27839251310179\n",
      "time_since_restore: 17955.506504535675\n",
      "time_this_iter_s: 53.05872416496277\n",
      "time_total_s: 17955.506504535675\n",
      "timers:\n",
      "  learn_throughput: 831.224\n",
      "  learn_time_ms: 571.567\n",
      "  load_throughput: 680734.407\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 7.768\n",
      "  sample_time_ms: 61160.926\n",
      "timestamp: 1639001867\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 133258\n",
      "training_iteration: 309\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:309 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 2.0189999999999593\n",
      "agent_timesteps_total: 267716\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-18-58\n",
      "done: false\n",
      "episode_len_mean: 319.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2372099999999575\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 555\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5394706726074219\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.526688575744629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012196004390716553\n",
      "        model: {}\n",
      "        policy_loss: -0.36904019117355347\n",
      "        total_loss: -0.3390384912490845\n",
      "        vf_explained_var: -0.6254937052726746\n",
      "        vf_loss: 0.011226315051317215\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.721541881561279\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008802822791039944\n",
      "        model: {}\n",
      "        policy_loss: -0.32138901948928833\n",
      "        total_loss: -0.2877746522426605\n",
      "        vf_explained_var: -0.6189038157463074\n",
      "        vf_loss: 0.01733398251235485\n",
      "  num_agent_steps_sampled: 267716\n",
      "  num_agent_steps_trained: 267716\n",
      "  num_steps_sampled: 133858\n",
      "  num_steps_trained: 133858\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 310\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.630927835051544\n",
      "  ram_util_percent: 38.39484536082473\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5069999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.6019699999999857\n",
      "  blue_1: 0.6352399999999885\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742572285646305\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.9176126034106\n",
      "  mean_inference_ms: 2.799804221762557\n",
      "  mean_raw_obs_processing_ms: 24.26090699576215\n",
      "time_since_restore: 18020.457832098007\n",
      "time_this_iter_s: 64.95132756233215\n",
      "time_total_s: 18020.457832098007\n",
      "timers:\n",
      "  learn_throughput: 831.373\n",
      "  learn_time_ms: 571.464\n",
      "  load_throughput: 680827.439\n",
      "  load_time_ms: 0.698\n",
      "  sample_throughput: 7.802\n",
      "  sample_time_ms: 60891.009\n",
      "timestamp: 1639001938\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 133858\n",
      "training_iteration: 310\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:310 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 508 -2.0 0.5349999999999682\n",
      "blue_1 True True 508 0.005 2.5269999999999695\n",
      "agent_timesteps_total: 268732\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-19-51\n",
      "done: false\n",
      "episode_len_mean: 323.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2781799999999566\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 556\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5394706726074219\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.99015474319458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008565478026866913\n",
      "        model: {}\n",
      "        policy_loss: -0.4983275532722473\n",
      "        total_loss: -0.46636873483657837\n",
      "        vf_explained_var: 0.26920872926712036\n",
      "        vf_loss: 0.018772434443235397\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.602284908294678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009591316804289818\n",
      "        model: {}\n",
      "        policy_loss: -0.48907196521759033\n",
      "        total_loss: -0.4691120684146881\n",
      "        vf_explained_var: -0.6405308842658997\n",
      "        vf_loss: 0.002221188973635435\n",
      "  num_agent_steps_sampled: 268732\n",
      "  num_agent_steps_trained: 268732\n",
      "  num_steps_sampled: 134366\n",
      "  num_steps_trained: 134366\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 311\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.93150684931507\n",
      "  ram_util_percent: 38.38219178082192\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5269999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.6024699999999854\n",
      "  blue_1: 0.6757099999999884\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742449593228388\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.88953533606798\n",
      "  mean_inference_ms: 2.7998074732254503\n",
      "  mean_raw_obs_processing_ms: 24.242226904930632\n",
      "time_since_restore: 18067.812934160233\n",
      "time_this_iter_s: 47.35510206222534\n",
      "time_total_s: 18067.812934160233\n",
      "timers:\n",
      "  learn_throughput: 833.216\n",
      "  learn_time_ms: 571.761\n",
      "  load_throughput: 595277.036\n",
      "  load_time_ms: 0.8\n",
      "  sample_throughput: 7.854\n",
      "  sample_time_ms: 60653.759\n",
      "timestamp: 1639001991\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 134366\n",
      "training_iteration: 311\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Evaluation at steps:310 starting ! -----------------\n",
      "agent_timesteps_total: 268732\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-19-51\n",
      "done: false\n",
      "episode_len_mean: 323.57\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2781799999999566\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 556\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5394706726074219\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.99015474319458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008565478026866913\n",
      "        model: {}\n",
      "        policy_loss: -0.4983275532722473\n",
      "        total_loss: -0.46636873483657837\n",
      "        vf_explained_var: 0.26920872926712036\n",
      "        vf_loss: 0.018772434443235397\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 4.602284908294678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009591316804289818\n",
      "        model: {}\n",
      "        policy_loss: -0.48907196521759033\n",
      "        total_loss: -0.4691120684146881\n",
      "        vf_explained_var: -0.6405308842658997\n",
      "        vf_loss: 0.002221188973635435\n",
      "  num_agent_steps_sampled: 268732\n",
      "  num_agent_steps_trained: 268732\n",
      "  num_steps_sampled: 134366\n",
      "  num_steps_trained: 134366\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 311\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 56.93150684931507\n",
      "  ram_util_percent: 38.38219178082192\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5269999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.6024699999999854\n",
      "  blue_1: 0.6757099999999884\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742449593228388\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.88953533606798\n",
      "  mean_inference_ms: 2.7998074732254503\n",
      "  mean_raw_obs_processing_ms: 24.242226904930632\n",
      "time_since_restore: 18067.812934160233\n",
      "time_this_iter_s: 47.35510206222534\n",
      "time_total_s: 18067.812934160233\n",
      "timers:\n",
      "  learn_throughput: 833.216\n",
      "  learn_time_ms: 571.761\n",
      "  load_throughput: 595277.036\n",
      "  load_time_ms: 0.8\n",
      "  sample_throughput: 7.854\n",
      "  sample_time_ms: 60653.759\n",
      "timestamp: 1639001991\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 134366\n",
      "training_iteration: 311\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:311 start! -----------------\n",
      "blue_0DOWN\n",
      "LOSE\n",
      "blue_0 True True 279 -2.0 -0.6100000000000076\n",
      "blue_1 True True 279 0.005 1.3949999999999922\n",
      "agent_timesteps_total: 269290\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-20-30\n",
      "done: false\n",
      "episode_len_mean: 325.37\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.2961799999999561\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 557\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5394706726074219\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.179487228393555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025646964088082314\n",
      "        model: {}\n",
      "        policy_loss: -0.2504478991031647\n",
      "        total_loss: -0.031416598707437515\n",
      "        vf_explained_var: 0.5339087247848511\n",
      "        vf_loss: 0.17954856157302856\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 8.621105194091797\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01082509383559227\n",
      "        model: {}\n",
      "        policy_loss: -0.26620882749557495\n",
      "        total_loss: -0.24141110479831696\n",
      "        vf_explained_var: 0.36391130089759827\n",
      "        vf_loss: 0.004777276888489723\n",
      "  num_agent_steps_sampled: 269290\n",
      "  num_agent_steps_trained: 269290\n",
      "  num_steps_sampled: 134645\n",
      "  num_steps_trained: 134645\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 312\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.229629629629635\n",
      "  ram_util_percent: 38.37777777777778\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5269999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.5914199999999854\n",
      "  blue_1: 0.7047599999999883\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742326423775502\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.86123603968299\n",
      "  mean_inference_ms: 2.7998105357182004\n",
      "  mean_raw_obs_processing_ms: 24.22348189200936\n",
      "time_since_restore: 18101.492740631104\n",
      "time_this_iter_s: 33.67980647087097\n",
      "time_total_s: 18101.492740631104\n",
      "timers:\n",
      "  learn_throughput: 829.522\n",
      "  learn_time_ms: 572.257\n",
      "  load_throughput: 677614.984\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 7.867\n",
      "  sample_time_ms: 60336.914\n",
      "timestamp: 1639002030\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 134645\n",
      "training_iteration: 312\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:312 start! -----------------\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.994 2.022999999999956\n",
      "agent_timesteps_total: 270490\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-21-23\n",
      "done: false\n",
      "episode_len_mean: 329.72\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.3399599999999552\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 558\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.309206008911133\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4909825325012207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008619686588644981\n",
      "        model: {}\n",
      "        policy_loss: -0.37162163853645325\n",
      "        total_loss: -0.3244965970516205\n",
      "        vf_explained_var: -0.18112067878246307\n",
      "        vf_loss: 0.02722044847905636\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.792447328567505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008508970029652119\n",
      "        model: {}\n",
      "        policy_loss: -0.37295424938201904\n",
      "        total_loss: -0.33468741178512573\n",
      "        vf_explained_var: -0.4110942780971527\n",
      "        vf_loss: 0.02252986840903759\n",
      "  num_agent_steps_sampled: 270490\n",
      "  num_agent_steps_trained: 270490\n",
      "  num_steps_sampled: 135245\n",
      "  num_steps_trained: 135245\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 313\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.57123287671233\n",
      "  ram_util_percent: 38.38904109589041\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5269999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.6232199999999849\n",
      "  blue_1: 0.7167399999999877\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742219878299918\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.83104889655488\n",
      "  mean_inference_ms: 2.7998217847984974\n",
      "  mean_raw_obs_processing_ms: 24.204135544570107\n",
      "time_since_restore: 18148.568667411804\n",
      "time_this_iter_s: 47.075926780700684\n",
      "time_total_s: 18148.568667411804\n",
      "timers:\n",
      "  learn_throughput: 837.07\n",
      "  learn_time_ms: 570.562\n",
      "  load_throughput: 681661.82\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 8.21\n",
      "  sample_time_ms: 58175.698\n",
      "timestamp: 1639002083\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 135245\n",
      "training_iteration: 313\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:313 start! -----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 123 0.005 0.6150000000000004\n",
      "blue_1 True True 123 -2.0 -1.3899999999999997\n",
      "LOSE\n",
      "blue_0 False True 419 -0.995 1.0949999999999775\n",
      "blue_1 False True 419 -0.993 1.4449999999999932\n",
      "agent_timesteps_total: 271574\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-22-37\n",
      "done: false\n",
      "episode_len_mean: 326.64\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.3113999999999564\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 560\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.309206008911133\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.618456840515137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009162690490484238\n",
      "        model: {}\n",
      "        policy_loss: -0.17531463503837585\n",
      "        total_loss: -0.06399530917406082\n",
      "        vf_explained_var: -0.35879993438720703\n",
      "        vf_loss: 0.0901607945561409\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.975839138031006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012934479862451553\n",
      "        model: {}\n",
      "        policy_loss: -0.1517399102449417\n",
      "        total_loss: 0.21674397587776184\n",
      "        vf_explained_var: -0.556681215763092\n",
      "        vf_loss: 0.34456220269203186\n",
      "  num_agent_steps_sampled: 271574\n",
      "  num_agent_steps_trained: 271574\n",
      "  num_steps_sampled: 135787\n",
      "  num_steps_trained: 135787\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 314\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.194117647058825\n",
      "  ram_util_percent: 38.3450980392157\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5269999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.6078199999999851\n",
      "  blue_1: 0.703579999999988\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5742018585816155\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.77436403431334\n",
      "  mean_inference_ms: 2.79983562699925\n",
      "  mean_raw_obs_processing_ms: 24.167377192235286\n",
      "time_since_restore: 18217.258956193924\n",
      "time_this_iter_s: 68.69028878211975\n",
      "time_total_s: 18217.258956193924\n",
      "timers:\n",
      "  learn_throughput: 835.287\n",
      "  learn_time_ms: 603.744\n",
      "  load_throughput: 719647.355\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 8.327\n",
      "  sample_time_ms: 60561.54\n",
      "timestamp: 1639002157\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 135787\n",
      "training_iteration: 314\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:314 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 132 0.005 0.6600000000000005\n",
      "blue_1 True True 132 -2.0 -1.3449999999999995\n",
      "LOSE\n",
      "blue_0 False True 543 -0.995 1.7149999999999643\n",
      "blue_1 False True 543 -0.996 1.9349999999999916\n",
      "agent_timesteps_total: 272924\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-23-57\n",
      "done: false\n",
      "episode_len_mean: 326.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.3095699999999553\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 562\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.309206008911133\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.167046070098877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008929411880671978\n",
      "        model: {}\n",
      "        policy_loss: -0.2087843120098114\n",
      "        total_loss: -0.15313595533370972\n",
      "        vf_explained_var: 0.01978297345340252\n",
      "        vf_loss: 0.03502850979566574\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 6.1926798820495605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013609051704406738\n",
      "        model: {}\n",
      "        policy_loss: -0.13619384169578552\n",
      "        total_loss: 0.1425524353981018\n",
      "        vf_explained_var: -0.22573241591453552\n",
      "        vf_loss: 0.25357702374458313\n",
      "  num_agent_steps_sampled: 272924\n",
      "  num_agent_steps_trained: 272924\n",
      "  num_steps_sampled: 136462\n",
      "  num_steps_trained: 136462\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 315\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.22767857142857\n",
      "  ram_util_percent: 38.43750000000001\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5269999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.6072199999999852\n",
      "  blue_1: 0.7023499999999882\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5741856263492877\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.71640120524026\n",
      "  mean_inference_ms: 2.7998701314992127\n",
      "  mean_raw_obs_processing_ms: 24.130621117174705\n",
      "time_since_restore: 18292.123186588287\n",
      "time_this_iter_s: 74.8642303943634\n",
      "time_total_s: 18292.123186588287\n",
      "timers:\n",
      "  learn_throughput: 831.835\n",
      "  learn_time_ms: 638.949\n",
      "  load_throughput: 758462.363\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 8.504\n",
      "  sample_time_ms: 62503.661\n",
      "timestamp: 1639002237\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 136462\n",
      "training_iteration: 315\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:315 start! -----------------\n",
      "blue_1DOWN\n",
      "LOSE\n",
      "blue_0 True True 128 0.005 0.6400000000000005\n",
      "blue_1 True True 128 -2.0 -1.3649999999999995\n",
      "LOSE\n",
      "blue_0 False False 600 -0.995 1.9999999999999583\n",
      "blue_1 False False 600 -0.995 1.9859999999999598\n",
      "agent_timesteps_total: 274380\n",
      "custom_metrics: {}\n",
      "date: 2021-12-09_07-25-36\n",
      "done: false\n",
      "episode_len_mean: 328.55\n",
      "episode_media: {}\n",
      "episode_reward_max: 4.175999999999897\n",
      "episode_reward_mean: 1.3273799999999552\n",
      "episode_reward_min: -1.645\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 564\n",
      "experiment_id: 3258f0277f3a4fa98a0ca2d0d9f5979d\n",
      "hostname: DESKTOP\n",
      "info:\n",
      "  learner:\n",
      "    blue_0:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.309206008911133\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.4537482261657715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007536970544606447\n",
      "        model: {}\n",
      "        policy_loss: -0.3174406886100769\n",
      "        total_loss: -0.26861563324928284\n",
      "        vf_explained_var: -0.4034378230571747\n",
      "        vf_loss: 0.03142065182328224\n",
      "    blue_1:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8494508266448975\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 7.582132339477539\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020064253360033035\n",
      "        model: {}\n",
      "        policy_loss: -0.2035180926322937\n",
      "        total_loss: 0.15882395207881927\n",
      "        vf_explained_var: -0.6123420000076294\n",
      "        vf_loss: 0.32523423433303833\n",
      "  num_agent_steps_sampled: 274380\n",
      "  num_agent_steps_trained: 274380\n",
      "  num_steps_sampled: 137190\n",
      "  num_steps_trained: 137190\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 316\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 62.3162962962963\n",
      "  ram_util_percent: 38.636296296296294\n",
      "pid: 21824\n",
      "policy_reward_max:\n",
      "  blue_0: 3.159999999999964\n",
      "  blue_1: 2.5269999999999695\n",
      "policy_reward_mean:\n",
      "  blue_0: 0.6374199999999849\n",
      "  blue_1: 0.689959999999988\n",
      "policy_reward_min:\n",
      "  blue_0: -1.825\n",
      "  blue_1: -1.7399999999999998\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.5741774380138244\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 107.65834926293593\n",
      "  mean_inference_ms: 2.799940597521781\n",
      "  mean_raw_obs_processing_ms: 24.093732652603002\n",
      "time_since_restore: 18385.204211235046\n",
      "time_this_iter_s: 93.08102464675903\n",
      "time_total_s: 18385.204211235046\n",
      "timers:\n",
      "  learn_throughput: 827.959\n",
      "  learn_time_ms: 657.399\n",
      "  load_throughput: 776754.676\n",
      "  load_time_ms: 0.701\n",
      "  sample_throughput: 8.167\n",
      "  sample_time_ms: 66646.115\n",
      "timestamp: 1639002336\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 137190\n",
      "training_iteration: 316\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "----------------- Training at steps:316 start! -----------------\n"
     ]
    }
   ],
   "source": [
    "#def getkey(key):\n",
    "    # return 111\n",
    "#    return(bool(ctypes.windll.user32.GetAsyncKeyState(key) & 0x8000))\n",
    "# Training & evaluation\n",
    "\n",
    "record_mode = 1\n",
    "results_dir = os.path.join('./' + PROJECT + '/results/')\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "results_file = results_dir + TRIAL + '.pkl'\n",
    "for steps in range(10001):\n",
    "    # Training\n",
    "    print(f'\\n----------------- Training at steps:{steps} start! -----------------')\n",
    "    eval_env.reset()\n",
    "    results = trainer.train()\n",
    "    print(pretty_print(results))\n",
    "    check_point = trainer.save(checkpoint_dir=check_point_dir)\n",
    "    # Evaluation\n",
    "    if steps % EVAL_FREQ == 0:\n",
    "        print(f'\\n----------------- Evaluation at steps:{steps} starting ! -----------------')\n",
    "        print(pretty_print(results))\n",
    "        check_point = trainer.save(checkpoint_dir=check_point_dir)\n",
    "        win = 0\n",
    "        for i in range(NUM_EVAL*0):\n",
    "            # print(f'\\nEvaluation {i}:')\n",
    "            obs = eval_env.reset()\n",
    "            done = False\n",
    "            \n",
    "            step_num = 0\n",
    "            fig = plt.figure(1)\n",
    "            ESC = 0x1B          # ESCキーの仮想キーコード\n",
    "            trajectory_length = 100\n",
    "            env_blue_pos = [0]\n",
    "            env_red_pos = [0]\n",
    "            env_mrm_pos = [0]\n",
    "            if record_mode == 0:\n",
    "                file_name = \"test_num\" + str(steps) +str(i)\n",
    "                video = cv2.VideoWriter(file_name+'.mp4',0x00000020,20.0,(eval_env.WINNDOW_SIZE_lon,eval_env.WINDOW_SIZE_lat))\n",
    "\n",
    "            while True:\n",
    "                action_dict = {}\n",
    "                for j in range(eval_env.blue_num):\n",
    "                    #if not eval_env.blue[j].hitpoint == 0:\n",
    "                    #action_dict['blue_' + str(j)] = trainer.compute_action(obs['blue_' + str(j)])\n",
    "                    action_dict['blue_' + str(j)] = trainer.compute_single_action(obs['blue_' + str(j)],policy_id='blue_' + str(j))\n",
    "                obs, rewards, dones, infos = eval_env.step(action_dict)\n",
    "                env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp= render_env.copy_from_env(eval_env)\n",
    "                env_blue_pos.append(env_blue_pos_temp)\n",
    "                env_red_pos.append(env_red_pos_temp)\n",
    "                env_mrm_pos.append(env_mrm_pos_temp)\n",
    "                if step_num == 0:\n",
    "                    del env_blue_pos[0]\n",
    "                    del env_red_pos[0]\n",
    "                    del env_mrm_pos[0]\n",
    "\n",
    "                hist_blue_pos = np.vstack(env_blue_pos)\n",
    "                hist_red_pos = np.vstack(env_red_pos)\n",
    "                hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "                plt.clf()\n",
    "                render_env.rend_3d(eval_env,hist_blue_pos,\"b\",1)\n",
    "                render_env.rend_3d(eval_env,hist_red_pos,\"r\",1)\n",
    "                render_env.rend_3d(eval_env,hist_mrm_pos,\"k\",1)\n",
    "                fig.canvas.draw()\n",
    "                plt.pause(.05)\n",
    "                if record_mode == 0:\n",
    "                    img = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)\n",
    "                    # cv2.imshow('test', img)\n",
    "                    # cv2.waitKey(1)\n",
    "                    # cv2.destroyAllWindows()\n",
    "                    video.write(img.astype('uint8'))\n",
    "\n",
    "                \n",
    "                step_num = step_num + 1\n",
    "                \n",
    "                done = dones[\"__all__\"]\n",
    "                #print(f'rewards:{rewards}')\n",
    "                #if record_mode == 0:\n",
    "                #    img = eval_env.render_movie(file_name,step_num)\n",
    "                #    video.write(img.astype('unit8'))\n",
    "                #elif record_mode == 1:\n",
    "                #    eval_env.render()\n",
    "                #elif record_mode == 2:\n",
    "                #    eval_env.render()\n",
    "                    \n",
    "                #env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp = render_env.copy_from_env(eval_env)\n",
    "                \n",
    "                #env_blue_pos.append(env_blue_pos_temp)\n",
    "                #env_red_pos.append(env_red_pos_temp)\n",
    "                #env_mrm_pos.append(env_mrm_pos_temp)\n",
    "                #step_num = step_num + 1\n",
    "                # エピソードの終了処理\n",
    "                if dones['__all__']:\n",
    "                    # print(f'all done at {env.steps}')\n",
    "                    break\n",
    "                \n",
    "            #del env_blue_pos[0]\n",
    "            #del env_red_pos[0]\n",
    "            #del env_mrm_pos[0]\n",
    "            \n",
    "            #hist_blue_pos = np.vstack(env_blue_pos)\n",
    "            #hist_red_pos = np.vstack(env_red_pos)\n",
    "            #hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "            \n",
    "            #f = open(results_file,'wb')\n",
    "            #pickle.dump(emv_blue_pos,f)\n",
    "            #pickle.dump(emv_red_pos,f)\n",
    "            #pickle.dump(emv_mrm_pos,f)\n",
    "            #f.close()\n",
    "            \n",
    "            if record_mode == 0:\n",
    "                video.release()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bfaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
