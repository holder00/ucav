{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6504bb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DMU\\AppData\\Roaming\\Python\\Python37\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n",
      "C:\\Users\\DMU\\AppData\\Roaming\\Python\\Python37\\site-packages\\quaternion\\numba_wrapper.py:23: UserWarning: \n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Could not import from numba, which means that some\n",
      "parts of this code may run MUCH more slowly.  You\n",
      "may wish to install numba.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "  warnings.warn(warning_text)\n",
      "2022-03-24 11:17:07,394\tINFO trainer.py:2055 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-03-24 11:17:07,394\tINFO trainer.py:792 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-03-24 11:17:12,558\tWARNING deprecation.py:46 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-24 11:17:12,578\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import Logger, UnifiedLogger, pretty_print\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.rllib.examples.models.shared_weights_model import TF2SharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.agents.ppo import ppo, PPOTrainer, PPOTFPolicy\n",
    "from ray.rllib.agents.impala.vtrace_tf_policy import VTraceTFPolicy\n",
    "from ray.rllib.agents.impala import impala, ImpalaTrainer\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from environment_rllib_3d import MyEnv\n",
    "#from test_env_for_lstm import MyEnv\n",
    "from settings.initial_settings import *\n",
    "from settings.reset_conditions import reset_conditions\n",
    "#from modules.models import MyConv2DModel_v0B_Small_CBAM_1DConv_Share\n",
    "#from modules.models import MyRNNUAVClass\n",
    "#from modules.models import DenseNetModelLarge\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from modules.savers import save_conditions\n",
    "from utility.result_env import render_env\n",
    "from utility.terminate_uavsimproc import teminate_proc\n",
    "from utility.latest_learned_file_path import latest_learned_file_path\n",
    "from utility.save_logs import save_logs_IMPALA\n",
    "from utility.save_logs import save_hists\n",
    "from utility.save_logs import save_env_info\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import ctypes\n",
    "import warnings\n",
    "\n",
    "#UCAV.exeが起動している場合、プロセスキルする。\n",
    "teminate_proc.UAVsimprockill(proc_name=\"UCAV.exe\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.filterwarnings('ignore', category=matplotlib.MatplotlibDeprecationWarning)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "PROJECT = \"UCAV\"\n",
    "TRIAL_ID = 2\n",
    "TRIAL = 'test_' + str(TRIAL_ID)\n",
    "EVAL_FREQ = 1\n",
    "CONTINUAL = False\n",
    "NUM_EVAL = 1\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "    timestr = datetime.datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "\n",
    "    def logger_creator(config):\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "#ModelCatalog.register_custom_model('my_model', MyRNNUAVClass)\n",
    "\n",
    "eval_env = MyEnv()\n",
    "policies = {\n",
    "    #\"blue_1\": PolicySpec(config={\"gamma\": 0.99}),\n",
    "    #\"blue_2\": PolicySpec(config={\"gamma\": 0.95}),\n",
    "    #\"blue_0\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space,\n",
    "    #           {\"model\":{\"vf_share_layers\": False,\"use_lstm\": True,\"max_seq_len\": 200},}),\n",
    "    #\"blue_1\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space,\n",
    "    #           {\"model\":{\"vf_share_layers\": False,\"use_lstm\": True,\"max_seq_len\": 200},}),\n",
    "    \"blue_0\": (VTraceTFPolicy, eval_env.observation_space, eval_env.action_space,\n",
    "               {\"model\":{\"vf_share_layers\": False,\"use_lstm\": True,\"max_seq_len\": 200},}),\n",
    "    \"blue_1\": (VTraceTFPolicy, eval_env.observation_space, eval_env.action_space,\n",
    "               {\"model\":{\"vf_share_layers\": False,\"use_lstm\": True,\"max_seq_len\": 200},}),\n",
    "}\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    #print(agent_id,episode)\n",
    "    #pol_id = policy_ids[agent_id]\n",
    "\n",
    "    pol_id = agent_id\n",
    "    return pol_id\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "config = impala.DEFAULT_CONFIG.copy()\n",
    "config = {\"env\": MyEnv,\"num_gpus\": 0,\"num_workers\": 0, \"num_cpus_per_worker\": 0,\"num_gpus_per_worker\": 0,\n",
    "          \"train_batch_size\": 600*5*2,\n",
    "          \"batch_mode\": \"complete_episodes\",\n",
    "          \"gamma\":0.995, \"lr\": 2.5e-4,\n",
    "          #\"clip_actions\":True,\"normalize_actions\":True,\n",
    "          \"observation_space\":eval_env.observation_space,\"action_space\":eval_env.action_space,\n",
    "          \"explore\":True,\n",
    "          \"rollout_fragment_length\":300,\"num_sgd_iter\": 20,\"learner_queue_size\": 300,\n",
    "          #\"sgd_minibatch_size\": 300, \"num_sgd_iter\":20,\n",
    "          #\"exploration_config\": {\"type\": \"StochasticSampling\",\"random_timesteps\":0}, #PPO デフォルト \"random_timesteps\":0\n",
    "          #\"model\":{\"fcnet_activation\": \"relu\",\"fcnet_hiddens\": [256, 256, 256],\"post_fcnet_activation\": \"linear\",\n",
    "          #         \"vf_share_layers\": True,},#\"linear\",\"relu\",\"tanh\" \"use_lstm\":True,\"lstm_cell_size\":256,\"max_seq_len\":128\n",
    "          \"learner_queue_timeout\": 900,\n",
    "          #\"model\": {\"custom_model\": \"my_model\"},\n",
    "          \"multiagent\": {\"policies\": policies,  \"policy_mapping_fn\": policy_mapping_fn}\n",
    "         }\n",
    "#res_name = \"sgd\"+str(config[\"sgd_minibatch_size\"])+\"sgd_num\"+str(config[\"num_sgd_iter\"])+\"lr\"+str(config[\"lr\"])+\"gamma\"+str(config[\"gamma\"])\n",
    "res_name = \"test\"\n",
    "conditions_dir = os.path.join('./' + PROJECT + '/conditions/')\n",
    "\n",
    "if not os.path.exists(conditions_dir):\n",
    "    os.makedirs(conditions_dir)\n",
    "save_conditions(conditions_dir)\n",
    "\n",
    "# PPOTrainer()は、try_import_tfを使うと、なぜかTensorflowのeager modeのエラーになる。\n",
    "\n",
    "trainer = impala.ImpalaTrainer(config=config,\n",
    "                         logger_creator=custom_log_creator(\n",
    "                             os.path.expanduser(\"./\" + PROJECT + \"/logs\"), TRIAL))\n",
    "\n",
    "if CONTINUAL:\n",
    "    # Continual learning: Need to specify the checkpoint\n",
    "    # model_path = PROJECT + '/checkpoints/' + TRIAL + '/checkpoint_000197/checkpoint-197'\n",
    "    model_path = latest_learned_file_path('./UCAV/checkpoints/test_2/*')\n",
    "    trainer.restore(checkpoint_path=model_path)\n",
    "\n",
    "models_dir = os.path.join('./' + PROJECT + '/models/')\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "for j in range(2):\n",
    "    text_name = models_dir + TRIAL + \"blue_\"+str(j) +'.txt'\n",
    "    with open(text_name, \"w\") as fp:\n",
    "        trainer.get_policy(\"blue_\"+str(j)).model.base_model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))\n",
    "    png_name = models_dir + TRIAL + '.png'\n",
    "    plot_model(trainer.get_policy(\"blue_\"+str(j)).model.base_model, to_file=png_name, show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define checkpoint dir\n",
    "check_point_dir = os.path.join('./' + PROJECT + '/checkpoints/', TRIAL)\n",
    "if not os.path.exists(check_point_dir):\n",
    "    os.makedirs(check_point_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebbfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "\n",
      "----------------- Training at steps:0 start! -----------------\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "121 blue_1 DOWN\n",
      "482 blue_0 Shoot at red_1 launch distance : 56372.93295488006 True True\n",
      "493 blue_0 Shoot at red_1 launch distance : 52473.834320350965 True True\n",
      "536 red_0 Shoot at blue_0\n",
      "547 red_0 Shoot at blue_0\n",
      "610 blue_0 Splash :red_1\n",
      "640 blue_0: Destroyed\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 640 -0.13994421051278277 113.31906618727442\n",
      "blue_1 False False 640 -0.10099981051278278 -1.3071002848420847\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "133 blue_1 DOWN\n",
      "224 blue_0 DOWN\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 224 -1.1010029311937106 -2.6646747510607898\n",
      "blue_1 False False 224 -0.10100186452704389 -1.4181754768746053\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "361 blue_1 DOWN\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar  7 21:50:01 2022\n",
    "\n",
    "@author: Takumi\n",
    "\"\"\"\n",
    "eval_env.reset()\n",
    "save_env_info(eval_env)\n",
    "record_mode = 0\n",
    "results_dir = os.path.join('./' + PROJECT + '/results/')\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "results_file = results_dir + TRIAL + '.pkl'\n",
    "for steps in range(10001):\n",
    "    # Training\n",
    "    print(f'\\n----------------- Training at steps:{steps} start! -----------------')\n",
    "    eval_env.eval = False\n",
    "    eval_env.reset()\n",
    "    results = trainer.train()\n",
    "    save_logs_IMPALA(res_name,results,steps,CONTINUAL)\n",
    "    print(pretty_print(results))\n",
    "    #check_point = trainer.save(checkpoint_dir=check_point_dir)\n",
    "    # Evaluation\n",
    "    if steps % EVAL_FREQ == 0:\n",
    "        print(f'\\n-------------- Evaluation at steps:{steps} starting ! --------------')\n",
    "\n",
    "        check_point = trainer.save(checkpoint_dir=check_point_dir)\n",
    "        for i in range(NUM_EVAL):\n",
    "            # print(f'\\nEvaluation {i}:')\n",
    "            model_path = latest_learned_file_path('./UCAV/checkpoints/test_2/*')\n",
    "            trainer.restore(checkpoint_path=model_path)\n",
    "            eval_env.eval = True\n",
    "            obs = eval_env.reset()\n",
    "            done = False\n",
    "            \n",
    "            step_num = 0\n",
    "            #fig = plt.figure(1,figsize=(8.0, 6.0))\n",
    "            ESC = 0x1B          # ESCキーの仮想キーコード\n",
    "            trajectory_length = 100\n",
    "\n",
    "            cell_size = 256\n",
    "            state_0=[np.zeros(cell_size, np.float32),np.zeros(cell_size, np.float32)]\n",
    "            state_1=[np.zeros(cell_size, np.float32),np.zeros(cell_size, np.float32)]\n",
    "            action_dict0 = [0,0]\n",
    "            action_dict1 = [0,0]\n",
    "            rewards = {\"blue_0\":0,\"blue_1\":0}\n",
    "            if record_mode == 0:\n",
    "                file_name = \"test_num\" + str(steps) +str(i)\n",
    "                #video = cv2.VideoWriter(file_name+'.mp4',0x00000020,20.0,(800,600))\n",
    "\n",
    "            while True:\n",
    "                action_dict = {}\n",
    "                action_dict0 = trainer.compute_single_action(obs['blue_0'],\n",
    "                                                             state=state_0,prev_action=None,prev_reward=None,\n",
    "                                                             policy_id='blue_0',explore=False)\n",
    "                action_dict1 = trainer.compute_single_action(obs['blue_1'],\n",
    "                                                             state=state_1,prev_action=None,prev_reward=None,\n",
    "                                                             policy_id='blue_1',explore=False)\n",
    "                \n",
    "                #action_dict0 = trainer.compute_single_action(obs['blue_0'],policy_id='blue_0')\n",
    "                #action_dict1 = trainer.compute_single_action(obs['blue_1'],policy_id='blue_1')\n",
    "                state_0 = action_dict0[1]\n",
    "                state_1 = action_dict1[1]\n",
    "                obs, rewards, dones, infos = eval_env.step({'blue_0': action_dict0[0], 'blue_1': action_dict1[0]})\n",
    "\n",
    "                env_blue_pos_temp_mod, env_red_pos_temp_mod, env_mrm_pos_temp_mod = render_env.copy_from_env_mod(eval_env)\n",
    "                if eval_env.timer == 1:\n",
    "                    env_blue_pos_mod = env_blue_pos_temp_mod\n",
    "                    env_red_pos_mod = env_red_pos_temp_mod\n",
    "                    env_mrm_pos_mod = env_mrm_pos_temp_mod\n",
    "                else:\n",
    "                    env_blue_pos_mod = np.vstack([env_blue_pos_mod,env_blue_pos_temp_mod])\n",
    "                    env_red_pos_mod = np.vstack([env_red_pos_mod,env_red_pos_temp_mod])\n",
    "                    env_mrm_pos_mod = np.vstack([env_mrm_pos_mod,env_mrm_pos_temp_mod])\n",
    "\n",
    "                # plt.clf()\n",
    "\n",
    "                # plt.subplots_adjust(left=-0.1,right=1.1,bottom=-0.1,top=1.1)\n",
    "                # fig.canvas.draw()\n",
    "                # plt.pause(.01)\n",
    "\n",
    "                #if record_mode == 0:\n",
    "                    #img = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "                    #img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)\n",
    "                    #video.write(img.astype('uint8'))\n",
    "\n",
    "                \n",
    "                step_num = step_num + 1\n",
    "                \n",
    "                done = dones[\"__all__\"]\n",
    "                \n",
    "                #print(f'rewards:{rewards}')\n",
    "                #if record_mode == 0:\n",
    "                #    img = eval_env.render_movie(file_name,step_num)\n",
    "                #    video.write(img.astype('unit8'))\n",
    "                #elif record_mode == 1:\n",
    "                #    eval_env.render()\n",
    "                #elif record_mode == 2:\n",
    "                #    eval_env.render()\n",
    "                    \n",
    "                #env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp = render_env.copy_from_env(eval_env)\n",
    "                \n",
    "                #env_blue_pos.append(env_blue_pos_temp)\n",
    "                #env_red_pos.append(env_red_pos_temp)\n",
    "                #env_mrm_pos.append(env_mrm_pos_temp)\n",
    "                #step_num = step_num + 1\n",
    "                # エピソードの終了処理\n",
    "                if dones['__all__']:\n",
    "                    save_hists(\"blue\",steps,env_blue_pos_mod)\n",
    "                    save_hists(\"red\",steps,env_red_pos_mod)\n",
    "                    save_hists(\"mrm\",steps,env_mrm_pos_mod)\n",
    "                    # print(f'all done at {env.steps}')\n",
    "                    break\n",
    "                \n",
    "\n",
    "            \n",
    "            #if record_mode == 0:\n",
    "               # video.release()\n",
    "\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
