{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee02754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\quaternion\\numba_wrapper.py:23: UserWarning: \n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Could not import from numba, which means that some\n",
      "parts of this code may run MUCH more slowly.  You\n",
      "may wish to install numba.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "  warnings.warn(warning_text)\n",
      "2022-02-21 18:51:49,822\tINFO services.py:1265 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-02-21 18:51:59,021\tINFO trainer.py:714 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2022-02-21 18:51:59,021\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-02-21 18:51:59,021\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DMU\\miniconda3\\envs\\AI2\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 18:52:01,425\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-21 18:52:01,776\tINFO trainable.py:383 -- Restored on 192.168.56.1 from checkpoint: ./UCAV/checkpoints/test_2\\checkpoint_001516\\checkpoint-1516\n",
      "2022-02-21 18:52:01,776\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 1516, '_timesteps_total': None, '_time_total': 1842482.89387393, '_episodes_total': 42933}\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import Logger, UnifiedLogger, pretty_print\n",
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "#from ray.rllib.examples.models.shared_weights_model import TF2SharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.agents.ppo import ppo, PPOTrainer, PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from environment_rllib_3d import MyEnv\n",
    "from settings.initial_settings import *\n",
    "from settings.reset_conditions import reset_conditions\n",
    "#from modules.models import MyConv2DModel_v0B_Small_CBAM_1DConv_Share\n",
    "#from modules.models import DenseNetModelLarge\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from modules.savers import save_conditions\n",
    "from utility.result_env import render_env\n",
    "from utility.terminate_uavsimproc import teminate_proc\n",
    "from utility.latest_learned_file_path import latest_learned_file_path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import ctypes\n",
    "import warnings\n",
    "\n",
    "#UCAV.exeが起動している場合、プロセスキルする。\n",
    "teminate_proc.UAVsimprockill(proc_name=\"UCAV.exe\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.filterwarnings('ignore', category=matplotlib.MatplotlibDeprecationWarning)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "PROJECT = \"UCAV\"\n",
    "TRIAL_ID = 2\n",
    "TRIAL = 'test_' + str(TRIAL_ID)\n",
    "EVAL_FREQ = 10\n",
    "NUM_EVAL = 10\n",
    "CONTINUAL = True\n",
    "\n",
    "def custom_log_creator(custom_path, custom_str):\n",
    "    timestr = datetime.datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    logdir_prefix = \"{}_{}\".format(custom_str, timestr)\n",
    "\n",
    "    def logger_creator(config):\n",
    "        if not os.path.exists(custom_path):\n",
    "            os.makedirs(custom_path)\n",
    "        logdir = tempfile.mkdtemp(prefix=logdir_prefix, dir=custom_path)\n",
    "        return UnifiedLogger(config, logdir, loggers=None)\n",
    "\n",
    "    return logger_creator\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "#ModelCatalog.register_custom_model('my_model', DenseNetModelLarge)\n",
    "\n",
    "# config = {\"env\": MyEnv,\n",
    "#           \"num_workers\": NUM_WORKERS,\n",
    "#           \"num_gpus\": NUM_GPUS,\n",
    "#           \"num_cpus_per_worker\": NUM_CPUS_PER_WORKER,\n",
    "#           \"num_sgd_iter\": NUM_SGD_ITER,\n",
    "#           \"lr\": LEARNING_RATE,\n",
    "#           \"gamma\": GAMMA,  # default=0.99\n",
    "#           \"model\": {\"custom_model\": \"my_model\"}\n",
    "#           # \"framework\": framework\n",
    "#           }  # use tensorflow 2\n",
    "eval_env = MyEnv({})\n",
    "policies = {\n",
    "    #\"blue_1\": PolicySpec(config={\"gamma\": 0.99}),\n",
    "    #\"blue_2\": PolicySpec(config={\"gamma\": 0.95}),\n",
    "    \"blue_0\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "    \"blue_1\": (PPOTFPolicy, eval_env.observation_space, eval_env.action_space, {}),\n",
    "}\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    #print(agent_id,episode)\n",
    "    #pol_id = policy_ids[agent_id]\n",
    "\n",
    "    pol_id = agent_id\n",
    "    return pol_id\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config = {\"env\": MyEnv,\"num_gpus\": 0,\"num_workers\": 0, \"num_cpus_per_worker\": 0,\"num_gpus_per_worker\": 0,\n",
    "          \"train_batch_size\": 3000*5,\n",
    "          \"batch_mode\": \"complete_episodes\",\n",
    "          \"gamma\":0.995, \"lr\": 2.5e-4,\"shuffle_sequences\": True,\n",
    "          #\"clip_actions\":True,\"normalize_actions\":True,\n",
    "          \"observation_space\":eval_env.observation_space,\"action_space\":eval_env.action_space,\n",
    "          \"explore\":True,\n",
    "          \"sgd_minibatch_size\": 300, \"num_sgd_iter\":20,\n",
    "          \"exploration_config\": {\"type\": \"StochasticSampling\",\"random_timesteps\":1200*5*5}, #PPO デフォルト \"random_timesteps\":0\n",
    "          #\"model\":{\"fcnet_activation\": \"relu\",\"fcnet_hiddens\": [256, 256, 256],\"post_fcnet_activation\": \"linear\",\n",
    "          #         \"vf_share_layers\": True,},#\"linear\",\"relu\",\"tanh\" \"use_lstm\":True,\"lstm_cell_size\":256,\"max_seq_len\":128\n",
    "          #\"model\":{\"vf_share_layers\": True,\"use_lstm\": True},\n",
    "          #\"model\": {\"custom_model\": \"my_model\"},\n",
    "          \"multiagent\": {\"policies\": policies,  \"policy_mapping_fn\": policy_mapping_fn}\n",
    "         }\n",
    "#res_name = \"sgd\"+str(config[\"sgd_minibatch_size\"])+\"sgd_num\"+str(config[\"num_sgd_iter\"])+\"lr\"+str(config[\"lr\"])+\"gamma\"+str(config[\"gamma\"])\n",
    "res_name = \"test\"\n",
    "\n",
    "conditions_dir = os.path.join('./' + PROJECT + '/conditions/')\n",
    "\n",
    "if not os.path.exists(conditions_dir):\n",
    "    os.makedirs(conditions_dir)\n",
    "save_conditions(conditions_dir)\n",
    "\n",
    "# PPOTrainer()は、try_import_tfを使うと、なぜかTensorflowのeager modeのエラーになる。\n",
    "\n",
    "trainer = ppo.PPOTrainer(config=config,\n",
    "                         logger_creator=custom_log_creator(\n",
    "                             os.path.expanduser(\"./\" + PROJECT + \"/logs\"), TRIAL))\n",
    "\n",
    "if CONTINUAL:\n",
    "    # Continual learning: Need to specify the checkpoint\n",
    "    model_path = latest_learned_file_path('./UCAV/checkpoints/test_2/*')\n",
    "    #model_path = PROJECT + '/checkpoints/' + TRIAL + '/checkpoint_000121/checkpoint-121'\n",
    "    \n",
    "    trainer.restore(checkpoint_path=model_path)\n",
    "\n",
    "# Instanciate the evaluation env\n",
    "eval_env = MyEnv({})\n",
    "record_mode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4383e321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "-------------------------- Scene: 1 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 673 -0.19 44.250000000000156\n",
      "blue_1 False False 673 0.0 24.3612\n",
      "==============================================================\n",
      "-------------------------- Scene: 1 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 671 -0.19 43.6900000000001\n",
      "blue_1 False False 671 0.0 24.282000000000007\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 420 -0.11 17.529999999999973\n",
      "blue_1 False False 420 0.0804 7.368000000000002\n",
      "==============================================================\n",
      "-------------------------- Scene: 1 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 671 -0.19 45.090000000000046\n",
      "blue_1 False False 671 0.0 25.288600000000006\n",
      "==============================================================\n",
      "-------------------------- Scene: 1 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 672 -0.19 43.830000000000055\n",
      "blue_1 False False 672 0.0 22.712999999999983\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 414 -0.11 21.62999999999998\n",
      "blue_1 False False 414 0.0804 7.999800000000002\n",
      "==============================================================\n",
      "-------------------------- Scene: 1 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 667 -0.19 38.730000000000004\n",
      "blue_1 False False 667 0.0 16.24079999999991\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 416 -0.11 20.68999999999998\n",
      "blue_1 False False 416 0.0804 8.1834\n",
      "==============================================================\n",
      "-------------------------- Scene: 0 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 358 -0.11 13.65\n",
      "blue_1 False False 358 0.0804 3.1434000000000006\n",
      "==============================================================\n",
      "-------------------------- Scene: 1 --------------------------\n",
      "==============================================================\n",
      "Blue_0Too Close Finish\n",
      "TIME LIMIT LOSE\n",
      "blue_0 False False 677 -0.19 42.04000000000002\n",
      "blue_1 False False 677 0.0 23.522599999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DMU\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel\\eventloops.py:258: RuntimeWarning: coroutine 'Kernel.do_one_iteration' was never awaited\n",
      "  self.func()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EVAL):\n",
    "    # print(f'\\nEvaluation {i}:')\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    eval_env.eval = True\n",
    "    step_num = 0\n",
    "    fig = plt.figure(1,figsize=(8.0, 6.0))\n",
    "    ESC = 0x1B          # ESCキーの仮想キーコード\n",
    "    trajectory_length = 100\n",
    "    env_blue_pos = [0]\n",
    "    env_red_pos = [0]\n",
    "    env_mrm_pos = [0]\n",
    "    if record_mode == 0:\n",
    "        file_name = \"test_num\" +str(i)\n",
    "        video = cv2.VideoWriter(file_name+'.mp4',0x00000020,20.0,(800,600))\n",
    "\n",
    "    while True:\n",
    "        action_dict = {}\n",
    "        #for j in range(eval_env.blue_num):\n",
    "            #if not eval_env.blue[j].hitpoint == 0:\n",
    "                #action_dict['blue_' + str(j)] = trainer.compute_action(obs['blue_' + str(j)])\n",
    "                #action_dict['blue_' + str(j)] = trainer.compute_single_action(obs['blue_' + str(j)],policy_id='blue_' + str(j),\n",
    "                                                                   #clip_actions=True,explore=False)\n",
    "                #print(action_dict['blue_' + str(j)][\"vector_psi_x\"],action_dict['blue_' + str(j)][\"vector_gam_x\"],\n",
    "                      #action_dict['blue_' + str(j)][\"velocity\"])\n",
    "        #obs, rewards, dones, infos = eval_env.step(action_dict)\n",
    "        action_dict0 = trainer.compute_single_action(obs['blue_0'],policy_id='blue_0',explore=False)\n",
    "        action_dict1 = trainer.compute_single_action(obs['blue_1'],policy_id='blue_1',explore=False)\n",
    "            #action_dict0 = trainer.compute_single_action(obs['blue_0'],policy_id='blue_0')\n",
    "            #action_dict1 = trainer.compute_single_action(obs['blue_1'],policy_id='blue_1')\n",
    "        obs, rewards, dones, infos = eval_env.step({'blue_0': action_dict0, 'blue_1': action_dict1})\n",
    "        #for j in range(eval_env.blue_num): \n",
    "\n",
    "            #print('blue_' + str(j),eval_env.blue[j].V, np.rad2deg(eval_env.blue[j].psi),  np.rad2deg(eval_env.blue[j].gam),\n",
    "            #     eval_env.blue[j].V_ref,np.rad2deg(eval_env.blue[j].psi_ref),  np.rad2deg(eval_env.blue[j].gam_ref))\n",
    "        env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp= render_env.copy_from_env(eval_env)\n",
    "        env_blue_pos.append(env_blue_pos_temp)\n",
    "        env_red_pos.append(env_red_pos_temp)\n",
    "        env_mrm_pos.append(env_mrm_pos_temp)\n",
    "        if step_num == 0:\n",
    "            del env_blue_pos[0]\n",
    "            del env_red_pos[0]\n",
    "            del env_mrm_pos[0]\n",
    "\n",
    "        hist_blue_pos = np.vstack(env_blue_pos)\n",
    "        hist_red_pos = np.vstack(env_red_pos)\n",
    "        hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "        plt.clf()\n",
    "        render_env.rend_3d(eval_env,hist_blue_pos,\"b\",1)\n",
    "        render_env.rend_3d(eval_env,hist_red_pos,\"r\",1)\n",
    "        render_env.rend_3d(eval_env,hist_mrm_pos,\"k\",1)\n",
    "        plt.subplots_adjust(left=-0.1,right=1.1,bottom=-0.1,top=1.1)\n",
    "        plt.pause(.01)\n",
    "        step_num = step_num + 1\n",
    "\n",
    "        done = dones[\"__all__\"]\n",
    "        if record_mode == 0:\n",
    "            img = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)\n",
    "            # cv2.imshow('test', img)\n",
    "            # cv2.waitKey(1)\n",
    "            # cv2.destroyAllWindows()\n",
    "            video.write(img.astype('uint8'))\n",
    "        elif record_mode == 1:\n",
    "            env.render()\n",
    "        elif record_mode == 2:\n",
    "            True #将来のため処理予約\n",
    "\n",
    "        #env_blue_pos_temp, env_red_pos_temp, env_mrm_pos_temp = render_env.copy_from_env(eval_env)\n",
    "\n",
    "        #env_blue_pos.append(env_blue_pos_temp)\n",
    "        #env_red_pos.append(env_red_pos_temp)\n",
    "        #env_mrm_pos.append(env_mrm_pos_temp)\n",
    "        #step_num = step_num + 1\n",
    "        # エピソードの終了処理\n",
    "        if dones['__all__']:\n",
    "            # print(f'all done at {env.steps}')\n",
    "            if record_mode == 0:\n",
    "                video.release()\n",
    "            break\n",
    "\n",
    "    #del env_blue_pos[0]\n",
    "    #del env_red_pos[0]\n",
    "    #del env_mrm_pos[0]\n",
    "\n",
    "    #hist_blue_pos = np.vstack(env_blue_pos)\n",
    "    #hist_red_pos = np.vstack(env_red_pos)\n",
    "    #hist_mrm_pos = np.vstack(env_mrm_pos)\n",
    "\n",
    "    #f = open(results_file,'wb')\n",
    "    #f = open(\"log.pkl\",\"wb\")\n",
    "    #pickle.dump(emv_blue_pos,f)\n",
    "    #pickle.dump(emv_blue_pos,f)\n",
    "    #pickle.dump(emv_red_pos,f)\n",
    "    #pickle.dump(emv_mrm_pos,f)\n",
    "    #f.close()\n",
    "\n",
    "    if record_mode == 0:\n",
    "        video.release()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b07aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
